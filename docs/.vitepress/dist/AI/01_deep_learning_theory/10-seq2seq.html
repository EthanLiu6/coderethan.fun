<!DOCTYPE html>
<html lang="zh-CN" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>1 从RNN 到 Seq2Seq | 码医森</title>
    <meta name="description" content="计算机知识的学习站点">
    <meta name="generator" content="VitePress v1.3.4">
    <link rel="preload stylesheet" href="/assets/style.CflK-Lwn.css" as="style">
    
    <script type="module" src="/assets/app.BoLbqWbx.js"></script>
    <link rel="preload" href="/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/assets/chunks/theme.DAwg5rED.js">
    <link rel="modulepreload" href="/assets/chunks/framework.DA-Pb-tg.js">
    <link rel="modulepreload" href="/assets/AI_01_deep_learning_theory_10-seq2seq.md.6D8B4Rlq.lean.js">
    <link rel="icon" type="image/svg+xml" href="/imgs/home-page-logo.svg">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css" crossorigin="">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-5d98c3a5><!--[--><!--]--><!--[--><span tabindex="-1" data-v-0f60ec36></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-0f60ec36> Skip to content </a><!--]--><!----><header class="VPNav" data-v-5d98c3a5 data-v-ae24b3ad><div class="VPNavBar" data-v-ae24b3ad data-v-6aa21345><div class="wrapper" data-v-6aa21345><div class="container" data-v-6aa21345><div class="title" data-v-6aa21345><div class="VPNavBarTitle has-sidebar" data-v-6aa21345 data-v-ab179fa1><a class="title" href="/" data-v-ab179fa1><!--[--><!--]--><!--[--><img class="VPImage logo" src="/imgs/home-page-logo.svg" alt data-v-8426fc1a><!--]--><span data-v-ab179fa1>CoderEthan学习站</span><!--[--><!--]--></a></div></div><div class="content" data-v-6aa21345><div class="content-body" data-v-6aa21345><!--[--><!--]--><div class="VPNavBarSearch search" data-v-6aa21345><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-6aa21345 data-v-dc692963><span id="main-nav-aria-label" class="visually-hidden" data-v-dc692963> Main Navigation </span><!--[--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-dc692963 data-v-b6c34ac9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-b6c34ac9><span class="text" data-v-b6c34ac9><!----><span data-v-b6c34ac9>AI</span><span class="vpi-chevron-down text-icon" data-v-b6c34ac9></span></span></button><div class="menu" data-v-b6c34ac9><div class="VPMenu" data-v-b6c34ac9 data-v-b98bc113><div class="items" data-v-b98bc113><!--[--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/AI/01_deep_learning_theory/" data-v-43f1e123><!--[-->DL基础理论<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/AI/02_distribute_training/" data-v-43f1e123><!--[-->分布式训练<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/AI/03_Transformer/" data-v-43f1e123><!--[-->Transformer个人梳理<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/AI/04_some_notes/" data-v-43f1e123><!--[-->DL个人笔记<!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-dc692963 data-v-b6c34ac9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-b6c34ac9><span class="text" data-v-b6c34ac9><!----><span data-v-b6c34ac9>计算机学科内容</span><span class="vpi-chevron-down text-icon" data-v-b6c34ac9></span></span></button><div class="menu" data-v-b6c34ac9><div class="VPMenu" data-v-b6c34ac9 data-v-b98bc113><div class="items" data-v-b98bc113><!--[--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/IT-learning/408/" data-v-43f1e123><!--[-->408知识<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/IT-learning/c++/" data-v-43f1e123><!--[-->C++基础<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/IT-learning/Java/" data-v-43f1e123><!--[-->Java后端<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/IT-learning/Linux/" data-v-43f1e123><!--[-->Linux技术<!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-dc692963 data-v-b6c34ac9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-b6c34ac9><span class="text" data-v-b6c34ac9><!----><span data-v-b6c34ac9>求职面试</span><span class="vpi-chevron-down text-icon" data-v-b6c34ac9></span></span></button><div class="menu" data-v-b6c34ac9><div class="VPMenu" data-v-b6c34ac9 data-v-b98bc113><div class="items" data-v-b98bc113><!--[--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/Job_Interview/Java/" data-v-43f1e123><!--[-->Java面经<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/Job_Interview/Algorithm_post/" data-v-43f1e123><!--[-->算法岗<!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-dc692963 data-v-b6c34ac9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-b6c34ac9><span class="text" data-v-b6c34ac9><!----><span data-v-b6c34ac9>其他维护</span><span class="vpi-chevron-down text-icon" data-v-b6c34ac9></span></span></button><div class="menu" data-v-b6c34ac9><div class="VPMenu" data-v-b6c34ac9 data-v-b98bc113><div class="items" data-v-b98bc113><!--[--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/update/update_log.html" data-v-43f1e123><!--[-->站点更新<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/question_list/" data-v-43f1e123><!--[-->问题清单<!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-dc692963 data-v-b6c34ac9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-b6c34ac9><span class="text" data-v-b6c34ac9><!----><span data-v-b6c34ac9>感悟和日常</span><span class="vpi-chevron-down text-icon" data-v-b6c34ac9></span></span></button><div class="menu" data-v-b6c34ac9><div class="VPMenu" data-v-b6c34ac9 data-v-b98bc113><div class="items" data-v-b98bc113><!--[--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/about_me/" data-v-43f1e123><!--[-->关于我<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/my_think/" data-v-43f1e123><!--[-->站长感悟<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link vp-external-link-icon" href="https://EthanLiu6.github.io" target="_blank" rel="noreferrer" data-v-43f1e123><!--[-->旧版博客<!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-6aa21345 data-v-6c893767><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-6c893767 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-6aa21345 data-v-0394ad82 data-v-7bc22406><!--[--><a class="VPSocialLink no-icon" href="https://github.com/ethanliu6/" aria-label target="_blank" rel="noopener" data-v-7bc22406 data-v-eee4e7cb><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path fill="#6841d2" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3 .3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5 .3-6.2 2.3zm44.2-1.7c-2.9 .7-4.9 2.6-4.6 4.9 .3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3 .7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3 .3 2.9 2.3 3.9 1.6 1 3.6 .7 4.3-.7 .7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3 .7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3 .7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></a><a class="VPSocialLink no-icon" href="https://space.bilibili.com/1327099977/" aria-label target="_blank" rel="noopener" data-v-7bc22406 data-v-eee4e7cb><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path fill="#6841d2" d="M488.6 104.1C505.3 122.2 513 143.8 511.9 169.8V372.2C511.5 398.6 502.7 420.3 485.4 437.3C468.2 454.3 446.3 463.2 419.9 464H92C65.6 463.2 43.8 454.2 26.7 436.8C9.7 419.4 .8 396.5 0 368.2V169.8C.8 143.8 9.7 122.2 26.7 104.1C43.8 87.8 65.6 78.8 92 78H121.4L96.1 52.2C90.3 46.5 87.4 39.2 87.4 30.4C87.4 21.6 90.3 14.3 96.1 8.6C101.8 2.9 109.1 0 117.9 0C126.7 0 134 2.9 139.8 8.6L213.1 78H301.1L375.6 8.6C381.7 2.9 389.2 0 398 0C406.8 0 414.1 2.9 419.9 8.6C425.6 14.3 428.5 21.6 428.5 30.4C428.5 39.2 425.6 46.5 419.9 52.2L394.6 78L423.9 78C450.3 78.8 471.9 87.8 488.6 104.1H488.6zM449.8 173.8C449.4 164.2 446.1 156.4 439.1 150.3C433.9 144.2 425.1 140.9 416.4 140.5H96.1C86.5 140.9 78.6 144.2 72.5 150.3C66.3 156.4 63.1 164.2 62.7 173.8V368.2C62.7 377.4 66 385.2 72.5 391.7C79 398.2 86.9 401.5 96.1 401.5H416.4C425.6 401.5 433.4 398.2 439.7 391.7C446 385.2 449.4 377.4 449.8 368.2L449.8 173.8zM185.5 216.5C191.8 222.8 195.2 230.6 195.6 239.7V273C195.2 282.2 191.9 289.9 185.8 296.2C179.6 302.5 171.8 305.7 162.2 305.7C152.6 305.7 144.7 302.5 138.6 296.2C132.5 289.9 129.2 282.2 128.8 273V239.7C129.2 230.6 132.6 222.8 138.9 216.5C145.2 210.2 152.1 206.9 162.2 206.5C171.4 206.9 179.2 210.2 185.5 216.5H185.5zM377 216.5C383.3 222.8 386.7 230.6 387.1 239.7V273C386.7 282.2 383.4 289.9 377.3 296.2C371.2 302.5 363.3 305.7 353.7 305.7C344.1 305.7 336.3 302.5 330.1 296.2C323.1 289.9 320.7 282.2 320.4 273V239.7C320.7 230.6 324.1 222.8 330.4 216.5C336.7 210.2 344.5 206.9 353.7 206.5C362.9 206.9 370.7 210.2 377 216.5H377z"/></svg></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-6aa21345 data-v-bb2aa2f0 data-v-b6c34ac9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-b6c34ac9><span class="vpi-more-horizontal icon" data-v-b6c34ac9></span></button><div class="menu" data-v-b6c34ac9><div class="VPMenu" data-v-b6c34ac9 data-v-b98bc113><!----><!--[--><!--[--><!----><div class="group" data-v-bb2aa2f0><div class="item appearance" data-v-bb2aa2f0><p class="label" data-v-bb2aa2f0>深色模式</p><div class="appearance-action" data-v-bb2aa2f0><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-bb2aa2f0 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div></div></div><div class="group" data-v-bb2aa2f0><div class="item social-links" data-v-bb2aa2f0><div class="VPSocialLinks social-links-list" data-v-bb2aa2f0 data-v-7bc22406><!--[--><a class="VPSocialLink no-icon" href="https://github.com/ethanliu6/" aria-label target="_blank" rel="noopener" data-v-7bc22406 data-v-eee4e7cb><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path fill="#6841d2" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3 .3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5 .3-6.2 2.3zm44.2-1.7c-2.9 .7-4.9 2.6-4.6 4.9 .3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3 .7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3 .3 2.9 2.3 3.9 1.6 1 3.6 .7 4.3-.7 .7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3 .7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3 .7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></a><a class="VPSocialLink no-icon" href="https://space.bilibili.com/1327099977/" aria-label target="_blank" rel="noopener" data-v-7bc22406 data-v-eee4e7cb><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path fill="#6841d2" d="M488.6 104.1C505.3 122.2 513 143.8 511.9 169.8V372.2C511.5 398.6 502.7 420.3 485.4 437.3C468.2 454.3 446.3 463.2 419.9 464H92C65.6 463.2 43.8 454.2 26.7 436.8C9.7 419.4 .8 396.5 0 368.2V169.8C.8 143.8 9.7 122.2 26.7 104.1C43.8 87.8 65.6 78.8 92 78H121.4L96.1 52.2C90.3 46.5 87.4 39.2 87.4 30.4C87.4 21.6 90.3 14.3 96.1 8.6C101.8 2.9 109.1 0 117.9 0C126.7 0 134 2.9 139.8 8.6L213.1 78H301.1L375.6 8.6C381.7 2.9 389.2 0 398 0C406.8 0 414.1 2.9 419.9 8.6C425.6 14.3 428.5 21.6 428.5 30.4C428.5 39.2 425.6 46.5 419.9 52.2L394.6 78L423.9 78C450.3 78.8 471.9 87.8 488.6 104.1H488.6zM449.8 173.8C449.4 164.2 446.1 156.4 439.1 150.3C433.9 144.2 425.1 140.9 416.4 140.5H96.1C86.5 140.9 78.6 144.2 72.5 150.3C66.3 156.4 63.1 164.2 62.7 173.8V368.2C62.7 377.4 66 385.2 72.5 391.7C79 398.2 86.9 401.5 96.1 401.5H416.4C425.6 401.5 433.4 398.2 439.7 391.7C446 385.2 449.4 377.4 449.8 368.2L449.8 173.8zM185.5 216.5C191.8 222.8 195.2 230.6 195.6 239.7V273C195.2 282.2 191.9 289.9 185.8 296.2C179.6 302.5 171.8 305.7 162.2 305.7C152.6 305.7 144.7 302.5 138.6 296.2C132.5 289.9 129.2 282.2 128.8 273V239.7C129.2 230.6 132.6 222.8 138.9 216.5C145.2 210.2 152.1 206.9 162.2 206.5C171.4 206.9 179.2 210.2 185.5 216.5H185.5zM377 216.5C383.3 222.8 386.7 230.6 387.1 239.7V273C386.7 282.2 383.4 289.9 377.3 296.2C371.2 302.5 363.3 305.7 353.7 305.7C344.1 305.7 336.3 302.5 330.1 296.2C323.1 289.9 320.7 282.2 320.4 273V239.7C320.7 230.6 324.1 222.8 330.4 216.5C336.7 210.2 344.5 206.9 353.7 206.5C362.9 206.9 370.7 210.2 377 216.5H377z"/></svg></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-6aa21345 data-v-e5dd9c1c><span class="container" data-v-e5dd9c1c><span class="top" data-v-e5dd9c1c></span><span class="middle" data-v-e5dd9c1c></span><span class="bottom" data-v-e5dd9c1c></span></span></button></div></div></div></div><div class="divider" data-v-6aa21345><div class="divider-line" data-v-6aa21345></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-5d98c3a5 data-v-a6f0e41e><div class="container" data-v-a6f0e41e><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-a6f0e41e><span class="vpi-align-left menu-icon" data-v-a6f0e41e></span><span class="menu-text" data-v-a6f0e41e>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-a6f0e41e data-v-17a5e62e><button data-v-17a5e62e>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-5d98c3a5 data-v-319d5ca6><div class="curtain" data-v-319d5ca6></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-319d5ca6><span class="visually-hidden" id="sidebar-aria-label" data-v-319d5ca6> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-c40bc020><section class="VPSidebarItem level-0 has-active" data-v-c40bc020 data-v-b7550ba0><!----><div class="items" data-v-b7550ba0><!--[--><section class="VPSidebarItem level-1 collapsible collapsed has-active" data-v-b7550ba0 data-v-b7550ba0><div class="item" role="button" tabindex="0" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><h3 class="text" data-v-b7550ba0>01_deep_learning_theory</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-b7550ba0><span class="vpi-chevron-right caret-icon" data-v-b7550ba0></span></div></div><div class="items" data-v-b7550ba0><!--[--><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/01-feedforward_network.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>01-feedforward_network</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/02-back_propagation.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>02-back_propagation</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/03-bp_example_demo.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>03-bp_example_demo</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/04-convolution_neural_network.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>04-convolution_neural_network</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/05-deep_learning_model.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>05-deep_learning_model</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/06-pytorch_install.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>06-pytorch_install</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/07-operators.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>07-operators</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/08-activation_functions.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>08-activation_functions</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/09-recurrent_neural_network.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>09-recurrent_neural_network</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/10-seq2seq.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>10-seq2seq</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/11-1attentions.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>11-1attentions</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/11-2attention-extension.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>11-2attention-extension</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/12-weight-initialization.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>12-weight-initialization</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/13-optimizers.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>13-optimizers</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/14-regularization.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>14-regularization</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/15-deep-learning-tuning-guide.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>15-deep-learning-tuning-guide</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/20-pytorch-tensor.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>20-pytorch-tensor</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/21-pytorch-autograd.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>21-pytorch-autograd</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/22-pytorch-module.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>22-pytorch-module</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/23-1training-example-1.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>23-1training-example-1</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/23-2decoder.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>23-2decoder</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/23-3encoder.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>23-3encoder</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/23-4transformer.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>23-4transformer</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/24-pytorch-optimizer.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>24-pytorch-optimizer</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/25-pytorch-lr-scheduler.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>25-pytorch-lr-scheduler</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/26-pytorch-dataloader.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>26-pytorch-dataloader</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/27-pytorch-model-save.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>27-pytorch-model-save</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/28-pytorch-tensorboard.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>28-pytorch-tensorboard</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/29-pytorch-graph-mode.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>29-pytorch-graph-mode</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/30-1training-example-cv.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>30-1training-example-cv</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/30-3main.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>30-3main</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/31-1stable-diffusion.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>31-1stable-diffusion</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/31-2SDXL.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>31-2SDXL</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/31-3VAE.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>31-3VAE</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/40-nlp-bert_ner.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>40-nlp-bert_ner</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/41-nlp-t5_question-answering.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>41-nlp-t5_question-answering</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/42-nlp-gpt.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>42-nlp-gpt</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/43-scaling-law.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>43-scaling-law</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/44-distribute-training.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>44-distribute-training</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/45-LLM-History.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>45-LLM-History</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/46-LLM-GPT-Extension.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>46-LLM-GPT-Extension</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/46-nlp-llama.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>46-nlp-llama</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/47-LLM-DeepSeek-Structure.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>47-LLM-DeepSeek-Structure</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/01_deep_learning_theory/47-nlp-deepseek.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>47-nlp-deepseek</p><!--]--></a><!----></div><!----></div><!--]--></div></section><section class="VPSidebarItem level-1 collapsible collapsed" data-v-b7550ba0 data-v-b7550ba0><div class="item" role="button" tabindex="0" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><h3 class="text" data-v-b7550ba0>02_distribute_training</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-b7550ba0><span class="vpi-chevron-right caret-icon" data-v-b7550ba0></span></div></div><div class="items" data-v-b7550ba0><!--[--><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/00_large-scale-model-trainning.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>00_large-scale-model-trainning</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/01_coding.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>01_coding</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/01_offload-and-recompute.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>01_offload-and-recompute</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/02_amp.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>02_amp</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/03_coding.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>03_coding</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/03_pytorch-DP.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>03_pytorch-DP</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/04_pytorch-DDP.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>04_pytorch-DDP</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/05_pytorch-DDP-IMPL.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>05_pytorch-DDP-IMPL</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/05_pytorch-DDP-IMPL_DDP_ORIGIN.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>05_pytorch-DDP-IMPL_DDP_ORIGIN</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/06_collective-comm.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>06_collective-comm</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/06_torchrun-process-group.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>06_torchrun-process-group</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/07_ZeRO-Optimizer.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>07_ZeRO-Optimizer</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/08_pytorch-ZeRO-1.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>08_pytorch-ZeRO-1</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/09_pytorch-FSDP-v1.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>09_pytorch-FSDP-v1</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/10_pytorch-FSDP-v2.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>10_pytorch-FSDP-v2</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/11_deepspeed-ZeRO-1-2-IMPL.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>11_deepspeed-ZeRO-1-2-IMPL</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/12_deepspeed-ZeRO-3-IMPL.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>12_deepspeed-ZeRO-3-IMPL</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/13_megatron-ZeRO-1-IMPL.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>13_megatron-ZeRO-1-IMPL</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/14_TP-Theory.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>14_TP-Theory</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/15_megatron-TP-IMPL.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>15_megatron-TP-IMPL</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/16_pytorch-TP-IMPL.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>16_pytorch-TP-IMPL</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/17_PP-Theory.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>17_PP-Theory</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/18_pytorch-PP-IMPL.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>18_pytorch-PP-IMPL</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/19_deepspeed-PP-IMPL.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>19_deepspeed-PP-IMPL</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/20_megatron-PP-IMPL.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>20_megatron-PP-IMPL</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/21_SP-Theory.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>21_SP-Theory</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/22_megatron-SP-IMPL.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>22_megatron-SP-IMPL</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/23_3D-Parallel-Theory.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>23_3D-Parallel-Theory</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/24_megatron-3D-Parallel-IMPL.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>24_megatron-3D-Parallel-IMPL</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/25_pytorch-3D-Parallel-IMPL.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>25_pytorch-3D-Parallel-IMPL</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/26_CP-Theory.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>26_CP-Theory</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/27_megatron-CP-IMPL.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>27_megatron-CP-IMPL</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/28_MOE-Theory.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>28_MOE-Theory</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/28_MOE-Theory_DeepSeekMOE.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>28_MOE-Theory_DeepSeekMOE</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/29_megatron-MOE-IMPL.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>29_megatron-MOE-IMPL</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/30_deepspeed-MOE-IMPL.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>30_deepspeed-MOE-IMPL</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/31_deepspeed-code-IMPL.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>31_deepspeed-code-IMPL</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/32_collective-operations.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>32_collective-operations</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/02_distribute_training/33_pytorch_distribute.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>33_pytorch_distribute</p><!--]--></a><!----></div><!----></div><!--]--></div></section><section class="VPSidebarItem level-1 collapsible collapsed" data-v-b7550ba0 data-v-b7550ba0><div class="item" role="button" tabindex="0" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><h3 class="text" data-v-b7550ba0>03_Transformer</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-b7550ba0><span class="vpi-chevron-right caret-icon" data-v-b7550ba0></span></div></div><div class="items" data-v-b7550ba0><!--[--><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/03_Transformer/01-Transformer的由来.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>01-Transformer的由来</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/03_Transformer/02-Transformer架构解读.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>02-Transformer架构解读</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/03_Transformer/03-Transformer源码构建.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>03-Transformer源码构建</p><!--]--></a><!----></div><!----></div><!--]--></div></section><section class="VPSidebarItem level-1 collapsible collapsed" data-v-b7550ba0 data-v-b7550ba0><div class="item" role="button" tabindex="0" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><h3 class="text" data-v-b7550ba0>04_some_notes</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-b7550ba0><span class="vpi-chevron-right caret-icon" data-v-b7550ba0></span></div></div><div class="items" data-v-b7550ba0><!--[--><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/04_some_notes/00-DL_base_notes.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>00-DL_base_notes</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/04_some_notes/01-class_logs.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>01-class_logs</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/04_some_notes/02-some_detials.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>02-some_detials</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/04_some_notes/03-Bert理解.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>03-Bert理解</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/04_some_notes/04-个人补充内容.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>04-个人补充内容</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/04_some_notes/05-Review_DL.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>05-Review_DL</p><!--]--></a><!----></div><!----></div><!--]--></div></section><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-5d98c3a5 data-v-1428d186><div class="VPDoc has-sidebar has-aside" data-v-1428d186 data-v-39a288b8><!--[--><!--]--><div class="container" data-v-39a288b8><div class="aside" data-v-39a288b8><div class="aside-curtain" data-v-39a288b8></div><div class="aside-container" data-v-39a288b8><div class="aside-content" data-v-39a288b8><div class="VPDocAside" data-v-39a288b8 data-v-3f215769><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-3f215769 data-v-a5bbad30><div class="content" data-v-a5bbad30><div class="outline-marker" data-v-a5bbad30></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-a5bbad30>本文目录</div><ul class="VPDocOutlineItem root" data-v-a5bbad30 data-v-b933a997><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-3f215769></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-39a288b8><div class="content-container" data-v-39a288b8><!--[--><!--]--><main class="main" data-v-39a288b8><div style="position:relative;" class="vp-doc _AI_01_deep_learning_theory_10-seq2seq" data-v-39a288b8><div><h1 id="_1-从rnn-到-seq2seq" tabindex="-1">1 从RNN 到 Seq2Seq <a class="header-anchor" href="#_1-从rnn-到-seq2seq" aria-label="Permalink to &quot;1 从RNN 到 Seq2Seq&quot;">​</a></h1><h2 id="_1-1-rnn-简述" tabindex="-1">1.1 RNN 简述 <a class="header-anchor" href="#_1-1-rnn-简述" aria-label="Permalink to &quot;1.1 RNN 简述&quot;">​</a></h2><p>        实际场景中，会遇到很多序列型输入数据的情况或问题：</p><ul><li>自然语言处理(nlp)问题。x1可以看做是第一个单词，x2可以看做是第二个单词，依次类推；</li><li>语音处理。此时，x1、x2、x3……是每帧的声音信号；</li><li>时间序列问题。例如每天的股票价格等等。</li></ul><p>        这些序列形的数据就不太好用原始的神经网络处理了。为了建模序列问题，RNN引入了隐状态h（hidden state）的概念，h可以对序列形的数据提取特征，接着再转换为输出，如下图典型RNN 原理图所示：</p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/seq2seq-figure1.jpg" alt="RNN 原理图"></p><p><strong>其中：</strong></p><ul><li>圆圈或方块表示的是向量;</li><li>一个箭头就表示对该向量做一次变换(如上图中h0和x1分别有一个箭头连接，就表示对h0和x1各做了一次变换).</li></ul><h2 id="_1-2-rnn-应用场景" tabindex="-1">1.2 RNN 应用场景 <a class="header-anchor" href="#_1-2-rnn-应用场景" aria-label="Permalink to &quot;1.2 RNN 应用场景&quot;">​</a></h2><p>        值得注意的是，RNN的输入是 x_{1}, x_{2}, \dots, x_{n} ，输出为 y_{1}, y_{2}, \dots, y_{n} ，也就是说，输入和输出序列必须要是等长的。由于这个限制的存在，经典RNN的适用范围比较小，但也有一些问题适合用经典的RNN结构建模.</p><h3 id="_1-2-1-rnn-解决-n-vs-n-问题" tabindex="-1">1.2.1 RNN 解决 N VS N 问题 <a class="header-anchor" href="#_1-2-1-rnn-解决-n-vs-n-问题" aria-label="Permalink to &quot;1.2.1 RNN 解决 N VS N 问题&quot;">​</a></h3><ul><li>ner 问题；</li><li>完形填空问题等；</li></ul><h3 id="_1-2-2-rnn-解决-n-versus-1-问题" tabindex="-1">1.2.2 RNN 解决 N Versus 1 问题 <a class="header-anchor" href="#_1-2-2-rnn-解决-n-versus-1-问题" aria-label="Permalink to &quot;1.2.2 RNN 解决 N Versus 1 问题&quot;">​</a></h3><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/seq2seq-figure2.jpg" alt="N v 1"></p><p>        这种结构通常用来处理序列分类问题。如输入一段文字判别它所属的类别，输入一个句子判断其情感倾向，输入一段视频并判断它的类别等等。</p><h3 id="_1-2-3-rnn-解决-1-vs-n-问题" tabindex="-1">1.2.3 RNN 解决 1 VS N 问题 <a class="header-anchor" href="#_1-2-3-rnn-解决-1-vs-n-问题" aria-label="Permalink to &quot;1.2.3 RNN 解决 1 VS N 问题&quot;">​</a></h3><p>        当输入不是序列而输出为序列的情况怎么处理？我们可以只在序列开始进行输入计算：</p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/seq2seq-figure3.jpg" alt="1 vs N"></p><p>        还有一种结构是把输入信息X作为每个阶段的输入:</p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/seq2seq-figure4.jpg" alt="1 vs N"></p><p>其等价表示为：</p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/seq2seq-figure5.jpg" alt="1 vs N"></p><p>        这种1 VS N的结构可以处理的问题有：</p><ul><li>从图像生成文字（image caption），此时输入的X就是图像的特征，而输出的y序列就是一段句子</li><li>从类别生成语音或音乐等</li></ul><h2 id="_1-3-n-vs-m-型任务" tabindex="-1">1.3 N VS M 型任务 <a class="header-anchor" href="#_1-3-n-vs-m-型任务" aria-label="Permalink to &quot;1.3 N VS M 型任务&quot;">​</a></h2><p>        假如输入序列长度为N，输出序列长度M(<strong>不确定具体长度</strong>)， M≠N时，RNN就无法直接解决，然而我们遇到的大部分问题序列都是不等长的，如机器翻译中，源语言和目标语言的句子往往并没有相同的长度。</p><p>        因此出现了RNN最重要的一种任务：N vs M 问题。解决此问题的结构称之为<strong>Encoder-Decoder模型</strong>，也可以称之为<strong>Seq2Seq模型</strong>。</p><h1 id="_2-seq2seq-模型" tabindex="-1">2 Seq2Seq 模型 <a class="header-anchor" href="#_2-seq2seq-模型" aria-label="Permalink to &quot;2 Seq2Seq 模型&quot;">​</a></h1><h1 id="_2-1-seq2seq-定义" tabindex="-1">2.1 Seq2Seq 定义 <a class="header-anchor" href="#_2-1-seq2seq-定义" aria-label="Permalink to &quot;2.1 Seq2Seq 定义&quot;">​</a></h1><ul><li><a href="https://zh.wikipedia.org/wiki/Seq2Seq%E6%A8%A1%E5%9E%8B" target="_blank" rel="noreferrer">wikipedia 定义</a></li></ul><p>        Seq2seq（Sequence to sequence）模型，是将序列（Sequence）映射到序列的神经网络（neural network）机器学习模型。这个模型最初设计用于改进机器翻译技术，可容许机器通过此模型发现及学习将一种语言的语句（词语序列）映射到另一种语言的对应语句上。除此之外，Seq2Seq也能广泛地应用到各种不同的技术上，如聊天机器人、Inbox by Gmail等，但需要有配对好的文本集才能训练出对应的模型。</p><p>        Seq2seq将输入序列转换为输出序列。它通过利用循环神经网络（递归神经网络）或更常用的<strong>LSTM、 GRU</strong>网络来避免梯度消失问题。当前项的内容总来源于前一步的输出。Seq2seq主要由一个编码器和一个解码器组成。 编码器将输入转换为一个隐藏状态向量，其中包含输入项的内容。 解码器进行相反的过程，将向量转换成输出序列，并使用前一步的输出作为下一步的输入。[4]</p><h1 id="_2-2-seq2seq-模型结构" tabindex="-1">2.2 seq2seq 模型结构 <a class="header-anchor" href="#_2-2-seq2seq-模型结构" aria-label="Permalink to &quot;2.2 seq2seq 模型结构&quot;">​</a></h1><p>        在Seq2Seq结构中，编码器(Encoder)把所有的输入序列都编码成一个统一的语义向量<strong>Context</strong>，然后再由解码器(Decoder)解码。在解码器(Decoder)解码的过程中，不断地将前一个时刻 t-1 的输出作为后一个时刻 t -1 的输入，循环解码，直到输出停止符为止。         与经典RNN结构不同的是，<strong>Seq2Seq结构不再要求输入和输出序列有相同的时间长度！</strong></p><h3 id="_2-2-1-encoder-decoder-架构" tabindex="-1">2.2.1 encoder-decoder 架构 <a class="header-anchor" href="#_2-2-1-encoder-decoder-架构" aria-label="Permalink to &quot;2.2.1 encoder-decoder 架构&quot;">​</a></h3><p><strong>encoder - decoder 经典结构可表示如下：</strong></p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/seq2seq-figure7.jpg" alt="典型结构1"></p><p><strong>按序列展开形式如下：</strong></p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/seq2seq-figure6.jpg" alt="典型seq2seq模型"></p><h3 id="_2-2-2-encoder-部分" tabindex="-1">2.2.2 encoder 部分 <a class="header-anchor" href="#_2-2-2-encoder-部分" aria-label="Permalink to &quot;2.2.2 encoder 部分&quot;">​</a></h3><p>         seq2seq 的 encoder 部分 的主要作用是将输入序列编码成一个<strong>上下文向量(Context vector)</strong>, 这个Context vector 包含了整个输入序列的信息:</p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/seq2seq-figure8.jpg" alt="context vector"></p><p>        如图所示，得到Context的方式有很多种：</p><ul><li>最简单的方法就是把Encoder的最后一个隐状态赋值给Context vector;</li><li>还可以对最后的隐状态做一个变换得到Context vector，也可以对所有的隐状态做变换;</li><li>还可以对每层的输出做加权后做变换得到Context vector.</li></ul><h3 id="_2-2-3-decoder-部分" tabindex="-1">2.2.3 decoder 部分 <a class="header-anchor" href="#_2-2-3-decoder-部分" aria-label="Permalink to &quot;2.2.3 decoder 部分&quot;">​</a></h3><p>        decoder 部分的作用就是接受encoder 输出的context vector，然后一步步将输出序列解码出来, 有不同的解码方式可供选择。</p><p><strong>形式1: Context vector 作为 decoder 的 h0输入到Decoder中：</strong></p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/seq2seq-figure9.jpg" alt="decoder 1"></p><p><strong>形式2: 将Context vector 当做每一步的输入 输入到 Decoder 的每个时间步中：</strong></p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/seq2seq-figure10.jpg" alt="decoder 2"></p><p><strong>思考：NLP 中 哪种形式用的比较多呢？？？</strong></p><h2 id="_2-3-seq2seq-实现举例" tabindex="-1">2.3 Seq2Seq 实现举例 <a class="header-anchor" href="#_2-3-seq2seq-实现举例" aria-label="Permalink to &quot;2.3 Seq2Seq 实现举例&quot;">​</a></h2><p>        为了展示Seq2Seq模型的运行过程，我们以机器翻译为例，整个encoder-decoder 过程为:</p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/seq2seq-figure11.jpg" alt="MT 任务"></p><p><strong>Decoder 细节展开图如下</strong></p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/seq2seq-figure12.jpg" alt="decoder 展开图"></p><p>其中：</p><ul><li>红点是embdding 后的输入向量；</li><li>绿点是RNN单元；</li><li>蓝点是某一时刻的输出向量；</li><li>橘黄点是线性变换后的值；</li><li>最上点是此时间步的输出，一般为 词汇表的index 索引.</li></ul><h1 id="_3-seq2seq-中的-attention-机制" tabindex="-1">3 Seq2Seq 中的 Attention 机制 <a class="header-anchor" href="#_3-seq2seq-中的-attention-机制" aria-label="Permalink to &quot;3 Seq2Seq 中的 Attention 机制&quot;">​</a></h1><p>        在Encoder-Decoder结构中，Encoder把所有的输入序列都编码成一个统一的语义特征context vector再解码。因此 context 中必须包含原始序列中的所有信息，输入序列的长度就成了限制模型性能的瓶颈。如机器翻译问题，当要翻译的句子较长时，一个context可能存不下那么多信息，就会造成翻译精度的下降。</p><p><strong>思考：如何解决这个问题并改进 Seq2Seq结构呢？？？</strong>*</p><h2 id="_3-1-seq2seq-中的-attention-机制" tabindex="-1">3.1 Seq2Seq 中的 Attention 机制 <a class="header-anchor" href="#_3-1-seq2seq-中的-attention-机制" aria-label="Permalink to &quot;3.1 Seq2Seq 中的 Attention 机制&quot;">​</a></h2><p>        所以如果要改进Seq2Seq结构，最好的切入角度就是：利用Encoder所有隐藏层状态 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mrow><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">h_{t}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">h</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">t</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> 解决Context长度限制问题。</p><p><strong>思路图如下：</strong><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/seq2seq-figure13.jpg" alt="attention 原理"></p><p>        而工程上通常使用<strong>Luong Attention</strong> 来改善或解决长度限制问题，Luong Attention 的原理图如下： <img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/seq2seq-figure14.jpg" alt="luong attention"></p><p>        整个seq2seq流程可以表述如下：</p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/seq2seq-figure15.jpg" alt="luong attention2"></p><ul><li><a href="https://arxiv.org/pdf/1508.04025v5.pdf" target="_blank" rel="noreferrer">luong attention 论文</a></li></ul><h1 id="_4-seq2seq-的工作流程" tabindex="-1">4 Seq2Seq 的工作流程 <a class="header-anchor" href="#_4-seq2seq-的工作流程" aria-label="Permalink to &quot;4 Seq2Seq 的工作流程&quot;">​</a></h1><h2 id="_4-1-预测时流程" tabindex="-1">4.1 预测时流程 <a class="header-anchor" href="#_4-1-预测时流程" aria-label="Permalink to &quot;4.1 预测时流程&quot;">​</a></h2><p>        预测时(Inference/predict)，Encoder端没什么变化，在Decoder端，使用<strong>自产自销的策略</strong>：每一步的预测结果，都送给下一步作为输入，直至输出end就结束, 这种模式我们称之为<strong>free running</strong> 。这时的Decoder就是一个<strong>语言模型(LM)</strong>。由于这个语言模型是根据context vector来进行文本的生成的，因此这种类型的语言模型，被称为<strong>条件语言模型(Conditional LM)</strong>。         条件语言模型可以使用一些预训练好的语言模型来对Decoder的参数进行初始化，从而可以加快迭代过程(具体见4.3节)。</p><p><em>(语言模型（Language Modeling）：对给定的上下文进行下一个单词或字符的预测，用于自动补全、机器翻译等任务)</em></p><p><strong>预测或推理时的流程图如下：</strong></p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/seq2seq-figure16.jpg" alt="encoder-decoder"></p><h2 id="_4-2-训练时流程" tabindex="-1">4.2 训练时流程 <a class="header-anchor" href="#_4-2-训练时流程" aria-label="Permalink to &quot;4.2 训练时流程&quot;">​</a></h2><p><strong>思考：在训练的时候，可以直接使用预测时语言模型(LM)的模式，使用上一步的预测来作为下一步的输入吗？？？</strong></p><p>        <strong>free running</strong> 的模式不能在训练时使用吗？——当然是可以的！从理论上没有任何的问题，又不是不能跑。但是，在实践中人们发现，这样训练非常困难。因为没有任何的引导，一开始会完全是瞎预测，正所谓<strong>一步错，步步错</strong>，而且越错越离谱，这样会导致训练时的累积损失太大（「误差爆炸」问题，exposure bias），训练起来非常艰难。</p><p><strong>思考：如何解决这个问题呢？？？</strong></p><h3 id="_4-2-1-teacher-forcing" tabindex="-1">4.2.1 Teacher Forcing <a class="header-anchor" href="#_4-2-1-teacher-forcing" aria-label="Permalink to &quot;4.2.1 Teacher Forcing&quot;">​</a></h3><p>        一种自然的思路是，在每一步的预测时，让老师来指导一下，即提示一下上一个词的正确答案，decoder就可以快速步入正轨，训练过程也可以更快收敛。因此大家把这种方法称为<strong>teacher forcing</strong>。所以，这种操作的目的就是为了使得训练过程更容易，过程如下图所示。</p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/seq2seq-figure17.jpg" alt="Teacher Forcing"></p><p><strong>思考：Teacher Forcing 就没问题吗？？？</strong></p><h3 id="_4-2-2-scheduled-sampling" tabindex="-1">4.2.2 Scheduled sampling <a class="header-anchor" href="#_4-2-2-scheduled-sampling" aria-label="Permalink to &quot;4.2.2 Scheduled sampling&quot;">​</a></h3><p>        更高端的教学模式是老师只给适量的引导，学生也积极学习。即我们设置一个概率p，每一步，以概率p靠自己上一步的输入来预测，以概率1-p根据老师的提示来预测，这种方法称为 <strong>计划采样(scheduled sampling)</strong>。</p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/seq2seq-figure18.jpg" alt="scheduled sampling"></p><p><em>(注意: 在seq2seq的训练过程中，decoder即使遇到了end标识也不会结束，因为训练的时候并不是一个生成的过程 ，我们需要等到“标准答案”都输入完才结束。)</em></p><h2 id="_4-3-decoder的预训练" tabindex="-1">4.3 Decoder的预训练 <a class="header-anchor" href="#_4-3-decoder的预训练" aria-label="Permalink to &quot;4.3 Decoder的预训练&quot;">​</a></h2><p>        前面提到过，Decoder其实是一种<strong>条件语言模型(Conditional LM)</strong>。条件语言模型意味着Decoder 训练好了之后，具有了某种能力，可以处理不同的Context vector 产生不同的结果。此时，Decoder 无需再变化，只需要更新Context vector就好了。</p><p><strong>思考：这是否意味着Decoder模型学到了一些通用的知识呢？？？</strong></p><p>        实际工程中，在训练过程中常使用<strong>预训练</strong>的语言模型来初始化Decoder的参数，原因如下：</p><ul><li>加速收敛：预训练的语言模型通常经过大规模的数据和计算资源训练得到，具有较好的语言表示能力。通过使用预训练模型初始化Decoder的参数，可以将这些丰富的语言表示能力引入到模型中，从而为模型提供一个较好的起点。这有助于加快模型的收敛速度，减少训练时间和资源消耗。</li><li>提供语义信息：预训练的语言模型在大规模数据上学习到了丰富的语义信息和语言规律。通过初始化Decoder参数，模型可以从预训练模型中继承这些有用的语言特征和知识，从而更好地理解和生成文本。这有助于生成更准确、流畅的语句，并提高生成文本的语义质量。</li><li>缓解数据稀疏性问题：在训练过程中，特别是当训练数据较少时，语言模型可能面临数据稀疏性的问题。通过使用预训练模型初始化Decoder的参数，可以利用预训练模型在大规模数据上学习到的语言分布信息，缓解数据稀疏性问题，提高模型的泛化能力和生成能力。         需要注意的是，预训练的语言模型通常是在大规模的无监督数据上进行预训练，而在具体任务上进行微调。这种预训练-微调的方式可以在任务特定的数据上进行更好的参数优化，同时保留了预训练模型所学到的通用语言表示能力。这种迁移学习的思想使得使用预训练语言模型来初始化Decoder参数成为一种有效的策略。</li></ul><h1 id="_5-seq2seq-的损失函数" tabindex="-1">5 Seq2Seq 的损失函数 <a class="header-anchor" href="#_5-seq2seq-的损失函数" aria-label="Permalink to &quot;5 Seq2Seq 的损失函数&quot;">​</a></h1><p>        前面我们详细介绍了seq2seq的内部的结构，明白了内部结构，想知道是怎么训练的就很容易了。</p><p>         在之前图中，我们看到decoder的每一步产生隐状态 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mrow><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">h_{t}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">h</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">t</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> 后，会通过一个projection层映射到对应的词。那怎么去计算每一步的损失呢？实际上，这个projection层，通常是一个softmax神经网络层，假设词汇量是V，则会输出一个V维度的向量，每一维代表是某个词的概率。映射的过程就是把最大概率的那个词找出来作为预测出的词。</p><p>        在计算损失的时候，我们使用交叉熵作为损失函数，所以我们要找出这个V维向量中，正确预测对应的词的那一维的概率大小 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><mrow><mi>p</mi></mrow><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{p}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord textstyle cramped"><span class="mord mathit">p</span></span></span><span style="top:0em;margin-left:0.16668em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>^</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> ，则这一步的损失就是它的负导数 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>−</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><mover accent="true"><mrow><mi>p</mi></mrow><mo>^</mo></mover><mo>)</mo></mrow><annotation encoding="application/x-tex">-log(\hat{p})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord">−</span><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mord mathit">o</span><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord textstyle cramped"><span class="mord mathit">p</span></span></span><span style="top:0em;margin-left:0.16668em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>^</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span> ，将每一步的损失求和，即得到总体的损失函数：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>J</mi><mo>=</mo><mo>−</mo><mi>log</mi><mo>(</mo><mi>p</mi><mo>(</mo><mover accent="true"><mrow><msub><mi>y</mi><mrow><mn>1</mn></mrow></msub></mrow><mo>^</mo></mover><mo>)</mo><mo>)</mo><mo>−</mo><mi>log</mi><mo>(</mo><mi>p</mi><mo>(</mo><mover accent="true"><mrow><msub><mi>y</mi><mrow><mn>2</mn></mrow></msub></mrow><mo>^</mo></mover><mo>)</mo><mo>)</mo><mo>−</mo><mo>…</mo><mo>−</mo><mi>log</mi><mo>(</mo><mi>p</mi><mo>(</mo><mover accent="true"><mrow><msub><mi>y</mi><mrow><mi>n</mi></mrow></msub></mrow><mo>^</mo></mover><mo>)</mo><mo>)</mo><mo>−</mo><mi>log</mi><mo>(</mo><mi>p</mi><mo>(</mo><mo>[</mo><mi>E</mi><mi>O</mi><mi>S</mi><mo>]</mo><mo>)</mo><mo>)</mo><mo>=</mo><mo>−</mo><mfrac><mrow><mn>1</mn></mrow><mrow><mi>T</mi></mrow></mfrac><msubsup><mo>∑</mo><mrow><mi>i</mi></mrow><mrow><mi>T</mi></mrow></msubsup><mi>log</mi><mo>(</mo><mi>p</mi><mo>(</mo><mover accent="true"><mrow><msub><mi>y</mi><mrow><mi>i</mi></mrow></msub></mrow><mo>^</mo></mover><mo>)</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">J =-\log (p (\hat{y_{1}}))-\log (p(\hat{y_{2}}))-\ldots-\log (p (\hat{y_{n}}))-\log (p([E O S])) =-\frac{1}{T} \sum_{i}^{T} \log (p(\hat{y_{i}})) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.8283360000000002em;"></span><span class="strut bottom" style="height:3.106005em;vertical-align:-1.277669em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:0.09618em;">J</span><span class="mrel">=</span><span class="mord">−</span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord displaystyle textstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03588em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span><span style="top:0em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>^</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mclose">)</span><span class="mbin">−</span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord displaystyle textstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03588em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathrm">2</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span><span style="top:0em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>^</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mclose">)</span><span class="mbin">−</span><span class="minner">…</span><span class="mbin">−</span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord displaystyle textstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03588em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">n</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span><span style="top:0em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>^</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mclose">)</span><span class="mbin">−</span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord mathit">p</span><span class="mopen">(</span><span class="mopen">[</span><span class="mord mathit" style="margin-right:0.05764em;">E</span><span class="mord mathit" style="margin-right:0.02778em;">O</span><span class="mord mathit" style="margin-right:0.05764em;">S</span><span class="mclose">]</span><span class="mclose">)</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord">−</span><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.686em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathit" style="margin-right:0.13889em;">T</span></span></span></span><span style="top:-0.22999999999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.677em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mop op-limits"><span class="vlist"><span style="top:1.1776689999999999em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span></span></span></span><span style="top:-0.000005000000000143778em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span><span class="op-symbol large-op mop">∑</span></span></span><span style="top:-1.2500050000000003em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">T</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord displaystyle textstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03588em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span><span style="top:0em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>^</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span></p><p>其中：</p><ul><li>T代表Decoder有多少步；</li><li>[EOS]代表 <strong>end of sentence</strong> 这个特殊标记(token).</li></ul><h1 id="_6-decoding-中的-beam-search" tabindex="-1">6 Decoding 中的 Beam search <a class="header-anchor" href="#_6-decoding-中的-beam-search" aria-label="Permalink to &quot;6 Decoding 中的 Beam search&quot;">​</a></h1><h2 id="_6-1-贪心decoding" tabindex="-1">6.1 贪心decoding <a class="header-anchor" href="#_6-1-贪心decoding" aria-label="Permalink to &quot;6.1 贪心decoding&quot;">​</a></h2><p>        前面画的几个图展示的预测过程，其实是最简单的decoding方式: <strong>Greedy Decoding</strong> ，即每一步，都预测出概率最大的那个词，然后输入给下一步, 如下图所示。</p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/seq2seq-figure19.jpg" alt="Greedy Decoding"></p><p>        这种Greedy的方式，简单快速，但是既然叫“贪心”，肯定会有问题，那就是<strong>每一步最优，不一定全局最优</strong>，这种方式很可能“捡了芝麻，丢了西瓜” 。         改进的方法，就是使用 <strong>Beam Search</strong>方法：每一步，多选几个作为候选，最后综合考虑，选出最优的组合。</p><h2 id="_6-2-beam-search-原理" tabindex="-1">6.2 Beam search 原理 <a class="header-anchor" href="#_6-2-beam-search-原理" aria-label="Permalink to &quot;6.2 Beam search 原理&quot;">​</a></h2><p>        Beam Search的操作步骤如下：</p><ul><li>首先，我们需要设定一个候选集的大小beam size=k；</li><li>每一步的开始，我们从每个当前输入对应的所有可能输出，计算每一条路的“序列得分”；</li><li>保留“序列得分”最大的k个作为下一步的输入；</li><li>不断重复上述过程，直至结束，选择“序列得分”最大的那个序列作为最终结果。</li></ul><p><strong>Beam search 原理图</strong></p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/seq2seq-figure20.jpg" alt="beam search"></p><p>        在每一步，我们都会去对所有的可能输出，计算一次score，假设beam size为k，词汇量为V，那么每一步就需要分出k×V个分支并逐一计算score。所以在图中我们可以看到除了第一步，后面每一步都是分出来2×3=6支。然后综合这k×V个score的结果，只选择其中最大的k个保留。</p><h2 id="_6-3-公式分析" tabindex="-1">6.3 公式分析 <a class="header-anchor" href="#_6-3-公式分析" aria-label="Permalink to &quot;6.3 公式分析&quot;">​</a></h2><p>        这里的重点就在于这个“序列得分”的计算, 我们使用如下的score函数来定义序列得分：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo>(</mo><msub><mi>y</mi><mrow><mn>1</mn></mrow></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>y</mi><mrow><mi>t</mi></mrow></msub><mo>)</mo><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>t</mi></mrow></msubsup><mi>log</mi><mi>P</mi><mo>(</mo><msub><mi>y</mi><mrow><mi>i</mi></mrow></msub><mo>∣</mo><msub><mi>y</mi><mrow><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mi>y</mi><mrow><mn>2</mn></mrow></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>y</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">score(y_{1}, \ldots, y_{t})=\sum_{i=1}^{t} \log P(y_{i} \mid y_{1}, y_{2}, \ldots, y_{i-1}, x) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.7805610000000003em;"></span><span class="strut bottom" style="height:3.05823em;vertical-align:-1.277669em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit">s</span><span class="mord mathit">c</span><span class="mord mathit">o</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit">e</span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03588em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="minner">…</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03588em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">t</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mrel">=</span><span class="mop op-limits"><span class="vlist"><span style="top:1.1776689999999999em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mrel">=</span><span class="mord mathrm">1</span></span></span></span><span style="top:-0.000005000000000143778em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span><span class="op-symbol large-op mop">∑</span></span></span><span style="top:-1.2500050000000003em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">t</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03588em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">∣</span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03588em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03588em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathrm">2</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="minner">…</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03588em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mbin">−</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span></span></p><p>        这个score代表了当前到第t步的输出序列的一个综合得分，越高越好。其中 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mi>P</mi><mo>(</mo><msub><mi>y</mi><mrow><mi>i</mi></mrow></msub><mo>∣</mo><msub><mi>y</mi><mrow><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mi>y</mi><mrow><mn>2</mn></mrow></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>y</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">log P(y_{i} \mid y_{1}, y_{2}, \ldots, y_{i-1}, x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mord mathit">o</span><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03588em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">∣</span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03588em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03588em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathrm">2</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="minner">…</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03588em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mbin">−</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span> 类似于前面我们写的第t步的交叉熵损失的负数。 所以这个score越大， 就意味着到当前这一步为止，输出序列的累积损失越小。</p><h2 id="_6-4-beam-search-分析" tabindex="-1">6.4 Beam search 分析 <a class="header-anchor" href="#_6-4-beam-search-分析" aria-label="Permalink to &quot;6.4 Beam search 分析&quot;">​</a></h2><p><strong>思考：每一步会选择k个候选项作为下一步的扩展，但是随着序列长度的增加，就意味着候选结果会一直增多吗？？？。</strong></p><p>        在每一步选择k个候选项时，Beam Search会根据某种评估指标（例如概率、得分）对候选项进行排序，然后选择概率或得分最高的k个候选项。这意味着在下一步扩展时，只有k个候选项会被保留，而其他的候选项会被丢弃。因此，虽然在每一步都会考虑k个候选项，但是随着序列长度的增加，只有其中最好的k个候选项会被保留，而其他的候选项会被淘汰。这样，随着序列长度的增加，候选结果的数量并不会一直增多，而是保持在k个。Beam Search算法通过这种方式在保持一定搜索广度的同时，控制了候选结果的数量，从而在搜索空间中找到可能性较高的序列。</p><p><strong>思考：要等到所有句子 end 整个decoding 过程才结束吗？？？</strong></p><p>        由于分支有时候会较多，如果等每一个分支都遇到end才停的话，可能耗时太久，因此一般我们会设定一些规则，比如已经走了T步，或者已经积累了N条已完成的句子，就终止beam search过程。</p><p><strong>思考：最后N个序列选哪个，根据总分还是平均分？？？</strong></p><p>        在search结束之后，我们需要对已完成的N个序列做一个抉择，挑选出最好的那个，那不就是通过前面定义的score函数来比较吗？确实可以，但是如果直接使用score来挑选的话，会导致那些很短的句子更容易被选出。因为score函数的每一项都是负的，序列越长，score往往就越小。因此我们可以使用长度来对score函数进行细微的调整：对每个序列的得分，除以序列的长度。根据调整后的结果来选择best one。</p><h1 id="_7-nlp-从机器学习到深度学习" tabindex="-1">7 NLP 从机器学习到深度学习 <a class="header-anchor" href="#_7-nlp-从机器学习到深度学习" aria-label="Permalink to &quot;7 NLP 从机器学习到深度学习&quot;">​</a></h1><h2 id="_7-1-nlp-中常见任务" tabindex="-1">7.1 NLP 中常见任务 <a class="header-anchor" href="#_7-1-nlp-中常见任务" aria-label="Permalink to &quot;7.1 NLP 中常见任务&quot;">​</a></h2><p>        NLP（自然语言处理）领域有许多常见的任务，涵盖了对自然语言进行理解和生成的各个方面。以下是一些常见的NLP任务：</p><ol><li>文本分类（Text Classification）：将文本分为不同的类别或标签，如情感分类、主题分类等。</li><li>命名实体识别（Named Entity Recognition，NER）：从文本中识别和提取命名实体，如人名、地名、组织机构等。</li><li>信息抽取（Information Extraction）：从非结构化文本中提取结构化信息，如关系抽取、事件抽取等。</li><li>问答系统（Question Answering）：回答用户提出的自然语言问题，可以是基于检索的问答或阅读理解型问答。</li><li>机器翻译（Machine Translation，MT）：将一种语言的文本翻译成另一种语言的文本。</li><li>情感分析（Sentiment Analysis）：分析文本的情感倾向，判断文本是正面的、负面的还是中性的。</li><li>文本生成（Text Generation）：生成符合语法和语义规则的文本，如文本摘要、对话生成等。</li><li>语言模型（Language Modeling）：对给定的上下文进行下一个单词或字符的预测，用于自动补全、机器翻译等任务。</li><li>序列标注（Sequence Labeling）：给定输入序列，为每个序列元素分配一个标签，如词性标注、命名实体识别等。</li><li>文本聚类（Text Clustering）：将文本集合分成不同的群组，每个群组包含相似的文本。</li><li>文本摘要（Text Summarization）：从长文本中提取关键信息，生成较短的摘要。</li><li>对话系统（Dialogue Systems）：处理人机对话，并与用户进行自然语言交互。</li><li>语义角色标注（Semantic Role Labeling）：为句子中的谓词和论元分配语义角色，描述句子中的事件和参与者。</li><li>语言生成（Language Generation）：生成自然语言文本，如机器翻译、文本摘要、对话生成等。 这些任务代表了NLP领域中的一些核心问题和应用，研究人员和从业者致力于开发和改进相应的算法和技术，以提高自然语言处理系统的性能和效果。</li></ol><h2 id="_7-2-机器翻译的发展历程" tabindex="-1">7.2 机器翻译的发展历程 <a class="header-anchor" href="#_7-2-机器翻译的发展历程" aria-label="Permalink to &quot;7.2 机器翻译的发展历程&quot;">​</a></h2><ul><li>早期的(1950s)机器翻译的思路十分简单，通过设置大量的翻译规则，构建一个大型的双语对照表，来将源语言翻译成目标语言。这种做法过程简单，效果也一般。</li><li>后来（1990s-2010s）我们有了更为先进复杂的机器翻译技术: <strong>统计机器翻译(Statistical Machine Translation, SMT).</strong></li><li>再后来 在深度学习时代，我们有了更好的方法：<strong>神经机器翻译(Neural Machine Translation，NMT).</strong></li></ul><h2 id="_7-3-smt-方法简介" tabindex="-1">7.3 SMT 方法简介 <a class="header-anchor" href="#_7-3-smt-方法简介" aria-label="Permalink to &quot;7.3 SMT 方法简介&quot;">​</a></h2><p>        SMT的主要思想就是从大量的数据中学习一个概率模型 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mrow><mi>y</mi><mi mathvariant="normal">∣</mi><mi>x</mi></mrow></mrow><annotation encoding="application/x-tex">P{y | x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mord textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mord mathrm">∣</span><span class="mord mathit">x</span></span></span></span></span> ，其中x是源语言（source language），y是目标语言（target language），即x翻译成y的概率有多大。在翻译时，我们只需要通过求 $$argmax_{y}P{y | x}$$ 就行了，即找到概率最大的那个y，就是x的最佳翻译了。</p><p>由贝叶斯公式：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><mi>y</mi><mo>∣</mo><mi>x</mi><mo>)</mo><mo>=</mo><mfrac><mrow><mi>P</mi><mo>(</mo><mi>y</mi><mo>)</mo><mi>P</mi><mo>(</mo><mi>x</mi><mo>∣</mo><mi>y</mi><mo>)</mo></mrow><mrow><mi>P</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">P(y \mid x)=\frac{P(y) P(x \mid y)}{P(x)} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.427em;"></span><span class="strut bottom" style="height:2.363em;vertical-align:-0.936em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mrel">∣</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.686em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span><span style="top:-0.2300000000000001em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.677em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mrel">∣</span><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span></span></p><p>我们可以将目标转变为：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>a</mi><msub><mi>x</mi><mrow><mi>y</mi></mrow></msub><mi>P</mi><mo>(</mo><mi>y</mi><mo>∣</mo><mi>x</mi><mo>)</mo><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>a</mi><msub><mi>x</mi><mrow><mi>y</mi></mrow></msub><mfrac><mrow><mi>P</mi><mo>(</mo><mi>y</mi><mo>)</mo><mi>P</mi><mo>(</mo><mi>x</mi><mo>∣</mo><mi>y</mi><mo>)</mo></mrow><mrow><mi>P</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>a</mi><msub><mi>x</mi><mrow><mi>y</mi></mrow></msub><mi>P</mi><mo>(</mo><mi>y</mi><mo>)</mo><mi>P</mi><mo>(</mo><mi>x</mi><mo>∣</mo><mi>y</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">argmax_{y} P(y \mid x)= argmax_{y} \frac{P(y) P(x \mid y)}{P(x)}=argmax_{y} P(y) P(x \mid y) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.427em;"></span><span class="strut bottom" style="height:2.363em;vertical-align:-0.936em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit">a</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="mord mathit">m</span><span class="mord mathit">a</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03588em;">y</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mrel">∣</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathit">a</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="mord mathit">m</span><span class="mord mathit">a</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03588em;">y</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.686em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span><span style="top:-0.2300000000000001em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.677em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mrel">∣</span><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mrel">=</span><span class="mord mathit">a</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="mord mathit">m</span><span class="mord mathit">a</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03588em;">y</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mrel">∣</span><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span></span></p><p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><mi>y</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">P(y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span> 就是求y这个句子的概率，这就是一个<strong>语言模型(LM)</strong>。而后者 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><mi>x</mi><mo>∣</mo><mi>y</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">P(x \mid y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mrel">∣</span><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span> 则被称为<strong>翻译模型(TM)</strong>。LM可以通过目标语言的语料进行训练，TM则需要通过**平行语料(parallel corpus，即源和目标两种语言的互相对照的语料)进行训练。</p><p>        翻译模型通过大量的平行语料，学习到的主要是两种语言之间的对应关系，而语言模型则侧重于学习一种语言内部的语法结构，不同词汇是怎么流畅地组合成句子的。原本的公式只有一个翻译模型，会导致我们训练出来的模型在翻译结果的语言通畅性方面很差。因此，我们经过公式变换，将一个TM任务转化成TM+LM两种任务，可以模型学习的结果更好。</p><p>        对于TM的学习，一般我们会进一步进行分解，考虑两种语言之间的各种对齐方式（alignment），即在原有的翻译模型上，引入一个隐变量a，得到 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><mi>x</mi><mo separator="true">,</mo><mi>a</mi><mo>∣</mo><mi>y</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">P(x, a \mid y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mpunct">,</span><span class="mord mathit">a</span><span class="mrel">∣</span><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span> ，可以理解为给定句子y，按照对齐方式a翻译成x的概率。具体什么是对齐方式alignment呢？它的意思就是在两种语言A和B之间，A的词是跟B的词怎么对应的。很明显，这种对应关系可以是一对一、一对多、多对一、多对多的。</p><p>        在学习了LM和TM这两个模型之后，是不是就完事儿了呢？当然没有，别忘了公式里还有一个argmax，我们要找出最佳的翻译是什么。根据LM和TM寻找最佳y的过程，就称为“decoding”，即解码。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/QLDSy3Cx3YIn4IzP3UVrS6HfxiatGYDIP9y66ibIsR2YE9ibjFMfLDHtX28TYw1fF5xiaC4tcibOib62ndmSicfmziby3A/640?wx_fmt=png&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="beam search"></p><p>        一个最直接的方法就是，遍历所有可能的y，选择概率最大的那个，当然就是最佳的翻译。明显，这种方式带来的开销是我们无法忍受的。如果学习过CRF或者HMM，我们应该知道对于这种解码的过程，我们一般使用动态规划、启发式搜索的方法来处理。在SMT中具体怎么解码，我们这里也暂时不做深入的研究。</p><p><strong>通过以上过程我们发现其中涉及的原理、推导过程非常复杂，数据处理也异常繁琐</strong></p><p>        统计机器翻译——SMT，在深度学习时代之前，风光无限，一直是机器翻译的巅峰技术。但是，SMT的门槛也是很高的，那些表现优异的SMT模型，通常都是极其复杂的，里面涉及到大量的特征工程，海量的专家知识，无数的资源积累，繁多的功能模块，还需要庞大的人力去维护。这也是我根本不想去深入了解这个技术的原因。</p><h2 id="_7-4-nmt" tabindex="-1">7.4 NMT <a class="header-anchor" href="#_7-4-nmt" aria-label="Permalink to &quot;7.4 NMT&quot;">​</a></h2><p>        深度学习的“可恨之处”在于，它把那些需要大量人力的工作都吃掉了，导致行业专家和搬砖工人门纷纷下岗。NMT就是这样，企图就是用一个简洁的神经网络结构，就把机器翻译这么大的一个工程给包下来。我画了一个形象生动的图来示意SMT和NMT的区别： <img src="https://mmbiz.qpic.cn/mmbiz_png/QLDSy3Cx3YIn4IzP3UVrS6HfxiatGYDIPpHIrxBonogUezNd326socPXmYbx5CoyINBKnWzod4BWaQK0m21X9pw/640?wx_fmt=png&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="beam search"></p><p>        之后，按照我们之前encoder - Decoder 架构 设计训练模型即可.</p><h1 id="_8-参考文献" tabindex="-1">8 参考文献 <a class="header-anchor" href="#_8-参考文献" aria-label="Permalink to &quot;8 参考文献&quot;">​</a></h1><ul><li><a href="https://spaces.ac.cn/archives/5861" target="_blank" rel="noreferrer">参考文献1</a></li><li><a href="https://arxiv.org/pdf/1409.3215.pdf" target="_blank" rel="noreferrer">参考文献2</a></li><li><a href="https://mp.weixin.qq.com/s/dXqAdb524o3lBZcQiXQacw" target="_blank" rel="noreferrer">参考文献3</a></li><li><a href="https://arxiv.org/abs/1508.04025" target="_blank" rel="noreferrer">Luong Attention</a></li><li><a href="https://arxiv.org/pdf/1508.04025v5.pdf" target="_blank" rel="noreferrer">luong attention</a></li><li><a href="https://zhuanlan.zhihu.com/p/51383402" target="_blank" rel="noreferrer">参考网站</a></li></ul></div></div></main><footer class="VPDocFooter" data-v-39a288b8 data-v-e257564d><!--[--><!--]--><div class="edit-info" data-v-e257564d><div class="edit-link" data-v-e257564d><a class="VPLink link vp-external-link-icon no-icon edit-link-button" href="https://github.com/EthanLiu6/coderethan.fun/edit/master/docs/AI/01_deep_learning_theory/10-seq2seq.md" target="_blank" rel="noreferrer" data-v-e257564d><!--[--><span class="vpi-square-pen edit-link-icon" data-v-e257564d></span> 在 GitHub 上编辑此页面 OR 提出修改意见<!--]--></a></div><div class="last-updated" data-v-e257564d><p class="VPLastUpdated" data-v-e257564d data-v-e98dd255>最后更新于: <time datetime="2025-03-27T09:51:05.000Z" data-v-e98dd255></time></p></div></div><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-e257564d><span class="visually-hidden" id="doc-footer-aria-label" data-v-e257564d>Pager</span><div class="pager" data-v-e257564d><a class="VPLink link pager-link prev" href="/AI/01_deep_learning_theory/09-recurrent_neural_network.html" data-v-e257564d><!--[--><span class="desc" data-v-e257564d>上一篇</span><span class="title" data-v-e257564d>09-recurrent_neural_network</span><!--]--></a></div><div class="pager" data-v-e257564d><a class="VPLink link pager-link next" href="/AI/01_deep_learning_theory/11-1attentions.html" data-v-e257564d><!--[--><span class="desc" data-v-e257564d>下一篇</span><span class="title" data-v-e257564d>11-1attentions</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-5d98c3a5 data-v-e315a0ad><div class="container" data-v-e315a0ad><p class="message" data-v-e315a0ad>ICP备案号: <a href="https://beian.miit.gov.cn/" target="_blank">蜀ICP备2024103116号</a><br>公安备案号: <a href="https://beian.mps.gov.cn/#/query/webSearch?code=51012202001928" rel="noreferrer" target="_blank">川公网安备51012202001928</a></p><p class="copyright" data-v-e315a0ad>版权所有 © 2024-present  <a href="mailto:16693226842@163.com" target="_blank">Ethan.Liu</a></p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"about_me_index.md\":\"mz4kR9Fi\",\"ai_01_deep_learning_theory_01-feedforward_network.md\":\"CcvQrxgt\",\"ai_01_deep_learning_theory_02-back_propagation.md\":\"BtjZwJU_\",\"ai_01_deep_learning_theory_03-bp_example_demo.md\":\"DDBE4U1G\",\"ai_01_deep_learning_theory_04-convolution_neural_network.md\":\"CxA35loH\",\"ai_01_deep_learning_theory_05-deep_learning_model.md\":\"Ct7liYj1\",\"ai_01_deep_learning_theory_06-pytorch_install.md\":\"Clcubooi\",\"ai_01_deep_learning_theory_07-operators.md\":\"DPdxImHV\",\"ai_01_deep_learning_theory_08-activation_functions.md\":\"B1C9PY4H\",\"ai_01_deep_learning_theory_09-recurrent_neural_network.md\":\"CzW3agM_\",\"ai_01_deep_learning_theory_10-seq2seq.md\":\"6D8B4Rlq\",\"ai_01_deep_learning_theory_11-1attentions.md\":\"Cvpw08iV\",\"ai_01_deep_learning_theory_11-2attention-extension.md\":\"DC_8b5oC\",\"ai_01_deep_learning_theory_12-weight-initialization.md\":\"C1v8tMs3\",\"ai_01_deep_learning_theory_13-optimizers.md\":\"BJt7iJF7\",\"ai_01_deep_learning_theory_14-regularization.md\":\"Bx5ZYl-4\",\"ai_01_deep_learning_theory_15-deep-learning-tuning-guide.md\":\"CWN3mKn4\",\"ai_01_deep_learning_theory_20-pytorch-tensor.md\":\"BFcyl0Px\",\"ai_01_deep_learning_theory_21-pytorch-autograd.md\":\"DS522bLB\",\"ai_01_deep_learning_theory_22-pytorch-module.md\":\"DjCl8s56\",\"ai_01_deep_learning_theory_23-1training-example-1.md\":\"DH8M-Obn\",\"ai_01_deep_learning_theory_23-2decoder.md\":\"Cq0AY57n\",\"ai_01_deep_learning_theory_23-3encoder.md\":\"D0g53JSA\",\"ai_01_deep_learning_theory_23-4transformer.md\":\"DHbayc0z\",\"ai_01_deep_learning_theory_24-pytorch-optimizer.md\":\"Bn_0Gw79\",\"ai_01_deep_learning_theory_25-pytorch-lr-scheduler.md\":\"DxyT4Fmr\",\"ai_01_deep_learning_theory_26-pytorch-dataloader.md\":\"RYWx6b8M\",\"ai_01_deep_learning_theory_27-pytorch-model-save.md\":\"B7Q4ONf_\",\"ai_01_deep_learning_theory_28-pytorch-tensorboard.md\":\"VHNB3XsJ\",\"ai_01_deep_learning_theory_29-pytorch-graph-mode.md\":\"BL6cmN4L\",\"ai_01_deep_learning_theory_30-1training-example-cv.md\":\"DRhcPsAc\",\"ai_01_deep_learning_theory_30-3main.md\":\"Ci1wkkl4\",\"ai_01_deep_learning_theory_31-1stable-diffusion.md\":\"7ssTYEJp\",\"ai_01_deep_learning_theory_31-2sdxl.md\":\"B5ng9y6A\",\"ai_01_deep_learning_theory_31-3vae.md\":\"Cxzp-WBt\",\"ai_01_deep_learning_theory_40-nlp-bert_ner.md\":\"CcuO1-mQ\",\"ai_01_deep_learning_theory_41-nlp-t5_question-answering.md\":\"_6IyzXZC\",\"ai_01_deep_learning_theory_42-nlp-gpt.md\":\"_Nxmo9wM\",\"ai_01_deep_learning_theory_43-scaling-law.md\":\"xoZlgvkY\",\"ai_01_deep_learning_theory_44-distribute-training.md\":\"CNK33TAz\",\"ai_01_deep_learning_theory_45-llm-history.md\":\"Czmd8z8v\",\"ai_01_deep_learning_theory_46-llm-gpt-extension.md\":\"DehUuq2v\",\"ai_01_deep_learning_theory_46-nlp-llama.md\":\"Cvowveq1\",\"ai_01_deep_learning_theory_47-llm-deepseek-structure.md\":\"REY-Au_D\",\"ai_01_deep_learning_theory_47-nlp-deepseek.md\":\"C1weFA21\",\"ai_01_deep_learning_theory_index.md\":\"DULtN52e\",\"ai_02_distribute_training_00_large-scale-model-trainning.md\":\"ScK-rTES\",\"ai_02_distribute_training_01_coding.md\":\"BpOoZXvs\",\"ai_02_distribute_training_01_offload-and-recompute.md\":\"DH1t2mgl\",\"ai_02_distribute_training_02_amp.md\":\"UHS_a1Tv\",\"ai_02_distribute_training_03_coding.md\":\"CmvCR_VM\",\"ai_02_distribute_training_03_pytorch-dp.md\":\"DlSpdrVa\",\"ai_02_distribute_training_04_pytorch-ddp.md\":\"BdX4eQnE\",\"ai_02_distribute_training_05_pytorch-ddp-impl.md\":\"C3ka-I-H\",\"ai_02_distribute_training_05_pytorch-ddp-impl_ddp_origin.md\":\"B0ExxsWg\",\"ai_02_distribute_training_06_collective-comm.md\":\"Cl7RSZCa\",\"ai_02_distribute_training_06_torchrun-process-group.md\":\"0t1Ne9wO\",\"ai_02_distribute_training_07_zero-optimizer.md\":\"C6XSUzAS\",\"ai_02_distribute_training_08_pytorch-zero-1.md\":\"ySTPyj3V\",\"ai_02_distribute_training_09_pytorch-fsdp-v1.md\":\"B72E4KDO\",\"ai_02_distribute_training_10_pytorch-fsdp-v2.md\":\"icJHYy7W\",\"ai_02_distribute_training_11_deepspeed-zero-1-2-impl.md\":\"qP_Pfxun\",\"ai_02_distribute_training_12_deepspeed-zero-3-impl.md\":\"Csv92XRa\",\"ai_02_distribute_training_13_megatron-zero-1-impl.md\":\"Db7_6tfb\",\"ai_02_distribute_training_14_tp-theory.md\":\"esuC_Liu\",\"ai_02_distribute_training_15_megatron-tp-impl.md\":\"DEVXuzJf\",\"ai_02_distribute_training_16_pytorch-tp-impl.md\":\"D8qf5ngw\",\"ai_02_distribute_training_17_pp-theory.md\":\"CM8M3C9i\",\"ai_02_distribute_training_18_pytorch-pp-impl.md\":\"B29AeAKF\",\"ai_02_distribute_training_19_deepspeed-pp-impl.md\":\"ChAcoPgP\",\"ai_02_distribute_training_20_megatron-pp-impl.md\":\"BUm1IzVm\",\"ai_02_distribute_training_21_sp-theory.md\":\"d7qyOraH\",\"ai_02_distribute_training_22_megatron-sp-impl.md\":\"D1orG262\",\"ai_02_distribute_training_23_3d-parallel-theory.md\":\"B1-vwyBl\",\"ai_02_distribute_training_24_megatron-3d-parallel-impl.md\":\"zPy3o2Oy\",\"ai_02_distribute_training_25_pytorch-3d-parallel-impl.md\":\"BVILcMDg\",\"ai_02_distribute_training_26_cp-theory.md\":\"B1coDnHY\",\"ai_02_distribute_training_27_megatron-cp-impl.md\":\"DdJsMNrL\",\"ai_02_distribute_training_28_moe-theory.md\":\"DMjnS_8q\",\"ai_02_distribute_training_28_moe-theory_deepseekmoe.md\":\"OTnWcHUV\",\"ai_02_distribute_training_29_megatron-moe-impl.md\":\"pJxFgCRW\",\"ai_02_distribute_training_30_deepspeed-moe-impl.md\":\"DGgDLvC2\",\"ai_02_distribute_training_31_deepspeed-code-impl.md\":\"WjUqYD_P\",\"ai_02_distribute_training_32_collective-operations.md\":\"DnU5ghvq\",\"ai_02_distribute_training_33_pytorch_distribute.md\":\"BJeB5n4-\",\"ai_02_distribute_training_index.md\":\"UwwL-uUg\",\"ai_03_transformer_01-transformer的由来.md\":\"CJuvJ6VA\",\"ai_03_transformer_02-transformer架构解读.md\":\"D7Q52fUF\",\"ai_03_transformer_03-transformer源码构建.md\":\"DNOcryKw\",\"ai_03_transformer_index.md\":\"CUW2DYfu\",\"ai_04_some_notes_00-dl_base_notes.md\":\"CugAcIEx\",\"ai_04_some_notes_01-class_logs.md\":\"CRBe6gh8\",\"ai_04_some_notes_02-some_detials.md\":\"WK9iM9IR\",\"ai_04_some_notes_03-bert理解.md\":\"D4ypihZd\",\"ai_04_some_notes_04-个人补充内容.md\":\"D67da2ij\",\"ai_04_some_notes_05-review_dl.md\":\"CB6AAWFj\",\"ai_04_some_notes_index.md\":\"BWjDxHIM\",\"ai_index.md\":\"FceT-BLG\",\"index.md\":\"DK1IDkdq\",\"it-learning_408_index.md\":\"DQ7Ub8n-\",\"it-learning_408_os-4.1 进程同步.md\":\"ImosRfCq\",\"it-learning_408_os-4.4 信号量机制.md\":\"CnMjdS0S\",\"it-learning_408_os-4.4 信号量机制pv操作之“可见”.md\":\"KW2s8fDp\",\"it-learning_c___01_开发环境搭建与基础数据类型.md\":\"DVAfnfKI\",\"it-learning_c___02_控制流语句与复合数据类型.md\":\"o_Iw4hag\",\"it-learning_c___03_指针与引用.md\":\"Bw6bYMMc\",\"it-learning_c___04_自定义数据类型与函数.md\":\"Cyyzd5R-\",\"it-learning_c___05_头文件与指针的算术运算.md\":\"DejaB1_U\",\"it-learning_c___06_字符串、数组、指针与函数.md\":\"D3aHSP55\",\"it-learning_c___07_函数进阶与内存管理.md\":\"DqUS2M8n\",\"it-learning_c___08_运算符优先级表.md\":\"CtazDwSH\",\"it-learning_c___09_指针、内存管理和类的基础.md\":\"L48egUic\",\"it-learning_c___10_深入类和对象.md\":\"BpLLILo4\",\"it-learning_c___11_类的大小、继承与权限控制.md\":\"DYFJyGgE\",\"it-learning_c___12_继承进阶.md\":\"CqNcG1Ig\",\"it-learning_c___13_类型转换和多态与虚函数.md\":\"DaeCEerH\",\"it-learning_c___14_纯虚函数、抽象类、深浅拷贝及智能指针.md\":\"B5cvBNtL\",\"it-learning_c___15_运算符重载与 string 类详解.md\":\"CZX-FnOl\",\"it-learning_c___16_有序容器与无序容器.md\":\"CJwgGEAo\",\"it-learning_c___17_模板.md\":\"4CdEsdGf\",\"it-learning_c___18_迭代器与其应用.md\":\"DmftJhgX\",\"it-learning_c___19_c__ 标准库常用算法.md\":\"Bymm15dk\",\"it-learning_c___20_c__ 异常处理 - 第19次课.md\":\"Sn8BieQI\",\"it-learning_c___21_友元及友元相关内容.md\":\"D1xuHqMb\",\"it-learning_c___22_c__ io 流详解-feadbc607d7f.md\":\"BD7P3tXk\",\"it-learning_c___23_c__ io 流详解.md\":\"azsZ5g77\",\"it-learning_c___24_位运算符总结.md\":\"BOkaz89o\",\"it-learning_c___25_c__三种继承方式.md\":\"DUn7n_et\",\"it-learning_c___26_c__11 高级特性.md\":\"CyrPVDkT\",\"it-learning_c___27_c__14 新特性.md\":\"6YQuRCGz\",\"it-learning_c___28_c__17 新特性.md\":\"BqJAdDo-\",\"it-learning_c___29_多文件和 makefile工程管理.md\":\"DvfAbEZ0\",\"it-learning_c___30_c__大型项目cmake工程管理.md\":\"CJvCgGVT\",\"it-learning_c___31_c__ 主要就业方向与技术能力分析报告.md\":\"CqJbtbjv\",\"it-learning_c___32_c__ 基础知识回顾.md\":\"Dx5_xIPx\",\"it-learning_c___index.md\":\"BAg94tAw\",\"it-learning_index.md\":\"D1maUb85\",\"it-learning_java_01.java-se.md\":\"CV9z-Ph2\",\"it-learning_java_02.sql.md\":\"gilr6jOh\",\"it-learning_java_03.java-web.md\":\"DXRkrx3K\",\"it-learning_java_05.mybatis.md\":\"HjU_MlJz\",\"it-learning_java_index.md\":\"DkRBc8cW\",\"it-learning_linux_01.linux基础.md\":\"BBs1RcBf\",\"it-learning_linux_02.shell.md\":\"BCfUk33T\",\"it-learning_linux_03.mpi并行计算.md\":\"gpRl9TGp\",\"it-learning_linux_04.docker.md\":\"DN_C173y\",\"it-learning_linux_index.md\":\"BVALaZgc\",\"job_interview_algorithm_post_index.md\":\"BC7-ey5V\",\"job_interview_algorithm_post_model_framework_001_transformer.md\":\"BGkLAUXG\",\"job_interview_algorithm_post_model_framework_002_normalization.md\":\"Bk3voDkd\",\"job_interview_algorithm_post_model_framework_003_position-embedding.md\":\"DhassiCn\",\"job_interview_algorithm_post_model_framework_004_activation.md\":\"XypIZF0Z\",\"job_interview_algorithm_post_model_framework_005_transformer-other.md\":\"DObM4AQX\",\"job_interview_algorithm_post_model_framework_006_llm-frame.md\":\"ZOdzAeBB\",\"job_interview_algorithm_post_model_framework_007_moe.md\":\"Pt227rSS\",\"job_interview_algorithm_post_model_framework_基础内容面试点.md\":\"A6jl_xQw\",\"job_interview_algorithm_post_model_framework_手撕内容.md\":\"D7eOyPIJ\",\"job_interview_algorithm_post_p0-00_场景题.md\":\"Bys-oUMI\",\"job_interview_algorithm_post_p1-01_分词器.md\":\"InWxpfrB\",\"job_interview_algorithm_post_p1-02_词向量.md\":\"CT43EKlP\",\"job_interview_algorithm_post_p2-01_注意力机制.md\":\"DOrDH746\",\"job_interview_algorithm_post_p2-02_位置编码.md\":\"DzImgEyR\",\"job_interview_algorithm_post_p2-03_归一化.md\":\"KV9ISQ90\",\"job_interview_algorithm_post_p2-04_残差连接.md\":\"DdTov5k5\",\"job_interview_algorithm_post_p3-01_多层感知机.md\":\"HnxbY8f4\",\"job_interview_algorithm_post_p3-02_激活函数.md\":\"B-78DfK3\",\"job_interview_algorithm_post_p3-03_损失函数.md\":\"N9_4qJRj\",\"job_interview_algorithm_post_p4-01_预训练技术.md\":\"D7fRaBTQ\",\"job_interview_algorithm_post_p5-01_后训练技术.md\":\"BCt3ZZ2Q\",\"job_interview_algorithm_post_p6-01_推理优化.md\":\"BDluuO6n\",\"job_interview_algorithm_post_p7-01_大模型架构.md\":\"CKvgBtK3\",\"job_interview_algorithm_post_p8_01_大模型应用.md\":\"C0eLfiiG\",\"job_interview_algorithm_post_p9-01_torch的数据.md\":\"DGN9VYcj\",\"job_interview_index.md\":\"CK06T0i_\",\"job_interview_java_index.md\":\"C0TwjbqV\",\"my_think_01_不同商家的视野.md\":\"oNKuD8ev\",\"my_think_02_学而篇.md\":\"DIXp_fou\",\"my_think_03_重温士兵突击.md\":\"iuGDsQYW\",\"my_think_04_你很好，慢慢来.md\":\"CdALlN7D\",\"my_think_index.md\":\"Ys8bgN2a\",\"question_list_doccano账户管理.md\":\"4xMMl7Bn\",\"question_list_index.md\":\"Bx3eFS0B\",\"question_list_专英翻转课堂—pytorch.md\":\"C2nclqWo\",\"question_list_虚拟机网络问题.md\":\"Cxh_Plki\",\"readme.md\":\"DJufORId\",\"update_update_log.md\":\"DkDYsiD-\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"zh-CN\",\"dir\":\"ltr\",\"title\":\"码医森\",\"description\":\"计算机知识的学习站点\",\"base\":\"/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"siteTitle\":\"CoderEthan学习站\",\"logo\":\"/imgs/home-page-logo.svg\",\"outline\":{\"label\":\"本文目录\",\"level\":[2,4]},\"search\":{\"provider\":\"local\"},\"socialLinks\":[{\"icon\":{\"svg\":\"<svg xmlns=\\\"http://www.w3.org/2000/svg\\\" viewBox=\\\"0 0 496 512\\\"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path fill=\\\"#6841d2\\\" d=\\\"M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3 .3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5 .3-6.2 2.3zm44.2-1.7c-2.9 .7-4.9 2.6-4.6 4.9 .3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3 .7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3 .3 2.9 2.3 3.9 1.6 1 3.6 .7 4.3-.7 .7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3 .7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3 .7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z\\\"/></svg>\"},\"link\":\"https://github.com/ethanliu6/\"},{\"icon\":{\"svg\":\"<svg xmlns=\\\"http://www.w3.org/2000/svg\\\" viewBox=\\\"0 0 512 512\\\"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path fill=\\\"#6841d2\\\" d=\\\"M488.6 104.1C505.3 122.2 513 143.8 511.9 169.8V372.2C511.5 398.6 502.7 420.3 485.4 437.3C468.2 454.3 446.3 463.2 419.9 464H92C65.6 463.2 43.8 454.2 26.7 436.8C9.7 419.4 .8 396.5 0 368.2V169.8C.8 143.8 9.7 122.2 26.7 104.1C43.8 87.8 65.6 78.8 92 78H121.4L96.1 52.2C90.3 46.5 87.4 39.2 87.4 30.4C87.4 21.6 90.3 14.3 96.1 8.6C101.8 2.9 109.1 0 117.9 0C126.7 0 134 2.9 139.8 8.6L213.1 78H301.1L375.6 8.6C381.7 2.9 389.2 0 398 0C406.8 0 414.1 2.9 419.9 8.6C425.6 14.3 428.5 21.6 428.5 30.4C428.5 39.2 425.6 46.5 419.9 52.2L394.6 78L423.9 78C450.3 78.8 471.9 87.8 488.6 104.1H488.6zM449.8 173.8C449.4 164.2 446.1 156.4 439.1 150.3C433.9 144.2 425.1 140.9 416.4 140.5H96.1C86.5 140.9 78.6 144.2 72.5 150.3C66.3 156.4 63.1 164.2 62.7 173.8V368.2C62.7 377.4 66 385.2 72.5 391.7C79 398.2 86.9 401.5 96.1 401.5H416.4C425.6 401.5 433.4 398.2 439.7 391.7C446 385.2 449.4 377.4 449.8 368.2L449.8 173.8zM185.5 216.5C191.8 222.8 195.2 230.6 195.6 239.7V273C195.2 282.2 191.9 289.9 185.8 296.2C179.6 302.5 171.8 305.7 162.2 305.7C152.6 305.7 144.7 302.5 138.6 296.2C132.5 289.9 129.2 282.2 128.8 273V239.7C129.2 230.6 132.6 222.8 138.9 216.5C145.2 210.2 152.1 206.9 162.2 206.5C171.4 206.9 179.2 210.2 185.5 216.5H185.5zM377 216.5C383.3 222.8 386.7 230.6 387.1 239.7V273C386.7 282.2 383.4 289.9 377.3 296.2C371.2 302.5 363.3 305.7 353.7 305.7C344.1 305.7 336.3 302.5 330.1 296.2C323.1 289.9 320.7 282.2 320.4 273V239.7C320.7 230.6 324.1 222.8 330.4 216.5C336.7 210.2 344.5 206.9 353.7 206.5C362.9 206.9 370.7 210.2 377 216.5H377z\\\"/></svg>\"},\"link\":\"https://space.bilibili.com/1327099977/\"}],\"nav\":[{\"text\":\"AI\",\"items\":[{\"text\":\"DL基础理论\",\"link\":\"/AI/01_deep_learning_theory/\"},{\"text\":\"分布式训练\",\"link\":\"/AI/02_distribute_training/\"},{\"text\":\"Transformer个人梳理\",\"link\":\"/AI/03_Transformer/\"},{\"text\":\"DL个人笔记\",\"link\":\"/AI/04_some_notes/\"}]},{\"text\":\"计算机学科内容\",\"items\":[{\"text\":\"408知识\",\"link\":\"/IT-learning/408/\"},{\"text\":\"C++基础\",\"link\":\"/IT-learning/c++/\"},{\"text\":\"Java后端\",\"link\":\"/IT-learning/Java/\"},{\"text\":\"Linux技术\",\"link\":\"/IT-learning/Linux/\"}]},{\"text\":\"求职面试\",\"items\":[{\"text\":\"Java面经\",\"link\":\"/Job_Interview/Java/\"},{\"text\":\"算法岗\",\"link\":\"/Job_Interview/Algorithm_post/\"}]},{\"text\":\"其他维护\",\"items\":[{\"text\":\"站点更新\",\"link\":\"/update/update_log\"},{\"text\":\"问题清单\",\"link\":\"/question_list/\"}]},{\"text\":\"感悟和日常\",\"items\":[{\"text\":\"关于我\",\"link\":\"/about_me/\"},{\"text\":\"站长感悟\",\"link\":\"/my_think/\"},{\"text\":\"旧版博客\",\"link\":\"https://EthanLiu6.github.io\"}]}],\"footer\":{\"message\":\"ICP备案号: <a href=\\\"https://beian.miit.gov.cn/\\\" target=\\\"_blank\\\">蜀ICP备2024103116号</a><br>公安备案号: <a href=\\\"https://beian.mps.gov.cn/#/query/webSearch?code=51012202001928\\\" rel=\\\"noreferrer\\\" target=\\\"_blank\\\">川公网安备51012202001928</a>\",\"copyright\":\"版权所有 © 2024-present  <a href=\\\"mailto:16693226842@163.com\\\" target=\\\"_blank\\\">Ethan.Liu</a>\"},\"editLink\":{\"pattern\":\"https://github.com/EthanLiu6/coderethan.fun/edit/master/docs/:path\",\"text\":\"在 GitHub 上编辑此页面 OR 提出修改意见\"},\"lastUpdated\":{\"text\":\"最后更新于\",\"formatOptions\":{\"dateStyle\":\"long\",\"timeStyle\":\"short\"}},\"docFooter\":{\"prev\":\"上一篇\",\"next\":\"下一篇\"},\"darkModeSwitchLabel\":\"深色模式\",\"lightModeSwitchTitle\":\"切换到浅色模式\",\"darkModeSwitchTitle\":\"切换到深色模式\",\"sidebar\":{\"/AI/\":[{\"items\":[{\"text\":\"01_deep_learning_theory\",\"items\":[{\"text\":\"01-feedforward_network\",\"link\":\"/AI/01_deep_learning_theory/01-feedforward_network.html\"},{\"text\":\"02-back_propagation\",\"link\":\"/AI/01_deep_learning_theory/02-back_propagation.html\"},{\"text\":\"03-bp_example_demo\",\"link\":\"/AI/01_deep_learning_theory/03-bp_example_demo.html\"},{\"text\":\"04-convolution_neural_network\",\"link\":\"/AI/01_deep_learning_theory/04-convolution_neural_network.html\"},{\"text\":\"05-deep_learning_model\",\"link\":\"/AI/01_deep_learning_theory/05-deep_learning_model.html\"},{\"text\":\"06-pytorch_install\",\"link\":\"/AI/01_deep_learning_theory/06-pytorch_install.html\"},{\"text\":\"07-operators\",\"link\":\"/AI/01_deep_learning_theory/07-operators.html\"},{\"text\":\"08-activation_functions\",\"link\":\"/AI/01_deep_learning_theory/08-activation_functions.html\"},{\"text\":\"09-recurrent_neural_network\",\"link\":\"/AI/01_deep_learning_theory/09-recurrent_neural_network.html\"},{\"text\":\"10-seq2seq\",\"link\":\"/AI/01_deep_learning_theory/10-seq2seq.html\"},{\"text\":\"11-1attentions\",\"link\":\"/AI/01_deep_learning_theory/11-1attentions.html\"},{\"text\":\"11-2attention-extension\",\"link\":\"/AI/01_deep_learning_theory/11-2attention-extension.html\"},{\"text\":\"12-weight-initialization\",\"link\":\"/AI/01_deep_learning_theory/12-weight-initialization.html\"},{\"text\":\"13-optimizers\",\"link\":\"/AI/01_deep_learning_theory/13-optimizers.html\"},{\"text\":\"14-regularization\",\"link\":\"/AI/01_deep_learning_theory/14-regularization.html\"},{\"text\":\"15-deep-learning-tuning-guide\",\"link\":\"/AI/01_deep_learning_theory/15-deep-learning-tuning-guide.html\"},{\"text\":\"20-pytorch-tensor\",\"link\":\"/AI/01_deep_learning_theory/20-pytorch-tensor.html\"},{\"text\":\"21-pytorch-autograd\",\"link\":\"/AI/01_deep_learning_theory/21-pytorch-autograd.html\"},{\"text\":\"22-pytorch-module\",\"link\":\"/AI/01_deep_learning_theory/22-pytorch-module.html\"},{\"text\":\"23-1training-example-1\",\"link\":\"/AI/01_deep_learning_theory/23-1training-example-1.html\"},{\"text\":\"23-2decoder\",\"link\":\"/AI/01_deep_learning_theory/23-2decoder.html\"},{\"text\":\"23-3encoder\",\"link\":\"/AI/01_deep_learning_theory/23-3encoder.html\"},{\"text\":\"23-4transformer\",\"link\":\"/AI/01_deep_learning_theory/23-4transformer.html\"},{\"text\":\"24-pytorch-optimizer\",\"link\":\"/AI/01_deep_learning_theory/24-pytorch-optimizer.html\"},{\"text\":\"25-pytorch-lr-scheduler\",\"link\":\"/AI/01_deep_learning_theory/25-pytorch-lr-scheduler.html\"},{\"text\":\"26-pytorch-dataloader\",\"link\":\"/AI/01_deep_learning_theory/26-pytorch-dataloader.html\"},{\"text\":\"27-pytorch-model-save\",\"link\":\"/AI/01_deep_learning_theory/27-pytorch-model-save.html\"},{\"text\":\"28-pytorch-tensorboard\",\"link\":\"/AI/01_deep_learning_theory/28-pytorch-tensorboard.html\"},{\"text\":\"29-pytorch-graph-mode\",\"link\":\"/AI/01_deep_learning_theory/29-pytorch-graph-mode.html\"},{\"text\":\"30-1training-example-cv\",\"link\":\"/AI/01_deep_learning_theory/30-1training-example-cv.html\"},{\"text\":\"30-3main\",\"link\":\"/AI/01_deep_learning_theory/30-3main.html\"},{\"text\":\"31-1stable-diffusion\",\"link\":\"/AI/01_deep_learning_theory/31-1stable-diffusion.html\"},{\"text\":\"31-2SDXL\",\"link\":\"/AI/01_deep_learning_theory/31-2SDXL.html\"},{\"text\":\"31-3VAE\",\"link\":\"/AI/01_deep_learning_theory/31-3VAE.html\"},{\"text\":\"40-nlp-bert_ner\",\"link\":\"/AI/01_deep_learning_theory/40-nlp-bert_ner.html\"},{\"text\":\"41-nlp-t5_question-answering\",\"link\":\"/AI/01_deep_learning_theory/41-nlp-t5_question-answering.html\"},{\"text\":\"42-nlp-gpt\",\"link\":\"/AI/01_deep_learning_theory/42-nlp-gpt.html\"},{\"text\":\"43-scaling-law\",\"link\":\"/AI/01_deep_learning_theory/43-scaling-law.html\"},{\"text\":\"44-distribute-training\",\"link\":\"/AI/01_deep_learning_theory/44-distribute-training.html\"},{\"text\":\"45-LLM-History\",\"link\":\"/AI/01_deep_learning_theory/45-LLM-History.html\"},{\"text\":\"46-LLM-GPT-Extension\",\"link\":\"/AI/01_deep_learning_theory/46-LLM-GPT-Extension.html\"},{\"text\":\"46-nlp-llama\",\"link\":\"/AI/01_deep_learning_theory/46-nlp-llama.html\"},{\"text\":\"47-LLM-DeepSeek-Structure\",\"link\":\"/AI/01_deep_learning_theory/47-LLM-DeepSeek-Structure.html\"},{\"text\":\"47-nlp-deepseek\",\"link\":\"/AI/01_deep_learning_theory/47-nlp-deepseek.html\"}],\"collapsed\":true},{\"text\":\"02_distribute_training\",\"items\":[{\"text\":\"00_large-scale-model-trainning\",\"link\":\"/AI/02_distribute_training/00_large-scale-model-trainning.html\"},{\"text\":\"01_coding\",\"link\":\"/AI/02_distribute_training/01_coding.html\"},{\"text\":\"01_offload-and-recompute\",\"link\":\"/AI/02_distribute_training/01_offload-and-recompute.html\"},{\"text\":\"02_amp\",\"link\":\"/AI/02_distribute_training/02_amp.html\"},{\"text\":\"03_coding\",\"link\":\"/AI/02_distribute_training/03_coding.html\"},{\"text\":\"03_pytorch-DP\",\"link\":\"/AI/02_distribute_training/03_pytorch-DP.html\"},{\"text\":\"04_pytorch-DDP\",\"link\":\"/AI/02_distribute_training/04_pytorch-DDP.html\"},{\"text\":\"05_pytorch-DDP-IMPL\",\"link\":\"/AI/02_distribute_training/05_pytorch-DDP-IMPL.html\"},{\"text\":\"05_pytorch-DDP-IMPL_DDP_ORIGIN\",\"link\":\"/AI/02_distribute_training/05_pytorch-DDP-IMPL_DDP_ORIGIN.html\"},{\"text\":\"06_collective-comm\",\"link\":\"/AI/02_distribute_training/06_collective-comm.html\"},{\"text\":\"06_torchrun-process-group\",\"link\":\"/AI/02_distribute_training/06_torchrun-process-group.html\"},{\"text\":\"07_ZeRO-Optimizer\",\"link\":\"/AI/02_distribute_training/07_ZeRO-Optimizer.html\"},{\"text\":\"08_pytorch-ZeRO-1\",\"link\":\"/AI/02_distribute_training/08_pytorch-ZeRO-1.html\"},{\"text\":\"09_pytorch-FSDP-v1\",\"link\":\"/AI/02_distribute_training/09_pytorch-FSDP-v1.html\"},{\"text\":\"10_pytorch-FSDP-v2\",\"link\":\"/AI/02_distribute_training/10_pytorch-FSDP-v2.html\"},{\"text\":\"11_deepspeed-ZeRO-1-2-IMPL\",\"link\":\"/AI/02_distribute_training/11_deepspeed-ZeRO-1-2-IMPL.html\"},{\"text\":\"12_deepspeed-ZeRO-3-IMPL\",\"link\":\"/AI/02_distribute_training/12_deepspeed-ZeRO-3-IMPL.html\"},{\"text\":\"13_megatron-ZeRO-1-IMPL\",\"link\":\"/AI/02_distribute_training/13_megatron-ZeRO-1-IMPL.html\"},{\"text\":\"14_TP-Theory\",\"link\":\"/AI/02_distribute_training/14_TP-Theory.html\"},{\"text\":\"15_megatron-TP-IMPL\",\"link\":\"/AI/02_distribute_training/15_megatron-TP-IMPL.html\"},{\"text\":\"16_pytorch-TP-IMPL\",\"link\":\"/AI/02_distribute_training/16_pytorch-TP-IMPL.html\"},{\"text\":\"17_PP-Theory\",\"link\":\"/AI/02_distribute_training/17_PP-Theory.html\"},{\"text\":\"18_pytorch-PP-IMPL\",\"link\":\"/AI/02_distribute_training/18_pytorch-PP-IMPL.html\"},{\"text\":\"19_deepspeed-PP-IMPL\",\"link\":\"/AI/02_distribute_training/19_deepspeed-PP-IMPL.html\"},{\"text\":\"20_megatron-PP-IMPL\",\"link\":\"/AI/02_distribute_training/20_megatron-PP-IMPL.html\"},{\"text\":\"21_SP-Theory\",\"link\":\"/AI/02_distribute_training/21_SP-Theory.html\"},{\"text\":\"22_megatron-SP-IMPL\",\"link\":\"/AI/02_distribute_training/22_megatron-SP-IMPL.html\"},{\"text\":\"23_3D-Parallel-Theory\",\"link\":\"/AI/02_distribute_training/23_3D-Parallel-Theory.html\"},{\"text\":\"24_megatron-3D-Parallel-IMPL\",\"link\":\"/AI/02_distribute_training/24_megatron-3D-Parallel-IMPL.html\"},{\"text\":\"25_pytorch-3D-Parallel-IMPL\",\"link\":\"/AI/02_distribute_training/25_pytorch-3D-Parallel-IMPL.html\"},{\"text\":\"26_CP-Theory\",\"link\":\"/AI/02_distribute_training/26_CP-Theory.html\"},{\"text\":\"27_megatron-CP-IMPL\",\"link\":\"/AI/02_distribute_training/27_megatron-CP-IMPL.html\"},{\"text\":\"28_MOE-Theory\",\"link\":\"/AI/02_distribute_training/28_MOE-Theory.html\"},{\"text\":\"28_MOE-Theory_DeepSeekMOE\",\"link\":\"/AI/02_distribute_training/28_MOE-Theory_DeepSeekMOE.html\"},{\"text\":\"29_megatron-MOE-IMPL\",\"link\":\"/AI/02_distribute_training/29_megatron-MOE-IMPL.html\"},{\"text\":\"30_deepspeed-MOE-IMPL\",\"link\":\"/AI/02_distribute_training/30_deepspeed-MOE-IMPL.html\"},{\"text\":\"31_deepspeed-code-IMPL\",\"link\":\"/AI/02_distribute_training/31_deepspeed-code-IMPL.html\"},{\"text\":\"32_collective-operations\",\"link\":\"/AI/02_distribute_training/32_collective-operations.html\"},{\"text\":\"33_pytorch_distribute\",\"link\":\"/AI/02_distribute_training/33_pytorch_distribute.html\"}],\"collapsed\":true},{\"text\":\"03_Transformer\",\"items\":[{\"text\":\"01-Transformer的由来\",\"link\":\"/AI/03_Transformer/01-Transformer的由来.html\"},{\"text\":\"02-Transformer架构解读\",\"link\":\"/AI/03_Transformer/02-Transformer架构解读.html\"},{\"text\":\"03-Transformer源码构建\",\"link\":\"/AI/03_Transformer/03-Transformer源码构建.html\"}],\"collapsed\":true},{\"text\":\"04_some_notes\",\"items\":[{\"text\":\"00-DL_base_notes\",\"link\":\"/AI/04_some_notes/00-DL_base_notes.html\"},{\"text\":\"01-class_logs\",\"link\":\"/AI/04_some_notes/01-class_logs.html\"},{\"text\":\"02-some_detials\",\"link\":\"/AI/04_some_notes/02-some_detials.html\"},{\"text\":\"03-Bert理解\",\"link\":\"/AI/04_some_notes/03-Bert理解.html\"},{\"text\":\"04-个人补充内容\",\"link\":\"/AI/04_some_notes/04-个人补充内容.html\"},{\"text\":\"05-Review_DL\",\"link\":\"/AI/04_some_notes/05-Review_DL.html\"}],\"collapsed\":true}]}],\"/IT-learning/\":[{\"items\":[{\"text\":\"408\",\"items\":[{\"text\":\"OS-4.1 进程同步\",\"link\":\"/IT-learning/408/OS-4.1 进程同步.html\"},{\"text\":\"OS-4.4 信号量机制\",\"link\":\"/IT-learning/408/OS-4.4 信号量机制.html\"},{\"text\":\"OS-4.4 信号量机制pv操作之“可见”\",\"link\":\"/IT-learning/408/OS-4.4 信号量机制pv操作之“可见”.html\"}],\"collapsed\":true},{\"text\":\"Java\",\"items\":[{\"text\":\"01.java-se\",\"link\":\"/IT-learning/Java/01.java-se.html\"},{\"text\":\"02.sql\",\"link\":\"/IT-learning/Java/02.sql.html\"},{\"text\":\"03.java-web\",\"link\":\"/IT-learning/Java/03.java-web.html\"},{\"text\":\"05.MyBatis\",\"link\":\"/IT-learning/Java/05.MyBatis.html\"}],\"collapsed\":true},{\"text\":\"Linux\",\"items\":[{\"text\":\"01.Linux基础\",\"link\":\"/IT-learning/Linux/01.Linux基础.html\"},{\"text\":\"02.Shell\",\"link\":\"/IT-learning/Linux/02.Shell.html\"},{\"text\":\"03.MPI并行计算\",\"link\":\"/IT-learning/Linux/03.MPI并行计算.html\"},{\"text\":\"04.Docker\",\"link\":\"/IT-learning/Linux/04.Docker.html\"}],\"collapsed\":true},{\"text\":\"c++\",\"items\":[{\"text\":\"01_开发环境搭建与基础数据类型\",\"link\":\"/IT-learning/c++/01_开发环境搭建与基础数据类型.html\"},{\"text\":\"02_控制流语句与复合数据类型\",\"link\":\"/IT-learning/c++/02_控制流语句与复合数据类型.html\"},{\"text\":\"03_指针与引用\",\"link\":\"/IT-learning/c++/03_指针与引用.html\"},{\"text\":\"04_自定义数据类型与函数\",\"link\":\"/IT-learning/c++/04_自定义数据类型与函数.html\"},{\"text\":\"05_头文件与指针的算术运算\",\"link\":\"/IT-learning/c++/05_头文件与指针的算术运算.html\"},{\"text\":\"06_字符串、数组、指针与函数\",\"link\":\"/IT-learning/c++/06_字符串、数组、指针与函数.html\"},{\"text\":\"07_函数进阶与内存管理\",\"link\":\"/IT-learning/c++/07_函数进阶与内存管理.html\"},{\"text\":\"08_运算符优先级表\",\"link\":\"/IT-learning/c++/08_运算符优先级表.html\"},{\"text\":\"09_指针、内存管理和类的基础\",\"link\":\"/IT-learning/c++/09_指针、内存管理和类的基础.html\"},{\"text\":\"10_深入类和对象\",\"link\":\"/IT-learning/c++/10_深入类和对象.html\"},{\"text\":\"11_类的大小、继承与权限控制\",\"link\":\"/IT-learning/c++/11_类的大小、继承与权限控制.html\"},{\"text\":\"12_继承进阶\",\"link\":\"/IT-learning/c++/12_继承进阶.html\"},{\"text\":\"13_类型转换和多态与虚函数\",\"link\":\"/IT-learning/c++/13_类型转换和多态与虚函数.html\"},{\"text\":\"14_纯虚函数、抽象类、深浅拷贝及智能指针\",\"link\":\"/IT-learning/c++/14_纯虚函数、抽象类、深浅拷贝及智能指针.html\"},{\"text\":\"15_运算符重载与 String 类详解\",\"link\":\"/IT-learning/c++/15_运算符重载与 String 类详解.html\"},{\"text\":\"16_有序容器与无序容器\",\"link\":\"/IT-learning/c++/16_有序容器与无序容器.html\"},{\"text\":\"17_模板\",\"link\":\"/IT-learning/c++/17_模板.html\"},{\"text\":\"18_迭代器与其应用\",\"link\":\"/IT-learning/c++/18_迭代器与其应用.html\"},{\"text\":\"19_C++ 标准库常用算法\",\"link\":\"/IT-learning/c++/19_C++ 标准库常用算法.html\"},{\"text\":\"20_C++ 异常处理 - 第19次课\",\"link\":\"/IT-learning/c++/20_C++ 异常处理 - 第19次课.html\"},{\"text\":\"21_友元及友元相关内容\",\"link\":\"/IT-learning/c++/21_友元及友元相关内容.html\"},{\"text\":\"22_C++ IO 流详解-feadbc607d7f\",\"link\":\"/IT-learning/c++/22_C++ IO 流详解-feadbc607d7f.html\"},{\"text\":\"23_C++ IO 流详解\",\"link\":\"/IT-learning/c++/23_C++ IO 流详解.html\"},{\"text\":\"24_位运算符总结\",\"link\":\"/IT-learning/c++/24_位运算符总结.html\"},{\"text\":\"25_C++三种继承方式\",\"link\":\"/IT-learning/c++/25_C++三种继承方式.html\"},{\"text\":\"26_C++11 高级特性\",\"link\":\"/IT-learning/c++/26_C++11 高级特性.html\"},{\"text\":\"27_C++14 新特性\",\"link\":\"/IT-learning/c++/27_C++14 新特性.html\"},{\"text\":\"28_C++17 新特性\",\"link\":\"/IT-learning/c++/28_C++17 新特性.html\"},{\"text\":\"29_多文件和 Makefile工程管理\",\"link\":\"/IT-learning/c++/29_多文件和 Makefile工程管理.html\"},{\"text\":\"30_C++大型项目CMake工程管理\",\"link\":\"/IT-learning/c++/30_C++大型项目CMake工程管理.html\"},{\"text\":\"31_C++ 主要就业方向与技术能力分析报告\",\"link\":\"/IT-learning/c++/31_C++ 主要就业方向与技术能力分析报告.html\"},{\"text\":\"32_C++ 基础知识回顾\",\"link\":\"/IT-learning/c++/32_C++ 基础知识回顾.html\"}],\"collapsed\":true}]}],\"/Job_Interview/\":[{\"items\":[{\"text\":\"Algorithm_post\",\"items\":[{\"text\":\"model_framework\",\"items\":[{\"text\":\"001_Transformer\",\"link\":\"/Job_Interview/Algorithm_post/model_framework/001_Transformer.html\"},{\"text\":\"002_Normalization\",\"link\":\"/Job_Interview/Algorithm_post/model_framework/002_Normalization.html\"},{\"text\":\"003_Position-Embedding\",\"link\":\"/Job_Interview/Algorithm_post/model_framework/003_Position-Embedding.html\"},{\"text\":\"004_Activation\",\"link\":\"/Job_Interview/Algorithm_post/model_framework/004_Activation.html\"},{\"text\":\"005_Transformer-Other\",\"link\":\"/Job_Interview/Algorithm_post/model_framework/005_Transformer-Other.html\"},{\"text\":\"006_LLM-Frame\",\"link\":\"/Job_Interview/Algorithm_post/model_framework/006_LLM-Frame.html\"},{\"text\":\"007_MoE\",\"link\":\"/Job_Interview/Algorithm_post/model_framework/007_MoE.html\"},{\"text\":\"基础内容面试点\",\"link\":\"/Job_Interview/Algorithm_post/model_framework/基础内容面试点.html\"},{\"text\":\"手撕内容\",\"link\":\"/Job_Interview/Algorithm_post/model_framework/手撕内容.html\"}],\"collapsed\":true},{\"text\":\"p0-00_场景题\",\"link\":\"/Job_Interview/Algorithm_post/p0-00_场景题.html\"},{\"text\":\"p1-01_分词器\",\"link\":\"/Job_Interview/Algorithm_post/p1-01_分词器.html\"},{\"text\":\"p1-02_词向量\",\"link\":\"/Job_Interview/Algorithm_post/p1-02_词向量.html\"},{\"text\":\"p2-01_注意力机制\",\"link\":\"/Job_Interview/Algorithm_post/p2-01_注意力机制.html\"},{\"text\":\"p2-02_位置编码\",\"link\":\"/Job_Interview/Algorithm_post/p2-02_位置编码.html\"},{\"text\":\"p2-03_归一化\",\"link\":\"/Job_Interview/Algorithm_post/p2-03_归一化.html\"},{\"text\":\"p2-04_残差连接\",\"link\":\"/Job_Interview/Algorithm_post/p2-04_残差连接.html\"},{\"text\":\"p3-01_多层感知机\",\"link\":\"/Job_Interview/Algorithm_post/p3-01_多层感知机.html\"},{\"text\":\"p3-02_激活函数\",\"link\":\"/Job_Interview/Algorithm_post/p3-02_激活函数.html\"},{\"text\":\"p3-03_损失函数\",\"link\":\"/Job_Interview/Algorithm_post/p3-03_损失函数.html\"},{\"text\":\"p4-01_预训练技术\",\"link\":\"/Job_Interview/Algorithm_post/p4-01_预训练技术.html\"},{\"text\":\"p5-01_后训练技术\",\"link\":\"/Job_Interview/Algorithm_post/p5-01_后训练技术.html\"},{\"text\":\"p6-01_推理优化\",\"link\":\"/Job_Interview/Algorithm_post/p6-01_推理优化.html\"},{\"text\":\"p7-01_大模型架构\",\"link\":\"/Job_Interview/Algorithm_post/p7-01_大模型架构.html\"},{\"text\":\"p8_01_大模型应用\",\"link\":\"/Job_Interview/Algorithm_post/p8_01_大模型应用.html\"},{\"text\":\"p9-01_torch的数据\",\"link\":\"/Job_Interview/Algorithm_post/p9-01_torch的数据.html\"}],\"collapsed\":true}]}],\"/my_think/\":[{\"items\":[{\"text\":\"01_不同商家的视野\",\"link\":\"/my_think/01_不同商家的视野.html\"},{\"text\":\"02_学而篇\",\"link\":\"/my_think/02_学而篇.html\"},{\"text\":\"03_重温士兵突击\",\"link\":\"/my_think/03_重温士兵突击.html\"},{\"text\":\"04_你很好，慢慢来\",\"link\":\"/my_think/04_你很好，慢慢来.html\"}]}],\"/question_list/\":[{\"items\":[{\"text\":\"doccano账户管理\",\"link\":\"/question_list/doccano账户管理.html\"},{\"text\":\"专英翻转课堂—PyTorch\",\"link\":\"/question_list/专英翻转课堂—PyTorch.html\"},{\"text\":\"虚拟机网络问题\",\"link\":\"/question_list/虚拟机网络问题.html\"}]}],\"/update/\":[{\"items\":[{\"text\":\"update_log\",\"link\":\"/update/update_log.html\"}]}]}},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>