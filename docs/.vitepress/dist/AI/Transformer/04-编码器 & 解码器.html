<!DOCTYPE html>
<html lang="zh-CN" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>探秘Transformer系列之（4）--- 编码器 &amp; 解码器 | 码医森</title>
    <meta name="description" content="计算机知识的学习站点">
    <meta name="generator" content="VitePress v1.3.4">
    <link rel="preload stylesheet" href="/assets/style.CflK-Lwn.css" as="style">
    
    <script type="module" src="/assets/app.CP5QrAO4.js"></script>
    <link rel="preload" href="/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/assets/chunks/theme.DC78O2cf.js">
    <link rel="modulepreload" href="/assets/chunks/framework.DA-Pb-tg.js">
    <link rel="modulepreload" href="/assets/AI_Transformer_04-编码器 _ 解码器.md.zSoU6yVz.lean.js">
    <link rel="icon" type="image/svg+xml" href="/imgs/home-page-logo.svg">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css" crossorigin="">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-5d98c3a5><!--[--><!--]--><!--[--><span tabindex="-1" data-v-0f60ec36></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-0f60ec36> Skip to content </a><!--]--><!----><header class="VPNav" data-v-5d98c3a5 data-v-ae24b3ad><div class="VPNavBar" data-v-ae24b3ad data-v-6aa21345><div class="wrapper" data-v-6aa21345><div class="container" data-v-6aa21345><div class="title" data-v-6aa21345><div class="VPNavBarTitle has-sidebar" data-v-6aa21345 data-v-ab179fa1><a class="title" href="/" data-v-ab179fa1><!--[--><!--]--><!--[--><img class="VPImage logo" src="/imgs/home-page-logo.svg" alt data-v-8426fc1a><!--]--><span data-v-ab179fa1>CoderEthan学习站</span><!--[--><!--]--></a></div></div><div class="content" data-v-6aa21345><div class="content-body" data-v-6aa21345><!--[--><!--]--><div class="VPNavBarSearch search" data-v-6aa21345><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-6aa21345 data-v-dc692963><span id="main-nav-aria-label" class="visually-hidden" data-v-dc692963> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/" tabindex="0" data-v-dc692963 data-v-9c663999><!--[--><span data-v-9c663999>主页</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/guide/" tabindex="0" data-v-dc692963 data-v-9c663999><!--[--><span data-v-9c663999>博客指南</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/生活与算法/" tabindex="0" data-v-dc692963 data-v-9c663999><!--[--><span data-v-9c663999>生活与算法</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-dc692963 data-v-b6c34ac9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-b6c34ac9><span class="text" data-v-b6c34ac9><!----><span data-v-b6c34ac9>AI</span><span class="vpi-chevron-down text-icon" data-v-b6c34ac9></span></span></button><div class="menu" data-v-b6c34ac9><div class="VPMenu" data-v-b6c34ac9 data-v-b98bc113><div class="items" data-v-b98bc113><!--[--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/AI/deep_learning_theory/" data-v-43f1e123><!--[-->DL基础理论<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/AI/Transformer/" data-v-43f1e123><!--[-->Transformer系列<!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-dc692963 data-v-b6c34ac9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-b6c34ac9><span class="text" data-v-b6c34ac9><!----><span data-v-b6c34ac9>计算机学科内容</span><span class="vpi-chevron-down text-icon" data-v-b6c34ac9></span></span></button><div class="menu" data-v-b6c34ac9><div class="VPMenu" data-v-b6c34ac9 data-v-b98bc113><div class="items" data-v-b98bc113><!--[--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/IT-learning/408知识/" data-v-43f1e123><!--[-->计算机知识<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/IT-learning/c++基础/" data-v-43f1e123><!--[-->C++基础<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/IT-learning/Java/" data-v-43f1e123><!--[-->Java后端<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/IT-learning/Linux/" data-v-43f1e123><!--[-->Linux技术<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/IT-learning/计算机图形学/" data-v-43f1e123><!--[-->计算机图形学<!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-dc692963 data-v-b6c34ac9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-b6c34ac9><span class="text" data-v-b6c34ac9><!----><span data-v-b6c34ac9>自我提升</span><span class="vpi-chevron-down text-icon" data-v-b6c34ac9></span></span></button><div class="menu" data-v-b6c34ac9><div class="VPMenu" data-v-b6c34ac9 data-v-b98bc113><div class="items" data-v-b98bc113><!--[--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/improve/阅读/" data-v-43f1e123><!--[-->阅读<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/improve/金融投资/" data-v-43f1e123><!--[-->金融投资<!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-dc692963 data-v-b6c34ac9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-b6c34ac9><span class="text" data-v-b6c34ac9><!----><span data-v-b6c34ac9>求职面试</span><span class="vpi-chevron-down text-icon" data-v-b6c34ac9></span></span></button><div class="menu" data-v-b6c34ac9><div class="VPMenu" data-v-b6c34ac9 data-v-b98bc113><div class="items" data-v-b98bc113><!--[--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/面试求职/Java面经/" data-v-43f1e123><!--[-->Java面经<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/面试求职/场景问题/" data-v-43f1e123><!--[-->场景问题<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/面试求职/经验分享/" data-v-43f1e123><!--[-->经验分享<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/面试求职/算法岗/" data-v-43f1e123><!--[-->算法岗<!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-dc692963 data-v-b6c34ac9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-b6c34ac9><span class="text" data-v-b6c34ac9><!----><span data-v-b6c34ac9>其他维护</span><span class="vpi-chevron-down text-icon" data-v-b6c34ac9></span></span></button><div class="menu" data-v-b6c34ac9><div class="VPMenu" data-v-b6c34ac9 data-v-b98bc113><div class="items" data-v-b98bc113><!--[--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/update/%E6%9B%B4%E6%96%B0%E6%97%A5%E5%BF%97.html" data-v-43f1e123><!--[-->站点更新<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/技术问题清单/" data-v-43f1e123><!--[-->问题清单<!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-dc692963 data-v-b6c34ac9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-b6c34ac9><span class="text" data-v-b6c34ac9><!----><span data-v-b6c34ac9>感悟和日常</span><span class="vpi-chevron-down text-icon" data-v-b6c34ac9></span></span></button><div class="menu" data-v-b6c34ac9><div class="VPMenu" data-v-b6c34ac9 data-v-b98bc113><div class="items" data-v-b98bc113><!--[--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/我的感悟/" data-v-43f1e123><!--[-->站长感悟<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link vp-external-link-icon" href="https://EthanLiu6.github.io" target="_blank" rel="noreferrer" data-v-43f1e123><!--[-->站长旧版博客<!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-6aa21345 data-v-6c893767><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-6c893767 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-6aa21345 data-v-0394ad82 data-v-7bc22406><!--[--><a class="VPSocialLink no-icon" href="https://github.com/ethanliu6/" aria-label target="_blank" rel="noopener" data-v-7bc22406 data-v-eee4e7cb><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path fill="#6841d2" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3 .3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5 .3-6.2 2.3zm44.2-1.7c-2.9 .7-4.9 2.6-4.6 4.9 .3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3 .7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3 .3 2.9 2.3 3.9 1.6 1 3.6 .7 4.3-.7 .7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3 .7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3 .7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></a><a class="VPSocialLink no-icon" href="https://space.bilibili.com/1327099977/" aria-label target="_blank" rel="noopener" data-v-7bc22406 data-v-eee4e7cb><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path fill="#6841d2" d="M488.6 104.1C505.3 122.2 513 143.8 511.9 169.8V372.2C511.5 398.6 502.7 420.3 485.4 437.3C468.2 454.3 446.3 463.2 419.9 464H92C65.6 463.2 43.8 454.2 26.7 436.8C9.7 419.4 .8 396.5 0 368.2V169.8C.8 143.8 9.7 122.2 26.7 104.1C43.8 87.8 65.6 78.8 92 78H121.4L96.1 52.2C90.3 46.5 87.4 39.2 87.4 30.4C87.4 21.6 90.3 14.3 96.1 8.6C101.8 2.9 109.1 0 117.9 0C126.7 0 134 2.9 139.8 8.6L213.1 78H301.1L375.6 8.6C381.7 2.9 389.2 0 398 0C406.8 0 414.1 2.9 419.9 8.6C425.6 14.3 428.5 21.6 428.5 30.4C428.5 39.2 425.6 46.5 419.9 52.2L394.6 78L423.9 78C450.3 78.8 471.9 87.8 488.6 104.1H488.6zM449.8 173.8C449.4 164.2 446.1 156.4 439.1 150.3C433.9 144.2 425.1 140.9 416.4 140.5H96.1C86.5 140.9 78.6 144.2 72.5 150.3C66.3 156.4 63.1 164.2 62.7 173.8V368.2C62.7 377.4 66 385.2 72.5 391.7C79 398.2 86.9 401.5 96.1 401.5H416.4C425.6 401.5 433.4 398.2 439.7 391.7C446 385.2 449.4 377.4 449.8 368.2L449.8 173.8zM185.5 216.5C191.8 222.8 195.2 230.6 195.6 239.7V273C195.2 282.2 191.9 289.9 185.8 296.2C179.6 302.5 171.8 305.7 162.2 305.7C152.6 305.7 144.7 302.5 138.6 296.2C132.5 289.9 129.2 282.2 128.8 273V239.7C129.2 230.6 132.6 222.8 138.9 216.5C145.2 210.2 152.1 206.9 162.2 206.5C171.4 206.9 179.2 210.2 185.5 216.5H185.5zM377 216.5C383.3 222.8 386.7 230.6 387.1 239.7V273C386.7 282.2 383.4 289.9 377.3 296.2C371.2 302.5 363.3 305.7 353.7 305.7C344.1 305.7 336.3 302.5 330.1 296.2C323.1 289.9 320.7 282.2 320.4 273V239.7C320.7 230.6 324.1 222.8 330.4 216.5C336.7 210.2 344.5 206.9 353.7 206.5C362.9 206.9 370.7 210.2 377 216.5H377z"/></svg></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-6aa21345 data-v-bb2aa2f0 data-v-b6c34ac9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-b6c34ac9><span class="vpi-more-horizontal icon" data-v-b6c34ac9></span></button><div class="menu" data-v-b6c34ac9><div class="VPMenu" data-v-b6c34ac9 data-v-b98bc113><!----><!--[--><!--[--><!----><div class="group" data-v-bb2aa2f0><div class="item appearance" data-v-bb2aa2f0><p class="label" data-v-bb2aa2f0>深色模式</p><div class="appearance-action" data-v-bb2aa2f0><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-bb2aa2f0 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div></div></div><div class="group" data-v-bb2aa2f0><div class="item social-links" data-v-bb2aa2f0><div class="VPSocialLinks social-links-list" data-v-bb2aa2f0 data-v-7bc22406><!--[--><a class="VPSocialLink no-icon" href="https://github.com/ethanliu6/" aria-label target="_blank" rel="noopener" data-v-7bc22406 data-v-eee4e7cb><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path fill="#6841d2" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3 .3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5 .3-6.2 2.3zm44.2-1.7c-2.9 .7-4.9 2.6-4.6 4.9 .3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3 .7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3 .3 2.9 2.3 3.9 1.6 1 3.6 .7 4.3-.7 .7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3 .7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3 .7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></a><a class="VPSocialLink no-icon" href="https://space.bilibili.com/1327099977/" aria-label target="_blank" rel="noopener" data-v-7bc22406 data-v-eee4e7cb><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path fill="#6841d2" d="M488.6 104.1C505.3 122.2 513 143.8 511.9 169.8V372.2C511.5 398.6 502.7 420.3 485.4 437.3C468.2 454.3 446.3 463.2 419.9 464H92C65.6 463.2 43.8 454.2 26.7 436.8C9.7 419.4 .8 396.5 0 368.2V169.8C.8 143.8 9.7 122.2 26.7 104.1C43.8 87.8 65.6 78.8 92 78H121.4L96.1 52.2C90.3 46.5 87.4 39.2 87.4 30.4C87.4 21.6 90.3 14.3 96.1 8.6C101.8 2.9 109.1 0 117.9 0C126.7 0 134 2.9 139.8 8.6L213.1 78H301.1L375.6 8.6C381.7 2.9 389.2 0 398 0C406.8 0 414.1 2.9 419.9 8.6C425.6 14.3 428.5 21.6 428.5 30.4C428.5 39.2 425.6 46.5 419.9 52.2L394.6 78L423.9 78C450.3 78.8 471.9 87.8 488.6 104.1H488.6zM449.8 173.8C449.4 164.2 446.1 156.4 439.1 150.3C433.9 144.2 425.1 140.9 416.4 140.5H96.1C86.5 140.9 78.6 144.2 72.5 150.3C66.3 156.4 63.1 164.2 62.7 173.8V368.2C62.7 377.4 66 385.2 72.5 391.7C79 398.2 86.9 401.5 96.1 401.5H416.4C425.6 401.5 433.4 398.2 439.7 391.7C446 385.2 449.4 377.4 449.8 368.2L449.8 173.8zM185.5 216.5C191.8 222.8 195.2 230.6 195.6 239.7V273C195.2 282.2 191.9 289.9 185.8 296.2C179.6 302.5 171.8 305.7 162.2 305.7C152.6 305.7 144.7 302.5 138.6 296.2C132.5 289.9 129.2 282.2 128.8 273V239.7C129.2 230.6 132.6 222.8 138.9 216.5C145.2 210.2 152.1 206.9 162.2 206.5C171.4 206.9 179.2 210.2 185.5 216.5H185.5zM377 216.5C383.3 222.8 386.7 230.6 387.1 239.7V273C386.7 282.2 383.4 289.9 377.3 296.2C371.2 302.5 363.3 305.7 353.7 305.7C344.1 305.7 336.3 302.5 330.1 296.2C323.1 289.9 320.7 282.2 320.4 273V239.7C320.7 230.6 324.1 222.8 330.4 216.5C336.7 210.2 344.5 206.9 353.7 206.5C362.9 206.9 370.7 210.2 377 216.5H377z"/></svg></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-6aa21345 data-v-e5dd9c1c><span class="container" data-v-e5dd9c1c><span class="top" data-v-e5dd9c1c></span><span class="middle" data-v-e5dd9c1c></span><span class="bottom" data-v-e5dd9c1c></span></span></button></div></div></div></div><div class="divider" data-v-6aa21345><div class="divider-line" data-v-6aa21345></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-5d98c3a5 data-v-a6f0e41e><div class="container" data-v-a6f0e41e><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-a6f0e41e><span class="vpi-align-left menu-icon" data-v-a6f0e41e></span><span class="menu-text" data-v-a6f0e41e>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-a6f0e41e data-v-17a5e62e><button data-v-17a5e62e>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-5d98c3a5 data-v-319d5ca6><div class="curtain" data-v-319d5ca6></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-319d5ca6><span class="visually-hidden" id="sidebar-aria-label" data-v-319d5ca6> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-c40bc020><section class="VPSidebarItem level-0 has-active" data-v-c40bc020 data-v-b7550ba0><!----><div class="items" data-v-b7550ba0><!--[--><section class="VPSidebarItem level-1 collapsible has-active" data-v-b7550ba0 data-v-b7550ba0><div class="item" role="button" tabindex="0" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><h3 class="text" data-v-b7550ba0>Transformer</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-b7550ba0><span class="vpi-chevron-right caret-icon" data-v-b7550ba0></span></div></div><div class="items" data-v-b7550ba0><!--[--><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/Transformer/01-注意力机制.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>01-注意力机制</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/Transformer/02-总体架构.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>02-总体架构</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/Transformer/03-数据处理.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>03-数据处理</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/Transformer/04-编码器 &amp; 解码器.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>04-编码器 & 解码器</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/Transformer/05-训练&amp;推理.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>05-训练&推理</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/Transformer/06-token.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>06-token</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/Transformer/07-embedding.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>07-embedding</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/Transformer/08-位置编码.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>08-位置编码</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/Transformer/09-位置编码分类.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>09-位置编码分类</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/Transformer/10-自注意力.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>10-自注意力</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/Transformer/11-掩码.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>11-掩码</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/Transformer/12-多头自注意力.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>12-多头自注意力</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/Transformer/13- FFN.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>13- FFN</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/Transformer/14-残差网络和归一化.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>14-残差网络和归一化</p><!--]--></a><!----></div><!----></div><!--]--></div></section><section class="VPSidebarItem level-1 collapsible" data-v-b7550ba0 data-v-b7550ba0><div class="item" role="button" tabindex="0" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><h3 class="text" data-v-b7550ba0>deep_learning_theory</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-b7550ba0><span class="vpi-chevron-right caret-icon" data-v-b7550ba0></span></div></div><div class="items" data-v-b7550ba0><!--[--><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/00-DL_Base_Notes.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>00-DL_Base_Notes</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/01-feedforward_network.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>01-feedforward_network</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/02-back_propagation.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>02-back_propagation</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/03-bp_example_demo.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>03-bp_example_demo</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/04-convolution_neural_network.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>04-convolution_neural_network</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/05-deep_learning_model.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>05-deep_learning_model</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/06-pytorch_install.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>06-pytorch_install</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/07-operators.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>07-operators</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/08-activation_functions.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>08-activation_functions</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/09-recurrent_neural_network.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>09-recurrent_neural_network</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/10-seq2seq.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>10-seq2seq</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/11-1attentions.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>11-1attentions</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/11-2attention-extension.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>11-2attention-extension</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/12-weight-initialization.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>12-weight-initialization</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/13-optimizers.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>13-optimizers</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/14-regularization.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>14-regularization</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/15-deep-learning-tuning-guide.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>15-deep-learning-tuning-guide</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/20-pytorch-tensor.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>20-pytorch-tensor</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/21-pytorch-autograd.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>21-pytorch-autograd</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/22-pytorch-module.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>22-pytorch-module</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/23-1training-example-1.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>23-1training-example-1</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/23-2decoder.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>23-2decoder</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/23-3encoder.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>23-3encoder</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/23-4transformer.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>23-4transformer</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/24-pytorch-optimizer.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>24-pytorch-optimizer</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/25-pytorch-lr-scheduler.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>25-pytorch-lr-scheduler</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/26-pytorch-dataloader.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>26-pytorch-dataloader</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/27-pytorch-model-save.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>27-pytorch-model-save</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/28-pytorch-tensorboard.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>28-pytorch-tensorboard</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/29-pytorch-graph-mode.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>29-pytorch-graph-mode</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/30-3main.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>30-3main</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/30-training-example-2.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>30-training-example-2</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/40-ner.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>40-ner</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/41-question-answering.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>41-question-answering</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/42-1stable-diffusion.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>42-1stable-diffusion</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/42-2SDXL.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>42-2SDXL</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/42-3VAE.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>42-3VAE</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/44-scaling-law.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>44-scaling-law</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/45-distribute-training.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>45-distribute-training</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/46-nlp-llama.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>46-nlp-llama</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/47-nlp-deepseek.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>47-nlp-deepseek</p><!--]--></a><!----></div><!----></div><!--]--></div></section><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-5d98c3a5 data-v-1428d186><div class="VPDoc has-sidebar has-aside" data-v-1428d186 data-v-39a288b8><!--[--><!--]--><div class="container" data-v-39a288b8><div class="aside" data-v-39a288b8><div class="aside-curtain" data-v-39a288b8></div><div class="aside-container" data-v-39a288b8><div class="aside-content" data-v-39a288b8><div class="VPDocAside" data-v-39a288b8 data-v-3f215769><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-3f215769 data-v-a5bbad30><div class="content" data-v-a5bbad30><div class="outline-marker" data-v-a5bbad30></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-a5bbad30>本文目录</div><ul class="VPDocOutlineItem root" data-v-a5bbad30 data-v-b933a997><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-3f215769></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-39a288b8><div class="content-container" data-v-39a288b8><!--[--><!--]--><main class="main" data-v-39a288b8><div style="position:relative;" class="vp-doc _AI_Transformer_04-%E7%BC%96%E7%A0%81%E5%99%A8%20&amp;%20%E8%A7%A3%E7%A0%81%E5%99%A8" data-v-39a288b8><div><h1 id="探秘transformer系列之-4-编码器-解码器" tabindex="-1">探秘Transformer系列之（4）--- 编码器 &amp; 解码器 <a class="header-anchor" href="#探秘transformer系列之-4-编码器-解码器" aria-label="Permalink to &quot;探秘Transformer系列之（4）--- 编码器 &amp; 解码器&quot;">​</a></h1><h2 id="_0x00-摘要" tabindex="-1">0x00 摘要 <a class="header-anchor" href="#_0x00-摘要" aria-label="Permalink to &quot;0x00 摘要&quot;">​</a></h2><p>对于机器翻译，Transformer完整的前向计算过程如下图所示（与总体架构那章的流程图相比较，此处重点突出了编码器和解码器内部以及之间的关联）。图左侧是编码器栈，右侧是解码器栈，这两个构成了 Transformer 的“躯干”。具体流程如下。</p><ul><li><p>将输入序列转换为嵌入矩阵，再加上位置编码（表示每个单词的位置）之后构成word embedding，然后把word embedding输入解码器。此步骤对应下图的标号1。</p></li><li><p>编码器接收输入序列的word embedding并生成相关隐向量。编码器是并行处理，因此只会进行一次前向传播。编码器栈内部通过自注意力机制完成了对源序列的特征的提取，得到了源序列内部元素之间的彼此相关性，保留了高维度潜藏的逻辑信息。或者说，自注意力负责基于其全部输入向量来建模每个输出均可以借鉴的隐向量。此步骤对应下图的标号2。</p></li><li><p>与编码器不同，解码器会循环执行，直到输出所有结果。解码器以标记和编码器的输出作为起点，生成输出序列的下一个 token。就像我们对编码器的输入所做的那样，我们会生成嵌入并添加位置编码来传给那些解码器。此步骤对应下图的标号3。</p><ul><li>解码器栈内部通过掩码自注意力机制完成了对目标序列特征的提取，得到了目标序列内部元素之间的彼此相关性，保留了高维度潜藏的逻辑信息。或者说，掩码自注意力负责基于解码器的输入向量来建模每个解码器的输出向量。</li><li>自注意力机制只能提炼解构本序列的关联性特征，因此编码器栈和解码器栈之间通过交叉注意力在编码器和解码器之间传递信息，完成彼此的联系，确保了对特征进行非对称的压缩和还原。或者说，交叉注意力层则负责基于编码器的所有输出隐向量来进一步建模每个解码器的输出向量。</li></ul></li><li><p>使用一个线性层来生成 logits。此步骤对应下图的标号4。</p></li><li><p>应用一个 softmax 层来生成概率，最终依据某种策略输出一个最可能的单词。此步骤对应下图的标号5。</p></li><li><p>解码器使用解码器的新输出token和先前生成的 token 作为输入序列来生成输出序列的下一个 token。此步骤对应下图的标号6。</p></li><li><p>重复步骤 3-6循环（对于步骤3，每次输入是变化的）来对下一个时刻的输出进行解码预测，直到生成 EOS 标记表示解码结束或者达到指定长度后停止。</p></li></ul><p>这个解码过程其实就是标准的seq2seq流程。因此，注意力机制是Transformer 的“灵魂”，Transformer 实际上是通过三重注意力机制建立起了序列内部以及序列之间的全局联系。</p><p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250220203206865-113142576.jpg" alt=""></p><p>本章依然用机器翻译来分析说明。</p><h2 id="_0x01-编码器" tabindex="-1">0x01 编码器 <a class="header-anchor" href="#_0x01-编码器" aria-label="Permalink to &quot;0x01 编码器&quot;">​</a></h2><p>编码器的输入是word embedding序列，这是一个低阶语义向量序列。编码器的目标就是对这个低阶语义向量序列进行特征提取、转换，并且最终映射到一个新的语义空间，从而得到一个高阶语义向量序列。因为编码器使用了注意力机制，所以这个高阶语义向量序列具有更加丰富和完整的语义，也是上下文感知的。这个高阶语义向量序列将被后续的解码器使用并生成最终输出序列。而且，编码器是为每一个待预测词都生成一个上下文向量。为何要在每一步针对每一个待预测词都生成一个新的上下文向量？我们可以通过例子来解答。</p><ul><li>中文：我吃了一个苹果，然后吃了一个香蕉。</li><li>英文：I ate an apple and then a banana。</li></ul><p>如果逐字翻译，翻译到”我吃了一个“时候，得到的英文应该是”I ate a“？还是”I ate an”？这就需要依据后面的“苹果”来判断。所以，翻译“苹果”之后，需要依据“苹果”才能确定是&quot;a&quot;还是“an”，进而更新之前”一个“这个词对于的上下文向量。</p><h3 id="_1-1-结构" tabindex="-1">1.1 结构 <a class="header-anchor" href="#_1-1-结构" aria-label="Permalink to &quot;1.1 结构&quot;">​</a></h3><p>Transformer的编码器模块结构如下图紫色方框所示。编码器是由多个相同的EncoderLayer（编码器层，即下图的黄色部分）堆叠而成的。Transformer论文原图中只画了一个EncoderLayer，然后旁边写了个 Nx，这个Nx就表示编码器是由几个EncoderLayer堆叠而成。论文中设置N = 6。</p><p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250220203220253-2083029478.jpg" alt=""></p><p>每个EncoderLayer由以下模块组成：</p><ul><li>多头自注意力机制（Multi-Head Self-Attention/MHA），其特点如下： <ul><li>MHA是对输入序列自身进行的注意力计算，用于获取输入序列中不同单词之间的相关性。</li><li>编码器的输入被转换（即输入经过嵌入层和位置编码后，分别与Query、Key 和 Value 的参数矩阵相乘）后，作为Query、Key 和 Value这三个参数传递给MHA，即QKV均来自一个序列。</li><li>MHA可以使网络在进行预测时对输入句子的不同位置的分配不同的注意力。多头注意力意味着模型有多组不同的注意力参数，每组都会输出一个依据注意力权重来加权求和的向量，这些向量会被合并成最终的向量进行输出。</li></ul></li><li>第一个残差连接（Residual Connection）。残差连接使注意力机制中产生的新数据和最开始输入的原始数据合并在一起，这个合并其实就是简单的加法，这样可以避免深度神经网络中的梯度消失问题。残差连接对应上图中的“Add”。</li><li>第一个Layer Normalization（层归一化）。该模块可以让数据更稳定，便于后续处理。具体对应图上的“Norm”，其会对层的输入进行归一化处理，使得其均值为0，方差为1。</li><li>FFN（Feed-Forward Networks/前馈神经网络）。这个模块由两个线性变换组成，中间夹有一个ReLU激活函数。它对每个位置的词向量独立地进行变换。这个层对经过注意力处理后的向量进一步进行处理和优化，产生一个新的表示。FFN输出是一个更具抽象性、更丰富的上下文表示，可以增加模型的非线性表示能力。</li><li>第二个残差连接，作用同第一个残差连接。</li><li>第二个Layer Normalization，作用同第一个Layer Normalization。</li></ul><p>或者简化来看，编码器模块由一系列相同层构成，每个层分为两个重要子模块：MHA和FFN。每个重要子模块周围有一个残差连接，并且每个重要子模块的输出都会经过Layer Normalization。</p><h3 id="_1-2-输入和输出" tabindex="-1">1.2 输入和输出 <a class="header-anchor" href="#_1-2-输入和输出" aria-label="Permalink to &quot;1.2 输入和输出&quot;">​</a></h3><p>因为编码器是层叠的栈结构，因此不同EncoderLayer的输入输出不尽相同。</p><p>编码器栈第一个EncoderLayer的输入是单词的Embedding加上位置编码，即图上的Input Embedding和Positional Encoding相加之后的结果，我们称之为Word Embedding（词向量）。加上位置编码的原因是由于Transformer模型没有循环或卷积操作，为了让模型能够利用词的顺序信息，需要在输入嵌入层中加入位置编码。因为多个EncoderLayer是串联在一起，所以栈的其它EncoderLayer的输入是上一个EncoderLayer的输出。</p><p>经过多层计算之后，最后一个EncoderLayer的输出就是编码器的输出（编码器和解码器之间的隐状态）。该输出会送入解码器堆栈中的每一个DecoderLayer中。通常在代码实现中把这个输出叫做memory。编码器的输出就是对原始输入的高阶抽象表达，是在更高维的向量空间中的表示。</p><p>输入的维度一般是[batch_size, seq_len, embedding_dim]。为了方便残差连接，每一个EncoderLayer输出的矩阵维度与输入完全一致。其形状也是[batch_size, seq_len, embedding_dim]。这样的设计也确保了模型在多个编码器层之间能够有效地传递和处理信息，同时也为更复杂的计算和解码阶段做好了准备。</p><h3 id="_1-3-流程" tabindex="-1">1.3 流程 <a class="header-anchor" href="#_1-3-流程" aria-label="Permalink to &quot;1.3 流程&quot;">​</a></h3><p>我们继续细化编码流程。一个Transformer编码块做的事情如下图所示。图中分为两部分，既包括编码器，也包括其输入（为了更好的说明，把处理输入部分也涵盖进来）。</p><ul><li>上面部分（ #2）就是Encoder模块里的一个独立的EncoderLayer。这个模块想要做的事情就是想把输入X转换为另外一个向量R，这两个向量的维度是一样的。然后向量R作为上一层的输入，会一层层往上传。</li><li>下面部分（#1）的是两个单词的embedding处理部分，即EncoderLayer的输入处理部分。</li></ul><p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250220203232174-1107324218.jpg" alt=""></p><p>我们对上图流程分析如下。</p><ul><li>第一步会用token embedding和位置编码来生成word embedding，对应图上圆形标号1。</li><li>第二步是自注意力机制，对应图上圆形标号2，具体操作是softmax(QKT)Vsoftmax(QKT)Vsoftmax(QK^T)V。所有的输入向量共同参与了这个过程，也就是说，X1和X2通过某种信息交换和杂糅，分别得到了中间变量Z1和Z2。自注意力机制就是句子中每个单词看看其它单词对自己的影响力有多大，本单词更应该关注在哪些单词上。在输入状态下，X1和X2互相不知道对方的信息，但因为在自注意力机制中发生了信息交换，所以Z1和Z2各自都有从X1和X2得来的信息。</li><li>第三步是残差连接和层归一化，对应图上圆形标号3。其具体操作是Norm(x+Sublayer(x))Norm(x+Sublayer(x))Norm(x+Sublayer(x))。</li><li>第四步是FFN层，对应图上圆形标号4。对应操作是max(0,xW1+b1)W2+b2max(0,xW1+b1)W2+b2max(0, xW_1 + b_1)W_2 + b_2，即两层线性映射并且中间使用激活函数来激活。因为FFN是割裂开的，所以Z1和Z2各自独立通过全连接神经网络，得到了R1和R2。</li><li>第五步是第二个残差连接和层归一化，对应图上圆形标号5。其具体操作是Norm(x+Sublayer(x))Norm(x+Sublayer(x))Norm(x+Sublayer(x))。</li></ul><p>至此，一个EncoderLayer就执行完毕，其输出可以作为下一个EncoderLayer的输入，然后重复2~5步骤，直至N个EncoderLayer都处理完毕。我们也可以看到，每个输出项的计算和其他项的计算是独立的，即每一层的EncoderLayer都对输入序列的所有位置同时进行操作，而不是像RNN那样逐个位置处理，这是Transformer模型高效并行处理的关键。</p><h3 id="_1-4-张量形状变化" tabindex="-1">1.4 张量形状变化 <a class="header-anchor" href="#_1-4-张量形状变化" aria-label="Permalink to &quot;1.4 张量形状变化&quot;">​</a></h3><p>我们来看看编码过程中的张量形状变化。编码器的输入是待推理的句子序列X: [batch_size, seq_len, d_model]。</p><p>注意：如果考虑到限制最大长度，则每次应该是[batch_size, max_seq_len, d_model]，此处进行了简化。</p><p>编码器内部数据转换时候的张量形状变化如下表所示，对于输入X的Input Embedding张量来说，其形状在编码器内部始终保持不变，具体如下。</p><ul><li>输入层中，在做embedding操作时，张量形状发生变化；在和位置编码相加时，张量形状保持不变。</li><li>在编码器层内部的流转过程中，张量形状保持不变。</li><li>在编码器内部，即多个编码器层之间交互的过程中，张量形状保持不变。</li></ul><p>下表给出了详细的操作和张量形状。</p><table tabindex="0"><thead><tr><th>视角</th><th>操作</th><th>操作结果张量的形状</th></tr></thead><tbody><tr><td>输入层</td><td>X（token index）</td><td>[batch_size, seq_len]</td></tr><tr><td>输入层</td><td>X = embedding(X)</td><td>[batch_size, seq_len, d_model]</td></tr><tr><td>输入层</td><td>X = X + PE</td><td>[batch_size, seq_len, d_model]</td></tr><tr><td>编码器层内部</td><td>X = MHA(X)</td><td>[batch_size, seq_len, d_model]</td></tr><tr><td>编码器层内部</td><td>X = X + MHA(X)</td><td>[batch_size, seq_len, d_model]</td></tr><tr><td>编码器层内部</td><td>X = LayerNorm(X)</td><td>[batch_size, seq_len, d_model]</td></tr><tr><td>编码器层内部</td><td>X = FFN(X)</td><td>[batch_size, seq_len, d_model]</td></tr><tr><td>编码器层</td><td>X = EncoderLayer(X)</td><td>[batch_size, seq_len, d_model]</td></tr><tr><td>编码器</td><td>X = Encoder(X) = 6 x EncoderLayer(X)</td><td>[batch_size, seq_len, d_model]</td></tr></tbody></table><h3 id="_1-6-实现" tabindex="-1">1.6 实现 <a class="header-anchor" href="#_1-6-实现" aria-label="Permalink to &quot;1.6 实现&quot;">​</a></h3><p>我们接下来看看哈佛源码中编码器的实现。</p><h4 id="encoder" tabindex="-1">Encoder <a class="header-anchor" href="#encoder" aria-label="Permalink to &quot;Encoder&quot;">​</a></h4><p>Encoder类是编码器的实现，它的forward()函数返回的就是编码之后的向量。</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 使用Encoder类来实现编码器，它继承了nn.Module类</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">class</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> Encoder</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">nn</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">Module</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">):</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;Core encoder is a stack of N layers&quot;</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">	# Encoder的核心部分是N个EncoderLayer堆叠而成的栈</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    </span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    def</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> __init__</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, layer, N):</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        初始化方法接受两个参数，分别是：</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        layer: 要堆叠的编码器层，对应下面的EncoderLayer类</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        N: 堆叠多少次，即EncoderLayer的数量</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        &quot;&quot;&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> </span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        super</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(Encoder, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">).</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">__init__</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">() </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 调用父类nn.Module的初始化方法</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # 使用clone()函数将layer克隆N份，并将这些层放在self.layers中</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.layers </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> clones(layer, N)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # 创建一个LayerNorm层，并赋值给self.norm，这是“Add &amp; Norm”中的“Norm”部分</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.norm </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> LayerNorm(layer.size)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> forward</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, x, mask):</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        &quot;Pass the input (and mask) through each layer in turn.&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        前向传播函数接受两个参数，分别是：</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        x: 输入数据，即经过Embedding处理和添加位置编码后的输入。形状为(batch_size, seq_len，embedding_dim)</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        mask：掩码</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        </span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # 使用EncoderLayer对输入x进行逐层处理，每次都会得到一个新的x，然后将x作为下一层的输入</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # 此循环的过程相当于输出的x经过了N个编码器层的逐步处理</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> layer </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.layers: </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 遍历self.layers中的每一个编码器层</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> layer(x, mask) </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 将x和mask传递给当前编码器层，编码器层进行运算，并将输出结果赋值给x</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        return</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.norm(x) </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 对最终的输出x应用层归一化，并将结果返回</span></span></code></pre></div><p>其中的clone函数的代码为</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> clones</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(module, N):</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;Produce N identical layers.&quot;</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.ModuleList([copy.deepcopy(module) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> _ </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> range</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(N)])</span></span></code></pre></div><h4 id="encoderlayer" tabindex="-1">EncoderLayer <a class="header-anchor" href="#encoderlayer" aria-label="Permalink to &quot;EncoderLayer&quot;">​</a></h4><p>EncoderLayer类是编码器层的实现，作为编码器的组成单元, 每个EncoderLayer完成一次对输入的特征提取过程, 即编码过程。</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">class</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> EncoderLayer</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">nn</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">Module</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">):</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;Encoder is made up of self-attn and feed forward (defined below)&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    def</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> __init__</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, size, self_attn, feed_forward, dropout):</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        初始化函数接受如下参数：</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        size: 对应d_model，即word embedding维度的大小，也是编码层的大小</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        self_attn: 多头自注意力模块的实例化对象</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        feed_forward: FFN层的实例化对象</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  		dropout: 置0比率</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        super</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(EncoderLayer, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">).</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">__init__</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">() </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 调用父类nn.Module的构造函数</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.self_attn </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> self_attn </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 设置成员变量</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.feed_forward </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> feed_forward </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 设置成员变量</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # 创建两个具有相同参数的SublayerConnection实例，一个用于自注意力，一个用于FFN</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.sublayer </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> clones(SublayerConnection(size, dropout), </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.size </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> size</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> forward</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, x, mask):</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        &quot;Follow Figure 1 (left) for connections.&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        对应论文中图1左侧Encoder的部分</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        前向函数的参数如下：</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        x: 源语句的嵌入向量或者前一个编码器的输出</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        mask: 掩码</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        </span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # 顺序运行两个函数：self_attn()，self.sublayer[0]()</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # 1. 对输入x进行自注意力操作</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # 2. 将自注意力结果传递给第一个SublayerConnection实例，SublayerConnection实例内部会做残差连接和层归一化</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.sublayer[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">](x, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">lambda</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.self_attn(x, x, x, mask))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        </span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # 用上面计算结果来顺序运行两个函数：self.feed_forward()和self.sublayer[1]</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # 1. FFN进行运算</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # 2. 将FFN计算结果传递给第一个SublayerConnection实例，SublayerConnection实例内部会做残差连接和层归一化</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        return</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.sublayer[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">](x, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.feed_forward)</span></span></code></pre></div><p>我们对代码中提到的SublayerConnection做下简要说明，后续文章会进行详述。从论文图上看，不管是自注意力模块还是FFN，它们都会先做自己的业务逻辑，然后做残差连接和层归一化，也会加入Dropout。因为有些逻辑可以复用，因此哈佛代码把他们封装在SublayerConnection类中。SublayerConnection类在其内部会构造 LayerNorm 和 Dropout的实例，自注意力或FFN还是放在EncoderLayer 中构造，然后在前向传播时候由EncoderLayer 传给SublayerConnection。解码器也是按照类似方式调用SublayerConnection。</p><p>但是SublayerConnection的实现和论文略有不同。</p><ul><li>原始论文的实现机制是：LayerNorm(x+Sublayer(x))LayerNorm(x+Sublayer(x))LayerNorm(x+ Sublayer(x))。</li><li>SublayerConnection则是：x+LayerNorm(Sublayer(x))x+LayerNorm(Sublayer(x))x+LayerNorm(Sublayer(x))。</li></ul><p>研究人员把Transformer论文中的实现叫做Post LN，因为是最后做LayerNorm，把SublayerConnection的实现方式叫做Pre LN。两种方式各有优劣，我们会在后续文章中进行分析。</p><h2 id="_0x02-解码器" tabindex="-1">0x02 解码器 <a class="header-anchor" href="#_0x02-解码器" aria-label="Permalink to &quot;0x02 解码器&quot;">​</a></h2><p>首先要提前说明下，解码器的输入有两个：编码器产生的隐状态和解码器之前预测的输出结果。解码器会基于这两个输入来预测下一个输出token。网上对编码器和解码器的关系有一个比较恰当的通俗比喻，作者依据自己的思考对该比喻做进一步调节：</p><ul><li>输入序列是一个需要组装的玩具。</li><li>编码器是售货员，售货员对该玩具的各个组件进行研究，编码器的输出结果（隐状态）就是玩具的组装说明书，里面说明了玩具每个组件的用法（需要怎么和其它组件相配合）。</li><li>解码器就是购买者。如果想把玩具组装好，购买者就需要在组装说明书中查询每个零件的说明，然后依据说明书的描述找到最相似的零件（注意力匹配）进行组装。在组装过程中的玩具就是解码器在之前步骤中的预测输出结果。因为组装需要一面查询组装说明书，一面查看在组装过程中的玩具。所以解码器有两个输入：组装说明书和组装过程中的玩具。</li><li>购买者最终输出一个组装好的玩具。</li></ul><h3 id="_2-1-结构" tabindex="-1">2.1 结构 <a class="header-anchor" href="#_2-1-结构" aria-label="Permalink to &quot;2.1 结构&quot;">​</a></h3><p>解码器的结构如下图所示，也是由多个解码器层组成。在解码器中，子层堆叠的目的是逐层细化和优化生成词汇的表示，使得模型能够生成更准确、更符合上下文的目标词。每个子层都有不同的功能和作用。</p><p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250220203250597-126667332.jpg" alt=""></p><p>每个解码器层包括三个重要子模块：掩码多头注意力（Masked Multi-Head Attention），交叉注意力和FFN。每个子模块的输出会传递到下一个子模块，进一步丰富和优化生成序列的表示。与编码器类似，每个重要子模块周围都有一个残差连接，并且每个重要子模块的输出都会经过层归一化。这三个子模块的作用如下：</p><ul><li>掩码多头注意力。这是输入序列对自身的注意力计算，每个解码步骤中，掩码自注意力层会通过自注意力机制计算输出序列中，当前词与已生成词的关系。其细节如下。 <ul><li>解码器的输入之一（解码器之前预测输出结果的拼接）被传递给所有三个参数，Query、Key和 Value，即QKV均来自一个序列。</li><li>掩码多头注意力模块会对“解码器之前预测的输出结果”这个序列进行编码，此编码执行类似编码器中全局自注意力层的工作。</li><li>掩码多头注意力与全局自注意力的不同之处在于在对序列中位置的处理上（掩码操作），我们马上会进行分析。</li><li>该层输出一个新的表示，是结合了已生成部分序列的信息的一个上下文向量，它包含当前生成的词与已生成词之间的上下文依赖关系。</li></ul></li><li>交叉注意力。该模块将源序列和目标序列进行对齐，是解码器和编码器之间的桥梁，这个子层的目的是让解码器结合编码器的输出，也就是在解码期间参考源句子的上下文信息。通过这一层，解码器可以对源句子的每个 token 进行注意力计算，确保解码时能够参考源句子的结构和语义。交叉注意力是编码器和解码器的第二个不同之处。其细节如下。 <ul><li>不带掩码。</li><li>交叉注意力的输入来源有两处：编码器的输出和掩码多头注意力的输出。K、V矩阵来自编码器的输出，Q来自掩码多头注意力的输出。作者之所以这样设计，是在模仿传统编码器-解码器模型的解码过程，可以综合考虑目标序列与源序列的内容。</li><li>这个层的输出结合了编码器输出的上下文信息以及解码器当前步骤生成的目标词的表示，进一步优化了目标词的表示。输出是一个包含源句子信息的上下文表示（融合了源句子的结构和目标序列的部分结构）。</li></ul></li><li>FFN，作用同编码器的FFN。</li></ul><p>解码器的掩码多头自注意力与编码器的多头自注意力不同之处的终极原因在于训练和推理的不同，即训练和推理在每个时间步的输入和操作的区别。</p><ul><li>训练过程中每个时间步的输入是全部目标序列，在Encoder的多头自注意力中，每个位置都可以自由地注意序列中的所有其他位置。这意味着计算注意力分数时，并没有位置上的限制。这种设置是因为在编码阶段，我们假定有完整的输入序列，并且每个词都可以依赖于上下文中的任何其他词来获得其表示。</li><li>推理过程中每个时间步的输入是直到当前时间步所产生的整个输出序列，推理的本质也是串行自回归的。或者说，解码器的本质就是自回归的。</li></ul><p>为了并行操作，人们使用了teacher forcing机制（需要结合掩码机制），这样可以让解码器同时对同一序列中的多个token进行解码和预测。因为解码器现在的输入是整个目标句子，而当预测某个位置的输出时，我们希望单词只能看到它以及它之前的的单词，不希望注意力在预测某个词时候就能关注到其后面的单词，那样模型有可能利用已经存在的未来词来辅助当前词的生成，就是“作弊”了。为了让前面的token不能观察到后面token的信息，所以使用了掩码技术。掩码的作用是确保解码器只能关注到它之前已经生成的词，而不能看到未来的词。掩码逻辑是为了训练来特殊打造。</p><p>另外，虽然推理时候所有输入都是已知输入，没有真正的“偷看未来词”的可能性，不需要掩码，但是为了保持与训练时的计算一致，推理时也保留了此处代码和模型结构。这样使得推理时的行为与训练时完全匹配，避免了训练与推理之间的行为差异。而且，虽然未来词还没有生成，掩码自注意力机制依然会起到限制作用，确保解码器在每个步骤只关注已经生成的上下文，而不会假设未来的信息存在。后续章节会对掩码做详细阐释。</p><h3 id="_2-2-输入和输出" tabindex="-1">2.2 输入和输出 <a class="header-anchor" href="#_2-2-输入和输出" aria-label="Permalink to &quot;2.2 输入和输出&quot;">​</a></h3><p>解码器也是层叠的栈结构，因此不同解码器层的输入输出不尽相同。</p><ul><li>第一个解码器层的输入有两个： <ul><li>解码器之前预测输出结果的拼接，即前一时刻解码器的输入 + 前一时刻解码器的输出（预测结果）。另外还要加上Shfited Right操作。</li><li>编码器的输出，即第一个解码器层中交叉注意力的K、V均来自编码器的输出（编码器堆栈中最后一个编码器的输出）。</li></ul></li><li>后续解码器层的输入有两个： <ul><li>前一个解码器层的输出。</li><li>编码器的输出，即后续每一个解码器层中交叉注意力的K、V均来自编码器的同一个输出（编码器堆栈中最后一个编码器的输出）。</li></ul></li></ul><p>解码器最终会输出一个实数向量，传给架构图中最上方的线性层，由最终的线性变换和softmax层最终转换为概率分布，借此预测下一个单词。前面提到，编码器的输出就是对原始输入在更高维的向量空间中的表示。解码器就是在高维空间内对向量进行操作，找到按照注意力分数匹配的高阶向量。后续会经过generator等模块把高维向量转换回人类可以理解的低维向量。</p><p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250220203303233-49756509.jpg" alt=""></p><h3 id="_2-3-流程" tabindex="-1">2.3 流程 <a class="header-anchor" href="#_2-3-流程" aria-label="Permalink to &quot;2.3 流程&quot;">​</a></h3><p>解码器的流程需要区分训练阶段和推理阶段，因为这两个阶段的流程不尽相同。</p><h4 id="训练" tabindex="-1">训练 <a class="header-anchor" href="#训练" aria-label="Permalink to &quot;训练&quot;">​</a></h4><p>注：解码器在训练时采用Teacher Forcing模式，会向解码器输入整个目标序列，可以并行预测目标序列的所有单词。我们会在后续章节对Teacher Forcing进行详细分析。</p><p>假如我们的训练任务得到了如下文本对：</p><ul><li>中文：我爱你。</li><li>英文：I love you。</li></ul><p>模型采用如下方式调用，其中batch.src是&quot;我爱你”，batch.tgt是&quot;I love you“。</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">out </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.forward(batch.src, batch.tgt, batch.src_mask, batch.tgt_mask)</span></span></code></pre></div><p>编码器接受&quot;我爱你”作为输入，“我爱你”首先被tokenizer处理成token，然后转换成一组向量进行后续处理，得到输出memory。</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">memory </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> encode(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, src, src_mask)</span></span></code></pre></div><p>解码器有两种输入：编码器的输出（对应下面代码的memory）和英文句子“I love you”（对应下面代码的tgt）。“I love you”经过tokenizer处理成token，然后转换成一组向量。解码器会把该组向量和编码器的输出结合起来，生成最终的翻译输出。</p><div class="language-scss vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">scss</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">decode(self, memory, src_mask, tgt, tgt_mask)</span></span></code></pre></div><p>我们要对解码器的输入再做一下说明。</p><ul><li>tgt：英文句子“I love you”是训练集的真值标签，需要结合掩码并且整体右移（Shifted Right）一位，最终得到是 shifted and masked ground truth。右移的原因解释如下：解码器在T-1时刻会预测T时刻的输出，所以需要在输入句子的最开始添加起始符，从而整个句子将整体右移一位，这样就把每个token的标签设置为它之后的token，方便预测第一个Token（在Teacher forcing模式下，也同时方便预测后续的token）。比如原始输入是“I love you”，右移之后变成“I love you”，这样我们就可以通过起始符预测“I”，也就是通过起始符l来预测实际的第一个输出。否则无法预测第一个输出。</li></ul><p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250220203634251-1474764331.jpg" alt=""></p><ul><li>src_mask和tgt_mask是源序列和目标序列的掩码。前面已经介绍过，加入掩码的原因是要隐藏未来信息。因为解码器的输入是整个目标句子，而当预测某个位置的输出时，我们希望单词只能它以及它之前的的单词，不希望注意力在预测某个词时候就能关注到其后面的单词，那样就是“作弊”了。因此需要借助掩码把后面单词的信息隐藏掉，这样才能在训练时候模拟实际推理的效果。</li></ul><p>具体训练时候的输入输出如下图，因为是并行训练，所以解码器的输入之一是&quot;I love you&quot;，在假设全部预测正确的情况下，输出也是&quot;I love you&quot;，此输出是一次性全部输出。</p><p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250220203332084-1349523742.jpg" alt=""></p><h4 id="推理" tabindex="-1">推理 <a class="header-anchor" href="#推理" aria-label="Permalink to &quot;推理&quot;">​</a></h4><p>解码器在推理时的工作流程相对简单多了，所以不再需要掩码。此时，解码器采用的是自回归模式，也就是这次的输出会加到上次的输入后面，作为下一次的输入，这样每次解码都会利用前面已经解码输出的所有单词嵌入信息。因此，推理任务不用真实目标序列来指导生成过程，只使用了中文句子“我爱你”。即，Encoder的输入不变，而Decoder的输入会是之前Decoder输出的组合（也包括Encoder的输出）。</p><p>具体如下图所示，模型将从特殊的起始序列标记开始依次生成输出序列。在生成下一个token时，模型会把刚预测的 token拼接在之前的输入序列上，以此作为新序列再输入给解码器，从而进行序列的自我生成。因此第一次推理输出“I”，然后将和“I”拼接起来一起输入到解码器，解码器第二次推理输出“love”，然后再将“ I love”输入给解码器，以此类推。当解码器生成标记时，它将停止生成。</p><p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250220203342512-1618792662.jpg" alt=""></p><p>具体代码流程如下。</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> inference_test</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">():</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    test_model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> make_model(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">11</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">11</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    test_model.eval()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    src </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.LongTensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">4</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">6</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">7</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">8</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">9</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]])</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    src_mask </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.ones(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    memory </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> test_model.encode(src, src_mask)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    ys </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.zeros(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">).type_as(src)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> i </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> range</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">9</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        out </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> test_model.decode(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            memory, src_mask, ys, subsequent_mask(ys.size(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)).type_as(src.data)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        )</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        prob </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> test_model.generator(out[:, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">])</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        _, next_word </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.max(prob, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dim</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        next_word </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> next_word.data[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        ys </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.cat(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            [ys, torch.empty(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">).type_as(src.data).fill_(next_word)], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dim</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        )</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Example Untrained Model Prediction:&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, ys)</span></span></code></pre></div><p>test_model.decode()对应如下代码。</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">class</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> EncoderDecoder</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">nn</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">Module</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">):</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> decode</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, memory, src_mask, tgt, tgt_mask):</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        return</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.decoder(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.tgt_embed(tgt), memory, src_mask, tgt_mask)</span></span></code></pre></div><p>编码器会逐步调用编码器层的逻辑。</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">class</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> Decoder</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">nn</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">Module</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">):</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> forward</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, x, memory, src_mask, tgt_mask):</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> layer </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.layers:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> layer(x, memory, src_mask, tgt_mask)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        return</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.norm(x)</span></span></code></pre></div><p>最终编码器层会调用业务逻辑，即注意力计算、残差连接等。</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">class</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> DecoderLayer</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">nn</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">Module</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">):</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> forward</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, x, memory, src_mask, tgt_mask):</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        &quot;Follow Figure 1 (right) for connections.&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> memory</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.sublayer[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">](x, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">lambda</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.self_attn(x, x, x, tgt_mask))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.sublayer[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">](x, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">lambda</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.src_attn(x, m, m, src_mask))</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        return</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.sublayer[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">](x, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.feed_forward)</span></span></code></pre></div><h3 id="_2-4-张量形状变化" tabindex="-1">2.4 张量形状变化 <a class="header-anchor" href="#_2-4-张量形状变化" aria-label="Permalink to &quot;2.4 张量形状变化&quot;">​</a></h3><p>我们来看看解码过程中的张量形状变化（为了更好的说明，我们把解码器的输入和输出都一起纳入）。解码器的输入是一个长度变化的张量Y：[batch_size, seq_len, d_model]，初始时，这个张量中，每个矩阵只有1行，即开始字符的编码。</p><p>注意：如果考虑到限制最大长度，则每次应该是[batch_size, max_seq_len, d_model]。</p><p>对于输入Y的Input Embedding张量来说，其形状在解码器内部始终保持不变，具体如下。</p><ul><li>输入层中，在做embedding操作时，张量形状发生变化；在和位置编码相加时，张量形状保持不变。</li><li>在解码器层内部的流转过程中，张量形状保持不变。</li><li>在解码器内部，即多个解码器层交互过程中，张量形状保持不变。</li><li>进入到输出层之后，张量形状开始变化。</li></ul><p>注意，在单次推理过程中，张量不变，但是每次推理之后，矩阵增加一行，seq_len加1。</p><table tabindex="0"><thead><tr><th>视角</th><th>操作</th><th>操作结果张量的形状</th></tr></thead><tbody><tr><td>输入层</td><td>Y（token index）</td><td>[batch_size, seq_len]</td></tr><tr><td>输入层</td><td>Y = embedding(Y)</td><td>[batch_size, seq_len, d_model]</td></tr><tr><td>输入层</td><td>Y = Y + PE</td><td>[batch_size, seq_len, d_model]</td></tr><tr><td>解码器层内部</td><td>Y = Masked-MHA(Y)</td><td>[batch_size, seq_len, d_model]</td></tr><tr><td>解码器层内部</td><td>Y = LayerNorm(Y + Masked-MHA(Y))</td><td>[batch_size, seq_len, d_model]</td></tr><tr><td>解码器层内部</td><td>Y = Cross-MHA(Y, M, M)</td><td>[batch_size, seq_len, d_model]</td></tr><tr><td>解码器层内部</td><td>Y = LayerNorm(Y + Cross-MHA(Y, M, M))</td><td>[batch_size, seq_len, d_model]</td></tr><tr><td>解码器层内部</td><td>Y = FFN(Y)</td><td>[batch_size, seq_len, d_model]</td></tr><tr><td>解码器层内部</td><td>Y = LayerNorm(Y + FFN(Y))</td><td>[batch_size, seq_len, d_model]</td></tr><tr><td>解码器层</td><td>Y = DecoderLayer(Y)</td><td>[batch_size, seq_len, d_model]</td></tr><tr><td>解码器</td><td>Y = Decoder(Y) = N x DecoderLayer(Y)</td><td>[batch_size, seq_len, d_model]</td></tr><tr><td>输出层</td><td>logits = Linear(Y)</td><td>[batch_size, seq_len, d_voc]</td></tr><tr><td>输出层</td><td>prob = softmax(logits)</td><td>[batch_size, seq_len, d_voc]</td></tr></tbody></table><h3 id="_2-5-实现" tabindex="-1">2.5 实现 <a class="header-anchor" href="#_2-5-实现" aria-label="Permalink to &quot;2.5 实现&quot;">​</a></h3><h4 id="decoder" tabindex="-1">Decoder <a class="header-anchor" href="#decoder" aria-label="Permalink to &quot;Decoder&quot;">​</a></h4><p>Decoder类是解码器的实现，是 N 个解码层堆叠的栈。编码器会将自己输出的隐向量编码矩阵C传递给解码器，这些隐向量可以帮助解码器知道它应该更加关注输入序列哪些位置。解码器的每个解码层都会使用同一个隐向量编码矩阵C，这些隐向量将被每个解码层用于自身的Encoder-Decoder交叉注意力模块，</p><p>Decoder类依次会根据当前翻译过的第i个单词，翻译下一个单词(i+1)。在解码过程中，翻译到第i+1单词时候需要通过Mask操作遮盖住(i+1)之后的单词。Decoder类的代码具体如下。</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">class</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> Decoder</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">nn</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">Module</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">):</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;Generic N layer decoder with masking.&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    def</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> __init__</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, layer, N):</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        初始化函数有两个参数。layer对应下面的DecoderLayer，是要堆叠的解码器层；N是解码器层的个数</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        super</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(Decoder, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">).</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">__init__</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()        </span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.layers </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> clones(layer, N) </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 使用clones()函数克隆了N个DecoderLayer，然后保存在layers这个列表中</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.norm </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> LayerNorm(layer.size) </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 层归一化的实例</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> forward</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, x, memory, src_mask, tgt_mask):</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        前向传播函数有四个参数：</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        x: 目标数据的嵌入表示，x的形状是(batch_size, seq_len, d_model)，在预测时，x的词数会不断增加，比如第一次是(1,1,512)，第二次是(1,2,512)，以此类推</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        memory: 编码器的输出</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        src_mask: 源序列的掩码</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        tgt_mask: 目标序列的掩码</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # 实现多个编码层堆叠起来的效果，并完成整个前向传播过程</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> layer </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.layers: </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 让x逐次在每个解码器层流通，进行处理</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> layer(x, memory, src_mask, tgt_mask)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # 对多个编码层的输出结果进行层归一化并返回最终的结果</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        return</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.norm(x)</span></span></code></pre></div><h4 id="decoderlayer" tabindex="-1">DecoderLayer <a class="header-anchor" href="#decoderlayer" aria-label="Permalink to &quot;DecoderLayer&quot;">​</a></h4><p>DecoderLayer类是解码器层的实现。作为解码器的组成单元，每个解码器层根据给定的输入向目标方向进行特征提取操作，即实施解码过程。DecoderLayer和EncoderLayer的内部非常相似，区别EncoderLayer只有一个多头自注意力模块，而DecoderLayer有两个多头自注意力模块，从代码上看，就是比EncoderLayer多了一个src_attn成员变量。self_attn和src_attn的实现完全一样，只不过使用的Query，Key 和 Value 的输入不同。</p><p>DecoderLayer主要成员变量如下：</p><ul><li>size：词嵌入的维度大小，即解码器层的尺寸。</li><li>self_attn: 掩码多头自注意力模块，负责对解码器之前的输出（即当前的输入）做自注意力计算。该模块需要Q=K=V。Self-Attention 的 Query，Key 和 Value 都是来自下层输入或者是原始输入。</li><li>src_attn：交叉注意力模块，负责对编码器的输出和解码器之前的输出做交叉注意力计算，但是Q!=K=V。Query 来自 self-attn 的输出，Key 和 Value则是编码器最后一层的输出（代码中是memory变量）。</li><li>feed_forward：FFN模块。</li><li>sublayer：应用了残差连接和层归一化。</li><li>drop：置零比率。</li></ul><p>上述这些成员变量通过参数传给初始化函数。</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">class</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> DecoderLayer</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">nn</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">Module</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 继承自PyTorch的nn.Module类</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;Decoder is made of self-attn, src-attn, and feed forward (defined below)&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    def</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> __init__</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, size, self_attn, src_attn, feed_forward, dropout):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        super</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(DecoderLayer, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">).</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">__init__</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.size </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> size</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.self_attn </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> self_attn</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.src_attn </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> src_attn</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.feed_forward </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> feed_forward</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # 创建三个SublayerConnection类实例，分别对应self_attn，src_attn和feed_forward</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.sublayer </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> clones(SublayerConnection(size, dropout), </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> forward</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, x, memory, src_mask, tgt_mask):</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        &quot;Follow Figure 1 (right) for connections.&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        前向传播函数有四个参数：</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        x: 目标数据的嵌入表示，x的形状是(batch_size, seq_len, d_model)，在预测时，x的词数会不断增加，比如第一次是(1,1,512)，第二次是(1,2,512)，以此类推。x可能是上一层的输出或者是整个解码器的输出</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        memory: 编码器的输出</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        src_mask: 源序列的掩码</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        tgt_mask: 目标序列的掩码</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        &quot;&quot;&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        </span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> memory </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 将memory表示成m方便之后使用</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # 第一个子层执行掩码多头自注意力计算。相当于顺序运行了两个函数：self_attn()和self.sublayer[0]()。这里的Q、K、V都是x。tgt_mask的作用是防止预测时看到未来的单词。</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.sublayer[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">](x, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">lambda</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.self_attn(x, x, x, tgt_mask))</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # 第二个子层执行交叉注意力操作，此时Q是输入x，K和V是编码器输出m。src_mask在此处的作用是遮挡填充符号，避免无意义的计算，提升模型效果和训练速度。此刻需要注意的是，两个注意力计算的mask参数不同，上一个是tgt_mask，此处是src_mask</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.sublayer[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">](x, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">lambda</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.src_attn(x, m, m, src_mask))</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # 第三个子层是FFN，经过它的处理后就可返回结果</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        return</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.sublayer[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">](x, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.feed_forward)</span></span></code></pre></div><h2 id="_0x03-交叉注意力深入" tabindex="-1">0x03 交叉注意力深入 <a class="header-anchor" href="#_0x03-交叉注意力深入" aria-label="Permalink to &quot;0x03 交叉注意力深入&quot;">​</a></h2><p>前面已经简单介绍过注意力的分类，此处结合代码对交叉注意力再进行深入分析。</p><h3 id="_3-1-分类" tabindex="-1">3.1 分类 <a class="header-anchor" href="#_3-1-分类" aria-label="Permalink to &quot;3.1 分类&quot;">​</a></h3><p>Transformer 中，注意力被用在三个地方，Encoder中的Multi-Head Attention；Decoder中的Masked Multi-Head Attention；Encoder和Decoder交互部分的Multi-Head Attention。注意力层（Self-attention 层及 Encoder-Decoder-attention 层）以三个参数的形式接受其输入：查询（Query）、键（Key）和值（Value）。我们接下来就分析下，看看Transformer中各个注意力模块的Q、K、V到底是怎么来的。</p><table tabindex="0"><thead><tr><th>名称</th><th>位置</th><th>Q</th><th>K/V</th></tr></thead><tbody><tr><td>多头自注意力</td><td>编码器</td><td>QKV均来自同一个序列</td><td>QKV均来自同一个序列</td></tr><tr><td>掩码多头自注意力</td><td>解码器</td><td>QKV均来自同一个序列</td><td>QKV均来自同一个序列</td></tr><tr><td>交叉注意力</td><td>解码器</td><td>掩码多头自注意力的输出</td><td>编码器输出被用做V和K。</td></tr></tbody></table><p>三个注意力模块的作用如下：</p><ul><li>多头自注意力：是输入序列对自身的注意力计算，允许每个位置自由地注意到整个序列，这样可以获取输入句子中不同单词之间的相关性。此处可以说是Transformer最大的创新。</li><li>掩码多头自注意力：是输入序列对自身的注意力计算，用于获取输出句子（已经翻译好的部分）中不同单词之间的相关性。同时通过序列掩码来限制注意范围，以保持自回归属性，确保生成过程的正确性。这种设计是Transformer模型能够并行训练的关键所在。</li><li>交叉注意力：是目标序列对输入序列的注意力计算。这种设计是Transformer模型能够有效处理类似文本翻译任务的关键所在。</li></ul><p>或者说， 注意力机制是寻找sequence内部（self-attention）或者sequence之间（cross-attention）不同位置上的相似性或者相关性。</p><h3 id="_3-2-业务逻辑" tabindex="-1">3.2 业务逻辑 <a class="header-anchor" href="#_3-2-业务逻辑" aria-label="Permalink to &quot;3.2 业务逻辑&quot;">​</a></h3><p>我们接下来看看交叉注意力的业务逻辑。交叉注意力计算的是每个源序列单词与每个目标序列单词之间的相互作用或者相似度，是在输入序列和输出序列之间进行对齐。</p><p>seq2seq场景存在一种因果关系，该因果性体现在上下文（即包括输入序列，也包括输出序列）上，因为显然源文本中每个位置的字符应该和目标翻译文本各位置字符存在一定的对照关系，因此源文本每个位置的token对于当下要预测的token应该有不一样的影响（权重分配）。我们以机器翻译任务为例，解码器的目标是在给定某源语言序列时产生正确的目标语言输出序列。为了实现这一点，解码器需要：</p><ul><li>学习到历史译文的所有信息。只有知道历史生成的内容，才有了正确输出下一个token的基础。</li><li>学习到于源文本中与当前输出的 token 相关的部分。这其实蕴含了解码器需要对编码器输出的所有信息都有所了解。只有解码器的每个位置都能够获取输入序列中的所有位置的信息，才能通过学习生成正确的输出 token。</li></ul><p>但是，掩码自注意力只能保证解码器学习到历史译文的内容。还需要一个方式来学习到源文本信息，以及把源文本和历史译文融合起来。因此，人们提出了交叉注意力来完成这个功能，这也是因果关系的一种体现。“编码器-解码器交叉注意力”从两个来源获得输入，这两个来源分别来自不同的范畴，因此交叉注意力可以理解为是自注意力的双塔实践。</p><ul><li>Q是译文，来自于解码器的输出。因为Q是来自解码器的掩码自注意力，所以此时天然已经获取了历史译文的内容。</li><li>K和V是原文，来自于编码器的输出，已经持有了输入序列（比如：“我喜欢苹果”）的信息。</li></ul><p>交叉注意力机制可以让解码器和编码器进行交互，确保了解码器可以&quot;询问&quot;编码器有关输入序列的信息，可以聚焦于源语言句子中的不同部分。或者说，编码器输出的隐向量本质是聚合了输入序列信息的一个数据库（V），而解码器的每一个输入token本质是一条查询语句（Q），负责查询数据库中与之最相似的（最需要注意的）token。最终 QKTQKTQK^T 这个矩阵的每一行都表示decoder的一个输入token对隐向量中所有token的注意力。例如，在进行“我喜欢苹果”到“I love apple”的翻译时，解码器可能会询问编码器：“根据你对‘我’的理解，接下来我应该输出什么？”。通过这种方式，解码器可以更准确地预测目标序列的下一个词。这种&quot;一问一答&quot;的描述是形象的，它说明了解码器是如何利用编码器的信息来确定最合适的输出的。</p><h3 id="_3-3-业务流程" tabindex="-1">3.3 业务流程 <a class="header-anchor" href="#_3-3-业务流程" aria-label="Permalink to &quot;3.3 业务流程&quot;">​</a></h3><p>然后我们再看交叉注意力是如何在编码器和解码器之间起作用的。</p><p>解码器在对当前时刻进行解码输出时，都会将当前时刻之前所有的预测结果作为输入来对下一个时刻的输出进行预测。假设现在需要将&quot;我吃了一个苹果翻译成英语&quot;I ate an apple&quot;，目前解码器已经输出了“I ate”两个单词，接下来需要对下一时刻的输出&quot;an&quot;进行预测，那么整个过程就可以通过下图来进行表示，图上蓝色部分是解码器，左边的小蓝框是解码器中的掩码多头自注意力模块，右面大蓝框是编码器的交叉注意力模块，左下方红色虚线框是编码器。</p><p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250220203404276-967640057.jpg" alt=""></p><p>具体流程说明如下：</p><ul><li>左上角的矩阵（对应序号1）是解码器中掩码自注意力机制对输入<code>&quot; I ate&quot;</code>这3个词编码后的结果。</li><li>左下角（对应序号2）是编码器对输入`&quot;我吃了一个苹果&quot;编码后的结果，这就是编码器最终输出的从多角度集自身与其他各个字关系的矩阵，记为memory。</li><li>因为上述两个矩阵分别都做了自注意力转换，所以每个矩阵中的每一个向量都包含了本序列其它位置上的编码信息。</li><li>把序号1看作是Q（对应序号3），把序号2看作是K和V（对应序号4和5）。</li><li>接下来，Q与K计算得到了一个注意力分数矩阵（对应序号6），矩阵的每一行就表示在对memory（图中的V）中的每一位置进行解码时，应该如何对注意力进行分配。</li><li>进行掩码操作（对应序号7）。</li><li>进行softmax操作（对应序号8）得到注意力权重A。此权重可以看做是Q（待解码向量）在K（本质上也就是memory）中查询memory中各个位置与Q有关的信息。</li><li>再将注意力权重A与V进行线性组合便得到了交叉注意力模块的输出向量（对应序号9）。此时这个输出向量可以看作是考虑了memory中各个位置编码信息的输出向量，也就是说它包含了在解码当前时刻时应该将注意力放在memory中哪些位置上的信息。</li></ul><h3 id="_3-4-代码逻辑" tabindex="-1">3.4 代码逻辑 <a class="header-anchor" href="#_3-4-代码逻辑" aria-label="Permalink to &quot;3.4 代码逻辑&quot;">​</a></h3><p>我们再从代码逻辑进行梳理。</p><p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250220203413643-339081530.jpg" alt=""></p><p>具体流程解读如下。</p><ul><li>run_epoch()函数调用模型的前向传播函数，即EncoderDecoder类的forward()函数（对应图上序号1），此时Batch类会提供数据和掩码（batch.src, batch.tgt, batch.src_mask, batch.tgt_mas）给模型进行前向传播。</li><li>EncoderDecoder类会通过encode()函数执行编码功能。 <ul><li>encode()函数利用src, src_mask调用Encoder类的forward()函数（对应图上序号2）。src里面是单词的索引，encode()函数中会调用self.src_embed(src)来生成src的word embedding。</li><li>Encoder.forward()函数又会调用EncoderLayer.forward()函数（对应图上序号3），此处是编码器的编码功能。</li><li>EncoderLayer.forward()函数最终调用到MultiHeadedAttention.forward()函数（对应图上序号4），此处是编码器层的编码功能。</li><li>MultiHeadedAttention.forward()函数最终调用到attention()函数完成注意力计算功能（对应图上序号5）。</li><li>encode()函数会返回memory，memory会被用作参数传递给decode()函数。</li></ul></li><li>EncoderDecoder类会通过decode()函数执行解码功能。 <ul><li>decode()函数利用memory，src_mask, tgt, tgt_mask来调用到Decoder.forward()函数（对应图上序号6）。tgt里面是单词的索引，decode()函数会调用self.tgt_embed(tgt)来生成tgt的word embeddng。</li><li>Decoder.forward()函数会调用DecoderLayer.forward()函数（对应图上序号7），此处是解码器的解码功能的实现。</li><li>DecoderLayer.forward()函数首先会调用MultiHeadedAttention.forward()函数。因为是使用self.self_attn(x, x, x, tgt_mask)来调用（对应图上序号8），x是tgt，所以这是掩码多头自注意力。</li><li>DecoderLayer.forward()函数其次会调用MultiHeadedAttention.forward()函数。因为是使用self.src_attn(x, m, m, src_mask)来调用（对应图上序号10），x是tgt，所以这是交叉注意力。</li><li>掩码多头自注意力的计算会使用tgt，tgt_mask来调用attenion()函数（对应图上序号9）。</li><li>交叉注意力的计算会使用tgt,memoy, memory, src_mask来调用attention()函数（对应图上序号11）。</li></ul></li></ul><p>其实这块与上文 Encoder 中 的 Multi-Head Attention 具体实现细节上完全相同。</p><p>我们总结下，交叉注意力机制使模型能够在生成输出序列的每一步都考虑到输入序列的全部信息，从而捕捉输入和输出之间的复杂依赖关系，在各种seq2seq任务中实现卓越的性能。</p><h2 id="_0x04-decoder-only" tabindex="-1">0x04 Decoder Only <a class="header-anchor" href="#_0x04-decoder-only" aria-label="Permalink to &quot;0x04 Decoder Only&quot;">​</a></h2><p>Transformer的架构并非一成不变，它可以表现为仅编码器（Encoder Only）、仅解码器（Decoder Only）或经典的编码器-解码器模型。每种架构变体都针对特定的学习目标和任务进行了定制。</p><h3 id="_4-1-分类" tabindex="-1">4.1 分类 <a class="header-anchor" href="#_4-1-分类" aria-label="Permalink to &quot;4.1 分类&quot;">​</a></h3><p>Transformer架构最初作为机器翻译任务的编码器-解码器模型引入。在此架构中，编码器将整个源语言句子作为输入，并将其通过多个Transformer编码器块，提取输入句子的高级特征。然后，这些提取的特征被一个接一个地馈送到解码器中，解码器基于来自编码器的源语言特征以及解码器之前生成的tokens来生成目标语言中的tokens。在随后的工作中，研究人员也引入仅编码器和仅解码器的架构，分别从原始编码器-解码器架构中仅取编码器和解码器组件，如图所示。</p><ul><li>(a)是仅包含编码器的模型，并行执行所有token的推理。</li><li>(b)是仅包含解码器的模型，以自回归方式进行推理。</li><li>(c)是包含编码器-解码器的模型，，它使用编码序列的输出作为交叉注意力模块的输入。</li></ul><p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250220203424082-1461469436.jpg" alt=""></p><p>下图给出了现代LLM的进化树，追溯了近年来语言模型的发展，并突出了一些最著名的模型。同一分支上的模型关系更密切。基于Transformer的模型以非灰色显示：</p><ul><li><p>蓝色分支 代表Decoder-Only 模型。随着时间的推移，越来越多的 Decoder-Only 模型被推出。</p></li><li><p>粉色分支 代表Encoder-Only 模型。这些模型主要用于编码和表示输入序列。</p></li><li><p>绿色分支 代表 Encoder-Decoder 模型。结合了前两者的特点，既能够编码输入序列，又能生成输出序列。</p></li></ul><p>模型在时间线上的垂直位置表示它们的发布日期。开源模型用实心方块表示，而闭源模型用空心方块表示。右下角的堆叠条形图显示了来自不同公司和机构的模型数量。</p><p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250220203432110-887693199.jpg" alt=""></p><p>从进化树中，我们得出与本篇相关的以下观察结果：</p><ul><li>仅解码器模型逐渐主导了LLM的发展。在LLM开发的早期阶段，仅解码器模型不如仅编码器和编码器-解码器模型流行。然而，在2021年之后，随着改变游戏规则的LLM-GPT-3的引入，仅解码器模型经历了显著的繁荣。与此同时，在BERT带来的最初爆炸性增长之后，仅编码器的模型逐渐开始逐渐消失。</li><li>编码器-解码器模型仍然很有前景，因为这种类型的架构仍在积极探索中，其中大多数都是开源的。谷歌对开源编码器-解码器架构做出了重大贡献。然而，仅解码器模型的灵活性和多功能性似乎使谷歌对这一方向的坚持不那么有希望。</li></ul><h3 id="_4-2-decoder-only" tabindex="-1">4.2 Decoder Only <a class="header-anchor" href="#_4-2-decoder-only" aria-label="Permalink to &quot;4.2 Decoder Only&quot;">​</a></h3><p>Decoder-Only 模型只使用标准 Transformer 的 Decoder 部分，但稍作改动，典型差异是少了编码器解码器注意层，即在 Decoder-Only 模型不需要接收编码器的信息输入。Decoder-Only 模型没有显式的编码器模块，不显式区分“理解”和“生成”阶段。模型在自注意力机制中隐式完成对用户输入的分析、理解和建模，同时为生成任务提供基础。</p><p>前文提到，在BERT带来的最初爆炸性增长之后，仅编码器的模型逐渐开始逐渐消失。因此目前只剩Encoder-Decoder模型和Decoder only模型。然而， Decoder Only模型也可以细分为Causal Decoder架构和Prefix Decoder架构。因此，现有LLM的主流架构在事实上大致可分为三种主要类型，即编码器-解码器、因果解码器和前缀解码器。下图给出了三种主流架构中注意力模式的比较。在这里，蓝色、绿色、黄色和灰色圆角矩形分别表示前缀token之间的注意力、前缀和目标token之间的注意力、目标token间的注意力和掩码注意力。</p><p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250220203439664-1249924971.jpg" alt=""></p><p>我们接下来分析两种Decoder Only架构。</p><ul><li>Causal Decoder（因果解码器）架构。因果解码器架构结合了单向注意力掩码，以确保每个输入token只能关注过去的token和自身。输入和输出token通过解码器以相同的方式进行处理。作为该架构的代表性语言模型，GPT系列模型是基于因果解码器架构开发的。</li><li>Prefix Decoder（前缀解码器）架构。前缀解码器架构（也称为非因果解码器]）修改了因果解码器的掩蔽机制，以实现对前缀token的双向关注和仅对生成的token的单向关注。这样，与编码器-解码器架构一样，前缀解码器可以对前缀序列进行双向编码，并逐一自回归预测输出token，其在编码和解码过程中共享相同的参数。与其从头开始进行预训练，一个实用的建议是不断训练因果解码器，然后将它们转换为前缀解码器以加速收敛，基于前缀解码器的现有代表性LLM包括GLM130B和U-PaLM。</li></ul><h3 id="_4-3-架构选择" tabindex="-1">4.3 架构选择 <a class="header-anchor" href="#_4-3-架构选择" aria-label="Permalink to &quot;4.3 架构选择&quot;">​</a></h3><p>许多研究都对仅解码器架构和编码器-解码器架构的性能进行了研究，但在有足够的训练和模型规模的情况下，确实没有确凿证据证明一种架构在最终性能上优于另一种架构。在知乎上有一个知名的帖子：[为什么现在的LLM都是Decoder only的架构？](为什么现在的LLM都是Decoder only的架构？)。各路大神都有很精彩的见解。不完全总结如下：</p><p>注：下面的decoder only主要指Causal Decoder（因果解码器）架构。</p><ul><li>适合生成任务。 <ul><li>Decoder-Only 模型的任务适配性更好。“纯生成”任务（如对话、续写）没有明确的“输入”和“输出”分界，引入Encoder会显得多余。</li><li>Decoder-Only 模型更适合生成任务。很多实际应用更关注生成的连贯性和语义丰富性，而不是对输入的复杂理解。</li></ul></li><li>泛化性能更好。 <ul><li>苏神提出了”注意力满秩问题“。即Decoder-only架构的Attention矩阵一定是满秩的，这代表更强的表达能力，而双向注意力反而会变得不足。 <ul><li>在纯解码器Decoder-Only架构中，由于因果掩码（防止模型看到未来的标记Token），注意力矩阵被限制为下三角形式，理论上可以保持其全秩状态：对角线上的每个元素（代表自注意力）都有助于使行列式为正（只有 Softmax 才能得到正结果）。全秩意味着理论上更强的表达能力。</li><li>另外两种生成式架构都引入了双向注意力，因此无法保证其注意力矩阵的全秩状态。直觉上这是有道理的。双向注意力是一把双刃剑：它能加快学习过程，但也会破坏模型学习生成所必需的更深层预测模式。你可以把它想象成学习如何写作：填空比逐字逐句地写出整篇文章更容易，但这是一种不太有效的练习方式。不过，经过大量训练后，这两种方法都能达到学习如何写作的目的。</li></ul></li><li>Encoder-Decoder模型因为可以看到双向，虽然预测时候有优势，但是训练时候降低了学习难度，导致上限不高。而Decoder-Only 模型在模型足够大、数据足够多时，其学习通用表征的上限更高。</li><li>论文&quot;What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization&quot; 比较了各种架构和预训练方法的组合。他们发现： <ul><li>Decoder-Only 模型在没有任何tuning数据的情况下、zero-shot表现最好。我们的实验表明，在纯粹的自监督预训练后，根据自回归语言建模目标训练的纯因果解码器模型表现出最强的零样本泛化能力。</li><li>而encoder-decoder则需要在一定量的标注数据上做multitask finetuning才能激发最佳性能。然而，在实验中，对具有非因果可见性的输入来说，先使用基于掩码语言建模目标训练，然后进行多任务微调的模型性能最好。</li></ul></li><li>Decoder-Only 模型是Casual attention，具备隐式的位置编码功能，可以打破Transformer的位置不变性。而双向attention的模型如果不带位置编码，则对语序区分能力较弱。</li><li>从提示词中进行上下文学习。在使用 LLM 时，我们可以采用提示词工程方法，例如提供少量实例来帮助 LLM 理解上下文或任务。在论文“Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers”中，研究人员用数学方法证明，这种上下文信息可以被视为具有与梯度下降类似的效果，可以更新零样本的注意力权重。如果我们把提示词看作是给注意力权重引入梯度，那么我们或许可以期待它对Decoder-Only模型产生更直接的效果，因为它在用于生成任务之前不需要先转化为中间语境的特征表示。从逻辑上讲，它应该仍然适用于Encoder-Decoder架构，但这需要对编码器进行仔细调整，使其达到最佳性能，而这可能比较困难。</li></ul></li><li>高效性。 <ul><li>Decoder-Only 模型在同一个模块中处理输入序列与输出序列，避免了模型结构的复杂化。根据奥卡姆剃刀原理：如果你有两个相互竞争的观点来解释同一现象，你应该选择更简单的观点。我们应该更倾向于只使用解码器的模型结构。</li><li>Decoder-Only 模型在推理过程只需一次向前传播，而不是 Encoder 和 Decoder 分别进行向前传播，推理效率更高。</li><li>Decoder-Only 模型支持一直复用KV Cache，对多轮对话更友好。而其它两种架构难以做到。在纯解码器模型（Decoder-Only）中，先前Token的键（K）和值（V）矩阵可以在解码过程中重复用于后面的标记Token。由于每个位置只关注之前的Token（由于因果注意力机制），因此这些标记Token的 K 和 V 矩阵保持不变。这种缓存机制避免了为已经处理过的标记Token重新计算 K 和 V 矩阵，从而提高了效率，有利于在自回归模型（如 GPT）的推理过程中加快生成速度并降低计算成本。</li><li>利用Scale Up。Encoder-Decoder 架构网络不是均匀对称的（不是线性而是有大量的分叉），导致数据依赖关系复杂，难以并行优化。而Decoder-Only 架构没有此问题。</li><li>训练数据效率高，训练成本低。 <ul><li>Decoder-Only 模型的训练目标是预测下一个 Token（Next Token Prediction），这是大规模预训练任务的核心目标。这种目标与网络架构直接对齐，能高效利用海量的非结构化文本数据。</li><li>Causal Decoder 模型因其强大的零样本泛化能力而表现出色，这与当前的惯例--在大规模语料库上进行自我监督学习十分契合。</li><li>而Encoder-Decoder 模型需要额外设计输入输出配对的数据。要实现 Encoder-Decoder结构的最大潜力，我们需要对标注数据进行多任务微调（基本上就是指令微调），这可能会非常昂贵，尤其是对于大型模型而言。</li></ul></li></ul></li></ul><h2 id="_0xff-参考" tabindex="-1">0xFF 参考 <a class="header-anchor" href="#_0xff-参考" aria-label="Permalink to &quot;0xFF 参考&quot;">​</a></h2><p><a href="https://zhuanlan.zhihu.com/p/552573482" target="_blank" rel="noreferrer">解剖Transformer 第二部分：你会用注意力机制组装出一个Transformer吗？</a> <a href="https://www.zhihu.com/people/anthony_shi" target="_blank" rel="noreferrer">大方</a></p><p><a href="http://citeseerx.ist.psu.edu/viewdoc/wnload;jsessionid=09A5BBC6F33B4AA3D7A0F06D8AEDB1A3?doi=10.1.1.52.9724&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noreferrer">A Learning Algorithm for Continually Running Fully Recurrent Neural Networks</a></p><p><a href="https://arxiv.org/abs/1506.03099" target="_blank" rel="noreferrer">Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks</a></p><p><a href="https://arxiv.org/abs/1610.09038" target="_blank" rel="noreferrer">Professor Forcing: A New Algorithm for Training Recurrent Networks</a>, 2016. Section 10.2.1, Teacher Forcing and Networks with Output Recurrence, <a href="http://amzn.to/2wHImcR" target="_blank" rel="noreferrer">Deep Learning</a>, Ian Goodfellow, Yoshua Bengio, Aaron Courville, 2016.</p><p><a href="https://zhuanlan.zhihu.com/p/656960458" target="_blank" rel="noreferrer">大模型时代，是 Infra 的春天还是冬天？</a> <a href="https://www.zhihu.com/people/cheng-cheng-69-56" target="_blank" rel="noreferrer">成诚</a></p><p><a href="https://kexue.fm/archives/9529" target="_blank" rel="noreferrer">为什么现在的LLM都是Decoder-only的架构？</a> 苏剑林</p><p><a href="https://mp.weixin.qq.com/s?__biz=MzA5NjQ4MDA2Nw==&amp;mid=2247483793&amp;idx=1&amp;sn=0c11aba4e6a65199a8142fdebf564359&amp;chksm=913928ad929a7c70fe8cb75c60dea5ae2a33db0ee02150435f4a5ef8b897772df42bfd98f812&amp;mpshare=1&amp;scene=1&amp;srcid=12020JCeh9IuyOT65l6A5fLe&amp;sharer_shareinfo=eb10bc2a3d37df56799aceda96901e39&amp;sharer_shareinfo_first=eb10bc2a3d37df56799aceda96901e39#rd" target="_blank" rel="noreferrer">Transformer 三大变体之Decoder-Only模型详解</a> 浪子 [牛山AI公园]</p><p><a href="https://www.jianshu.com/p/5933374ebb68" target="_blank" rel="noreferrer">Transformer系列：图文详解Decoder解码器原理</a> xiaogp</p><p><a href="https://mp.weixin.qq.com/s?__biz=MzA5NjQ4MDA2Nw==&amp;mid=2247483785&amp;idx=1&amp;sn=a0cf21001eae1999c3e3752d745adc7b&amp;chksm=91687765db12ae54fe91cb6473f7e72f4a90188f9d132b1b4d8c8630d6a62d78ba40913dbbd3&amp;mpshare=1&amp;scene=1&amp;srcid=1110Z4Cgypow8VKHM8J8CMBk&amp;sharer_shareinfo=11f59027fc25661acc1bd00b91e2b23e&amp;sharer_shareinfo_first=11f59027fc25661acc1bd00b91e2b23e#rd" target="_blank" rel="noreferrer">Transformer中的解码器详解</a> 浪子 牛山AI公园</p><p><a href="https://mp.weixin.qq.com/s?__biz=MzA5NjQ4MDA2Nw==&amp;mid=2247483772&amp;idx=1&amp;sn=df26667deb580c938b072dbf72df032f&amp;chksm=90ae3f50a7d9b6465e038bdf0e0df629abc27f35e3b4df990c39542c3e8801ec6f6ba6453332&amp;token=180317867&amp;lang=zh_CN#rd" target="_blank" rel="noreferrer">Transformer中的编码器详解</a> 浪子 牛山AI公园</p><p><a href="https://spaces.ac.cn/archives/10347" target="_blank" rel="noreferrer">Decoder-only的LLM为什么需要位置编码？</a> 苏剑林</p><p><a href="https://mp.weixin.qq.com/s?__biz=MzkzODI1NzQyNA==&amp;mid=2247492798&amp;idx=1&amp;sn=9e88356ecb9ff238d7aab54c5b06303b&amp;chksm=c3becffeece094117818485f727a0423dd4366b193cbb36dcc97855a380952cfa42d7eb47b2b&amp;mpshare=1&amp;scene=1&amp;srcid=0117jNqu3jpHSBsyiVbN21iz&amp;sharer_shareinfo=091ec7b66707ad93107ddc2514240c84&amp;sharer_shareinfo_first=091ec7b66707ad93107ddc2514240c84#rd" target="_blank" rel="noreferrer">为什么大多数LLM只使用Decoder-Only结构？</a> AI算法之道</p><p><a href="https://mp.weixin.qq.com/s?__biz=MzU5NjY0NzA4NA==&amp;mid=2247485884&amp;idx=1&amp;sn=d39737f2b8113ff41e34993a3db3024d&amp;chksm=ff9a76af300531f82f8d7c4afcd0331358b52332c56e7c46b4f161d933fa911b3412195baf75&amp;mpshare=1&amp;scene=1&amp;srcid=0109sZ5pyi3rCAxUG6wmOLgU&amp;sharer_shareinfo=35f5ce3148859bb1b77441cf2fecd486&amp;sharer_shareinfo_first=35f5ce3148859bb1b77441cf2fecd486#rd" target="_blank" rel="noreferrer">Transformers基本原理—Decoder如何进行解码？</a> Python伊甸园</p><p>Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers. Retrieved from <a href="https://arxiv.org/abs/2212.10559" target="_blank" rel="noreferrer">https://arxiv.org/abs/2212.10559</a> Dai, D., Sun, Y., Dong, L., Hao, Y., Sui, Z., &amp; Wei, F. (2022).</p><p>本文转自 <a href="https://www.cnblogs.com/rossiXYZ/p/18727704" target="_blank" rel="noreferrer">https://www.cnblogs.com/rossiXYZ/p/18727704</a>，如有侵权，请联系删除。</p></div></div></main><footer class="VPDocFooter" data-v-39a288b8 data-v-e257564d><!--[--><!--]--><div class="edit-info" data-v-e257564d><div class="edit-link" data-v-e257564d><a class="VPLink link vp-external-link-icon no-icon edit-link-button" href="https://github.com/EthanLiu6/coderethan.fun/edit/master/docs/AI/Transformer/04-编码器 &amp; 解码器.md" target="_blank" rel="noreferrer" data-v-e257564d><!--[--><span class="vpi-square-pen edit-link-icon" data-v-e257564d></span> 在 GitHub 上编辑此页面 OR 提出修改意见<!--]--></a></div><!----></div><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-e257564d><span class="visually-hidden" id="doc-footer-aria-label" data-v-e257564d>Pager</span><div class="pager" data-v-e257564d><a class="VPLink link pager-link prev" href="/AI/Transformer/03-数据处理.html" data-v-e257564d><!--[--><span class="desc" data-v-e257564d>上一篇</span><span class="title" data-v-e257564d>03-数据处理</span><!--]--></a></div><div class="pager" data-v-e257564d><a class="VPLink link pager-link next" href="/AI/Transformer/05-训练&amp;推理.html" data-v-e257564d><!--[--><span class="desc" data-v-e257564d>下一篇</span><span class="title" data-v-e257564d>05-训练&推理</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-5d98c3a5 data-v-e315a0ad><div class="container" data-v-e315a0ad><p class="message" data-v-e315a0ad>ICP备案号: <a href="https://beian.miit.gov.cn/" target="_blank">蜀ICP备2024103116号</a><br>公安备案号: <a href="https://beian.mps.gov.cn/#/query/webSearch?code=51012202001928" rel="noreferrer" target="_blank">川公网安备51012202001928</a></p><p class="copyright" data-v-e315a0ad>版权所有 © 2024-present  <a href="mailto:16693226842@163.com" target="_blank">Ethan.Liu</a></p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"ai_deep_learning_theory_00-dl_base_notes.md\":\"CAdB7Lmh\",\"ai_deep_learning_theory_01-feedforward_network.md\":\"Dc0-nIKA\",\"ai_deep_learning_theory_02-back_propagation.md\":\"t66HZ6KH\",\"ai_deep_learning_theory_03-bp_example_demo.md\":\"CdKkr9wb\",\"ai_deep_learning_theory_04-convolution_neural_network.md\":\"CKyKqVoU\",\"ai_deep_learning_theory_05-deep_learning_model.md\":\"B3Ib7-iT\",\"ai_deep_learning_theory_06-pytorch_install.md\":\"C08X_AVj\",\"ai_deep_learning_theory_07-operators.md\":\"BKFjU-RC\",\"ai_deep_learning_theory_08-activation_functions.md\":\"Como81rR\",\"ai_deep_learning_theory_09-recurrent_neural_network.md\":\"D7qCdZEy\",\"ai_deep_learning_theory_10-seq2seq.md\":\"CIAyZr6Y\",\"ai_deep_learning_theory_11-1attentions.md\":\"CB-r_QHy\",\"ai_deep_learning_theory_11-2attention-extension.md\":\"D9uu1g-H\",\"ai_deep_learning_theory_12-weight-initialization.md\":\"tjnwIO-Q\",\"ai_deep_learning_theory_13-optimizers.md\":\"Qk82FVfW\",\"ai_deep_learning_theory_14-regularization.md\":\"CWzAanem\",\"ai_deep_learning_theory_15-deep-learning-tuning-guide.md\":\"CMQIvWo9\",\"ai_deep_learning_theory_20-pytorch-tensor.md\":\"3GYXzMSK\",\"ai_deep_learning_theory_21-pytorch-autograd.md\":\"BPcbnDeO\",\"ai_deep_learning_theory_22-pytorch-module.md\":\"D3W47z47\",\"ai_deep_learning_theory_23-1training-example-1.md\":\"BSbXuKLm\",\"ai_deep_learning_theory_23-2decoder.md\":\"BCYdFt2l\",\"ai_deep_learning_theory_23-3encoder.md\":\"BpaKSKIz\",\"ai_deep_learning_theory_23-4transformer.md\":\"Deo9x8So\",\"ai_deep_learning_theory_24-pytorch-optimizer.md\":\"D38W_WfI\",\"ai_deep_learning_theory_25-pytorch-lr-scheduler.md\":\"BrPX5Pww\",\"ai_deep_learning_theory_26-pytorch-dataloader.md\":\"C1VXfMIC\",\"ai_deep_learning_theory_27-pytorch-model-save.md\":\"D0btqXT_\",\"ai_deep_learning_theory_28-pytorch-tensorboard.md\":\"CGiQm8lw\",\"ai_deep_learning_theory_29-pytorch-graph-mode.md\":\"Dh2sBoD6\",\"ai_deep_learning_theory_30-3main.md\":\"Cf3sjvsp\",\"ai_deep_learning_theory_30-training-example-2.md\":\"DVUt4gEj\",\"ai_deep_learning_theory_40-ner.md\":\"dpC65KCz\",\"ai_deep_learning_theory_41-question-answering.md\":\"CvkK4DNb\",\"ai_deep_learning_theory_42-1stable-diffusion.md\":\"CGgf4cC2\",\"ai_deep_learning_theory_42-2sdxl.md\":\"CC6w4ggh\",\"ai_deep_learning_theory_42-3vae.md\":\"CD94bkcz\",\"ai_deep_learning_theory_44-scaling-law.md\":\"B0JM9xx_\",\"ai_deep_learning_theory_45-distribute-training.md\":\"DZ9YG6ih\",\"ai_deep_learning_theory_46-nlp-llama.md\":\"8zU0XjzU\",\"ai_deep_learning_theory_47-nlp-deepseek.md\":\"CjtxEpY3\",\"ai_deep_learning_theory_index.md\":\"TlMKjhkI\",\"ai_index.md\":\"DZAAGaZE\",\"ai_transformer_01-注意力机制.md\":\"CAUQ-ZlO\",\"ai_transformer_02-总体架构.md\":\"ChkPOSG5\",\"ai_transformer_03-数据处理.md\":\"I5H3eYdL\",\"ai_transformer_04-编码器 _ 解码器.md\":\"zSoU6yVz\",\"ai_transformer_05-训练_推理.md\":\"BJLWHSwO\",\"ai_transformer_06-token.md\":\"6NkY4MDx\",\"ai_transformer_07-embedding.md\":\"CQvHLv6j\",\"ai_transformer_08-位置编码.md\":\"D1gfu21J\",\"ai_transformer_09-位置编码分类.md\":\"BTP3xgL5\",\"ai_transformer_10-自注意力.md\":\"cLYDSTGu\",\"ai_transformer_11-掩码.md\":\"D3BVR142\",\"ai_transformer_12-多头自注意力.md\":\"fflE1jcD\",\"ai_transformer_13- ffn.md\":\"D5pP50b2\",\"ai_transformer_14-残差网络和归一化.md\":\"CGidK7tQ\",\"ai_transformer_index.md\":\"D8JcJLy1\",\"guide_index.md\":\"0LVzN69w\",\"guide_博客结构.md\":\"x1FQx-jf\",\"improve_index.md\":\"CH8Euwro\",\"improve_金融投资_index.md\":\"Vw_D3eRw\",\"improve_阅读_index.md\":\"Dj8DDADt\",\"index.md\":\"mK1aVFiR\",\"it-learning_408知识_index.md\":\"DlbfR44a\",\"it-learning_408知识_os-4.1 进程同步.md\":\"BL8WmbXW\",\"it-learning_408知识_os-4.4 信号量机制.md\":\"ZCxX2ify\",\"it-learning_408知识_os-4.4 信号量机制pv操作之“可见”.md\":\"DcOjbMF_\",\"it-learning_c__基础_01_开发环境搭建与基础数据类型.md\":\"BDKy9qJ5\",\"it-learning_c__基础_02_控制流语句与复合数据类型.md\":\"SKbyo4op\",\"it-learning_c__基础_03_指针与引用.md\":\"DHwgRso8\",\"it-learning_c__基础_04_自定义数据类型与函数.md\":\"DRE6vCJj\",\"it-learning_c__基础_05_头文件与指针的算术运算.md\":\"D1SjIWSd\",\"it-learning_c__基础_06_字符串、数组、指针与函数.md\":\"CgPOPn1g\",\"it-learning_c__基础_07_函数进阶与内存管理.md\":\"6qEwY-bi\",\"it-learning_c__基础_08_运算符优先级表.md\":\"BJfq7pJh\",\"it-learning_c__基础_09_指针、内存管理和类的基础.md\":\"B67kgruM\",\"it-learning_c__基础_10_深入类和对象.md\":\"BWMMnRaW\",\"it-learning_c__基础_11_类的大小、继承与权限控制.md\":\"CRDT26Xm\",\"it-learning_c__基础_12_继承进阶.md\":\"6AJ13KSU\",\"it-learning_c__基础_13_类型转换和多态与虚函数.md\":\"CYE5ie6q\",\"it-learning_c__基础_14_纯虚函数、抽象类、深浅拷贝及智能指针.md\":\"UvlEbqGK\",\"it-learning_c__基础_15_运算符重载与 string 类详解.md\":\"1S48Obum\",\"it-learning_c__基础_16_有序容器与无序容器.md\":\"1sbF57hZ\",\"it-learning_c__基础_17_模板.md\":\"Bp6hiaoG\",\"it-learning_c__基础_18_迭代器与其应用.md\":\"DhitqtBz\",\"it-learning_c__基础_19_c__ 标准库常用算法.md\":\"DiYIRHYc\",\"it-learning_c__基础_20_c__ 异常处理 - 第19次课.md\":\"CJMeMX9w\",\"it-learning_c__基础_21_友元及友元相关内容.md\":\"Cx439b0I\",\"it-learning_c__基础_22_c__ io 流详解-feadbc607d7f.md\":\"BAPGehpe\",\"it-learning_c__基础_23_c__ io 流详解.md\":\"I4ZE0Tau\",\"it-learning_c__基础_24_位运算符总结.md\":\"CRY59I0U\",\"it-learning_c__基础_25_c__三种继承方式.md\":\"Bm4ge_Dq\",\"it-learning_c__基础_26_c__11 高级特性.md\":\"b4HnWfxf\",\"it-learning_c__基础_27_c__14 新特性.md\":\"C5KOu8gX\",\"it-learning_c__基础_28_c__17 新特性.md\":\"DGt5Tcb-\",\"it-learning_c__基础_29_多文件和 makefile工程管理.md\":\"BDridL3y\",\"it-learning_c__基础_30_c__大型项目cmake工程管理.md\":\"DOufPDWu\",\"it-learning_c__基础_31_c__ 主要就业方向与技术能力分析报告.md\":\"AOV7PjqE\",\"it-learning_c__基础_32_c__ 基础知识回顾.md\":\"A-5IxrZu\",\"it-learning_c__基础_index.md\":\"BTWU73w9\",\"it-learning_index.md\":\"B2k5NgZE\",\"it-learning_java_01.java-se.md\":\"CV9z-Ph2\",\"it-learning_java_02.sql.md\":\"gilr6jOh\",\"it-learning_java_03.java-web.md\":\"DXRkrx3K\",\"it-learning_java_05.mybatis.md\":\"HjU_MlJz\",\"it-learning_java_index.md\":\"DkRBc8cW\",\"it-learning_linux_01.linux基础.md\":\"BBs1RcBf\",\"it-learning_linux_02.shell.md\":\"BCfUk33T\",\"it-learning_linux_03.mpi并行计算.md\":\"gpRl9TGp\",\"it-learning_linux_04.docker.md\":\"DN_C173y\",\"it-learning_linux_index.md\":\"BVALaZgc\",\"it-learning_计算机图形学_02.直线光栅化.md\":\"C0uFebpu\",\"it-learning_计算机图形学_index.md\":\"Be4gO7hA\",\"readme.md\":\"m88s5xF2\",\"update_更新日志.md\":\"BVtQLf5o\",\"我的感悟_2024_index.md\":\"DNiQxEmG\",\"我的感悟_2024_不同商家的视野.md\":\"5Pu9Bj7-\",\"我的感悟_2024_学而篇.md\":\"C-hCBgCG\",\"我的感悟_2024_重温士兵突击.md\":\"C2-8pk9-\",\"我的感悟_2025_index.md\":\"DfcTEU3d\",\"我的感悟_2026_index.md\":\"ByieJetu\",\"我的感悟_index.md\":\"CridsN6k\",\"技术问题清单_doccano账户管理.md\":\"Ba1HQxQq\",\"技术问题清单_index.md\":\"-lH6BxFw\",\"技术问题清单_专英翻转课堂—pytorch.md\":\"Dodyf68o\",\"技术问题清单_虚拟机网络问题.md\":\"B5YN-iPU\",\"生活与算法_index.md\":\"Bz6rRKIq\",\"生活与算法_贪心算法_1.人的本性——贪心！.md\":\"ByHAGPXB\",\"生活与算法_贪心算法_2.初步感受贪心.md\":\"BKL_n967\",\"生活与算法_贪心算法_3. 分发饼干问题.md\":\"D-sOMxS6\",\"面试求职_java面经_index.md\":\"Dt5psx-c\",\"面试求职_场景问题_index.md\":\"Bq1aCqXq\",\"面试求职_算法岗_index.md\":\"B0mkLyz0\",\"面试求职_经验分享_index.md\":\"7Buq_LXu\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"zh-CN\",\"dir\":\"ltr\",\"title\":\"码医森\",\"description\":\"计算机知识的学习站点\",\"base\":\"/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"siteTitle\":\"CoderEthan学习站\",\"logo\":\"/imgs/home-page-logo.svg\",\"outline\":{\"label\":\"本文目录\",\"level\":[2,4]},\"search\":{\"provider\":\"local\"},\"socialLinks\":[{\"icon\":{\"svg\":\"<svg xmlns=\\\"http://www.w3.org/2000/svg\\\" viewBox=\\\"0 0 496 512\\\"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path fill=\\\"#6841d2\\\" d=\\\"M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3 .3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5 .3-6.2 2.3zm44.2-1.7c-2.9 .7-4.9 2.6-4.6 4.9 .3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3 .7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3 .3 2.9 2.3 3.9 1.6 1 3.6 .7 4.3-.7 .7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3 .7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3 .7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z\\\"/></svg>\"},\"link\":\"https://github.com/ethanliu6/\"},{\"icon\":{\"svg\":\"<svg xmlns=\\\"http://www.w3.org/2000/svg\\\" viewBox=\\\"0 0 512 512\\\"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path fill=\\\"#6841d2\\\" d=\\\"M488.6 104.1C505.3 122.2 513 143.8 511.9 169.8V372.2C511.5 398.6 502.7 420.3 485.4 437.3C468.2 454.3 446.3 463.2 419.9 464H92C65.6 463.2 43.8 454.2 26.7 436.8C9.7 419.4 .8 396.5 0 368.2V169.8C.8 143.8 9.7 122.2 26.7 104.1C43.8 87.8 65.6 78.8 92 78H121.4L96.1 52.2C90.3 46.5 87.4 39.2 87.4 30.4C87.4 21.6 90.3 14.3 96.1 8.6C101.8 2.9 109.1 0 117.9 0C126.7 0 134 2.9 139.8 8.6L213.1 78H301.1L375.6 8.6C381.7 2.9 389.2 0 398 0C406.8 0 414.1 2.9 419.9 8.6C425.6 14.3 428.5 21.6 428.5 30.4C428.5 39.2 425.6 46.5 419.9 52.2L394.6 78L423.9 78C450.3 78.8 471.9 87.8 488.6 104.1H488.6zM449.8 173.8C449.4 164.2 446.1 156.4 439.1 150.3C433.9 144.2 425.1 140.9 416.4 140.5H96.1C86.5 140.9 78.6 144.2 72.5 150.3C66.3 156.4 63.1 164.2 62.7 173.8V368.2C62.7 377.4 66 385.2 72.5 391.7C79 398.2 86.9 401.5 96.1 401.5H416.4C425.6 401.5 433.4 398.2 439.7 391.7C446 385.2 449.4 377.4 449.8 368.2L449.8 173.8zM185.5 216.5C191.8 222.8 195.2 230.6 195.6 239.7V273C195.2 282.2 191.9 289.9 185.8 296.2C179.6 302.5 171.8 305.7 162.2 305.7C152.6 305.7 144.7 302.5 138.6 296.2C132.5 289.9 129.2 282.2 128.8 273V239.7C129.2 230.6 132.6 222.8 138.9 216.5C145.2 210.2 152.1 206.9 162.2 206.5C171.4 206.9 179.2 210.2 185.5 216.5H185.5zM377 216.5C383.3 222.8 386.7 230.6 387.1 239.7V273C386.7 282.2 383.4 289.9 377.3 296.2C371.2 302.5 363.3 305.7 353.7 305.7C344.1 305.7 336.3 302.5 330.1 296.2C323.1 289.9 320.7 282.2 320.4 273V239.7C320.7 230.6 324.1 222.8 330.4 216.5C336.7 210.2 344.5 206.9 353.7 206.5C362.9 206.9 370.7 210.2 377 216.5H377z\\\"/></svg>\"},\"link\":\"https://space.bilibili.com/1327099977/\"}],\"nav\":[{\"text\":\"主页\",\"link\":\"/\"},{\"text\":\"博客指南\",\"link\":\"/guide/\"},{\"text\":\"生活与算法\",\"link\":\"/生活与算法/\"},{\"text\":\"AI\",\"items\":[{\"text\":\"DL基础理论\",\"link\":\"/AI/deep_learning_theory/\"},{\"text\":\"Transformer系列\",\"link\":\"/AI/Transformer/\"}]},{\"text\":\"计算机学科内容\",\"items\":[{\"text\":\"计算机知识\",\"link\":\"/IT-learning/408知识/\"},{\"text\":\"C++基础\",\"link\":\"/IT-learning/c++基础/\"},{\"text\":\"Java后端\",\"link\":\"/IT-learning/Java/\"},{\"text\":\"Linux技术\",\"link\":\"/IT-learning/Linux/\"},{\"text\":\"计算机图形学\",\"link\":\"/IT-learning/计算机图形学/\"}]},{\"text\":\"自我提升\",\"items\":[{\"text\":\"阅读\",\"link\":\"/improve/阅读/\"},{\"text\":\"金融投资\",\"link\":\"/improve/金融投资/\"}]},{\"text\":\"求职面试\",\"items\":[{\"text\":\"Java面经\",\"link\":\"/面试求职/Java面经/\"},{\"text\":\"场景问题\",\"link\":\"/面试求职/场景问题/\"},{\"text\":\"经验分享\",\"link\":\"/面试求职/经验分享/\"},{\"text\":\"算法岗\",\"link\":\"/面试求职/算法岗/\"}]},{\"text\":\"其他维护\",\"items\":[{\"text\":\"站点更新\",\"link\":\"/update/更新日志\"},{\"text\":\"问题清单\",\"link\":\"/技术问题清单/\"}]},{\"text\":\"感悟和日常\",\"items\":[{\"text\":\"站长感悟\",\"link\":\"/我的感悟/\"},{\"text\":\"站长旧版博客\",\"link\":\"https://EthanLiu6.github.io\"}]}],\"footer\":{\"message\":\"ICP备案号: <a href=\\\"https://beian.miit.gov.cn/\\\" target=\\\"_blank\\\">蜀ICP备2024103116号</a><br>公安备案号: <a href=\\\"https://beian.mps.gov.cn/#/query/webSearch?code=51012202001928\\\" rel=\\\"noreferrer\\\" target=\\\"_blank\\\">川公网安备51012202001928</a>\",\"copyright\":\"版权所有 © 2024-present  <a href=\\\"mailto:16693226842@163.com\\\" target=\\\"_blank\\\">Ethan.Liu</a>\"},\"editLink\":{\"pattern\":\"https://github.com/EthanLiu6/coderethan.fun/edit/master/docs/:path\",\"text\":\"在 GitHub 上编辑此页面 OR 提出修改意见\"},\"lastUpdated\":{\"text\":\"最后更新于\",\"formatOptions\":{\"dateStyle\":\"long\",\"timeStyle\":\"short\"}},\"docFooter\":{\"prev\":\"上一篇\",\"next\":\"下一篇\"},\"darkModeSwitchLabel\":\"深色模式\",\"lightModeSwitchTitle\":\"切换到浅色模式\",\"darkModeSwitchTitle\":\"切换到深色模式\",\"sidebar\":{\"/AI/\":[{\"items\":[{\"text\":\"Transformer\",\"items\":[{\"text\":\"01-注意力机制\",\"link\":\"/AI/Transformer/01-注意力机制.html\"},{\"text\":\"02-总体架构\",\"link\":\"/AI/Transformer/02-总体架构.html\"},{\"text\":\"03-数据处理\",\"link\":\"/AI/Transformer/03-数据处理.html\"},{\"text\":\"04-编码器 & 解码器\",\"link\":\"/AI/Transformer/04-编码器 & 解码器.html\"},{\"text\":\"05-训练&推理\",\"link\":\"/AI/Transformer/05-训练&推理.html\"},{\"text\":\"06-token\",\"link\":\"/AI/Transformer/06-token.html\"},{\"text\":\"07-embedding\",\"link\":\"/AI/Transformer/07-embedding.html\"},{\"text\":\"08-位置编码\",\"link\":\"/AI/Transformer/08-位置编码.html\"},{\"text\":\"09-位置编码分类\",\"link\":\"/AI/Transformer/09-位置编码分类.html\"},{\"text\":\"10-自注意力\",\"link\":\"/AI/Transformer/10-自注意力.html\"},{\"text\":\"11-掩码\",\"link\":\"/AI/Transformer/11-掩码.html\"},{\"text\":\"12-多头自注意力\",\"link\":\"/AI/Transformer/12-多头自注意力.html\"},{\"text\":\"13- FFN\",\"link\":\"/AI/Transformer/13- FFN.html\"},{\"text\":\"14-残差网络和归一化\",\"link\":\"/AI/Transformer/14-残差网络和归一化.html\"}],\"collapsed\":false},{\"text\":\"deep_learning_theory\",\"items\":[{\"text\":\"00-DL_Base_Notes\",\"link\":\"/AI/deep_learning_theory/00-DL_Base_Notes.html\"},{\"text\":\"01-feedforward_network\",\"link\":\"/AI/deep_learning_theory/01-feedforward_network.html\"},{\"text\":\"02-back_propagation\",\"link\":\"/AI/deep_learning_theory/02-back_propagation.html\"},{\"text\":\"03-bp_example_demo\",\"link\":\"/AI/deep_learning_theory/03-bp_example_demo.html\"},{\"text\":\"04-convolution_neural_network\",\"link\":\"/AI/deep_learning_theory/04-convolution_neural_network.html\"},{\"text\":\"05-deep_learning_model\",\"link\":\"/AI/deep_learning_theory/05-deep_learning_model.html\"},{\"text\":\"06-pytorch_install\",\"link\":\"/AI/deep_learning_theory/06-pytorch_install.html\"},{\"text\":\"07-operators\",\"link\":\"/AI/deep_learning_theory/07-operators.html\"},{\"text\":\"08-activation_functions\",\"link\":\"/AI/deep_learning_theory/08-activation_functions.html\"},{\"text\":\"09-recurrent_neural_network\",\"link\":\"/AI/deep_learning_theory/09-recurrent_neural_network.html\"},{\"text\":\"10-seq2seq\",\"link\":\"/AI/deep_learning_theory/10-seq2seq.html\"},{\"text\":\"11-1attentions\",\"link\":\"/AI/deep_learning_theory/11-1attentions.html\"},{\"text\":\"11-2attention-extension\",\"link\":\"/AI/deep_learning_theory/11-2attention-extension.html\"},{\"text\":\"12-weight-initialization\",\"link\":\"/AI/deep_learning_theory/12-weight-initialization.html\"},{\"text\":\"13-optimizers\",\"link\":\"/AI/deep_learning_theory/13-optimizers.html\"},{\"text\":\"14-regularization\",\"link\":\"/AI/deep_learning_theory/14-regularization.html\"},{\"text\":\"15-deep-learning-tuning-guide\",\"link\":\"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html\"},{\"text\":\"20-pytorch-tensor\",\"link\":\"/AI/deep_learning_theory/20-pytorch-tensor.html\"},{\"text\":\"21-pytorch-autograd\",\"link\":\"/AI/deep_learning_theory/21-pytorch-autograd.html\"},{\"text\":\"22-pytorch-module\",\"link\":\"/AI/deep_learning_theory/22-pytorch-module.html\"},{\"text\":\"23-1training-example-1\",\"link\":\"/AI/deep_learning_theory/23-1training-example-1.html\"},{\"text\":\"23-2decoder\",\"link\":\"/AI/deep_learning_theory/23-2decoder.html\"},{\"text\":\"23-3encoder\",\"link\":\"/AI/deep_learning_theory/23-3encoder.html\"},{\"text\":\"23-4transformer\",\"link\":\"/AI/deep_learning_theory/23-4transformer.html\"},{\"text\":\"24-pytorch-optimizer\",\"link\":\"/AI/deep_learning_theory/24-pytorch-optimizer.html\"},{\"text\":\"25-pytorch-lr-scheduler\",\"link\":\"/AI/deep_learning_theory/25-pytorch-lr-scheduler.html\"},{\"text\":\"26-pytorch-dataloader\",\"link\":\"/AI/deep_learning_theory/26-pytorch-dataloader.html\"},{\"text\":\"27-pytorch-model-save\",\"link\":\"/AI/deep_learning_theory/27-pytorch-model-save.html\"},{\"text\":\"28-pytorch-tensorboard\",\"link\":\"/AI/deep_learning_theory/28-pytorch-tensorboard.html\"},{\"text\":\"29-pytorch-graph-mode\",\"link\":\"/AI/deep_learning_theory/29-pytorch-graph-mode.html\"},{\"text\":\"30-3main\",\"link\":\"/AI/deep_learning_theory/30-3main.html\"},{\"text\":\"30-training-example-2\",\"link\":\"/AI/deep_learning_theory/30-training-example-2.html\"},{\"text\":\"40-ner\",\"link\":\"/AI/deep_learning_theory/40-ner.html\"},{\"text\":\"41-question-answering\",\"link\":\"/AI/deep_learning_theory/41-question-answering.html\"},{\"text\":\"42-1stable-diffusion\",\"link\":\"/AI/deep_learning_theory/42-1stable-diffusion.html\"},{\"text\":\"42-2SDXL\",\"link\":\"/AI/deep_learning_theory/42-2SDXL.html\"},{\"text\":\"42-3VAE\",\"link\":\"/AI/deep_learning_theory/42-3VAE.html\"},{\"text\":\"44-scaling-law\",\"link\":\"/AI/deep_learning_theory/44-scaling-law.html\"},{\"text\":\"45-distribute-training\",\"link\":\"/AI/deep_learning_theory/45-distribute-training.html\"},{\"text\":\"46-nlp-llama\",\"link\":\"/AI/deep_learning_theory/46-nlp-llama.html\"},{\"text\":\"47-nlp-deepseek\",\"link\":\"/AI/deep_learning_theory/47-nlp-deepseek.html\"}],\"collapsed\":false}]}],\"/IT-learning/\":[{\"items\":[{\"text\":\"408知识\",\"items\":[{\"text\":\"OS-4.1 进程同步\",\"link\":\"/IT-learning/408知识/OS-4.1 进程同步.html\"},{\"text\":\"OS-4.4 信号量机制\",\"link\":\"/IT-learning/408知识/OS-4.4 信号量机制.html\"},{\"text\":\"OS-4.4 信号量机制pv操作之“可见”\",\"link\":\"/IT-learning/408知识/OS-4.4 信号量机制pv操作之“可见”.html\"}],\"collapsed\":false},{\"text\":\"Java\",\"items\":[{\"text\":\"01.java-se\",\"link\":\"/IT-learning/Java/01.java-se.html\"},{\"text\":\"02.sql\",\"link\":\"/IT-learning/Java/02.sql.html\"},{\"text\":\"03.java-web\",\"link\":\"/IT-learning/Java/03.java-web.html\"},{\"text\":\"05.MyBatis\",\"link\":\"/IT-learning/Java/05.MyBatis.html\"}],\"collapsed\":false},{\"text\":\"Linux\",\"items\":[{\"text\":\"01.Linux基础\",\"link\":\"/IT-learning/Linux/01.Linux基础.html\"},{\"text\":\"02.Shell\",\"link\":\"/IT-learning/Linux/02.Shell.html\"},{\"text\":\"03.MPI并行计算\",\"link\":\"/IT-learning/Linux/03.MPI并行计算.html\"},{\"text\":\"04.Docker\",\"link\":\"/IT-learning/Linux/04.Docker.html\"}],\"collapsed\":false},{\"text\":\"c++基础\",\"items\":[{\"text\":\"01_开发环境搭建与基础数据类型\",\"link\":\"/IT-learning/c++基础/01_开发环境搭建与基础数据类型.html\"},{\"text\":\"02_控制流语句与复合数据类型\",\"link\":\"/IT-learning/c++基础/02_控制流语句与复合数据类型.html\"},{\"text\":\"03_指针与引用\",\"link\":\"/IT-learning/c++基础/03_指针与引用.html\"},{\"text\":\"04_自定义数据类型与函数\",\"link\":\"/IT-learning/c++基础/04_自定义数据类型与函数.html\"},{\"text\":\"05_头文件与指针的算术运算\",\"link\":\"/IT-learning/c++基础/05_头文件与指针的算术运算.html\"},{\"text\":\"06_字符串、数组、指针与函数\",\"link\":\"/IT-learning/c++基础/06_字符串、数组、指针与函数.html\"},{\"text\":\"07_函数进阶与内存管理\",\"link\":\"/IT-learning/c++基础/07_函数进阶与内存管理.html\"},{\"text\":\"08_运算符优先级表\",\"link\":\"/IT-learning/c++基础/08_运算符优先级表.html\"},{\"text\":\"09_指针、内存管理和类的基础\",\"link\":\"/IT-learning/c++基础/09_指针、内存管理和类的基础.html\"},{\"text\":\"10_深入类和对象\",\"link\":\"/IT-learning/c++基础/10_深入类和对象.html\"},{\"text\":\"11_类的大小、继承与权限控制\",\"link\":\"/IT-learning/c++基础/11_类的大小、继承与权限控制.html\"},{\"text\":\"12_继承进阶\",\"link\":\"/IT-learning/c++基础/12_继承进阶.html\"},{\"text\":\"13_类型转换和多态与虚函数\",\"link\":\"/IT-learning/c++基础/13_类型转换和多态与虚函数.html\"},{\"text\":\"14_纯虚函数、抽象类、深浅拷贝及智能指针\",\"link\":\"/IT-learning/c++基础/14_纯虚函数、抽象类、深浅拷贝及智能指针.html\"},{\"text\":\"15_运算符重载与 String 类详解\",\"link\":\"/IT-learning/c++基础/15_运算符重载与 String 类详解.html\"},{\"text\":\"16_有序容器与无序容器\",\"link\":\"/IT-learning/c++基础/16_有序容器与无序容器.html\"},{\"text\":\"17_模板\",\"link\":\"/IT-learning/c++基础/17_模板.html\"},{\"text\":\"18_迭代器与其应用\",\"link\":\"/IT-learning/c++基础/18_迭代器与其应用.html\"},{\"text\":\"19_C++ 标准库常用算法\",\"link\":\"/IT-learning/c++基础/19_C++ 标准库常用算法.html\"},{\"text\":\"20_C++ 异常处理 - 第19次课\",\"link\":\"/IT-learning/c++基础/20_C++ 异常处理 - 第19次课.html\"},{\"text\":\"21_友元及友元相关内容\",\"link\":\"/IT-learning/c++基础/21_友元及友元相关内容.html\"},{\"text\":\"22_C++ IO 流详解-feadbc607d7f\",\"link\":\"/IT-learning/c++基础/22_C++ IO 流详解-feadbc607d7f.html\"},{\"text\":\"23_C++ IO 流详解\",\"link\":\"/IT-learning/c++基础/23_C++ IO 流详解.html\"},{\"text\":\"24_位运算符总结\",\"link\":\"/IT-learning/c++基础/24_位运算符总结.html\"},{\"text\":\"25_C++三种继承方式\",\"link\":\"/IT-learning/c++基础/25_C++三种继承方式.html\"},{\"text\":\"26_C++11 高级特性\",\"link\":\"/IT-learning/c++基础/26_C++11 高级特性.html\"},{\"text\":\"27_C++14 新特性\",\"link\":\"/IT-learning/c++基础/27_C++14 新特性.html\"},{\"text\":\"28_C++17 新特性\",\"link\":\"/IT-learning/c++基础/28_C++17 新特性.html\"},{\"text\":\"29_多文件和 Makefile工程管理\",\"link\":\"/IT-learning/c++基础/29_多文件和 Makefile工程管理.html\"},{\"text\":\"30_C++大型项目CMake工程管理\",\"link\":\"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html\"},{\"text\":\"31_C++ 主要就业方向与技术能力分析报告\",\"link\":\"/IT-learning/c++基础/31_C++ 主要就业方向与技术能力分析报告.html\"},{\"text\":\"32_C++ 基础知识回顾\",\"link\":\"/IT-learning/c++基础/32_C++ 基础知识回顾.html\"}],\"collapsed\":false},{\"text\":\"计算机图形学\",\"items\":[{\"text\":\"02.直线光栅化\",\"link\":\"/IT-learning/计算机图形学/02.直线光栅化.html\"}],\"collapsed\":false}]}],\"/guide/\":[{\"items\":[{\"text\":\"博客结构\",\"link\":\"/guide/博客结构.html\"}]}],\"/update/\":[{\"items\":[{\"text\":\"更新日志\",\"link\":\"/update/更新日志.html\"}]}],\"/我的感悟/\":[{\"items\":[{\"text\":\"2024\",\"items\":[{\"text\":\"不同商家的视野\",\"link\":\"/我的感悟/2024/不同商家的视野.html\"},{\"text\":\"学而篇\",\"link\":\"/我的感悟/2024/学而篇.html\"},{\"text\":\"重温士兵突击\",\"link\":\"/我的感悟/2024/重温士兵突击.html\"}],\"collapsed\":false}]}],\"/技术问题清单/\":[{\"items\":[{\"text\":\"doccano账户管理\",\"link\":\"/技术问题清单/doccano账户管理.html\"},{\"text\":\"专英翻转课堂—PyTorch\",\"link\":\"/技术问题清单/专英翻转课堂—PyTorch.html\"},{\"text\":\"虚拟机网络问题\",\"link\":\"/技术问题清单/虚拟机网络问题.html\"}]}],\"/生活与算法/\":[{\"items\":[{\"text\":\"贪心算法\",\"items\":[{\"text\":\"1.人的本性——贪心！\",\"link\":\"/生活与算法/贪心算法/1.人的本性——贪心！.html\"},{\"text\":\"2.初步感受贪心\",\"link\":\"/生活与算法/贪心算法/2.初步感受贪心.html\"},{\"text\":\"3. 分发饼干问题\",\"link\":\"/生活与算法/贪心算法/3. 分发饼干问题.html\"}],\"collapsed\":false}]}]}},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>