<!DOCTYPE html>
<html lang="zh-CN" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>一. DL_Base_Notes | 码医森</title>
    <meta name="description" content="计算机知识的学习站点">
    <meta name="generator" content="VitePress v1.3.4">
    <link rel="preload stylesheet" href="/assets/style.CflK-Lwn.css" as="style">
    
    <script type="module" src="/assets/app.CP5QrAO4.js"></script>
    <link rel="preload" href="/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/assets/chunks/theme.DC78O2cf.js">
    <link rel="modulepreload" href="/assets/chunks/framework.DA-Pb-tg.js">
    <link rel="modulepreload" href="/assets/AI_deep_learning_theory_00-DL_Base_Notes.md.CAdB7Lmh.lean.js">
    <link rel="icon" type="image/svg+xml" href="/imgs/home-page-logo.svg">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css" crossorigin="">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-5d98c3a5><!--[--><!--]--><!--[--><span tabindex="-1" data-v-0f60ec36></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-0f60ec36> Skip to content </a><!--]--><!----><header class="VPNav" data-v-5d98c3a5 data-v-ae24b3ad><div class="VPNavBar" data-v-ae24b3ad data-v-6aa21345><div class="wrapper" data-v-6aa21345><div class="container" data-v-6aa21345><div class="title" data-v-6aa21345><div class="VPNavBarTitle has-sidebar" data-v-6aa21345 data-v-ab179fa1><a class="title" href="/" data-v-ab179fa1><!--[--><!--]--><!--[--><img class="VPImage logo" src="/imgs/home-page-logo.svg" alt data-v-8426fc1a><!--]--><span data-v-ab179fa1>CoderEthan学习站</span><!--[--><!--]--></a></div></div><div class="content" data-v-6aa21345><div class="content-body" data-v-6aa21345><!--[--><!--]--><div class="VPNavBarSearch search" data-v-6aa21345><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-6aa21345 data-v-dc692963><span id="main-nav-aria-label" class="visually-hidden" data-v-dc692963> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/" tabindex="0" data-v-dc692963 data-v-9c663999><!--[--><span data-v-9c663999>主页</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/guide/" tabindex="0" data-v-dc692963 data-v-9c663999><!--[--><span data-v-9c663999>博客指南</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/生活与算法/" tabindex="0" data-v-dc692963 data-v-9c663999><!--[--><span data-v-9c663999>生活与算法</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-dc692963 data-v-b6c34ac9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-b6c34ac9><span class="text" data-v-b6c34ac9><!----><span data-v-b6c34ac9>AI</span><span class="vpi-chevron-down text-icon" data-v-b6c34ac9></span></span></button><div class="menu" data-v-b6c34ac9><div class="VPMenu" data-v-b6c34ac9 data-v-b98bc113><div class="items" data-v-b98bc113><!--[--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/AI/deep_learning_theory/" data-v-43f1e123><!--[-->DL基础理论<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/AI/Transformer/" data-v-43f1e123><!--[-->Transformer系列<!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-dc692963 data-v-b6c34ac9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-b6c34ac9><span class="text" data-v-b6c34ac9><!----><span data-v-b6c34ac9>计算机学科内容</span><span class="vpi-chevron-down text-icon" data-v-b6c34ac9></span></span></button><div class="menu" data-v-b6c34ac9><div class="VPMenu" data-v-b6c34ac9 data-v-b98bc113><div class="items" data-v-b98bc113><!--[--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/IT-learning/408知识/" data-v-43f1e123><!--[-->计算机知识<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/IT-learning/c++基础/" data-v-43f1e123><!--[-->C++基础<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/IT-learning/Java/" data-v-43f1e123><!--[-->Java后端<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/IT-learning/Linux/" data-v-43f1e123><!--[-->Linux技术<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/IT-learning/计算机图形学/" data-v-43f1e123><!--[-->计算机图形学<!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-dc692963 data-v-b6c34ac9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-b6c34ac9><span class="text" data-v-b6c34ac9><!----><span data-v-b6c34ac9>自我提升</span><span class="vpi-chevron-down text-icon" data-v-b6c34ac9></span></span></button><div class="menu" data-v-b6c34ac9><div class="VPMenu" data-v-b6c34ac9 data-v-b98bc113><div class="items" data-v-b98bc113><!--[--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/improve/阅读/" data-v-43f1e123><!--[-->阅读<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/improve/金融投资/" data-v-43f1e123><!--[-->金融投资<!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-dc692963 data-v-b6c34ac9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-b6c34ac9><span class="text" data-v-b6c34ac9><!----><span data-v-b6c34ac9>求职面试</span><span class="vpi-chevron-down text-icon" data-v-b6c34ac9></span></span></button><div class="menu" data-v-b6c34ac9><div class="VPMenu" data-v-b6c34ac9 data-v-b98bc113><div class="items" data-v-b98bc113><!--[--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/面试求职/Java面经/" data-v-43f1e123><!--[-->Java面经<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/面试求职/场景问题/" data-v-43f1e123><!--[-->场景问题<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/面试求职/经验分享/" data-v-43f1e123><!--[-->经验分享<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/面试求职/算法岗/" data-v-43f1e123><!--[-->算法岗<!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-dc692963 data-v-b6c34ac9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-b6c34ac9><span class="text" data-v-b6c34ac9><!----><span data-v-b6c34ac9>其他维护</span><span class="vpi-chevron-down text-icon" data-v-b6c34ac9></span></span></button><div class="menu" data-v-b6c34ac9><div class="VPMenu" data-v-b6c34ac9 data-v-b98bc113><div class="items" data-v-b98bc113><!--[--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/update/%E6%9B%B4%E6%96%B0%E6%97%A5%E5%BF%97.html" data-v-43f1e123><!--[-->站点更新<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/技术问题清单/" data-v-43f1e123><!--[-->问题清单<!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-dc692963 data-v-b6c34ac9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-b6c34ac9><span class="text" data-v-b6c34ac9><!----><span data-v-b6c34ac9>感悟和日常</span><span class="vpi-chevron-down text-icon" data-v-b6c34ac9></span></span></button><div class="menu" data-v-b6c34ac9><div class="VPMenu" data-v-b6c34ac9 data-v-b98bc113><div class="items" data-v-b98bc113><!--[--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link" href="/我的感悟/" data-v-43f1e123><!--[-->站长感悟<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-43f1e123><a class="VPLink link vp-external-link-icon" href="https://EthanLiu6.github.io" target="_blank" rel="noreferrer" data-v-43f1e123><!--[-->站长旧版博客<!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-6aa21345 data-v-6c893767><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-6c893767 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-6aa21345 data-v-0394ad82 data-v-7bc22406><!--[--><a class="VPSocialLink no-icon" href="https://github.com/ethanliu6/" aria-label target="_blank" rel="noopener" data-v-7bc22406 data-v-eee4e7cb><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path fill="#6841d2" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3 .3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5 .3-6.2 2.3zm44.2-1.7c-2.9 .7-4.9 2.6-4.6 4.9 .3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3 .7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3 .3 2.9 2.3 3.9 1.6 1 3.6 .7 4.3-.7 .7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3 .7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3 .7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></a><a class="VPSocialLink no-icon" href="https://space.bilibili.com/1327099977/" aria-label target="_blank" rel="noopener" data-v-7bc22406 data-v-eee4e7cb><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path fill="#6841d2" d="M488.6 104.1C505.3 122.2 513 143.8 511.9 169.8V372.2C511.5 398.6 502.7 420.3 485.4 437.3C468.2 454.3 446.3 463.2 419.9 464H92C65.6 463.2 43.8 454.2 26.7 436.8C9.7 419.4 .8 396.5 0 368.2V169.8C.8 143.8 9.7 122.2 26.7 104.1C43.8 87.8 65.6 78.8 92 78H121.4L96.1 52.2C90.3 46.5 87.4 39.2 87.4 30.4C87.4 21.6 90.3 14.3 96.1 8.6C101.8 2.9 109.1 0 117.9 0C126.7 0 134 2.9 139.8 8.6L213.1 78H301.1L375.6 8.6C381.7 2.9 389.2 0 398 0C406.8 0 414.1 2.9 419.9 8.6C425.6 14.3 428.5 21.6 428.5 30.4C428.5 39.2 425.6 46.5 419.9 52.2L394.6 78L423.9 78C450.3 78.8 471.9 87.8 488.6 104.1H488.6zM449.8 173.8C449.4 164.2 446.1 156.4 439.1 150.3C433.9 144.2 425.1 140.9 416.4 140.5H96.1C86.5 140.9 78.6 144.2 72.5 150.3C66.3 156.4 63.1 164.2 62.7 173.8V368.2C62.7 377.4 66 385.2 72.5 391.7C79 398.2 86.9 401.5 96.1 401.5H416.4C425.6 401.5 433.4 398.2 439.7 391.7C446 385.2 449.4 377.4 449.8 368.2L449.8 173.8zM185.5 216.5C191.8 222.8 195.2 230.6 195.6 239.7V273C195.2 282.2 191.9 289.9 185.8 296.2C179.6 302.5 171.8 305.7 162.2 305.7C152.6 305.7 144.7 302.5 138.6 296.2C132.5 289.9 129.2 282.2 128.8 273V239.7C129.2 230.6 132.6 222.8 138.9 216.5C145.2 210.2 152.1 206.9 162.2 206.5C171.4 206.9 179.2 210.2 185.5 216.5H185.5zM377 216.5C383.3 222.8 386.7 230.6 387.1 239.7V273C386.7 282.2 383.4 289.9 377.3 296.2C371.2 302.5 363.3 305.7 353.7 305.7C344.1 305.7 336.3 302.5 330.1 296.2C323.1 289.9 320.7 282.2 320.4 273V239.7C320.7 230.6 324.1 222.8 330.4 216.5C336.7 210.2 344.5 206.9 353.7 206.5C362.9 206.9 370.7 210.2 377 216.5H377z"/></svg></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-6aa21345 data-v-bb2aa2f0 data-v-b6c34ac9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-b6c34ac9><span class="vpi-more-horizontal icon" data-v-b6c34ac9></span></button><div class="menu" data-v-b6c34ac9><div class="VPMenu" data-v-b6c34ac9 data-v-b98bc113><!----><!--[--><!--[--><!----><div class="group" data-v-bb2aa2f0><div class="item appearance" data-v-bb2aa2f0><p class="label" data-v-bb2aa2f0>深色模式</p><div class="appearance-action" data-v-bb2aa2f0><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-bb2aa2f0 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div></div></div><div class="group" data-v-bb2aa2f0><div class="item social-links" data-v-bb2aa2f0><div class="VPSocialLinks social-links-list" data-v-bb2aa2f0 data-v-7bc22406><!--[--><a class="VPSocialLink no-icon" href="https://github.com/ethanliu6/" aria-label target="_blank" rel="noopener" data-v-7bc22406 data-v-eee4e7cb><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path fill="#6841d2" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3 .3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5 .3-6.2 2.3zm44.2-1.7c-2.9 .7-4.9 2.6-4.6 4.9 .3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3 .7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3 .3 2.9 2.3 3.9 1.6 1 3.6 .7 4.3-.7 .7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3 .7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3 .7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></a><a class="VPSocialLink no-icon" href="https://space.bilibili.com/1327099977/" aria-label target="_blank" rel="noopener" data-v-7bc22406 data-v-eee4e7cb><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path fill="#6841d2" d="M488.6 104.1C505.3 122.2 513 143.8 511.9 169.8V372.2C511.5 398.6 502.7 420.3 485.4 437.3C468.2 454.3 446.3 463.2 419.9 464H92C65.6 463.2 43.8 454.2 26.7 436.8C9.7 419.4 .8 396.5 0 368.2V169.8C.8 143.8 9.7 122.2 26.7 104.1C43.8 87.8 65.6 78.8 92 78H121.4L96.1 52.2C90.3 46.5 87.4 39.2 87.4 30.4C87.4 21.6 90.3 14.3 96.1 8.6C101.8 2.9 109.1 0 117.9 0C126.7 0 134 2.9 139.8 8.6L213.1 78H301.1L375.6 8.6C381.7 2.9 389.2 0 398 0C406.8 0 414.1 2.9 419.9 8.6C425.6 14.3 428.5 21.6 428.5 30.4C428.5 39.2 425.6 46.5 419.9 52.2L394.6 78L423.9 78C450.3 78.8 471.9 87.8 488.6 104.1H488.6zM449.8 173.8C449.4 164.2 446.1 156.4 439.1 150.3C433.9 144.2 425.1 140.9 416.4 140.5H96.1C86.5 140.9 78.6 144.2 72.5 150.3C66.3 156.4 63.1 164.2 62.7 173.8V368.2C62.7 377.4 66 385.2 72.5 391.7C79 398.2 86.9 401.5 96.1 401.5H416.4C425.6 401.5 433.4 398.2 439.7 391.7C446 385.2 449.4 377.4 449.8 368.2L449.8 173.8zM185.5 216.5C191.8 222.8 195.2 230.6 195.6 239.7V273C195.2 282.2 191.9 289.9 185.8 296.2C179.6 302.5 171.8 305.7 162.2 305.7C152.6 305.7 144.7 302.5 138.6 296.2C132.5 289.9 129.2 282.2 128.8 273V239.7C129.2 230.6 132.6 222.8 138.9 216.5C145.2 210.2 152.1 206.9 162.2 206.5C171.4 206.9 179.2 210.2 185.5 216.5H185.5zM377 216.5C383.3 222.8 386.7 230.6 387.1 239.7V273C386.7 282.2 383.4 289.9 377.3 296.2C371.2 302.5 363.3 305.7 353.7 305.7C344.1 305.7 336.3 302.5 330.1 296.2C323.1 289.9 320.7 282.2 320.4 273V239.7C320.7 230.6 324.1 222.8 330.4 216.5C336.7 210.2 344.5 206.9 353.7 206.5C362.9 206.9 370.7 210.2 377 216.5H377z"/></svg></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-6aa21345 data-v-e5dd9c1c><span class="container" data-v-e5dd9c1c><span class="top" data-v-e5dd9c1c></span><span class="middle" data-v-e5dd9c1c></span><span class="bottom" data-v-e5dd9c1c></span></span></button></div></div></div></div><div class="divider" data-v-6aa21345><div class="divider-line" data-v-6aa21345></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-5d98c3a5 data-v-a6f0e41e><div class="container" data-v-a6f0e41e><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-a6f0e41e><span class="vpi-align-left menu-icon" data-v-a6f0e41e></span><span class="menu-text" data-v-a6f0e41e>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-a6f0e41e data-v-17a5e62e><button data-v-17a5e62e>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-5d98c3a5 data-v-319d5ca6><div class="curtain" data-v-319d5ca6></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-319d5ca6><span class="visually-hidden" id="sidebar-aria-label" data-v-319d5ca6> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-c40bc020><section class="VPSidebarItem level-0 has-active" data-v-c40bc020 data-v-b7550ba0><!----><div class="items" data-v-b7550ba0><!--[--><section class="VPSidebarItem level-1 collapsible" data-v-b7550ba0 data-v-b7550ba0><div class="item" role="button" tabindex="0" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><h3 class="text" data-v-b7550ba0>Transformer</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-b7550ba0><span class="vpi-chevron-right caret-icon" data-v-b7550ba0></span></div></div><div class="items" data-v-b7550ba0><!--[--><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/Transformer/01-注意力机制.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>01-注意力机制</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/Transformer/02-总体架构.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>02-总体架构</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/Transformer/03-数据处理.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>03-数据处理</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/Transformer/04-编码器 &amp; 解码器.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>04-编码器 & 解码器</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/Transformer/05-训练&amp;推理.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>05-训练&推理</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/Transformer/06-token.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>06-token</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/Transformer/07-embedding.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>07-embedding</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/Transformer/08-位置编码.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>08-位置编码</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/Transformer/09-位置编码分类.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>09-位置编码分类</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/Transformer/10-自注意力.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>10-自注意力</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/Transformer/11-掩码.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>11-掩码</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/Transformer/12-多头自注意力.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>12-多头自注意力</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/Transformer/13- FFN.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>13- FFN</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/Transformer/14-残差网络和归一化.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>14-残差网络和归一化</p><!--]--></a><!----></div><!----></div><!--]--></div></section><section class="VPSidebarItem level-1 collapsible has-active" data-v-b7550ba0 data-v-b7550ba0><div class="item" role="button" tabindex="0" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><h3 class="text" data-v-b7550ba0>deep_learning_theory</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-b7550ba0><span class="vpi-chevron-right caret-icon" data-v-b7550ba0></span></div></div><div class="items" data-v-b7550ba0><!--[--><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/00-DL_Base_Notes.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>00-DL_Base_Notes</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/01-feedforward_network.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>01-feedforward_network</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/02-back_propagation.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>02-back_propagation</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/03-bp_example_demo.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>03-bp_example_demo</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/04-convolution_neural_network.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>04-convolution_neural_network</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/05-deep_learning_model.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>05-deep_learning_model</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/06-pytorch_install.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>06-pytorch_install</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/07-operators.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>07-operators</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/08-activation_functions.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>08-activation_functions</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/09-recurrent_neural_network.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>09-recurrent_neural_network</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/10-seq2seq.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>10-seq2seq</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/11-1attentions.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>11-1attentions</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/11-2attention-extension.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>11-2attention-extension</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/12-weight-initialization.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>12-weight-initialization</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/13-optimizers.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>13-optimizers</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/14-regularization.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>14-regularization</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/15-deep-learning-tuning-guide.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>15-deep-learning-tuning-guide</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/20-pytorch-tensor.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>20-pytorch-tensor</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/21-pytorch-autograd.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>21-pytorch-autograd</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/22-pytorch-module.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>22-pytorch-module</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/23-1training-example-1.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>23-1training-example-1</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/23-2decoder.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>23-2decoder</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/23-3encoder.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>23-3encoder</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/23-4transformer.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>23-4transformer</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/24-pytorch-optimizer.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>24-pytorch-optimizer</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/25-pytorch-lr-scheduler.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>25-pytorch-lr-scheduler</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/26-pytorch-dataloader.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>26-pytorch-dataloader</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/27-pytorch-model-save.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>27-pytorch-model-save</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/28-pytorch-tensorboard.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>28-pytorch-tensorboard</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/29-pytorch-graph-mode.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>29-pytorch-graph-mode</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/30-3main.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>30-3main</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/30-training-example-2.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>30-training-example-2</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/40-ner.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>40-ner</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/41-question-answering.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>41-question-answering</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/42-1stable-diffusion.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>42-1stable-diffusion</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/42-2SDXL.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>42-2SDXL</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/42-3VAE.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>42-3VAE</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/44-scaling-law.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>44-scaling-law</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/45-distribute-training.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>45-distribute-training</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/46-nlp-llama.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>46-nlp-llama</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link link" href="/AI/deep_learning_theory/47-nlp-deepseek.html" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>47-nlp-deepseek</p><!--]--></a><!----></div><!----></div><!--]--></div></section><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-5d98c3a5 data-v-1428d186><div class="VPDoc has-sidebar has-aside" data-v-1428d186 data-v-39a288b8><!--[--><!--]--><div class="container" data-v-39a288b8><div class="aside" data-v-39a288b8><div class="aside-curtain" data-v-39a288b8></div><div class="aside-container" data-v-39a288b8><div class="aside-content" data-v-39a288b8><div class="VPDocAside" data-v-39a288b8 data-v-3f215769><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-3f215769 data-v-a5bbad30><div class="content" data-v-a5bbad30><div class="outline-marker" data-v-a5bbad30></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-a5bbad30>本文目录</div><ul class="VPDocOutlineItem root" data-v-a5bbad30 data-v-b933a997><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-3f215769></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-39a288b8><div class="content-container" data-v-39a288b8><!--[--><!--]--><main class="main" data-v-39a288b8><div style="position:relative;" class="vp-doc _AI_deep_learning_theory_00-DL_Base_Notes" data-v-39a288b8><div><h1 id="一-dl-base-notes" tabindex="-1">一. DL_Base_Notes <a class="header-anchor" href="#一-dl-base-notes" aria-label="Permalink to &quot;一. DL_Base_Notes&quot;">​</a></h1><h2 id="_1-🌟🌟🌟🌟🌟normalization" tabindex="-1">1. 🌟🌟🌟🌟🌟Normalization <a class="header-anchor" href="#_1-🌟🌟🌟🌟🌟normalization" aria-label="Permalink to &quot;1. 🌟🌟🌟🌟🌟Normalization&quot;">​</a></h2><p>Batch Norm，Layer Norm，Instance Norm，Group Norm</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi><mo>=</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mrow><mi mathvariant="normal">E</mi></mrow><mo>[</mo><mi>x</mi><mo>]</mo></mrow><mrow><msqrt><mrow><mrow><mi mathvariant="normal">V</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi></mrow><mo>[</mo><mi>x</mi><mo>]</mo><mo>+</mo><mi>ϵ</mi></mrow></msqrt></mrow></mfrac><mo>∗</mo><mi>γ</mi><mo>+</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.427em;"></span><span class="strut bottom" style="height:2.55701em;vertical-align:-1.13001em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mrel">=</span><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.825005em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="sqrt mord"><span class="sqrt-sign" style="top:-0.04500500000000007em;"><span class="style-wrap reset-textstyle textstyle uncramped"><span class="delimsizing size1">√</span></span></span><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="mord textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathrm" style="margin-right:0.01389em;">V</span><span class="mord mathrm">a</span><span class="mord mathrm">r</span></span><span class="mopen">[</span><span class="mord mathit">x</span><span class="mclose">]</span><span class="mbin">+</span><span class="mord mathit">ϵ</span></span></span><span style="top:-0.855005em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="reset-textstyle textstyle uncramped sqrt-line"></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span>​</span></span></span></span></span></span><span style="top:-0.22999999999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.677em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathit">x</span><span class="mbin">−</span><span class="mord textstyle uncramped"><span class="mord mathrm">E</span></span><span class="mopen">[</span><span class="mord mathit">x</span><span class="mclose">]</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mbin">∗</span><span class="mord mathit" style="margin-right:0.05556em;">γ</span><span class="mbin">+</span><span class="mord mathit" style="margin-right:0.05278em;">β</span></span></span></span></span></p><p><a href="https://blog.csdn.net/LoseInVain/article/details/86476010" target="_blank" rel="noreferrer">Batch Norm的技术博客</a></p><p><strong>思考：在训练和推理时有何不同？？？</strong></p><h2 id="_2-🌟🌟🌟activation" tabindex="-1">2. 🌟🌟🌟Activation <a class="header-anchor" href="#_2-🌟🌟🌟activation" aria-label="Permalink to &quot;2. 🌟🌟🌟Activation&quot;">​</a></h2><h3 id="_2-1-non-linear-activations-的两种类型" tabindex="-1">2.1 Non-linear Activations 的两种类型 <a class="header-anchor" href="#_2-1-non-linear-activations-的两种类型" aria-label="Permalink to &quot;2.1 Non-linear Activations 的两种类型&quot;">​</a></h3><p>一种是逐元素操作（Element wise 或者Point wise），eg:ReLU,Sigmoid,Tanh,等，另一种是操作对象（元素）之间具有相关性，eg.Softmax</p><h3 id="_2-2" tabindex="-1">2.2 <a class="header-anchor" href="#_2-2" aria-label="Permalink to &quot;2.2&quot;">​</a></h3><h2 id="_3-🌟loss-function" tabindex="-1">3. 🌟Loss Function <a class="header-anchor" href="#_3-🌟loss-function" aria-label="Permalink to &quot;3. 🌟Loss Function&quot;">​</a></h2><h2 id="_4-🌟🌟🌟🌟optimizer" tabindex="-1">4. 🌟🌟🌟🌟Optimizer <a class="header-anchor" href="#_4-🌟🌟🌟🌟optimizer" aria-label="Permalink to &quot;4. 🌟🌟🌟🌟Optimizer&quot;">​</a></h2><blockquote><p>动量后面的Admw那些据估计忘了</p></blockquote><h2 id="_5-🌟🌟🌟🌟🌟回顾attention" tabindex="-1">5. 🌟🌟🌟🌟🌟回顾Attention <a class="header-anchor" href="#_5-🌟🌟🌟🌟🌟回顾attention" aria-label="Permalink to &quot;5. 🌟🌟🌟🌟🌟回顾Attention&quot;">​</a></h2> Attention(Q, K, V ) = softmax(\frac{Q·K^T}{\sqrt{d_k}})V <h3 id="_5-1-为啥attention的时候要除以" tabindex="-1">5.1 为啥Attention的时候要除以<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msqrt><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{d_k}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8572200000000001em;"></span><span class="strut bottom" style="height:1.04em;vertical-align:-0.18278em;"></span><span class="base textstyle uncramped"><span class="sqrt mord"><span class="sqrt-sign" style="top:-0.017220000000000013em;"><span class="style-wrap reset-textstyle textstyle uncramped">√</span></span><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="mord textstyle cramped"><span class="mord"><span class="mord mathit">d</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span><span style="top:-0.77722em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="reset-textstyle textstyle uncramped sqrt-line"></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span>​</span></span></span></span></span></span>？ <a class="header-anchor" href="#_5-1-为啥attention的时候要除以" aria-label="Permalink to &quot;5.1 为啥Attention的时候要除以$\sqrt{d_k}$？&quot;">​</a></h3><p>当 dk<em>d**k</em> 的值比较小的时候，两种点积机制(additive 和 Dot-Product)的性能相差相近，当 dk<em>d**k</em> 比较大时，additive attention 比不带scale 的点积attention性能好。 我们怀疑，对于很大的 dk<em>d**k</em> 值，点积大幅度增长，将softmax函数推向具有极小梯度的区域。 为了抵消这种影响，我们缩小点积 1dk√<em>d**k</em>1 倍。</p><h3 id="_5-2-为啥拆多头-为啥效果好了" tabindex="-1">5.2 为啥拆多头？为啥效果好了？ <a class="header-anchor" href="#_5-2-为啥拆多头-为啥效果好了" aria-label="Permalink to &quot;5.2 为啥拆多头？为啥效果好了？&quot;">​</a></h3><h3 id="_5-3-cross-multi-head-attention" tabindex="-1">5.3 Cross Multi-Head Attention？ <a class="header-anchor" href="#_5-3-cross-multi-head-attention" aria-label="Permalink to &quot;5.3 Cross Multi-Head Attention？&quot;">​</a></h3><p>首先，Self- Attention与传统的Attention机制非常的不同：传统的Attention是基于source端和target端的隐变量（hidden state）计算Attention的，得到的结果是源端（source端）的每个词与目标端（target端）每个词之间的依赖关系。 其次，Self-Attention首先分别在source端和target端进行自身的attention，仅与source input或者target input自身相关的Self -Attention，以捕捉source端或target端自身的词与词之间的依赖关系；然后再把source端的得到的self -Attention加入到target端得到的Attention中，称作为<strong>Cross-Attention</strong>，以捕捉source端和target端词与词之间的依赖关系。</p><h3 id="_5-4-mask-multi-head-attention" tabindex="-1">5.4 Mask Multi-Head Attention <a class="header-anchor" href="#_5-4-mask-multi-head-attention" aria-label="Permalink to &quot;5.4 Mask Multi-Head Attention&quot;">​</a></h3><p>​ 与Encoder的Multi-Head Attention计算原理一样，只是多加了一个mask码。mask 表示掩码，它对某些值进行掩盖，使其在参数更新时不产生效果。Transformer 模型里面涉及两种 mask，分别是 padding mask 和 sequence mask。</p><h3 id="_5-5-masking实现机理" tabindex="-1">5.5 Masking实现机理 <a class="header-anchor" href="#_5-5-masking实现机理" aria-label="Permalink to &quot;5.5 Masking实现机理&quot;">​</a></h3><p>具体的做法是，把<strong>这些位置</strong>的值<strong>加上一个非常大的负数(负无穷)</strong>，这样的话，经过 softmax，这些位置的概率就会接近0！</p><h3 id="_5-6-mqa和gqa" tabindex="-1">5.6 MQA和GQA <a class="header-anchor" href="#_5-6-mqa和gqa" aria-label="Permalink to &quot;5.6 MQA和GQA&quot;">​</a></h3><p>MQA多头共用K，V</p><p>GQA将头分组，组内共用KV</p><h2 id="_6-🌟🌟🌟k-v-cache" tabindex="-1">6. 🌟🌟🌟K-V Cache <a class="header-anchor" href="#_6-🌟🌟🌟k-v-cache" aria-label="Permalink to &quot;6. 🌟🌟🌟K-V Cache&quot;">​</a></h2><h2 id="_7-🌟🌟🌟常见的正则化方法" tabindex="-1">7. 🌟🌟🌟常见的正则化方法 <a class="header-anchor" href="#_7-🌟🌟🌟常见的正则化方法" aria-label="Permalink to &quot;7. 🌟🌟🌟常见的正则化方法&quot;">​</a></h2><h1 id="二-课堂记录" tabindex="-1">二. 课堂记录 <a class="header-anchor" href="#二-课堂记录" aria-label="Permalink to &quot;二. 课堂记录&quot;">​</a></h1><h2 id="🌟0301-0302" tabindex="-1">🌟0301-0302 <a class="header-anchor" href="#🌟0301-0302" aria-label="Permalink to &quot;🌟0301-0302&quot;">​</a></h2><ul><li>0301:input序列长度大于embedding时候的seq_len时, input的输入序列会按照seq_len进行切割拼接到batch上吗? (老师讲了encoder时候input不足seq_len时候使用mask然后想问的另一个问题)</li><li>0302:<code>K-V cache</code>时候当预测下一个时间步的时候与之前的做Attention的时候, 中途会取出cache里的K—V吗还是只取出里面的K还是只在最后一个结束后才整体取一次 (我想问的也就是在一个batch或者一个seq的访存情况, 每一个时间步都需要访问cache一次吗)</li><li>是直接使用缓存的填充矩阵还是需要拿出缓存数据(读还是取)</li></ul><h2 id="_0308-0309——pytorch" tabindex="-1">0308-0309——PyTorch <a class="header-anchor" href="#_0308-0309——pytorch" aria-label="Permalink to &quot;0308-0309——PyTorch&quot;">​</a></h2><blockquote><p>提及: 混合精度训练</p></blockquote><h3 id="_1-1-tensor-中数据的连续性" tabindex="-1">1.1 Tensor 中数据的连续性 <a class="header-anchor" href="#_1-1-tensor-中数据的连续性" aria-label="Permalink to &quot;1.1 Tensor 中数据的连续性&quot;">​</a></h3><p>reshape, transpose, view, T(转置), permute</p><p>transpose会让raw data不变(共用), mata data的stride和shape等属性就变了 is_contiguous()不连续, 但reshape和permute这些是不会变的,因为他们会发生data copy, contiguous()会发生copy raw data数据</p><p>view和reshape的区别</p><p>view更加安全, 不会重新拷贝数据, 但数据不连续不能使用view,也就是stride不协调, reshape不会错误, 会重新拷贝数据, 数据也连续</p><p>permute和transpose会让stride属性改变, 从而发生数据不连续, 通常使用后要加一个contiguous()让数据连续</p><h3 id="_1-2-pytorch-autograd" tabindex="-1">1.2 pytorch autograd <a class="header-anchor" href="#_1-2-pytorch-autograd" aria-label="Permalink to &quot;1.2 pytorch autograd&quot;">​</a></h3><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250308163009045.png" alt="image-20250308163009045" style="zoom:36%;"><p>……………………</p><p>叶子结点+requests_grad=True才有最终的grad, 非叶子结点中途可能会计算grad, 但用了就会丢弃(requests_grad=True的)</p><p>梯度累加也有可能, 多个step的梯度累加, 隐式增加batch</p><p>若没进行xxx.grad.zero_()或者xxx.grad = None, 则会进行accumulate()累加grad, 这两种方法有一点区别, zero__()会置零,会占用显存, 但=None的话会释放显存, 两者各有好坏</p><h3 id="_1-3-inplace-op" tabindex="-1">1.3 inplace-op <a class="header-anchor" href="#_1-3-inplace-op" aria-label="Permalink to &quot;1.3 inplace-op&quot;">​</a></h3><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250309102559857.png" alt="image-20250309102559857"></p><p>叶子结点的Tensor变量不能进行in-place操作, 因为要更新梯度的时候要用叶子结点</p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250309113212792.png" alt="image-20250309113212792" style="zoom:50%;"><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250309114217344.png" alt="image-20250309114217344" style="zoom:50%;"><p>no_grad()底层是基于set_grad_enable(Flase)的</p><h3 id="_1-4-自动微分机制-auto-grad-重点" tabindex="-1">1.4 自动微分机制(auto grad) 重点： <a class="header-anchor" href="#_1-4-自动微分机制-auto-grad-重点" aria-label="Permalink to &quot;1.4 自动微分机制(auto grad) 重点：&quot;">​</a></h3><ul><li>pytorch中 正向forward 对我们用户是可见的，但是backward对我们用户是不可见的；</li><li>一般情况下，每一个正向的函数，都对应一个反向的函数（grad_fn--&gt; Tensor中）；</li><li>tensor：requires_grad = True</li><li>tensor: grad --&gt; tensor 中存储grad的地方；</li><li>tensor: grad_fn --&gt; 存储我们反向函数的地方</li><li>tesnor: is_leaf --&gt; 这个tensor 是不是 叶子节点；</li><li>net::all weight --&gt; 都是leaf</li><li>叶子节点的梯度会自动保存下来的（weight）；</li><li>中间的 activation 的梯度会计算，但是不保留；</li><li>pytorch 动态图 vs tensorflow 静态图；</li><li>我们不能改变一个非叶子节点的 requires_grad;</li><li>非叶子（一个函数的output）节点它的 requires_grad 自动推导的；</li><li>非叶子节点对应函数的inputs 中只要有一个 requires_grad = True, 那么这个非叶子节点的requires_grad = True;</li><li>torch.no_grad() 会使得里面的新的tensor requires_grad = False</li><li>inplace的操作，非常大的风险：覆盖了原来的值，导致反向传播时计算不准确；</li><li>标量的梯度才能被隐式创建，隐式创建（.backward(1)）；</li><li>一般情况下，.backward(gradient)是有输入的: ;</li></ul><h3 id="_2-1-torch-nn-module" tabindex="-1">2.1 torch.nn.Module <a class="header-anchor" href="#_2-1-torch-nn-module" aria-label="Permalink to &quot;2.1 torch.nn.Module&quot;">​</a></h3><p>train模式和veal模式不会对grad的情况做修改,只是对训练和推理的对应的算子做不同的处理(等价处理)</p><p>常用算子dropout和BachNorm</p><p>xxx.cuda()的时候搬迁的是_parameters到cuda, 还有buffer也搬迁到cuda, 并没有将模型结构进行搬迁.</p><p>按照深度优先遍历sub module,将里面的_parameters和buffer到cuda, 数据类型转换也是一样的操作</p><p>c++底层实现了一个dispather分发机制,按照device属性分发, 对应device会调用对应的fn算子, 计算部分才执行</p><p>_parameters()送参数给优化器的时候将所有的parameters送到optim, 但数据共用, 同时更新</p><p>钩子函数(没太懂)</p><hr><h2 id="_0315-0316-续pytorch" tabindex="-1">0315-0316（续PyTorch） <a class="header-anchor" href="#_0315-0316-续pytorch" aria-label="Permalink to &quot;0315-0316（续PyTorch）&quot;">​</a></h2><h3 id="_1-1-回顾" tabindex="-1">1.1 回顾 <a class="header-anchor" href="#_1-1-回顾" aria-label="Permalink to &quot;1.1 回顾&quot;">​</a></h3><p>1.Tensor类和重要属性 2.autograd，动态图 3.Module以及属性和方法</p><blockquote><p>training,_parameters,_buffers,_modules(hooks是主要用二次开发等情况)</p></blockquote><p>子模块啥时候定义的呢？</p><p>_parameters,_buffers哪些有哪些没有</p><p>将module里的parameters传给optim，会通过调用parameters()进行</p><p>一系列方法具体情况</p><h3 id="_1-2-问题合集" tabindex="-1">1.2 问题合集 <a class="header-anchor" href="#_1-2-问题合集" aria-label="Permalink to &quot;1.2 问题合集&quot;">​</a></h3><ol><li>在讲transformer的padding mask的时候想到，如果输入seq_len大于了定义的seq_len，会直接截断还是截断再拼接到下一个batch</li><li>在sequence mask的时候，忘了要问啥了</li><li>在normalization层的时候不是有两个学习的参数吗，这俩参数是一次forward训练一次还是单独有自己的训练？还有，这俩参数是咋更新的？</li><li>dataset会迭代的将所有数据加载到内存吗，然后dataloader再一批次的提取吗</li></ol><p>with torch.no_grad(): eval时候用，计算图不再进行，对require_grads=True的不进行梯度计算，显存占用量会减少，activation的就会丢弃</p><p>dataset会迭代的将所有数据加载到内存吗，然后dataloader再一批次一批次的提取吗？还是说dataloader准备拿一个batch，然后dataset根据batch_size迭代获取size条。</p><blockquote><p>是后者，也就是I/O的时候，batch_size太小的话会增加I/O负担</p></blockquote><h3 id="_2-1-torch-optim" tabindex="-1">2.1 torch.optim <a class="header-anchor" href="#_2-1-torch-optim" aria-label="Permalink to &quot;2.1 torch.optim&quot;">​</a></h3><p>参数传param的时候的传递和打包方式</p><p>self.param_groups</p><p>==self.state==：训练时候显存消耗的主要项（优化器的动量项有关） 他是一个dict，keys是tensor，values也是 模型</p><blockquote><p>移动指数平均是啥忘了</p></blockquote><p>def load_state_dict</p><h3 id="_2-2-learning-rate-调整方案" tabindex="-1">2.2 learning rate 调整方案 <a class="header-anchor" href="#_2-2-learning-rate-调整方案" aria-label="Permalink to &quot;2.2 learning rate 调整方案&quot;">​</a></h3><p>Torch.optim.lr_scheduler</p><p>震荡类型的学习率调整是减少进入局部最优解的情况</p><p>==状态字典==，三个地方见过，都类似，模型保存时候需要有</p><h3 id="_2-3-模型保存和加载" tabindex="-1">2.3 模型保存和加载 <a class="header-anchor" href="#_2-3-模型保存和加载" aria-label="Permalink to &quot;2.3 模型保存和加载&quot;">​</a></h3><p>==动态图==</p><p>1.save state_dict的时候只有参数，save model的时候无法直接保存整个网络，但是他的材料（init）的那些会保存，模型加载的时候能通过，但runing time时候，forward并没有，必须导入或者自己实现，需要原来Net的签名（具体定义可以不一致，会放入_modules）</p><p>2.如果是自己写的算子，在init时候也放入_modules吗？</p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/%7B6fbad3cc-1899-4404-b3b3-d91f7da5cb95%7D.png" alt="img"></p><p>3.==onnx==模型保存必须输入对应的input，自己run一遍，是一个静态图</p><p>4.训练中的保存和加载（check point）==模型保存的几种参数类型==）</p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250316113313785.png" alt="image-20250316113313785"></p><h3 id="_3-1-dataset-and-dataloader" tabindex="-1">3.1 Dataset and Dataloader <a class="header-anchor" href="#_3-1-dataset-and-dataloader" aria-label="Permalink to &quot;3.1 Dataset and Dataloader&quot;">​</a></h3><blockquote><p>只学习pytorch的，后续自己补hf的那些</p></blockquote><h3 id="_4-1-nlp" tabindex="-1">4.1 NLP <a class="header-anchor" href="#_4-1-nlp" aria-label="Permalink to &quot;4.1 NLP&quot;">​</a></h3><p>GPT：自监督训练得到预训练模型（采用迁移学习）</p><p>Bert：完形填空</p><p>迁移学习：预训练+微调（微调的数据集就是专业领域的数据集）</p><h3 id="_4-2-bert" tabindex="-1">4.2 Bert <a class="header-anchor" href="#_4-2-bert" aria-label="Permalink to &quot;4.2 Bert&quot;">​</a></h3><p>1.两个任务：MLM和NSP</p><p>2.Embedding，词嵌入</p><p>词，句子（分段），位置 嵌入</p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250316165604714.png" alt="image-20250316165604714"></p><blockquote><p>transformer的词嵌入式用三角位置嵌入</p></blockquote><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250316175009793.png" alt="image-20250316175009793"></p></div></div></main><footer class="VPDocFooter" data-v-39a288b8 data-v-e257564d><!--[--><!--]--><div class="edit-info" data-v-e257564d><div class="edit-link" data-v-e257564d><a class="VPLink link vp-external-link-icon no-icon edit-link-button" href="https://github.com/EthanLiu6/coderethan.fun/edit/master/docs/AI/deep_learning_theory/00-DL_Base_Notes.md" target="_blank" rel="noreferrer" data-v-e257564d><!--[--><span class="vpi-square-pen edit-link-icon" data-v-e257564d></span> 在 GitHub 上编辑此页面 OR 提出修改意见<!--]--></a></div><div class="last-updated" data-v-e257564d><p class="VPLastUpdated" data-v-e257564d data-v-e98dd255>最后更新于: <time datetime="2025-03-17T12:45:01.000Z" data-v-e98dd255></time></p></div></div><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-e257564d><span class="visually-hidden" id="doc-footer-aria-label" data-v-e257564d>Pager</span><div class="pager" data-v-e257564d><a class="VPLink link pager-link prev" href="/AI/Transformer/14-残差网络和归一化.html" data-v-e257564d><!--[--><span class="desc" data-v-e257564d>上一篇</span><span class="title" data-v-e257564d>14-残差网络和归一化</span><!--]--></a></div><div class="pager" data-v-e257564d><a class="VPLink link pager-link next" href="/AI/deep_learning_theory/01-feedforward_network.html" data-v-e257564d><!--[--><span class="desc" data-v-e257564d>下一篇</span><span class="title" data-v-e257564d>01-feedforward_network</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-5d98c3a5 data-v-e315a0ad><div class="container" data-v-e315a0ad><p class="message" data-v-e315a0ad>ICP备案号: <a href="https://beian.miit.gov.cn/" target="_blank">蜀ICP备2024103116号</a><br>公安备案号: <a href="https://beian.mps.gov.cn/#/query/webSearch?code=51012202001928" rel="noreferrer" target="_blank">川公网安备51012202001928</a></p><p class="copyright" data-v-e315a0ad>版权所有 © 2024-present  <a href="mailto:16693226842@163.com" target="_blank">Ethan.Liu</a></p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"ai_deep_learning_theory_00-dl_base_notes.md\":\"CAdB7Lmh\",\"ai_deep_learning_theory_01-feedforward_network.md\":\"Dc0-nIKA\",\"ai_deep_learning_theory_02-back_propagation.md\":\"t66HZ6KH\",\"ai_deep_learning_theory_03-bp_example_demo.md\":\"CdKkr9wb\",\"ai_deep_learning_theory_04-convolution_neural_network.md\":\"CKyKqVoU\",\"ai_deep_learning_theory_05-deep_learning_model.md\":\"B3Ib7-iT\",\"ai_deep_learning_theory_06-pytorch_install.md\":\"C08X_AVj\",\"ai_deep_learning_theory_07-operators.md\":\"BKFjU-RC\",\"ai_deep_learning_theory_08-activation_functions.md\":\"Como81rR\",\"ai_deep_learning_theory_09-recurrent_neural_network.md\":\"D7qCdZEy\",\"ai_deep_learning_theory_10-seq2seq.md\":\"CIAyZr6Y\",\"ai_deep_learning_theory_11-1attentions.md\":\"CB-r_QHy\",\"ai_deep_learning_theory_11-2attention-extension.md\":\"D9uu1g-H\",\"ai_deep_learning_theory_12-weight-initialization.md\":\"tjnwIO-Q\",\"ai_deep_learning_theory_13-optimizers.md\":\"Qk82FVfW\",\"ai_deep_learning_theory_14-regularization.md\":\"CWzAanem\",\"ai_deep_learning_theory_15-deep-learning-tuning-guide.md\":\"CMQIvWo9\",\"ai_deep_learning_theory_20-pytorch-tensor.md\":\"3GYXzMSK\",\"ai_deep_learning_theory_21-pytorch-autograd.md\":\"BPcbnDeO\",\"ai_deep_learning_theory_22-pytorch-module.md\":\"D3W47z47\",\"ai_deep_learning_theory_23-1training-example-1.md\":\"BSbXuKLm\",\"ai_deep_learning_theory_23-2decoder.md\":\"BCYdFt2l\",\"ai_deep_learning_theory_23-3encoder.md\":\"BpaKSKIz\",\"ai_deep_learning_theory_23-4transformer.md\":\"Deo9x8So\",\"ai_deep_learning_theory_24-pytorch-optimizer.md\":\"D38W_WfI\",\"ai_deep_learning_theory_25-pytorch-lr-scheduler.md\":\"BrPX5Pww\",\"ai_deep_learning_theory_26-pytorch-dataloader.md\":\"C1VXfMIC\",\"ai_deep_learning_theory_27-pytorch-model-save.md\":\"D0btqXT_\",\"ai_deep_learning_theory_28-pytorch-tensorboard.md\":\"CGiQm8lw\",\"ai_deep_learning_theory_29-pytorch-graph-mode.md\":\"Dh2sBoD6\",\"ai_deep_learning_theory_30-3main.md\":\"Cf3sjvsp\",\"ai_deep_learning_theory_30-training-example-2.md\":\"DVUt4gEj\",\"ai_deep_learning_theory_40-ner.md\":\"dpC65KCz\",\"ai_deep_learning_theory_41-question-answering.md\":\"CvkK4DNb\",\"ai_deep_learning_theory_42-1stable-diffusion.md\":\"CGgf4cC2\",\"ai_deep_learning_theory_42-2sdxl.md\":\"CC6w4ggh\",\"ai_deep_learning_theory_42-3vae.md\":\"CD94bkcz\",\"ai_deep_learning_theory_44-scaling-law.md\":\"B0JM9xx_\",\"ai_deep_learning_theory_45-distribute-training.md\":\"DZ9YG6ih\",\"ai_deep_learning_theory_46-nlp-llama.md\":\"8zU0XjzU\",\"ai_deep_learning_theory_47-nlp-deepseek.md\":\"CjtxEpY3\",\"ai_deep_learning_theory_index.md\":\"TlMKjhkI\",\"ai_index.md\":\"DZAAGaZE\",\"ai_transformer_01-注意力机制.md\":\"CAUQ-ZlO\",\"ai_transformer_02-总体架构.md\":\"ChkPOSG5\",\"ai_transformer_03-数据处理.md\":\"I5H3eYdL\",\"ai_transformer_04-编码器 _ 解码器.md\":\"zSoU6yVz\",\"ai_transformer_05-训练_推理.md\":\"BJLWHSwO\",\"ai_transformer_06-token.md\":\"6NkY4MDx\",\"ai_transformer_07-embedding.md\":\"CQvHLv6j\",\"ai_transformer_08-位置编码.md\":\"D1gfu21J\",\"ai_transformer_09-位置编码分类.md\":\"BTP3xgL5\",\"ai_transformer_10-自注意力.md\":\"cLYDSTGu\",\"ai_transformer_11-掩码.md\":\"D3BVR142\",\"ai_transformer_12-多头自注意力.md\":\"fflE1jcD\",\"ai_transformer_13- ffn.md\":\"D5pP50b2\",\"ai_transformer_14-残差网络和归一化.md\":\"CGidK7tQ\",\"ai_transformer_index.md\":\"D8JcJLy1\",\"guide_index.md\":\"0LVzN69w\",\"guide_博客结构.md\":\"x1FQx-jf\",\"improve_index.md\":\"CH8Euwro\",\"improve_金融投资_index.md\":\"Vw_D3eRw\",\"improve_阅读_index.md\":\"Dj8DDADt\",\"index.md\":\"mK1aVFiR\",\"it-learning_408知识_index.md\":\"DlbfR44a\",\"it-learning_408知识_os-4.1 进程同步.md\":\"BL8WmbXW\",\"it-learning_408知识_os-4.4 信号量机制.md\":\"ZCxX2ify\",\"it-learning_408知识_os-4.4 信号量机制pv操作之“可见”.md\":\"DcOjbMF_\",\"it-learning_c__基础_01_开发环境搭建与基础数据类型.md\":\"BDKy9qJ5\",\"it-learning_c__基础_02_控制流语句与复合数据类型.md\":\"SKbyo4op\",\"it-learning_c__基础_03_指针与引用.md\":\"DHwgRso8\",\"it-learning_c__基础_04_自定义数据类型与函数.md\":\"DRE6vCJj\",\"it-learning_c__基础_05_头文件与指针的算术运算.md\":\"D1SjIWSd\",\"it-learning_c__基础_06_字符串、数组、指针与函数.md\":\"CgPOPn1g\",\"it-learning_c__基础_07_函数进阶与内存管理.md\":\"6qEwY-bi\",\"it-learning_c__基础_08_运算符优先级表.md\":\"BJfq7pJh\",\"it-learning_c__基础_09_指针、内存管理和类的基础.md\":\"B67kgruM\",\"it-learning_c__基础_10_深入类和对象.md\":\"BWMMnRaW\",\"it-learning_c__基础_11_类的大小、继承与权限控制.md\":\"CRDT26Xm\",\"it-learning_c__基础_12_继承进阶.md\":\"6AJ13KSU\",\"it-learning_c__基础_13_类型转换和多态与虚函数.md\":\"CYE5ie6q\",\"it-learning_c__基础_14_纯虚函数、抽象类、深浅拷贝及智能指针.md\":\"UvlEbqGK\",\"it-learning_c__基础_15_运算符重载与 string 类详解.md\":\"1S48Obum\",\"it-learning_c__基础_16_有序容器与无序容器.md\":\"1sbF57hZ\",\"it-learning_c__基础_17_模板.md\":\"Bp6hiaoG\",\"it-learning_c__基础_18_迭代器与其应用.md\":\"DhitqtBz\",\"it-learning_c__基础_19_c__ 标准库常用算法.md\":\"DiYIRHYc\",\"it-learning_c__基础_20_c__ 异常处理 - 第19次课.md\":\"CJMeMX9w\",\"it-learning_c__基础_21_友元及友元相关内容.md\":\"Cx439b0I\",\"it-learning_c__基础_22_c__ io 流详解-feadbc607d7f.md\":\"BAPGehpe\",\"it-learning_c__基础_23_c__ io 流详解.md\":\"I4ZE0Tau\",\"it-learning_c__基础_24_位运算符总结.md\":\"CRY59I0U\",\"it-learning_c__基础_25_c__三种继承方式.md\":\"Bm4ge_Dq\",\"it-learning_c__基础_26_c__11 高级特性.md\":\"b4HnWfxf\",\"it-learning_c__基础_27_c__14 新特性.md\":\"C5KOu8gX\",\"it-learning_c__基础_28_c__17 新特性.md\":\"DGt5Tcb-\",\"it-learning_c__基础_29_多文件和 makefile工程管理.md\":\"BDridL3y\",\"it-learning_c__基础_30_c__大型项目cmake工程管理.md\":\"DOufPDWu\",\"it-learning_c__基础_31_c__ 主要就业方向与技术能力分析报告.md\":\"AOV7PjqE\",\"it-learning_c__基础_32_c__ 基础知识回顾.md\":\"A-5IxrZu\",\"it-learning_c__基础_index.md\":\"BTWU73w9\",\"it-learning_index.md\":\"B2k5NgZE\",\"it-learning_java_01.java-se.md\":\"CV9z-Ph2\",\"it-learning_java_02.sql.md\":\"gilr6jOh\",\"it-learning_java_03.java-web.md\":\"DXRkrx3K\",\"it-learning_java_05.mybatis.md\":\"HjU_MlJz\",\"it-learning_java_index.md\":\"DkRBc8cW\",\"it-learning_linux_01.linux基础.md\":\"BBs1RcBf\",\"it-learning_linux_02.shell.md\":\"BCfUk33T\",\"it-learning_linux_03.mpi并行计算.md\":\"gpRl9TGp\",\"it-learning_linux_04.docker.md\":\"DN_C173y\",\"it-learning_linux_index.md\":\"BVALaZgc\",\"it-learning_计算机图形学_02.直线光栅化.md\":\"C0uFebpu\",\"it-learning_计算机图形学_index.md\":\"Be4gO7hA\",\"readme.md\":\"m88s5xF2\",\"update_更新日志.md\":\"BVtQLf5o\",\"我的感悟_2024_index.md\":\"DNiQxEmG\",\"我的感悟_2024_不同商家的视野.md\":\"5Pu9Bj7-\",\"我的感悟_2024_学而篇.md\":\"C-hCBgCG\",\"我的感悟_2024_重温士兵突击.md\":\"C2-8pk9-\",\"我的感悟_2025_index.md\":\"DfcTEU3d\",\"我的感悟_2026_index.md\":\"ByieJetu\",\"我的感悟_index.md\":\"CridsN6k\",\"技术问题清单_doccano账户管理.md\":\"Ba1HQxQq\",\"技术问题清单_index.md\":\"-lH6BxFw\",\"技术问题清单_专英翻转课堂—pytorch.md\":\"Dodyf68o\",\"技术问题清单_虚拟机网络问题.md\":\"B5YN-iPU\",\"生活与算法_index.md\":\"Bz6rRKIq\",\"生活与算法_贪心算法_1.人的本性——贪心！.md\":\"ByHAGPXB\",\"生活与算法_贪心算法_2.初步感受贪心.md\":\"BKL_n967\",\"生活与算法_贪心算法_3. 分发饼干问题.md\":\"D-sOMxS6\",\"面试求职_java面经_index.md\":\"Dt5psx-c\",\"面试求职_场景问题_index.md\":\"Bq1aCqXq\",\"面试求职_算法岗_index.md\":\"B0mkLyz0\",\"面试求职_经验分享_index.md\":\"7Buq_LXu\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"zh-CN\",\"dir\":\"ltr\",\"title\":\"码医森\",\"description\":\"计算机知识的学习站点\",\"base\":\"/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"siteTitle\":\"CoderEthan学习站\",\"logo\":\"/imgs/home-page-logo.svg\",\"outline\":{\"label\":\"本文目录\",\"level\":[2,4]},\"search\":{\"provider\":\"local\"},\"socialLinks\":[{\"icon\":{\"svg\":\"<svg xmlns=\\\"http://www.w3.org/2000/svg\\\" viewBox=\\\"0 0 496 512\\\"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path fill=\\\"#6841d2\\\" d=\\\"M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3 .3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5 .3-6.2 2.3zm44.2-1.7c-2.9 .7-4.9 2.6-4.6 4.9 .3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3 .7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3 .3 2.9 2.3 3.9 1.6 1 3.6 .7 4.3-.7 .7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3 .7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3 .7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z\\\"/></svg>\"},\"link\":\"https://github.com/ethanliu6/\"},{\"icon\":{\"svg\":\"<svg xmlns=\\\"http://www.w3.org/2000/svg\\\" viewBox=\\\"0 0 512 512\\\"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path fill=\\\"#6841d2\\\" d=\\\"M488.6 104.1C505.3 122.2 513 143.8 511.9 169.8V372.2C511.5 398.6 502.7 420.3 485.4 437.3C468.2 454.3 446.3 463.2 419.9 464H92C65.6 463.2 43.8 454.2 26.7 436.8C9.7 419.4 .8 396.5 0 368.2V169.8C.8 143.8 9.7 122.2 26.7 104.1C43.8 87.8 65.6 78.8 92 78H121.4L96.1 52.2C90.3 46.5 87.4 39.2 87.4 30.4C87.4 21.6 90.3 14.3 96.1 8.6C101.8 2.9 109.1 0 117.9 0C126.7 0 134 2.9 139.8 8.6L213.1 78H301.1L375.6 8.6C381.7 2.9 389.2 0 398 0C406.8 0 414.1 2.9 419.9 8.6C425.6 14.3 428.5 21.6 428.5 30.4C428.5 39.2 425.6 46.5 419.9 52.2L394.6 78L423.9 78C450.3 78.8 471.9 87.8 488.6 104.1H488.6zM449.8 173.8C449.4 164.2 446.1 156.4 439.1 150.3C433.9 144.2 425.1 140.9 416.4 140.5H96.1C86.5 140.9 78.6 144.2 72.5 150.3C66.3 156.4 63.1 164.2 62.7 173.8V368.2C62.7 377.4 66 385.2 72.5 391.7C79 398.2 86.9 401.5 96.1 401.5H416.4C425.6 401.5 433.4 398.2 439.7 391.7C446 385.2 449.4 377.4 449.8 368.2L449.8 173.8zM185.5 216.5C191.8 222.8 195.2 230.6 195.6 239.7V273C195.2 282.2 191.9 289.9 185.8 296.2C179.6 302.5 171.8 305.7 162.2 305.7C152.6 305.7 144.7 302.5 138.6 296.2C132.5 289.9 129.2 282.2 128.8 273V239.7C129.2 230.6 132.6 222.8 138.9 216.5C145.2 210.2 152.1 206.9 162.2 206.5C171.4 206.9 179.2 210.2 185.5 216.5H185.5zM377 216.5C383.3 222.8 386.7 230.6 387.1 239.7V273C386.7 282.2 383.4 289.9 377.3 296.2C371.2 302.5 363.3 305.7 353.7 305.7C344.1 305.7 336.3 302.5 330.1 296.2C323.1 289.9 320.7 282.2 320.4 273V239.7C320.7 230.6 324.1 222.8 330.4 216.5C336.7 210.2 344.5 206.9 353.7 206.5C362.9 206.9 370.7 210.2 377 216.5H377z\\\"/></svg>\"},\"link\":\"https://space.bilibili.com/1327099977/\"}],\"nav\":[{\"text\":\"主页\",\"link\":\"/\"},{\"text\":\"博客指南\",\"link\":\"/guide/\"},{\"text\":\"生活与算法\",\"link\":\"/生活与算法/\"},{\"text\":\"AI\",\"items\":[{\"text\":\"DL基础理论\",\"link\":\"/AI/deep_learning_theory/\"},{\"text\":\"Transformer系列\",\"link\":\"/AI/Transformer/\"}]},{\"text\":\"计算机学科内容\",\"items\":[{\"text\":\"计算机知识\",\"link\":\"/IT-learning/408知识/\"},{\"text\":\"C++基础\",\"link\":\"/IT-learning/c++基础/\"},{\"text\":\"Java后端\",\"link\":\"/IT-learning/Java/\"},{\"text\":\"Linux技术\",\"link\":\"/IT-learning/Linux/\"},{\"text\":\"计算机图形学\",\"link\":\"/IT-learning/计算机图形学/\"}]},{\"text\":\"自我提升\",\"items\":[{\"text\":\"阅读\",\"link\":\"/improve/阅读/\"},{\"text\":\"金融投资\",\"link\":\"/improve/金融投资/\"}]},{\"text\":\"求职面试\",\"items\":[{\"text\":\"Java面经\",\"link\":\"/面试求职/Java面经/\"},{\"text\":\"场景问题\",\"link\":\"/面试求职/场景问题/\"},{\"text\":\"经验分享\",\"link\":\"/面试求职/经验分享/\"},{\"text\":\"算法岗\",\"link\":\"/面试求职/算法岗/\"}]},{\"text\":\"其他维护\",\"items\":[{\"text\":\"站点更新\",\"link\":\"/update/更新日志\"},{\"text\":\"问题清单\",\"link\":\"/技术问题清单/\"}]},{\"text\":\"感悟和日常\",\"items\":[{\"text\":\"站长感悟\",\"link\":\"/我的感悟/\"},{\"text\":\"站长旧版博客\",\"link\":\"https://EthanLiu6.github.io\"}]}],\"footer\":{\"message\":\"ICP备案号: <a href=\\\"https://beian.miit.gov.cn/\\\" target=\\\"_blank\\\">蜀ICP备2024103116号</a><br>公安备案号: <a href=\\\"https://beian.mps.gov.cn/#/query/webSearch?code=51012202001928\\\" rel=\\\"noreferrer\\\" target=\\\"_blank\\\">川公网安备51012202001928</a>\",\"copyright\":\"版权所有 © 2024-present  <a href=\\\"mailto:16693226842@163.com\\\" target=\\\"_blank\\\">Ethan.Liu</a>\"},\"editLink\":{\"pattern\":\"https://github.com/EthanLiu6/coderethan.fun/edit/master/docs/:path\",\"text\":\"在 GitHub 上编辑此页面 OR 提出修改意见\"},\"lastUpdated\":{\"text\":\"最后更新于\",\"formatOptions\":{\"dateStyle\":\"long\",\"timeStyle\":\"short\"}},\"docFooter\":{\"prev\":\"上一篇\",\"next\":\"下一篇\"},\"darkModeSwitchLabel\":\"深色模式\",\"lightModeSwitchTitle\":\"切换到浅色模式\",\"darkModeSwitchTitle\":\"切换到深色模式\",\"sidebar\":{\"/AI/\":[{\"items\":[{\"text\":\"Transformer\",\"items\":[{\"text\":\"01-注意力机制\",\"link\":\"/AI/Transformer/01-注意力机制.html\"},{\"text\":\"02-总体架构\",\"link\":\"/AI/Transformer/02-总体架构.html\"},{\"text\":\"03-数据处理\",\"link\":\"/AI/Transformer/03-数据处理.html\"},{\"text\":\"04-编码器 & 解码器\",\"link\":\"/AI/Transformer/04-编码器 & 解码器.html\"},{\"text\":\"05-训练&推理\",\"link\":\"/AI/Transformer/05-训练&推理.html\"},{\"text\":\"06-token\",\"link\":\"/AI/Transformer/06-token.html\"},{\"text\":\"07-embedding\",\"link\":\"/AI/Transformer/07-embedding.html\"},{\"text\":\"08-位置编码\",\"link\":\"/AI/Transformer/08-位置编码.html\"},{\"text\":\"09-位置编码分类\",\"link\":\"/AI/Transformer/09-位置编码分类.html\"},{\"text\":\"10-自注意力\",\"link\":\"/AI/Transformer/10-自注意力.html\"},{\"text\":\"11-掩码\",\"link\":\"/AI/Transformer/11-掩码.html\"},{\"text\":\"12-多头自注意力\",\"link\":\"/AI/Transformer/12-多头自注意力.html\"},{\"text\":\"13- FFN\",\"link\":\"/AI/Transformer/13- FFN.html\"},{\"text\":\"14-残差网络和归一化\",\"link\":\"/AI/Transformer/14-残差网络和归一化.html\"}],\"collapsed\":false},{\"text\":\"deep_learning_theory\",\"items\":[{\"text\":\"00-DL_Base_Notes\",\"link\":\"/AI/deep_learning_theory/00-DL_Base_Notes.html\"},{\"text\":\"01-feedforward_network\",\"link\":\"/AI/deep_learning_theory/01-feedforward_network.html\"},{\"text\":\"02-back_propagation\",\"link\":\"/AI/deep_learning_theory/02-back_propagation.html\"},{\"text\":\"03-bp_example_demo\",\"link\":\"/AI/deep_learning_theory/03-bp_example_demo.html\"},{\"text\":\"04-convolution_neural_network\",\"link\":\"/AI/deep_learning_theory/04-convolution_neural_network.html\"},{\"text\":\"05-deep_learning_model\",\"link\":\"/AI/deep_learning_theory/05-deep_learning_model.html\"},{\"text\":\"06-pytorch_install\",\"link\":\"/AI/deep_learning_theory/06-pytorch_install.html\"},{\"text\":\"07-operators\",\"link\":\"/AI/deep_learning_theory/07-operators.html\"},{\"text\":\"08-activation_functions\",\"link\":\"/AI/deep_learning_theory/08-activation_functions.html\"},{\"text\":\"09-recurrent_neural_network\",\"link\":\"/AI/deep_learning_theory/09-recurrent_neural_network.html\"},{\"text\":\"10-seq2seq\",\"link\":\"/AI/deep_learning_theory/10-seq2seq.html\"},{\"text\":\"11-1attentions\",\"link\":\"/AI/deep_learning_theory/11-1attentions.html\"},{\"text\":\"11-2attention-extension\",\"link\":\"/AI/deep_learning_theory/11-2attention-extension.html\"},{\"text\":\"12-weight-initialization\",\"link\":\"/AI/deep_learning_theory/12-weight-initialization.html\"},{\"text\":\"13-optimizers\",\"link\":\"/AI/deep_learning_theory/13-optimizers.html\"},{\"text\":\"14-regularization\",\"link\":\"/AI/deep_learning_theory/14-regularization.html\"},{\"text\":\"15-deep-learning-tuning-guide\",\"link\":\"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html\"},{\"text\":\"20-pytorch-tensor\",\"link\":\"/AI/deep_learning_theory/20-pytorch-tensor.html\"},{\"text\":\"21-pytorch-autograd\",\"link\":\"/AI/deep_learning_theory/21-pytorch-autograd.html\"},{\"text\":\"22-pytorch-module\",\"link\":\"/AI/deep_learning_theory/22-pytorch-module.html\"},{\"text\":\"23-1training-example-1\",\"link\":\"/AI/deep_learning_theory/23-1training-example-1.html\"},{\"text\":\"23-2decoder\",\"link\":\"/AI/deep_learning_theory/23-2decoder.html\"},{\"text\":\"23-3encoder\",\"link\":\"/AI/deep_learning_theory/23-3encoder.html\"},{\"text\":\"23-4transformer\",\"link\":\"/AI/deep_learning_theory/23-4transformer.html\"},{\"text\":\"24-pytorch-optimizer\",\"link\":\"/AI/deep_learning_theory/24-pytorch-optimizer.html\"},{\"text\":\"25-pytorch-lr-scheduler\",\"link\":\"/AI/deep_learning_theory/25-pytorch-lr-scheduler.html\"},{\"text\":\"26-pytorch-dataloader\",\"link\":\"/AI/deep_learning_theory/26-pytorch-dataloader.html\"},{\"text\":\"27-pytorch-model-save\",\"link\":\"/AI/deep_learning_theory/27-pytorch-model-save.html\"},{\"text\":\"28-pytorch-tensorboard\",\"link\":\"/AI/deep_learning_theory/28-pytorch-tensorboard.html\"},{\"text\":\"29-pytorch-graph-mode\",\"link\":\"/AI/deep_learning_theory/29-pytorch-graph-mode.html\"},{\"text\":\"30-3main\",\"link\":\"/AI/deep_learning_theory/30-3main.html\"},{\"text\":\"30-training-example-2\",\"link\":\"/AI/deep_learning_theory/30-training-example-2.html\"},{\"text\":\"40-ner\",\"link\":\"/AI/deep_learning_theory/40-ner.html\"},{\"text\":\"41-question-answering\",\"link\":\"/AI/deep_learning_theory/41-question-answering.html\"},{\"text\":\"42-1stable-diffusion\",\"link\":\"/AI/deep_learning_theory/42-1stable-diffusion.html\"},{\"text\":\"42-2SDXL\",\"link\":\"/AI/deep_learning_theory/42-2SDXL.html\"},{\"text\":\"42-3VAE\",\"link\":\"/AI/deep_learning_theory/42-3VAE.html\"},{\"text\":\"44-scaling-law\",\"link\":\"/AI/deep_learning_theory/44-scaling-law.html\"},{\"text\":\"45-distribute-training\",\"link\":\"/AI/deep_learning_theory/45-distribute-training.html\"},{\"text\":\"46-nlp-llama\",\"link\":\"/AI/deep_learning_theory/46-nlp-llama.html\"},{\"text\":\"47-nlp-deepseek\",\"link\":\"/AI/deep_learning_theory/47-nlp-deepseek.html\"}],\"collapsed\":false}]}],\"/IT-learning/\":[{\"items\":[{\"text\":\"408知识\",\"items\":[{\"text\":\"OS-4.1 进程同步\",\"link\":\"/IT-learning/408知识/OS-4.1 进程同步.html\"},{\"text\":\"OS-4.4 信号量机制\",\"link\":\"/IT-learning/408知识/OS-4.4 信号量机制.html\"},{\"text\":\"OS-4.4 信号量机制pv操作之“可见”\",\"link\":\"/IT-learning/408知识/OS-4.4 信号量机制pv操作之“可见”.html\"}],\"collapsed\":false},{\"text\":\"Java\",\"items\":[{\"text\":\"01.java-se\",\"link\":\"/IT-learning/Java/01.java-se.html\"},{\"text\":\"02.sql\",\"link\":\"/IT-learning/Java/02.sql.html\"},{\"text\":\"03.java-web\",\"link\":\"/IT-learning/Java/03.java-web.html\"},{\"text\":\"05.MyBatis\",\"link\":\"/IT-learning/Java/05.MyBatis.html\"}],\"collapsed\":false},{\"text\":\"Linux\",\"items\":[{\"text\":\"01.Linux基础\",\"link\":\"/IT-learning/Linux/01.Linux基础.html\"},{\"text\":\"02.Shell\",\"link\":\"/IT-learning/Linux/02.Shell.html\"},{\"text\":\"03.MPI并行计算\",\"link\":\"/IT-learning/Linux/03.MPI并行计算.html\"},{\"text\":\"04.Docker\",\"link\":\"/IT-learning/Linux/04.Docker.html\"}],\"collapsed\":false},{\"text\":\"c++基础\",\"items\":[{\"text\":\"01_开发环境搭建与基础数据类型\",\"link\":\"/IT-learning/c++基础/01_开发环境搭建与基础数据类型.html\"},{\"text\":\"02_控制流语句与复合数据类型\",\"link\":\"/IT-learning/c++基础/02_控制流语句与复合数据类型.html\"},{\"text\":\"03_指针与引用\",\"link\":\"/IT-learning/c++基础/03_指针与引用.html\"},{\"text\":\"04_自定义数据类型与函数\",\"link\":\"/IT-learning/c++基础/04_自定义数据类型与函数.html\"},{\"text\":\"05_头文件与指针的算术运算\",\"link\":\"/IT-learning/c++基础/05_头文件与指针的算术运算.html\"},{\"text\":\"06_字符串、数组、指针与函数\",\"link\":\"/IT-learning/c++基础/06_字符串、数组、指针与函数.html\"},{\"text\":\"07_函数进阶与内存管理\",\"link\":\"/IT-learning/c++基础/07_函数进阶与内存管理.html\"},{\"text\":\"08_运算符优先级表\",\"link\":\"/IT-learning/c++基础/08_运算符优先级表.html\"},{\"text\":\"09_指针、内存管理和类的基础\",\"link\":\"/IT-learning/c++基础/09_指针、内存管理和类的基础.html\"},{\"text\":\"10_深入类和对象\",\"link\":\"/IT-learning/c++基础/10_深入类和对象.html\"},{\"text\":\"11_类的大小、继承与权限控制\",\"link\":\"/IT-learning/c++基础/11_类的大小、继承与权限控制.html\"},{\"text\":\"12_继承进阶\",\"link\":\"/IT-learning/c++基础/12_继承进阶.html\"},{\"text\":\"13_类型转换和多态与虚函数\",\"link\":\"/IT-learning/c++基础/13_类型转换和多态与虚函数.html\"},{\"text\":\"14_纯虚函数、抽象类、深浅拷贝及智能指针\",\"link\":\"/IT-learning/c++基础/14_纯虚函数、抽象类、深浅拷贝及智能指针.html\"},{\"text\":\"15_运算符重载与 String 类详解\",\"link\":\"/IT-learning/c++基础/15_运算符重载与 String 类详解.html\"},{\"text\":\"16_有序容器与无序容器\",\"link\":\"/IT-learning/c++基础/16_有序容器与无序容器.html\"},{\"text\":\"17_模板\",\"link\":\"/IT-learning/c++基础/17_模板.html\"},{\"text\":\"18_迭代器与其应用\",\"link\":\"/IT-learning/c++基础/18_迭代器与其应用.html\"},{\"text\":\"19_C++ 标准库常用算法\",\"link\":\"/IT-learning/c++基础/19_C++ 标准库常用算法.html\"},{\"text\":\"20_C++ 异常处理 - 第19次课\",\"link\":\"/IT-learning/c++基础/20_C++ 异常处理 - 第19次课.html\"},{\"text\":\"21_友元及友元相关内容\",\"link\":\"/IT-learning/c++基础/21_友元及友元相关内容.html\"},{\"text\":\"22_C++ IO 流详解-feadbc607d7f\",\"link\":\"/IT-learning/c++基础/22_C++ IO 流详解-feadbc607d7f.html\"},{\"text\":\"23_C++ IO 流详解\",\"link\":\"/IT-learning/c++基础/23_C++ IO 流详解.html\"},{\"text\":\"24_位运算符总结\",\"link\":\"/IT-learning/c++基础/24_位运算符总结.html\"},{\"text\":\"25_C++三种继承方式\",\"link\":\"/IT-learning/c++基础/25_C++三种继承方式.html\"},{\"text\":\"26_C++11 高级特性\",\"link\":\"/IT-learning/c++基础/26_C++11 高级特性.html\"},{\"text\":\"27_C++14 新特性\",\"link\":\"/IT-learning/c++基础/27_C++14 新特性.html\"},{\"text\":\"28_C++17 新特性\",\"link\":\"/IT-learning/c++基础/28_C++17 新特性.html\"},{\"text\":\"29_多文件和 Makefile工程管理\",\"link\":\"/IT-learning/c++基础/29_多文件和 Makefile工程管理.html\"},{\"text\":\"30_C++大型项目CMake工程管理\",\"link\":\"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html\"},{\"text\":\"31_C++ 主要就业方向与技术能力分析报告\",\"link\":\"/IT-learning/c++基础/31_C++ 主要就业方向与技术能力分析报告.html\"},{\"text\":\"32_C++ 基础知识回顾\",\"link\":\"/IT-learning/c++基础/32_C++ 基础知识回顾.html\"}],\"collapsed\":false},{\"text\":\"计算机图形学\",\"items\":[{\"text\":\"02.直线光栅化\",\"link\":\"/IT-learning/计算机图形学/02.直线光栅化.html\"}],\"collapsed\":false}]}],\"/guide/\":[{\"items\":[{\"text\":\"博客结构\",\"link\":\"/guide/博客结构.html\"}]}],\"/update/\":[{\"items\":[{\"text\":\"更新日志\",\"link\":\"/update/更新日志.html\"}]}],\"/我的感悟/\":[{\"items\":[{\"text\":\"2024\",\"items\":[{\"text\":\"不同商家的视野\",\"link\":\"/我的感悟/2024/不同商家的视野.html\"},{\"text\":\"学而篇\",\"link\":\"/我的感悟/2024/学而篇.html\"},{\"text\":\"重温士兵突击\",\"link\":\"/我的感悟/2024/重温士兵突击.html\"}],\"collapsed\":false}]}],\"/技术问题清单/\":[{\"items\":[{\"text\":\"doccano账户管理\",\"link\":\"/技术问题清单/doccano账户管理.html\"},{\"text\":\"专英翻转课堂—PyTorch\",\"link\":\"/技术问题清单/专英翻转课堂—PyTorch.html\"},{\"text\":\"虚拟机网络问题\",\"link\":\"/技术问题清单/虚拟机网络问题.html\"}]}],\"/生活与算法/\":[{\"items\":[{\"text\":\"贪心算法\",\"items\":[{\"text\":\"1.人的本性——贪心！\",\"link\":\"/生活与算法/贪心算法/1.人的本性——贪心！.html\"},{\"text\":\"2.初步感受贪心\",\"link\":\"/生活与算法/贪心算法/2.初步感受贪心.html\"},{\"text\":\"3. 分发饼干问题\",\"link\":\"/生活与算法/贪心算法/3. 分发饼干问题.html\"}],\"collapsed\":false}]}]}},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>