import{_ as a,c as i,o as n,a2 as t}from"./chunks/framework.DA-Pb-tg.js";const g=JSON.parse('{"title":"1 概念理解","description":"","frontmatter":{},"headers":[],"relativePath":"AI/01_deep_learning_theory/02-back_propagation.md","filePath":"AI/01_deep_learning_theory/02-back_propagation.md","lastUpdated":null}'),p={name:"AI/01_deep_learning_theory/02-back_propagation.md"};function e(l,s,r,h,o,c){return n(),i("div",null,s[0]||(s[0]=[t(`<h1 id="_1-概念理解" tabindex="-1">1 概念理解 <a class="header-anchor" href="#_1-概念理解" aria-label="Permalink to &quot;1 概念理解&quot;">​</a></h1><h2 id="_1-1-神经网络训练流程概述" tabindex="-1">1.1 神经网络训练流程概述 <a class="header-anchor" href="#_1-1-神经网络训练流程概述" aria-label="Permalink to &quot;1.1 神经网络训练流程概述&quot;">​</a></h2><p>        当我们使用前馈神经网络（feedforward neural network）接收输入 x 并产生输出 y 时，信息通过网络向前流动。输入 x 提供初始信息，然后传播到每一层的隐藏单元，最终产生输出 y。这称之为前向传播（forward propagation）。 在训练过程中，前向传播可以持续向前直到它产生一个<strong>标量</strong> 的 损失函数 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>J</mi><mo>(</mo><mi>θ</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">J(\\theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.09618em;">J</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span></span> 。 反向传播（back propagation）算法经常简称为backprop，允许来自代价函数的信息通过网络向后流动，以便计算梯度。<br></p><h2 id="_1-2-反向传播的定义" tabindex="-1">1.2 反向传播的定义 <a class="header-anchor" href="#_1-2-反向传播的定义" aria-label="Permalink to &quot;1.2 反向传播的定义&quot;">​</a></h2><p>        反向传播（英语：Backpropagation，意为<strong>误差</strong>反向传播，缩写为BP）是对多层人工神经网络进行梯度下降的算法，也就是用<strong>链式法则</strong>以网络每层的<strong>权重</strong>为变量计算<strong>损失函数</strong>的梯度，以<strong>更新权重</strong>来最小化损失函数。<br></p><h1 id="_2-梯度下降算法简述" tabindex="-1">2 梯度下降算法简述 <a class="header-anchor" href="#_2-梯度下降算法简述" aria-label="Permalink to &quot;2 梯度下降算法简述&quot;">​</a></h1><ul><li><p>多元函数 f 的梯度定义为：<br><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/back-propagation-formula1.jpg" alt="梯度公式"></p></li><li><p>梯度有一个非常重要的性质：<strong>函数f沿梯度方向增加（上升）最快, 函数f沿负梯度方向减小（下降）最快。</strong></p></li><li><p>梯度下降法(SGD)算法, ：<br><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/back-propagation-figure1.jpg" alt="梯度下降法"></p></li><li><p>梯度下降法效果展示：<br><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/back-propagation-gif1.gif" alt="梯度下降法"></p></li><li><p>梯度下降法代码展示：<br></p></li></ul><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">#coding:utf8</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    </span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> fun</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(x,y):</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> y</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> dfun_x</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(x,y): </span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> </span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> dfun_y</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(x,y):</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">if</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> __name__</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> ==</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;__main__&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:    </span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 1</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    y </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 4</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    lr </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.01</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    iters </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 4000</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    for</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> iter</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> range</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(iters):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> lr</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> dfun_x(x, y)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        y </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> y </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> lr</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> dfun_y(x, y)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;loss = &#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, fun(x, y))</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;x=&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,x)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;y=&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,y)</span></span></code></pre></div><h1 id="_3-bp-或-深度神经网络训练需要明确的几个概念" tabindex="-1">3 BP 或 深度神经网络训练需要明确的几个概念 <a class="header-anchor" href="#_3-bp-或-深度神经网络训练需要明确的几个概念" aria-label="Permalink to &quot;3 BP 或 深度神经网络训练需要明确的几个概念&quot;">​</a></h1><p>一个典型的深度神经网络图如下：<br><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/back-propagation-figure2.jpg" alt="网络结构图"></p><p>进一步，一个深度学习模型中的所有数据可划分为如下类别：</p><ul><li>权重(weight) 或 参数(parameter)</li><li>激活(activation)</li><li>超参</li></ul><p><strong>思考：请分析上图中参数的类别 ？？？</strong></p><p>再进一步，按照逻辑先后顺序反向传播算法（BP 算法）可划分为两个阶段：<br></p><ul><li>激励传播(反向传播)</li><li>权重更新</li></ul><p><strong>思考： 反向传播的目的是求 激活的梯度 还是 权重的梯度 ？？？</strong> <br><strong>思考： 我们需要同时计算出 激活的梯度 和 权重的梯度吗 ？？？</strong> <br></p><h1 id="_4-链式求导法则" tabindex="-1">4 链式求导法则 <a class="header-anchor" href="#_4-链式求导法则" aria-label="Permalink to &quot;4 链式求导法则&quot;">​</a></h1><p>一个深度神经网络可以理解为一个复杂的复合函数：<br></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mo>=</mo><mi>f</mi><mo>(</mo><mi>w</mi><mo>)</mo><mo separator="true">;</mo><mi>y</mi><mo>=</mo><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo separator="true">;</mo><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo>=</mo><mi>f</mi><mo>(</mo><mi>y</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">x = f(w); y = f(x); loss = f(y) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit">x</span><span class="mrel">=</span><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="mclose">)</span><span class="mpunct">;</span><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mrel">=</span><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mpunct">;</span><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mord mathit">o</span><span class="mord mathit">s</span><span class="mord mathit">s</span><span class="mrel">=</span><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span></span></p><p>当计算 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>w</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\\frac{\\partial loss}{\\partial w}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8801079999999999em;"></span><span class="strut bottom" style="height:1.2251079999999999em;vertical-align:-0.345em;"></span><span class="base textstyle uncramped"><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.345em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathrm" style="margin-right:0.05556em;">∂</span><span class="mord mathit" style="margin-right:0.02691em;">w</span></span></span></span><span style="top:-0.22999999999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.394em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm" style="margin-right:0.05556em;">∂</span><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mord mathit">o</span><span class="mord mathit">s</span><span class="mord mathit">s</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span> 时就需要用到链式求导, 链式求导有两种情况需要考虑：</p><ul><li><p>情况一：无分支 <br><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/back-propagation-figure3.jpg" alt="链式求导1"></p></li><li><p>情况二：存在分支 <br><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/back-propagation-figure4.jpg" alt="链式求导2"></p></li></ul><p><strong>===== 有了以上背景知识，我们就可以进行反向传播(back propagation) 的计算了。======</strong></p><h1 id="_5-bp-流程图示" tabindex="-1">5 BP 流程图示 <a class="header-anchor" href="#_5-bp-流程图示" aria-label="Permalink to &quot;5 BP 流程图示&quot;">​</a></h1><p>        在前馈神经网络最后，网络的输出信号 y 与目标值(label)进行比较，这个目标值可以在训练数据集中找到。这个差异(difference)被称为输出层神经元的误差信号 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>δ</mi></mrow><annotation encoding="application/x-tex">\\delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03785em;">δ</span></span></span></span> 。</p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/back-propagation-figure5.jpg" alt="error signal"></p><p>        直接计算内部神经元的误差信号是不可能的，因为这些神经元的输出值是未知的。多层网络的有效训练方法长时间以来一直未知。直到上世纪八十年代中期，反向传播算法才被提出。其思想是将误差信号 d（在单个训练步骤中计算得出）传播回所有输出信号作为该神经元的输入的神经元中。<br></p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/back-propagation-figure6.jpg" alt="error signal propagation"></p><p>        用于传播误差的权重系数 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mrow><mi>m</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">w_{mn}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.02691em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">m</span><span class="mord mathit">n</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> 等于计算输出值时使用的权重系数。只是数据流的方向改变了（信号依次从输出传播到输入）。这种技术适用于所有网络层。如果传播的误差来自多个神经元，则进行相加。下面是示例图解：<br></p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/back-propagation-figure7.jpg" alt="error signal propagation"></p><p>当计算完每个神经元的误差信号后，可以修改每个神经元输入节点的权重系数。<br></p><ul><li><p>第一层权重修改：<br><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/back-propagation-figure8.jpg" alt="weight update"><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/back-propagation-figure9.jpg" alt="weight update"><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/back-propagation-figure10.jpg" alt="weight update"></p></li><li><p>第二层权重修改：<br><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/back-propagation-figure11.jpg" alt="weight update"><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/back-propagation-figure12.jpg" alt="weight update"></p></li><li><p>第三层权重修改：<br><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/back-propagation-figure13.jpg" alt="weight update"></p></li></ul><p><strong>思考：权重的梯度什么时候计算的 ？？</strong></p><h1 id="_6-反向传播数学推导" tabindex="-1">6 反向传播数学推导 <a class="header-anchor" href="#_6-反向传播数学推导" aria-label="Permalink to &quot;6 反向传播数学推导&quot;">​</a></h1><h2 id="_6-1-反向传播目的确认" tabindex="-1">6.1 反向传播目的确认 <a class="header-anchor" href="#_6-1-反向传播目的确认" aria-label="Permalink to &quot;6.1 反向传播目的确认&quot;">​</a></h2><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/back-propagation-figure14.jpg" alt="bp-target"></p><h2 id="_6-2-线性连接层-weight-的梯度" tabindex="-1">6.2 线性连接层 weight 的梯度 <a class="header-anchor" href="#_6-2-线性连接层-weight-的梯度" aria-label="Permalink to &quot;6.2 线性连接层 weight 的梯度&quot;">​</a></h2><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/back-propagation-figure15.jpg" alt="linear backward"></p><h2 id="_6-3-激活函数-input-的梯度" tabindex="-1">6.3 激活函数 input 的梯度 <a class="header-anchor" href="#_6-3-激活函数-input-的梯度" aria-label="Permalink to &quot;6.3 激活函数 input 的梯度&quot;">​</a></h2><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/back-propagation-figure16.jpg" alt="linear backward"></p><h2 id="_6-4-激活函数-output-的梯度" tabindex="-1">6.4 激活函数 output 的梯度 <a class="header-anchor" href="#_6-4-激活函数-output-的梯度" aria-label="Permalink to &quot;6.4 激活函数 output 的梯度&quot;">​</a></h2><ul><li><p>求解过程 <br><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/back-propagation-figure17.jpg" alt="linear backward"></p></li><li><p>公式化简 <br><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/back-propagation-figure18.jpg" alt="linear backward"></p></li><li><p>最终形式 <br><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/back-propagation-figure19.jpg" alt="linear backward"></p></li></ul><h2 id="_6-5-下层激活-input-z-and-z-梯度求解" tabindex="-1">6.5 下层激活 input(z&#39; and z&#39;&#39;) 梯度求解 <a class="header-anchor" href="#_6-5-下层激活-input-z-and-z-梯度求解" aria-label="Permalink to &quot;6.5 下层激活 input(z&#39; and z&#39;&#39;) 梯度求解&quot;">​</a></h2><ol><li><p>下层是output的情况：<br><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/back-propagation-figure20.jpg" alt="linear backward"></p></li><li><p>下层是中间层的情况：<br><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/back-propagation-figure21.jpg" alt="linear backward"></p></li></ol><h1 id="_7-反向传播总结" tabindex="-1">7 反向传播总结 <a class="header-anchor" href="#_7-反向传播总结" aria-label="Permalink to &quot;7 反向传播总结&quot;">​</a></h1><ul><li><p>所有激活梯度求解 <br><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/back-propagation-figure22.jpg" alt="linear backward"></p></li><li><p>所有权重梯度求解 <br><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/back-propagation-figure23.jpg" alt="linear backward"></p></li></ul><p>        损失C对W的权重有两部分，一部分是第一项，激活函数Z对W的偏导数（a）, 此项其实就是前向传播，另一个是第二项，C对激活函数Z的偏导数，此项就是反向传播。<br></p>`,46)]))}const k=a(p,[["render",e]]);export{g as __pageData,k as default};
