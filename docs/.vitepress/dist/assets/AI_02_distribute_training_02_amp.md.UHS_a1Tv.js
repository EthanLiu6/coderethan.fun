import{_ as a,c as i,o as n,a2 as l}from"./chunks/framework.DA-Pb-tg.js";const c=JSON.parse('{"title":"1. AMP 论文","description":"","frontmatter":{},"headers":[],"relativePath":"AI/02_distribute_training/02_amp.md","filePath":"AI/02_distribute_training/02_amp.md","lastUpdated":1743069065000}'),e={name:"AI/02_distribute_training/02_amp.md"};function t(p,s,h,r,k,o){return n(),i("div",null,s[0]||(s[0]=[l(`<h1 id="_1-amp-论文" tabindex="-1">1. AMP 论文 <a class="header-anchor" href="#_1-amp-论文" aria-label="Permalink to &quot;1. AMP 论文&quot;">​</a></h1><ul><li><a href="https://github.com/Elvin-Ma/ai_papers/blob/main/mixed_precision/mixed-precision.md" target="_blank" rel="noreferrer">我的论文链接</a></li><li><a href="https://arxiv.org/pdf/1710.03740" target="_blank" rel="noreferrer">论文链接-EN</a></li></ul><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image.png" alt="alt text"></p><h1 id="_2-apex" tabindex="-1">2. Apex <a class="header-anchor" href="#_2-apex" aria-label="Permalink to &quot;2. Apex&quot;">​</a></h1><ul><li>Pytorch 的 AMP 其实是从 apex 简化而来的，和 apex 的 O1 相当。</li></ul><p><strong>apex 几个级别</strong> <br></p><ul><li>O0: FP32 training <br>     Your incoming model should be FP32 already, so this is likely a no-op. O0 can be useful to establish an accuracy baseline. <br></li></ul><div class="language-c++ vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">c++</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Default properties set by O0:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">cast_model_type</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">torch.float32</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">patch_torch_functions</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">False</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">keep_batchnorm_fp32</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (effectively, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;not applicable,&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> everything is FP32)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">master_weights</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">False</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">loss_scale</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1.0</span></span></code></pre></div><ul><li>O1: Mixed Precision (recommended for typical use) <br>     Patch all Torch functions and Tensor methods to cast their inputs according to a whitelist-blacklist model. Whitelist ops (for example, Tensor Core-friendly ops like GEMMs and convolutions) are performed in FP16. Blacklist ops that benefit from FP32 precision (for example, softmax) are performed in FP32. O1 also uses dynamic loss scaling, unless overridden. <br></li></ul><div class="language-c++ vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">c++</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Default properties set by O1:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">cast_model_type</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> applicable)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">patch_torch_functions</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">True</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">keep_batchnorm_fp32</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (again, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> applicable, all model weights remain FP32)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">master_weights</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> applicable, model weights remain FP32)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">loss_scale</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;dynamic&quot;</span></span></code></pre></div><ul><li>O2: &quot;Almost FP16&quot; Mixed Precision <br>     O2 casts the model weights to FP16, patches the model&#39;s forward method to cast input data to FP16, keeps batchnorms in FP32, maintains FP32 master weights, updates the optimizer&#39;s param_groups so that the optimizer.step() acts directly on the FP32 weights (followed by FP32 master weight-&gt;FP16 model weight copies if necessary), and implements dynamic loss scaling (unless overridden). Unlike O1, O2 does not patch Torch functions or Tensor methods. <br></li></ul><div class="language-c++ vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">c++</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Default properties set by O2:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">cast_model_type</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">torch.float16</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">patch_torch_functions</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">False</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">keep_batchnorm_fp32</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">True</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">master_weights</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">True</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">loss_scale</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;dynamic&quot;</span></span></code></pre></div><ul><li>O3: FP16 training <br>     O3 may not achieve the stability of the true mixed precision options O1 and O2. However, it can be useful to establish a speed baseline for your model, against which the performance of O1 and O2 can be compared. If your model uses batch normalization, to establish &quot;speed of light&quot; you can try O3 with the additional property override keep_batchnorm_fp32=True (which enables cudnn batchnorm, as stated earlier). <br></li></ul><div class="language-c++ vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">c++</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Default properties set by O3:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">cast_model_type</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">torch.float16</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">patch_torch_functions</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">False</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">keep_batchnorm_fp32</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">False</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">master_weights</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">False</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">loss_scale</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1.0</span></span></code></pre></div><h1 id="_3-pytorch-中amp的用法" tabindex="-1">3. pytorch 中amp的用法 <a class="header-anchor" href="#_3-pytorch-中amp的用法" aria-label="Permalink to &quot;3. pytorch 中amp的用法&quot;">​</a></h1><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.cuda.amp </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> autocast </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> autocast</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Creates model and optimizer in default precision</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Net().cuda()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">optimizer </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> optim.SGD(model.parameters(), </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">...</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Creates a GradScaler once at the beginning of training.</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">scaler </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> GradScaler()</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> epoch </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> epochs:</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    for</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> input</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, target </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> data:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        optimizer.zero_grad()</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # Runs the forward pass with autocasting.</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        with</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> autocast(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">device_type</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;cuda&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">torch.float16):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            output </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            loss </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> loss_fn(output, target)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # Scales loss.  Calls backward() on scaled loss to create scaled gradients.</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # Backward passes under autocast are not recommended.</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # Backward ops run in the same dtype autocast chose for corresponding forward ops.</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        scaler.scale(loss).backward()</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # scaler.step() first unscales the gradients of the optimizer&#39;s assigned params.</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # If these gradients do not contain infs or NaNs, optimizer.step() is then called,</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # otherwise, optimizer.step() is skipped.</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        scaler.step(optimizer)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # Updates the scale for next iteration.</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        scaler.update()</span></span></code></pre></div><h1 id="_4-fp16-训练的优势和不足" tabindex="-1">4 fp16 训练的优势和不足 <a class="header-anchor" href="#_4-fp16-训练的优势和不足" aria-label="Permalink to &quot;4 fp16 训练的优势和不足&quot;">​</a></h1><h2 id="_4-1-使用fp16训练神经网络-相对比使用fp32带来的优点" tabindex="-1">4.1 使用FP16训练神经网络，相对比使用FP32带来的优点 <a class="header-anchor" href="#_4-1-使用fp16训练神经网络-相对比使用fp32带来的优点" aria-label="Permalink to &quot;4.1 使用FP16训练神经网络，相对比使用FP32带来的优点&quot;">​</a></h2><ul><li>减少内存占用：FP16的位宽是FP32的一半，因此权重等参数所占用的内存也是原来的一半，节省下来的内存可以放更大的网络模型或者使用更多的数据进行训练。</li><li>加快通讯效率：针对分布式训练，特别是在大模型训练的过程中，通讯的开销制约了网络模型训练的整体性能，通讯的位宽少了意味着可以提升通讯性能，减少等待时间，加快数据的流通。</li><li>计算效率更高：在特殊的AI加速芯片如华为Ascend 910和310系列，或者NVIDIA VOTAL架构的Titan V and Tesla V100的GPU上，使用FP16的执行运算性能比FP32更加快。</li></ul><h2 id="_4-2-fp16训练的问题" tabindex="-1">4.2 FP16训练的问题 <a class="header-anchor" href="#_4-2-fp16训练的问题" aria-label="Permalink to &quot;4.2 FP16训练的问题&quot;">​</a></h2><p>        但是使用FP16同样会带来一些问题，其中最重要的是1）精度溢出和2）舍入误差。<br></p><ul><li>数据溢出：数据溢出比较好理解，FP16的有效数据表示范围为 6.10×10−5∼65504 ，FP32的有效数据表示范围为 1.4×10−45 1.7×1038 。可见FP16相比FP32的有效范围要窄很多，使用FP16替换FP32会出现上溢（Overflow）和下溢（Underflow）的情况。而在深度学习中，需要计算网络模型中权重的梯度（一阶导数），因此梯度会比权重值更加小，往往容易出现下溢情况。<br></li><li>舍入误差：Rounding Error指示是当网络模型的反向梯度很小，一般FP32能够表示，但是转换到FP16会小于当前区间内的最小间隔，会导致数据溢出。如0.00006666666在FP32中能正常表示，转换到FP16后会表示成为0.000067，不满足FP16最小间隔的数会强制舍入。<br></li></ul><h1 id="_5-loss-进行-scale-缩放" tabindex="-1">5. loss 进行 scale 缩放 <a class="header-anchor" href="#_5-loss-进行-scale-缩放" aria-label="Permalink to &quot;5. loss 进行 scale 缩放&quot;">​</a></h1><h2 id="_5-1-损失缩放基本原理" tabindex="-1">5.1 损失缩放基本原理 <a class="header-anchor" href="#_5-1-损失缩放基本原理" aria-label="Permalink to &quot;5.1 损失缩放基本原理&quot;">​</a></h2><p>        损失放大是需要结合混合精度实现的，其主要的主要思路是：<br></p><ol><li>Scale up阶段，网络模型前向计算后在反响传播前，将得到的损失变化值DLoss增大2^K倍。</li><li>Scale down阶段，反向传播后，将权重梯度缩2^K倍，恢复FP32值进行存储。</li></ol><h2 id="_5-2-动态损失缩放-dynamic-loss-scaling" tabindex="-1">5.2 动态损失缩放（Dynamic Loss Scaling）： <a class="header-anchor" href="#_5-2-动态损失缩放-dynamic-loss-scaling" aria-label="Permalink to &quot;5.2 动态损失缩放（Dynamic Loss Scaling）：&quot;">​</a></h2><p>        上面提到的损失缩放都是使用一个默认值对损失值进行缩放，为了充分利用FP16的动态范围，可以更好地缓解舍入误差，<strong>尽量使用比较大的放大倍数</strong>。总结动态损失缩放算法，就是每当梯度溢出时候减少损失缩放规模，并且间歇性地尝试增加损失规模，从而实现在不引起溢出的情况下使用最高损失缩放因子，更好地恢复精度。<br></p><h2 id="_5-3-动态损失缩放的算法" tabindex="-1">5.3 动态损失缩放的算法 <a class="header-anchor" href="#_5-3-动态损失缩放的算法" aria-label="Permalink to &quot;5.3 动态损失缩放的算法&quot;">​</a></h2><ol><li>动态损失缩放的算法会从比较高的缩放因子开始（如2^24），然后开始进行训练迭代中检查数是否会溢出（Infs/Nans）；</li><li>如果没有梯度溢出，则不进行缩放，继续进行迭代；如果检测到梯度溢出，则缩放因子会减半，重新确认梯度更新情况，直到数不产生溢出的范围内；</li><li>在训练的后期，loss已经趋近收敛稳定，梯度更新的幅度往往小了，这个时候可以允许更高的损失缩放因子来再次防止数据下溢。</li><li>因此，动态损失缩放算法会尝试在每N（N=2000）次迭代将损失缩放增加F倍数，然后执行步骤2检查是否溢出。</li></ol><h1 id="_6-使用bf16-数据类型" tabindex="-1">6 使用bf16 数据类型 <a class="header-anchor" href="#_6-使用bf16-数据类型" aria-label="Permalink to &quot;6 使用bf16 数据类型&quot;">​</a></h1><ul><li><p>float16 的情况：</p><ul><li><p>float16 的动态范围较小，容易导致梯度下溢或上溢。</p></li><li><p>使用 float16 时，通常需要通过 scaler（如 GradScaler）对损失进行缩放，以保持梯度的数值稳定性。</p></li></ul></li><li><p>bfloat16 的情况：</p><ul><li><p>bfloat16 的动态范围与 float32 相近，数值稳定性较好。</p></li><li><p>使用 bfloat16 时，通常不需要额外的 scaler 进行缩放。</p></li></ul></li></ul><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>import torch</span></span>
<span class="line"><span>import torch.nn as nn</span></span>
<span class="line"><span>import torch.optim as optim</span></span>
<span class="line"><span></span></span>
<span class="line"><span># 模型和优化器</span></span>
<span class="line"><span>model = nn.Linear(10, 1).to(&#39;cuda&#39;)</span></span>
<span class="line"><span>optimizer = optim.SGD(model.parameters(), lr=0.01)</span></span>
<span class="line"><span></span></span>
<span class="line"><span># 使用 bfloat16</span></span>
<span class="line"><span>with torch.autocast(device_type=&#39;cuda&#39;, dtype=torch.bfloat16):</span></span>
<span class="line"><span>    output = model(torch.randn(16, 10).to(&#39;cuda&#39;))</span></span>
<span class="line"><span>    loss = nn.MSELoss()(output, torch.randn(16, 1).to(&#39;cuda&#39;))</span></span>
<span class="line"><span></span></span>
<span class="line"><span># 反向传播和优化</span></span>
<span class="line"><span>optimizer.zero_grad()</span></span>
<span class="line"><span>loss.backward()</span></span>
<span class="line"><span>optimizer.step()</span></span></code></pre></div><h1 id="_7-参考链接" tabindex="-1">7 参考链接 <a class="header-anchor" href="#_7-参考链接" aria-label="Permalink to &quot;7 参考链接&quot;">​</a></h1><ul><li><a href="https://developer.nvidia.com/blog/mixed-precision-training-deep-neural-networks/" target="_blank" rel="noreferrer">nvidia-developer</a></li><li><a href="https://docs.nvidia.com/deeplearning/performance/pdf/Training-Mixed-Precision-User-Guide.pdf" target="_blank" rel="noreferrer">user-guide-pdf</a></li><li><a href="https://github.com/NVIDIA/apex/blob/master/docs/source/amp.rst" target="_blank" rel="noreferrer">apex</a></li></ul>`,35)]))}const E=a(e,[["render",t]]);export{c as __pageData,E as default};
