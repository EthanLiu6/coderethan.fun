import{_ as i,c as a,o as n,a2 as l}from"./chunks/framework.DA-Pb-tg.js";const o=JSON.parse('{"title":"0 pytorch 3D parallel","description":"","frontmatter":{},"headers":[],"relativePath":"AI/02_distribute_training/25_pytorch-3D-Parallel-IMPL.md","filePath":"AI/02_distribute_training/25_pytorch-3D-Parallel-IMPL.md","lastUpdated":null}'),t={name:"AI/02_distribute_training/25_pytorch-3D-Parallel-IMPL.md"};function p(e,s,h,k,r,d){return n(),a("div",null,s[0]||(s[0]=[l(`<h1 id="_0-pytorch-3d-parallel" tabindex="-1">0 pytorch 3D parallel <a class="header-anchor" href="#_0-pytorch-3d-parallel" aria-label="Permalink to &quot;0 pytorch 3D parallel&quot;">​</a></h1><ul><li><a href="https://arxiv.org/abs/2401.10241" target="_blank" rel="noreferrer">参考论文</a></li></ul><table tabindex="0"><thead><tr><th>名称</th><th>描述</th></tr></thead><tbody><tr><td>DTensor</td><td>Expressive tensor abstraction to replace flat parameter to manage parameter shard.</td></tr><tr><td>DeviceMesh</td><td>Device abstraction: represents the distributed system as a multi-dimensional array.</td></tr><tr><td>FSDP2 + TP Composability</td><td>Incorporates user-selectable combinations of N-D parallelism training.</td></tr><tr><td>FSDP2 + TP + PP Composability</td><td>Incorporates user-selectable combinations of N-D parallelism training.</td></tr><tr><td>Meta Device Initialization</td><td>Init meta device on each device first and initialize the parameters according to sharding layouts and RNG (Random Number Generator).</td></tr><tr><td>Selective Activation Checkpoint</td><td>Flexible AC (activation checkpoint) and SAC (selective activation checkpoint) options utilizing <code>torch.utils.checkpoint</code>.</td></tr><tr><td>Region Compilation</td><td>通过区域编译，识别相同结构，缩短编译时间，同时和FSDP、TP相兼容，通过计算-通信重排提升吞吐和内存方面效率。</td></tr><tr><td>Asyn TP</td><td>微流水线实现TP中计算和通信的重叠，同时利用SymmetricMemory抽象，通过在每个GPU上分配共享缓冲区实现更快通信。</td></tr><tr><td>Mixed Precision Training with Float8</td><td>支持了使用Float8进行更高级的混合精度训练（逐张量缩放策略、与autograd、torch.compile、fsdp2、TP组合）。</td></tr><tr><td>Distributed Checkpointing</td><td>通过DTensor封装全局和局部张量信息实现DCP，并与异步检查点技术相结合进一步提升效率。</td></tr><tr><td>HSDP (Hybrid Sharded Data Parallel)</td><td>HSDP相对于FSDP的通信饱和点可以将总world size扩展3-6倍。</td></tr></tbody></table><h1 id="_1-torchtitan-未来发展" tabindex="-1">1 torchtitan 未来发展 <a class="header-anchor" href="#_1-torchtitan-未来发展" aria-label="Permalink to &quot;1 torchtitan 未来发展&quot;">​</a></h1><table tabindex="0"><thead><tr><th>名称</th><th>描述</th></tr></thead><tbody><tr><td>4D Parallel</td><td>整合Context parallel，实现4D-Parallel。</td></tr><tr><td>Zero-Bubble Pipeline Schedules</td><td>参考论文：<a href="https://arxiv.org/abs/2401.10241" target="_blank" rel="noreferrer">arXiv:2401.10241</a>。</td></tr><tr><td>External Contributions</td><td>构建和评估自定义创新。</td></tr></tbody></table><h1 id="_2-torchtitan-使用及配置流程" tabindex="-1">2 torchtitan 使用及配置流程 <a class="header-anchor" href="#_2-torchtitan-使用及配置流程" aria-label="Permalink to &quot;2 torchtitan 使用及配置流程&quot;">​</a></h1><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/figure1.png" alt="figure1"></p><h2 id="step-1-初始化模型并配置pp" tabindex="-1">step 1 初始化模型并配置PP <a class="header-anchor" href="#step-1-初始化模型并配置pp" aria-label="Permalink to &quot;step 1 初始化模型并配置PP&quot;">​</a></h2><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># meta init</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">with</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;meta&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model_cls.from_model_args(model_config)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># apply PP</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">pp_schedule, model_parts </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> models_pipelining_fns[model_name](</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    model, pp_mesh, parallel_dims, job_config, device, model_config, loss_fn</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># For PP with looped schedules, each item in model_parts is one stage-model-chunk.</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># We need to iterate through model_parts to apply SPMD parallelisms, compilation,</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># optimizer, and checkpointing</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model_parts:</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # apply SPMD-style distributed training techniques</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    models_parallelize_fns[model_name](m, world_mesh, parallel_dims, job_config)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # move sharded model to GPU and initialize weights via DTensor</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    m.to_empty(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">device</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">init_device)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    m.init_weights(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">buffer_device</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">buffer_device)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    m.train()</span></span></code></pre></div><p><strong>pp的具体配置流程</strong> <br></p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> pipeline_llama</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    model: nn.Module,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    pp_mesh: DeviceMesh,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    parallel_dims: ParallelDims,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    job_config: JobConfig,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    device: DeviceType,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    model_config: ModelArgs,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    loss_fn: Callable[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">...</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, torch.Tensor],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    stages, models </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pipeline_llama_manual_split(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        model, pp_mesh, parallel_dims, job_config, device, model_config</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    )</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    pp_schedule </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> build_pipeline_schedule(job_config, stages, loss_fn)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pp_schedule, models</span></span></code></pre></div><h2 id="step2-tp、sp-配置" tabindex="-1">step2 TP、SP 配置 <a class="header-anchor" href="#step2-tp、sp-配置" aria-label="Permalink to &quot;step2 TP、SP 配置&quot;">​</a></h2><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # Apply tensor + sequence parallelism to every transformer block</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">NOTE</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">: At the cost of model code change, we can accelerate Sequence Parallel</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    #       by folding (and unfolding) the batch dimension and the sequence dimension.</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    #       Examples can be found at https://github.com/pytorch/torchtitan/pull/437</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> layer_id, transformer_block </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.layers.items():</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        layer_plan </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> {</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">            &quot;attention_norm&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: SequenceParallel(),</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">            &quot;attention&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: prepare_module_input(</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">                input_layouts</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(Shard(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">),</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">                desired_input_layouts</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(Replicate(), </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">),</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            ),</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">            &quot;attention.wq&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: colwise_parallel(),</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">            &quot;attention.wk&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: colwise_parallel(),</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">            &quot;attention.wv&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: colwise_parallel(),</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">            &quot;attention.wo&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: rowwise_parallel(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">output_layouts</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Shard(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)),</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">            &quot;ffn_norm&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: SequenceParallel(),</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">            &quot;feed_forward&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: prepare_module_input(</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">                input_layouts</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(Shard(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">),),</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">                desired_input_layouts</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(Replicate(),),</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            ),</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">            &quot;feed_forward.w1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: colwise_parallel(),</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">            &quot;feed_forward.w2&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: rowwise_parallel(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">output_layouts</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Shard(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)),</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">            &quot;feed_forward.w3&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: colwise_parallel(),</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        }</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        parallelize_module(</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">            module</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">transformer_block,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">            device_mesh</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tp_mesh,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">            parallelize_plan</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">layer_plan,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        )</span></span></code></pre></div><h2 id="step3-配置fsdp" tabindex="-1">step3 配置FSDP <a class="header-anchor" href="#step3-配置fsdp" aria-label="Permalink to &quot;step3 配置FSDP&quot;">​</a></h2><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> apply_fsdp</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    model: nn.Module,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    dp_mesh: DeviceMesh,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    param_dtype: torch.dtype,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    reduce_dtype: torch.dtype,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    tp_enabled: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">bool</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    pp_enabled: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">bool</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    cpu_offload: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">bool</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> False</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">):</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    Apply data parallelism to the model. FSDP2 is used here.</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    mp_policy </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> MixedPrecisionPolicy(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">param_dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">param_dtype, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">reduce_dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">reduce_dtype)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    fsdp_config </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> {</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;mesh&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: dp_mesh, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;mp_policy&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: mp_policy}</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> cpu_offload:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        fsdp_config[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;offload_policy&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> CPUOffloadPolicy()</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> layer_id, transformer_block </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.layers.items():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pp_enabled:</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">            # For PP, do not reshard after forward to avoid per-microbatch</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">            # all-gathers, which can be expensive and non-overlapped</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            reshard_after_forward </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> False</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        else</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">            # As an optimization, do not reshard after forward for the last</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">            # transformer block since FSDP would prefetch it immediately</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            reshard_after_forward </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> int</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(layer_id) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> len</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(model.layers) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 1</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        fully_shard(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            transformer_block,</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">            **</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">fsdp_config,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">            reshard_after_forward</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">reshard_after_forward,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        )</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    fully_shard(model, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">**</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">fsdp_config, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">reshard_after_forward</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pp_enabled)</span></span></code></pre></div><h1 id="_3-模型配置" tabindex="-1">3 模型配置 <a class="header-anchor" href="#_3-模型配置" aria-label="Permalink to &quot;3 模型配置&quot;">​</a></h1><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/figure2.png" alt="figure1"></p>`,17)]))}const c=i(t,[["render",p]]);export{o as __pageData,c as default};
