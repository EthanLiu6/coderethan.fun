import{_ as a,c as n,o as t,a2 as e}from"./chunks/framework.DA-Pb-tg.js";const d=JSON.parse('{"title":"00-DL_Base_Notes","description":"","frontmatter":{},"headers":[],"relativePath":"AI/04_some_notes/00-DL_base_notes.md","filePath":"AI/04_some_notes/00-DL_base_notes.md","lastUpdated":null}'),i={name:"AI/04_some_notes/00-DL_base_notes.md"};function m(p,s,l,r,o,c){return t(),n("div",null,s[0]||(s[0]=[e('<h1 id="_00-dl-base-notes" tabindex="-1">00-DL_Base_Notes <a class="header-anchor" href="#_00-dl-base-notes" aria-label="Permalink to &quot;00-DL_Base_Notes&quot;">​</a></h1><h2 id="_1-normalization-还未coding-🌟🌟🌟🌟🌟" tabindex="-1">1. Normalization（还未coding）🌟🌟🌟🌟🌟 <a class="header-anchor" href="#_1-normalization-还未coding-🌟🌟🌟🌟🌟" aria-label="Permalink to &quot;1. Normalization（还未coding）🌟🌟🌟🌟🌟&quot;">​</a></h2><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/op-figure4.jpg" alt="figure4"></p><h3 id="_1-1-norm功能" tabindex="-1">1.1 Norm功能 <a class="header-anchor" href="#_1-1-norm功能" aria-label="Permalink to &quot;1.1 Norm功能&quot;">​</a></h3><p>Batch Norm，Layer Norm，Instance Norm，Group Norm</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi><mo>=</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mrow><mi mathvariant="normal">E</mi></mrow><mo>[</mo><mi>x</mi><mo>]</mo></mrow><mrow><msqrt><mrow><mrow><mi mathvariant="normal">V</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi></mrow><mo>[</mo><mi>x</mi><mo>]</mo><mo>+</mo><mi>ϵ</mi></mrow></msqrt></mrow></mfrac><mo>∗</mo><mi>γ</mi><mo>+</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.427em;"></span><span class="strut bottom" style="height:2.55701em;vertical-align:-1.13001em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mrel">=</span><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.825005em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="sqrt mord"><span class="sqrt-sign" style="top:-0.04500500000000007em;"><span class="style-wrap reset-textstyle textstyle uncramped"><span class="delimsizing size1">√</span></span></span><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="mord textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathrm" style="margin-right:0.01389em;">V</span><span class="mord mathrm">a</span><span class="mord mathrm">r</span></span><span class="mopen">[</span><span class="mord mathit">x</span><span class="mclose">]</span><span class="mbin">+</span><span class="mord mathit">ϵ</span></span></span><span style="top:-0.855005em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="reset-textstyle textstyle uncramped sqrt-line"></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span>​</span></span></span></span></span></span><span style="top:-0.22999999999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.677em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathit">x</span><span class="mbin">−</span><span class="mord textstyle uncramped"><span class="mord mathrm">E</span></span><span class="mopen">[</span><span class="mord mathit">x</span><span class="mclose">]</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mbin">∗</span><span class="mord mathit" style="margin-right:0.05556em;">γ</span><span class="mbin">+</span><span class="mord mathit" style="margin-right:0.05278em;">β</span></span></span></span></span></p><ul><li>去量纲，把数据调整到更强烈的数据分布</li><li>减少梯度消失和梯度爆炸</li><li>主要是有一个计算期望和方差的过程</li><li>做Norm的粒度不同，应用场景不同</li></ul><p>其他资料：<a href="https://blog.csdn.net/LoseInVain/article/details/86476010" target="_blank" rel="noreferrer">Batch Norm的技术博客</a></p><h3 id="_1-2-为啥不用bn来做nlp" tabindex="-1">1.2 为啥不用BN来做NLP？ <a class="header-anchor" href="#_1-2-为啥不用bn来做nlp" aria-label="Permalink to &quot;1.2 为啥不用BN来做NLP？&quot;">​</a></h3><ul><li>NLP的主要数据格式是[batch_szie, seq_len, embedding_dim]</li><li>Nlp 每个seq都是基本独立的，所以不能用Batch Norm</li><li>LN对应的维度就是对embedding_dim进行的</li></ul><p><strong>Batch Norm是逐channel（每个batch的同一个channel）进行标准化</strong>，也就是垮batch的。图片恰好需要这种方式。</p><p>LN是逐batch进行标准化的。NLP中往往是一个一个的seq进行训练的，而且长度不同，更适合这种。<strong>这让我想起了Attention的soft max操作是对一个行向量进行归一化的</strong></p><p>LayerNorm有助于稳定训练过程并提高收敛性。它的工作原理是对输入的各个特征进行归一化，确保激活的均值和方差一致。**普遍认为这种归一化有助于缓解与内部协变量偏移相关的问题，使模型能够更有效地学习并降低对初始权重的敏感性。**从架构图上看，LayerNorm在每个Transformer 块中应用两次，一次在自注意力机制之后，一次在FFN层之后，但是在实际工作中不一定如此。</p><p>文本长度不确定，而在LN层可以。</p><h3 id="_1-3-思考" tabindex="-1">1.3 思考 <a class="header-anchor" href="#_1-3-思考" aria-label="Permalink to &quot;1.3 思考&quot;">​</a></h3><ul><li><p>在训练和推理时有何不同？？？</p><blockquote><p>pytorch的模型有两种模式，在module模块里面有个‘training’属性，也有对应的API，里面明确指出了这个</p><p>在BatchNorm采用训练计算的结果（E和Var），应用到测试或者推理的时候</p><p>在Dropout后续会说，训练会drop掉，但推理不会，会成（1-rate）</p></blockquote><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> train</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self: T, mode: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">bool</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) -&gt; T:</span></span>\n<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        r</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;&quot;&quot;Set the module in training mode.</span></span>\n<span class="line"></span>\n<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        This has any effect only on certain modules. See documentations of</span></span>\n<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        particular modules for details of their behaviors in training/evaluation</span></span>\n<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,</span></span>\n<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        etc.</span></span>\n<span class="line"></span>\n<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        Args:</span></span>\n<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">            mode (bool): whether to set training mode (``True``) or evaluation</span></span>\n<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">                         mode (``False``). Default: ``True``.</span></span></code></pre></div></li><li><p>对于期望和方差计算策略？？？</p><blockquote><p><code>采用移动指数平均</code>，有点类似RNN了</p></blockquote></li></ul><h3 id="_1-4-rms-norm-大模型使用-🌟🌟🌟" tabindex="-1">1.4 RMS Norm(大模型使用)🌟🌟🌟 <a class="header-anchor" href="#_1-4-rms-norm-大模型使用-🌟🌟🌟" aria-label="Permalink to &quot;1.4 RMS Norm(大模型使用)🌟🌟🌟&quot;">​</a></h3><p>对LN做简化，对于NLP，对缩放敏感，对平移不敏感，所以分子不减<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>E</mi><mi>x</mi></msub></mrow><annotation encoding="application/x-tex">E_x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.05764em;">E</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.05764em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">x</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>，减少了很大计算量</p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250326224954810.png" alt="image-20250326224954810"></p><h2 id="_2-activation🌟🌟🌟" tabindex="-1">2. Activation🌟🌟🌟 <a class="header-anchor" href="#_2-activation🌟🌟🌟" aria-label="Permalink to &quot;2. Activation🌟🌟🌟&quot;">​</a></h2><h3 id="_2-1-non-linear-activations的两种类型" tabindex="-1">2.1 Non-linear Activations的两种类型 <a class="header-anchor" href="#_2-1-non-linear-activations的两种类型" aria-label="Permalink to &quot;2.1 Non-linear Activations的两种类型&quot;">​</a></h3><p>一种是逐元素操作（Element wise 或者Point wise），eg:ReLU,Sigmoid,Tanh,等，另一种是操作对象（元素）之间具有相关性，eg.Softmax</p><blockquote><p>element wise是操作的数据间彼此独立，并且输入与输出大小一致</p></blockquote><h3 id="_2-2" tabindex="-1">2.2 <a class="header-anchor" href="#_2-2" aria-label="Permalink to &quot;2.2&quot;">​</a></h3><h2 id="_3-loss-function🌟" tabindex="-1">3. Loss Function🌟 <a class="header-anchor" href="#_3-loss-function🌟" aria-label="Permalink to &quot;3. Loss Function🌟&quot;">​</a></h2><h2 id="_4-optimizer🌟🌟🌟🌟" tabindex="-1">4. Optimizer🌟🌟🌟🌟 <a class="header-anchor" href="#_4-optimizer🌟🌟🌟🌟" aria-label="Permalink to &quot;4. Optimizer🌟🌟🌟🌟&quot;">​</a></h2><blockquote><p>动量后面的Admw那些据估计忘了</p></blockquote><h2 id="_5-transformer🌟🌟🌟🌟🌟" tabindex="-1">5. Transformer🌟🌟🌟🌟🌟 <a class="header-anchor" href="#_5-transformer🌟🌟🌟🌟🌟" aria-label="Permalink to &quot;5. Transformer🌟🌟🌟🌟🌟&quot;">​</a></h2><p>深入理解请阅读Transformer系列文章<a href="/AI/03_Transformer/">Transformer</a></p><h3 id="_5-1-为啥attention的时候要除以" tabindex="-1">5.1 为啥Attention的时候要除以<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msqrt><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow></msqrt></mrow><annotation encoding="application/x-tex">\\sqrt{d_k}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8572200000000001em;"></span><span class="strut bottom" style="height:1.04em;vertical-align:-0.18278em;"></span><span class="base textstyle uncramped"><span class="sqrt mord"><span class="sqrt-sign" style="top:-0.017220000000000013em;"><span class="style-wrap reset-textstyle textstyle uncramped">√</span></span><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="mord textstyle cramped"><span class="mord"><span class="mord mathit">d</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span><span style="top:-0.77722em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="reset-textstyle textstyle uncramped sqrt-line"></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span>​</span></span></span></span></span></span>？ <a class="header-anchor" href="#_5-1-为啥attention的时候要除以" aria-label="Permalink to &quot;5.1 为啥Attention的时候要除以$\\sqrt{d_k}$？&quot;">​</a></h3> Attention(Q, K, V ) = softmax(\\frac{Q·K^T}{\\sqrt{d_k}})·V <ul><li>个人感觉跟Normalization的作用类似</li></ul><p>当 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">d_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">d</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> 的值比较小的时候，两种点积机制(additive 和 Dot-Product)的性能相差相近，当 dk<em>d**k</em> 比较大时，additive attention 比不带scale 的点积attention性能好。 我们怀疑，对于很大的 dk<em>d**k</em> 值，点积大幅度增长，将softmax函数推向具有极小梯度的区域。 为了抵消这种影响，我们缩小点积 1dk√<em>d**k</em>1 倍。</p><h3 id="_5-2-为啥拆多头-为啥效果好了" tabindex="-1">5.2 为啥拆多头？为啥效果好了？ <a class="header-anchor" href="#_5-2-为啥拆多头-为啥效果好了" aria-label="Permalink to &quot;5.2 为啥拆多头？为啥效果好了？&quot;">​</a></h3><ul><li><p>提取到了更多的信息（类似CNN的multi- kernel），数据分布组与组（子空间）之间独立</p><blockquote><p>Multi-head attention允许模型的不同表示子空间联合<strong>关注不同位置</strong>的信息。 如果只有一个attention head，它的平均值会削弱这个信息。</p></blockquote></li><li><p>减少计算量（应该可以在这一层减少原来的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mn>1</mn></mrow><mrow><mi>N</mi><mi>u</mi><msub><mi>m</mi><mrow><mi>h</mi><mi>e</mi><mi>a</mi><mi>d</mi></mrow></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\\frac{1}{Num_{head}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.845108em;"></span><span class="strut bottom" style="height:1.2959679999999998em;vertical-align:-0.4508599999999999em;"></span><span class="base textstyle uncramped"><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.345em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.10903em;">N</span><span class="mord mathit">u</span><span class="mord"><span class="mord mathit">m</span><span class="vlist"><span style="top:0.15122857142857138em;margin-right:0.07142857142857144em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord scriptscriptstyle cramped"><span class="mord mathit">h</span><span class="mord mathit">e</span><span class="mord mathit">a</span><span class="mord mathit">d</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span><span style="top:-0.22999999999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.394em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span>倍）</p></li></ul><h3 id="_5-3-cross-multi-head-attention" tabindex="-1">5.3 Cross Multi-Head Attention？ <a class="header-anchor" href="#_5-3-cross-multi-head-attention" aria-label="Permalink to &quot;5.3 Cross Multi-Head Attention？&quot;">​</a></h3><p>首先，Self- Attention与传统的Attention机制非常的不同：传统的Attention是基于source端和target端的隐变量（hidden state）计算Attention的，得到的结果是源端（source端）的每个词与目标端（target端）每个词之间的依赖关系。 其次，Self-Attention首先分别在source端和target端进行自身的attention，仅与source input或者target input自身相关的Self -Attention，以捕捉source端或target端自身的词与词之间的依赖关系；然后再把source端的得到的self -Attention加入到target端得到的Attention中，称作为<strong>Cross-Attention</strong>，以捕捉source端和target端词与词之间的依赖关系。</p><h3 id="_5-4-mask-multi-head-attention" tabindex="-1">5.4 Mask Multi-Head Attention <a class="header-anchor" href="#_5-4-mask-multi-head-attention" aria-label="Permalink to &quot;5.4 Mask Multi-Head Attention&quot;">​</a></h3><p>​ 与Encoder的Multi-Head Attention计算原理一样，只是多加了一个mask码。mask 表示掩码，它对某些值进行掩盖，使其在参数更新时不产生效果。Transformer 模型里面涉及两种 mask，分别是 padding mask 和 sequence mask。</p><h3 id="_5-5-masking实现机理" tabindex="-1">5.5 Masking实现机理 <a class="header-anchor" href="#_5-5-masking实现机理" aria-label="Permalink to &quot;5.5 Masking实现机理&quot;">​</a></h3><p>具体的做法是，把<strong>这些位置</strong>的值<strong>加上一个非常大的负数(负无穷)</strong>，这样的话，经过 softmax，这些位置的概率就会接近0！</p><h3 id="_5-6-mha、mqa和gqa" tabindex="-1">5.6 MHA、MQA和GQA <a class="header-anchor" href="#_5-6-mha、mqa和gqa" aria-label="Permalink to &quot;5.6 MHA、MQA和GQA&quot;">​</a></h3><p>MQA多头共用K，V</p><p>GQA将头分组，组内共用KV</p><h2 id="_5-7-position-embedding🌟🌟🌟🌟🌟" tabindex="-1">5.7 position embedding🌟🌟🌟🌟🌟 <a class="header-anchor" href="#_5-7-position-embedding🌟🌟🌟🌟🌟" aria-label="Permalink to &quot;5.7 position embedding🌟🌟🌟🌟🌟&quot;">​</a></h2><ul><li><p>attention并不会获取到输入序列（token与token之间）的位置信息，只有俩俩的相关性</p></li><li><p>最初的Transformer使用的是<strong>绝对位置编码</strong>，即：直接对第<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow><annotation encoding="application/x-tex">batch</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">b</span><span class="mord mathit">a</span><span class="mord mathit">t</span><span class="mord mathit">c</span><span class="mord mathit">h</span></span></span></span>个输入序列的第<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span></span>个token向量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>X</mi><mi>k</mi><mrow><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msubsup></mrow><annotation encoding="application/x-tex">X^{batch}_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.849108em;"></span><span class="strut bottom" style="height:1.132216em;vertical-align:-0.2831079999999999em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="vlist"><span style="top:0.2831079999999999em;margin-left:-0.07847em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">b</span><span class="mord mathit">a</span><span class="mord mathit">t</span><span class="mord mathit">c</span><span class="mord mathit">h</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>加上一个位置编码<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>P</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">P_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.13889em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>，然后做同样的attention</p></li><li><p>关于编码向量（或者矩阵）的生成，采用三角位置编码方式，对于第奇数(<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>2</mn><mi>i</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">2i+1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.65952em;"></span><span class="strut bottom" style="height:0.74285em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">2</span><span class="mord mathit">i</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span>)个token采用cos，对于偶数采用sin</p><blockquote><p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>p</mi><mrow><mi>k</mi><mo separator="true">,</mo><mn>2</mn><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>c</mi><mi>o</mi><mi>s</mi><mo>(</mo><mfrac><mrow><mi>k</mi></mrow><mrow><mn>1</mn><mn>0</mn><mn>0</mn><mn>0</mn><msup><mn>0</mn><mrow><mn>2</mn><mi>i</mi><mi mathvariant="normal">/</mi><mi>d</mi></mrow></msup></mrow></mfrac><mo>)</mo></mrow><annotation encoding="application/x-tex">p_{k,2i+1}=cos(\\frac{k}{10000^{2i/d}})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8801079999999999em;"></span><span class="strut bottom" style="height:1.2674079999999999em;vertical-align:-0.3872999999999999em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">p</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span><span class="mpunct">,</span><span class="mord mathrm">2</span><span class="mord mathit">i</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mord mathit">c</span><span class="mord mathit">o</span><span class="mord mathit">s</span><span class="mopen">(</span><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.3872999999999999em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathrm">1</span><span class="mord mathrm">0</span><span class="mord mathrm">0</span><span class="mord mathrm">0</span><span class="mord"><span class="mord mathrm">0</span><span class="vlist"><span style="top:-0.289em;margin-right:0.07142857142857144em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord scriptscriptstyle cramped"><span class="mord mathrm">2</span><span class="mord mathit">i</span><span class="mord mathrm">/</span><span class="mord mathit">d</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span><span style="top:-0.23000000000000004em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.394em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mclose">)</span></span></span></span></p><p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>p</mi><mrow><mi>k</mi><mo separator="true">,</mo><mn>2</mn><mi>i</mi></mrow></msub><mo>=</mo><mi>s</mi><mi>i</mi><mi>n</mi><mo>(</mo><mfrac><mrow><mi>k</mi></mrow><mrow><mn>1</mn><mn>0</mn><mn>0</mn><mn>0</mn><msup><mn>0</mn><mrow><mn>2</mn><mi>i</mi><mi mathvariant="normal">/</mi><mi>d</mi></mrow></msup></mrow></mfrac><mo>)</mo></mrow><annotation encoding="application/x-tex">p_{k,2i}=sin(\\frac{k}{10000^{2i/d}})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8801079999999999em;"></span><span class="strut bottom" style="height:1.2674079999999999em;vertical-align:-0.3872999999999999em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">p</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span><span class="mpunct">,</span><span class="mord mathrm">2</span><span class="mord mathit">i</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mord mathit">s</span><span class="mord mathit">i</span><span class="mord mathit">n</span><span class="mopen">(</span><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.3872999999999999em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathrm">1</span><span class="mord mathrm">0</span><span class="mord mathrm">0</span><span class="mord mathrm">0</span><span class="mord"><span class="mord mathrm">0</span><span class="vlist"><span style="top:-0.289em;margin-right:0.07142857142857144em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord scriptscriptstyle cramped"><span class="mord mathrm">2</span><span class="mord mathit">i</span><span class="mord mathrm">/</span><span class="mord mathit">d</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span><span style="top:-0.23000000000000004em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.394em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mclose">)</span></span></span></span></p><p><strong>其中：d表示位置向量的维度（应该也是输入次向量的embedding维度）</strong></p></blockquote></li><li><p>其实提取到位置信息是靠变量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span></span>获取的，相当于是seq index</p></li><li><p><strong>自己绘图发现该方案存在一定问题：</strong> 当embedding dim<strong>大于某一范围的</strong>的时候，对应编码信息会向0-1分布。我测试的[2, 32, 64]，当dim 在[12, 20]发生某种变化，然后超过20开始趋向0-1分布，其实从公式也可以明显发现这种现象</p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250326170819283.png" alt="image-20250326170819283" style="zoom:40%;"><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250326170943850.png" alt="image-20250326170943850"></p></li><li><p>该位置编码是在做Attention之前进行的，与embedding后的向量合并，再做attention</p></li></ul><h3 id="x-x-其他" tabindex="-1">x.x 其他 <a class="header-anchor" href="#x-x-其他" aria-label="Permalink to &quot;x.x 其他&quot;">​</a></h3><ul><li><p>工程中将QKV的权重矩阵直接放在一块，shape就是原来<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mi>e</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>d</mi><mi>d</mi><mi>i</mi><mi>n</mi><mi>g</mi><mi mathvariant="normal">_</mi><mi>d</mi><mi>i</mi><mi>m</mi><mo separator="true">,</mo><mi>e</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>d</mi><mi>d</mi><mi>i</mi><mi>n</mi><mi>g</mi><mi mathvariant="normal">_</mi><mi>d</mi><mi>i</mi><mi>m</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">(embedding\\_dim, embedding\\_dim)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1.06em;vertical-align:-0.31em;"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord mathit">e</span><span class="mord mathit">m</span><span class="mord mathit">b</span><span class="mord mathit">e</span><span class="mord mathit">d</span><span class="mord mathit">d</span><span class="mord mathit">i</span><span class="mord mathit">n</span><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="mord mathrm" style="margin-right:0.02778em;">_</span><span class="mord mathit">d</span><span class="mord mathit">i</span><span class="mord mathit">m</span><span class="mpunct">,</span><span class="mord mathit">e</span><span class="mord mathit">m</span><span class="mord mathit">b</span><span class="mord mathit">e</span><span class="mord mathit">d</span><span class="mord mathit">d</span><span class="mord mathit">i</span><span class="mord mathit">n</span><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="mord mathrm" style="margin-right:0.02778em;">_</span><span class="mord mathit">d</span><span class="mord mathit">i</span><span class="mord mathit">m</span><span class="mclose">)</span></span></span></span>到 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mi>e</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>d</mi><mi>d</mi><mi>i</mi><mi>n</mi><mi>g</mi><mi mathvariant="normal">_</mi><mi>d</mi><mi>i</mi><mi>m</mi><mo separator="true">,</mo><mi>e</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>d</mi><mi>d</mi><mi>i</mi><mi>n</mi><mi>g</mi><mi mathvariant="normal">_</mi><mi>d</mi><mi>i</mi><mi>m</mi><mo>×</mo><mn>3</mn><mo>)</mo></mrow><annotation encoding="application/x-tex">(embedding\\_dim, embedding\\_dim \\times 3)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1.06em;vertical-align:-0.31em;"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord mathit">e</span><span class="mord mathit">m</span><span class="mord mathit">b</span><span class="mord mathit">e</span><span class="mord mathit">d</span><span class="mord mathit">d</span><span class="mord mathit">i</span><span class="mord mathit">n</span><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="mord mathrm" style="margin-right:0.02778em;">_</span><span class="mord mathit">d</span><span class="mord mathit">i</span><span class="mord mathit">m</span><span class="mpunct">,</span><span class="mord mathit">e</span><span class="mord mathit">m</span><span class="mord mathit">b</span><span class="mord mathit">e</span><span class="mord mathit">d</span><span class="mord mathit">d</span><span class="mord mathit">i</span><span class="mord mathit">n</span><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="mord mathrm" style="margin-right:0.02778em;">_</span><span class="mord mathit">d</span><span class="mord mathit">i</span><span class="mord mathit">m</span><span class="mbin">×</span><span class="mord mathrm">3</span><span class="mclose">)</span></span></span></span></p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/1850883-20250209144655019-1620461538.jpg" alt="img"></p></li><li><p>Attention的时候是否需要对自身做？自回归的时候应当下一次token尽可能不是上一个词，所以矩阵对角线是否应当是趋于零的？</p></li><li><p>句子间的相似度计算方式有哪些？Attention为啥采用点积？</p><blockquote><p>addivation</p><p>dot-dart （为啥点积运算可以计算相似度）</p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250325155249034.png" alt="image-20250325155249034"></p></blockquote></li><li><p>FFN的时候为啥dim要✖️4放大纬度</p></li><li><p>encoder其实是特征提取的一个过程</p></li><li><p>为啥encoder的input和out（context vector）的纬度大小要一样？（因为有原始论文有6个encoder Layer或者cell）</p></li><li><p>encoder层的attention会注意到前面的词吗？</p></li><li><p>encoder的特征聚合是在什么时候</p></li><li><p>特征压缩</p></li><li><p>softmax的时候是对attention结果的哪个纬度进行归一化（词与词相关性考虑，应该是掩码后矩阵分行向量），为啥归一化？（词与词之间的联系性，因为还要点乘value，所以应当是一个权重）</p></li><li><p>mask的大小？（batch_size, seq_len, seq_len），应该和attention score一样，因为要码谁就跟谁一样</p></li></ul><h2 id="_6-🌟🌟🌟k-v-cache" tabindex="-1">6. 🌟🌟🌟K-V Cache <a class="header-anchor" href="#_6-🌟🌟🌟k-v-cache" aria-label="Permalink to &quot;6. 🌟🌟🌟K-V Cache&quot;">​</a></h2><h2 id="_7-🌟🌟🌟常见的正则化方法" tabindex="-1">7. 🌟🌟🌟常见的正则化方法 <a class="header-anchor" href="#_7-🌟🌟🌟常见的正则化方法" aria-label="Permalink to &quot;7. 🌟🌟🌟常见的正则化方法&quot;">​</a></h2><ul><li>Dropout</li><li></li></ul><h2 id="_8-bert🌟🌟🌟🌟" tabindex="-1">8. Bert🌟🌟🌟🌟 <a class="header-anchor" href="#_8-bert🌟🌟🌟🌟" aria-label="Permalink to &quot;8. Bert🌟🌟🌟🌟&quot;">​</a></h2><p>输入后的对15%的三个处理</p><p>bert有三个编码，分别是</p><h2 id="_9-位置编码总结" tabindex="-1">9. 位置编码总结 <a class="header-anchor" href="#_9-位置编码总结" aria-label="Permalink to &quot;9. 位置编码总结&quot;">​</a></h2><h3 id="_9-1-绝对位置编码" tabindex="-1">9.1 绝对位置编码 <a class="header-anchor" href="#_9-1-绝对位置编码" aria-label="Permalink to &quot;9.1 绝对位置编码&quot;">​</a></h3><blockquote><p>在Attention之前进行位置编码</p><p>对qkv都做位置编码</p></blockquote><ul><li><p>三角（固定）位置编码</p></li><li><p>可学习位置编码</p><blockquote><p>把编码矩阵当作可学习参数进行训练（大小与embedding后的输入一致）</p><p>Bert模型就是采用的这种编码</p></blockquote></li></ul><h3 id="_9-2-相对位置编码" tabindex="-1">9.2 相对位置编码 <a class="header-anchor" href="#_9-2-相对位置编码" aria-label="Permalink to &quot;9.2 相对位置编码&quot;">​</a></h3><p><strong>不使用每个 token 的绝对位置，而是表示 token 之间的相对位置。这样做的好处是，模型不需要对每个位置使用单独的编码，而是通过计算相对距离来捕捉位置信息</strong>。</p><blockquote><p>在Attention之后进行位置编码</p><p>只对q和k做位置编码，对value不做，value是结果或者说是token本身的特征信息</p><p>根据数学原理推导的（依赖之前的位置编码公式来推导，<strong>也就是由绝对位置编码启发而来</strong>）</p><p>采用分桶思想（T5的思想）</p></blockquote><h3 id="_9-3-旋转位置编码——rope-大模型常用" tabindex="-1">9.3 旋转位置编码——RoPE（大模型常用） <a class="header-anchor" href="#_9-3-旋转位置编码——rope-大模型常用" aria-label="Permalink to &quot;9.3 旋转位置编码——RoPE（大模型常用）&quot;">​</a></h3><blockquote><p>也是相对位置编码的一种</p></blockquote><h3 id="" tabindex="-1"><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250322144241104.png" alt="image-20250322144241104"> <a class="header-anchor" href="#" aria-label="Permalink to &quot;![image-20250322144241104](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250322144241104.png)&quot;">​</a></h3><blockquote><p>通过构建数学模型</p><p>根据想要得到的效果进行反推得到</p></blockquote><h2 id="_10-模型的参数和状态" tabindex="-1">10. 模型的参数和状态 <a class="header-anchor" href="#_10-模型的参数和状态" aria-label="Permalink to &quot;10. 模型的参数和状态&quot;">​</a></h2><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250327165128156.png" alt="image-20250327165128156" style="zoom:67%;"><h1 id="-1" tabindex="-1"><a class="header-anchor" href="#-1" aria-label="Permalink to &quot;&quot;">​</a></h1>',68)]))}const u=a(i,[["render",m]]);export{d as __pageData,u as default};
