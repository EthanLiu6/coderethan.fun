import{_ as e,c as t,o as l,a2 as o}from"./chunks/framework.DA-Pb-tg.js";const g=JSON.parse('{"title":"01-Class_Logs","description":"","frontmatter":{},"headers":[],"relativePath":"AI/04_some_notes/01-class_logs.md","filePath":"AI/04_some_notes/01-class_logs.md","lastUpdated":null}'),r={name:"AI/04_some_notes/01-class_logs.md"};function i(n,a,s,p,h,c){return l(),t("div",null,a[0]||(a[0]=[o(`<h1 id="_01-class-logs" tabindex="-1">01-Class_Logs <a class="header-anchor" href="#_01-class-logs" aria-label="Permalink to &quot;01-Class_Logs&quot;">​</a></h1><h2 id="_0301-0302-transformer🌟🌟🌟" tabindex="-1">0301-0302 Transformer🌟🌟🌟 <a class="header-anchor" href="#_0301-0302-transformer🌟🌟🌟" aria-label="Permalink to &quot;0301-0302 Transformer🌟🌟🌟&quot;">​</a></h2><ul><li>0301:input序列长度大于embedding时候的seq_len时, input的输入序列会按照seq_len进行切割拼接到batch上吗? (老师讲了encoder时候input不足seq_len时候使用mask然后想问的另一个问题)</li><li>0302:<code>K-V cache</code>时候当预测下一个时间步的时候与之前的做Attention的时候, 中途会取出cache里的K—V吗还是只取出里面的K还是只在最后一个结束后才整体取一次 (我想问的也就是在一个batch或者一个seq的访存情况, 每一个时间步都需要访问cache一次吗)</li><li>是直接使用缓存的填充矩阵还是需要拿出缓存数据(读还是取)</li></ul><h2 id="_0308-0309——pytorch" tabindex="-1">0308-0309——PyTorch <a class="header-anchor" href="#_0308-0309——pytorch" aria-label="Permalink to &quot;0308-0309——PyTorch&quot;">​</a></h2><blockquote><p>提及: 混合精度训练</p></blockquote><h3 id="_1-1-tensor-中数据的连续性" tabindex="-1">1.1 Tensor 中数据的连续性 <a class="header-anchor" href="#_1-1-tensor-中数据的连续性" aria-label="Permalink to &quot;1.1 Tensor 中数据的连续性&quot;">​</a></h3><p>reshape, transpose, view, T(转置), permute</p><p>transpose会让raw data不变(共用), mata data的stride和shape等属性就变了 is_contiguous()不连续, 但reshape和permute这些是不会变的,因为他们会发生data copy, contiguous()会发生copy raw data数据</p><p>view和reshape的区别</p><p>view更加安全, 不会重新拷贝数据, 但数据不连续不能使用view,也就是stride不协调, reshape不会错误, 会重新拷贝数据, 数据也连续</p><p>permute和transpose会让stride属性改变, 从而发生数据不连续, 通常使用后要加一个contiguous()让数据连续</p><h3 id="_1-2-pytorch-autograd" tabindex="-1">1.2 pytorch autograd <a class="header-anchor" href="#_1-2-pytorch-autograd" aria-label="Permalink to &quot;1.2 pytorch autograd&quot;">​</a></h3><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250308163009045.png" alt="image-20250308163009045" style="zoom:36%;"><p>……………………</p><p>叶子结点+requests_grad=True才有最终的grad, 非叶子结点中途可能会计算grad, 但用了就会丢弃(requests_grad=True的)</p><p>梯度累加也有可能, 多个step的梯度累加, 隐式增加batch</p><p>若没进行xxx.grad.zero_()或者xxx.grad = None, 则会进行accumulate()累加grad, 这两种方法有一点区别, zero__()会置零,会占用显存, 但=None的话会释放显存, 两者各有好坏</p><h3 id="_1-3-inplace-op" tabindex="-1">1.3 inplace-op <a class="header-anchor" href="#_1-3-inplace-op" aria-label="Permalink to &quot;1.3 inplace-op&quot;">​</a></h3><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250309102559857.png" alt="image-20250309102559857"></p><p>叶子结点的Tensor变量不能进行in-place操作, 因为要更新梯度的时候要用叶子结点</p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250309113212792.png" alt="image-20250309113212792" style="zoom:50%;"><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250309114217344.png" alt="image-20250309114217344" style="zoom:50%;"><p>no_grad()底层是基于set_grad_enable(Flase)的</p><h3 id="_1-4-自动微分机制-auto-grad-重点" tabindex="-1">1.4 自动微分机制(auto grad) 重点： <a class="header-anchor" href="#_1-4-自动微分机制-auto-grad-重点" aria-label="Permalink to &quot;1.4 自动微分机制(auto grad) 重点：&quot;">​</a></h3><ul><li>pytorch中 正向forward 对我们用户是可见的，但是backward对我们用户是不可见的；</li><li>一般情况下，每一个正向的函数，都对应一个反向的函数（grad_fn--&gt; Tensor中）；</li><li>tensor：requires_grad = True</li><li>tensor: grad --&gt; tensor 中存储grad的地方；</li><li>tensor: grad_fn --&gt; 存储我们反向函数的地方</li><li>tesnor: is_leaf --&gt; 这个tensor 是不是 叶子节点；</li><li>net::all weight --&gt; 都是leaf</li><li>叶子节点的梯度会自动保存下来的（weight）；</li><li>中间的 activation 的梯度会计算，但是不保留；</li><li>pytorch 动态图 vs tensorflow 静态图；</li><li>我们不能改变一个非叶子节点的 requires_grad;</li><li>非叶子（一个函数的output）节点它的 requires_grad 自动推导的；</li><li>非叶子节点对应函数的inputs 中只要有一个 requires_grad = True, 那么这个非叶子节点的requires_grad = True;</li><li>torch.no_grad() 会使得里面的新的tensor requires_grad = False</li><li>inplace的操作，非常大的风险：覆盖了原来的值，导致反向传播时计算不准确；</li><li>标量的梯度才能被隐式创建，隐式创建（.backward(1)）；</li><li>一般情况下，.backward(gradient)是有输入的: ;</li></ul><h3 id="_2-1-torch-nn-module" tabindex="-1">2.1 torch.nn.Module <a class="header-anchor" href="#_2-1-torch-nn-module" aria-label="Permalink to &quot;2.1 torch.nn.Module&quot;">​</a></h3><p>train模式和veal模式不会对grad的情况做修改,只是对训练和推理的对应的算子做不同的处理(等价处理)</p><p>常用算子dropout和BachNorm</p><p>xxx.cuda()的时候搬迁的是_parameters到cuda, 还有buffer也搬迁到cuda, 并没有将模型结构进行搬迁.</p><p>按照深度优先遍历sub module,将里面的_parameters和buffer到cuda, 数据类型转换也是一样的操作</p><p>c++底层实现了一个dispather分发机制,按照device属性分发, 对应device会调用对应的fn算子, 计算部分才执行</p><p>_parameters()送参数给优化器的时候将所有的parameters送到optim, 但数据共用, 同时更新</p><p>钩子函数(没太懂)</p><hr><h2 id="_0315-0316-续pytorch" tabindex="-1">0315-0316（续PyTorch） <a class="header-anchor" href="#_0315-0316-续pytorch" aria-label="Permalink to &quot;0315-0316（续PyTorch）&quot;">​</a></h2><h3 id="_1-1-回顾" tabindex="-1">1.1 回顾 <a class="header-anchor" href="#_1-1-回顾" aria-label="Permalink to &quot;1.1 回顾&quot;">​</a></h3><p>1.Tensor类和重要属性 2.autograd，动态图 3.Module以及属性和方法</p><blockquote><p>training,_parameters,_buffers,_modules(hooks是主要用二次开发等情况)</p></blockquote><p>子模块啥时候定义的呢？</p><p>_parameters,_buffers哪些有哪些没有</p><p>将module里的parameters传给optim，会通过调用parameters()进行</p><p>一系列方法具体情况</p><h3 id="_1-2-问题合集" tabindex="-1">1.2 问题合集 <a class="header-anchor" href="#_1-2-问题合集" aria-label="Permalink to &quot;1.2 问题合集&quot;">​</a></h3><ol><li>在讲transformer的padding mask的时候想到，如果输入seq_len大于了定义的seq_len，会直接截断还是截断再拼接到下一个batch</li><li>在sequence mask的时候，忘了要问啥了</li><li>在normalization层的时候不是有两个学习的参数吗，这俩参数是一次forward训练一次还是单独有自己的训练？还有，这俩参数是咋更新的？</li><li>dataset会迭代的将所有数据加载到内存吗，然后dataloader再一批次的提取吗</li></ol><p>with torch.no_grad(): eval时候用，计算图不再进行，对require_grads=True的不进行梯度计算，显存占用量会减少，activation的就会丢弃</p><p>dataset会迭代的将所有数据加载到内存吗，然后dataloader再一批次一批次的提取吗？还是说dataloader准备拿一个batch，然后dataset根据batch_size迭代获取size条。</p><blockquote><p>是后者，也就是I/O的时候，batch_size太小的话会增加I/O负担</p></blockquote><h3 id="_2-1-torch-optim" tabindex="-1">2.1 torch.optim <a class="header-anchor" href="#_2-1-torch-optim" aria-label="Permalink to &quot;2.1 torch.optim&quot;">​</a></h3><p>参数传param的时候的传递和打包方式</p><p>self.param_groups</p><p>==self.state==：训练时候显存消耗的主要项（优化器的动量项有关） 他是一个dict，keys是tensor，values也是 模型</p><blockquote><p>移动指数平均是啥忘了</p></blockquote><p>def load_state_dict</p><h3 id="_2-2-learning-rate-调整方案" tabindex="-1">2.2 learning rate 调整方案 <a class="header-anchor" href="#_2-2-learning-rate-调整方案" aria-label="Permalink to &quot;2.2 learning rate 调整方案&quot;">​</a></h3><p>Torch.optim.lr_scheduler</p><p>震荡类型的学习率调整是减少进入局部最优解的情况</p><p>==状态字典==，三个地方见过，都类似，模型保存时候需要有</p><h3 id="_2-3-模型保存和加载" tabindex="-1">2.3 模型保存和加载 <a class="header-anchor" href="#_2-3-模型保存和加载" aria-label="Permalink to &quot;2.3 模型保存和加载&quot;">​</a></h3><p>==动态图==</p><p>1.save state_dict的时候只有参数，save model的时候无法直接保存整个网络，但是他的材料（init）的那些会保存，模型加载的时候能通过，但runing time时候，forward并没有，必须导入或者自己实现，需要原来Net的签名（具体定义可以不一致，会放入_modules）</p><p>2.如果是自己写的算子，在init时候也放入_modules吗？</p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/%7B6fbad3cc-1899-4404-b3b3-d91f7da5cb95%7D.png" alt="img"></p><p>3.==onnx==模型保存必须输入对应的input，自己run一遍，是一个静态图</p><p>4.训练中的保存和加载（check point）==模型保存的几种参数类型==）</p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250316113313785.png" alt="image-20250316113313785"></p><h3 id="_3-1-dataset-and-dataloader" tabindex="-1">3.1 Dataset and Dataloader <a class="header-anchor" href="#_3-1-dataset-and-dataloader" aria-label="Permalink to &quot;3.1 Dataset and Dataloader&quot;">​</a></h3><blockquote><p>只学习pytorch的，后续自己补hf的那些</p></blockquote><h3 id="_4-1-nlp" tabindex="-1">4.1 NLP <a class="header-anchor" href="#_4-1-nlp" aria-label="Permalink to &quot;4.1 NLP&quot;">​</a></h3><p>GPT：自监督训练得到预训练模型（采用迁移学习）</p><p>Bert：完形填空</p><p>迁移学习：预训练+微调（微调的数据集就是专业领域的数据集）</p><h3 id="_4-2-bert" tabindex="-1">4.2 Bert <a class="header-anchor" href="#_4-2-bert" aria-label="Permalink to &quot;4.2 Bert&quot;">​</a></h3><p>1.两个任务：MLM和NSP</p><p>2.Embedding，词嵌入</p><p>词，句子（分段），位置 嵌入</p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250316165604714.png" alt="image-20250316165604714"></p><blockquote><p>transformer的词嵌入式用三角位置嵌入</p></blockquote><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250316175009793.png" alt="image-20250316175009793"></p><p>未讲知识：分词器tokenizer</p><h2 id="_0322" tabindex="-1">0322 <a class="header-anchor" href="#_0322" aria-label="Permalink to &quot;0322&quot;">​</a></h2><h3 id="_1-1-回顾-1" tabindex="-1">1.1 回顾 <a class="header-anchor" href="#_1-1-回顾-1" aria-label="Permalink to &quot;1.1 回顾&quot;">​</a></h3><p><code>bert4torch</code>的ner项目讲解和debug</p><h3 id="_2-1-t5讲解" tabindex="-1">2.1 T5讲解 <a class="header-anchor" href="#_2-1-t5讲解" aria-label="Permalink to &quot;2.1 T5讲解&quot;">​</a></h3><h3 id="_2-2-position-embedding🌟🌟🌟🌟🌟" tabindex="-1">2.2 position embedding🌟🌟🌟🌟🌟 <a class="header-anchor" href="#_2-2-position-embedding🌟🌟🌟🌟🌟" aria-label="Permalink to &quot;2.2 position embedding🌟🌟🌟🌟🌟&quot;">​</a></h3><p>绝对位置编码</p><ul><li>三角函数式(Sinusoidal)</li><li>可学习(Learnable)</li></ul><p>相对位置编码</p><ul><li><strong>是在Attention的时候才位置编码</strong></li><li>只对q和k做位置编码，对value不做，value是结果或者说是token本身的特征信息</li><li>T5的分桶思想</li></ul><p>==旋转位置编码==（大模型使用的方法）</p><ul><li>根据数学原理推导</li></ul><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250322144241104.png" alt="image-20250322144241104"></p><ul><li>想要得到的效果=&gt;反推</li></ul><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250322144600625.png" alt="image-20250322144600625"></p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250322145503812.png" alt="image-20250322145503812"></p><h3 id="_3-1-gpt" tabindex="-1">3.1 GPT <a class="header-anchor" href="#_3-1-gpt" aria-label="Permalink to &quot;3.1 GPT&quot;">​</a></h3><p>GPT-1 已经出现zero-shot迹象，层归一化还是之前的post-norm</p><p>GPT-2 零样本学习，即zero-shot，层归一化有点变化，改成per-Norm</p><blockquote><p>相当于纯预训练</p></blockquote><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250322170432945.png" alt="image-20250322170432945"></p><p>GPT-3 few-shot（给案例），发现模型规模可以提高能力，最后实现了无需微调到达一些较好的任务处理，架构基本和GPT-2一致，但加了一个新的‘交替的稠密和稀疏的’Attention，余弦衰减的学习率策略，batch-size从小变大，再加上0.1的权重衰减正则化</p><blockquote><p>Few-shot, one-shot, zero-shot</p><p>• <strong>Few-Shot（FS）：</strong> 模型在推理时给出K个任务示例作为上下文信息，同时提供任务的自然语言描述，但不允许模型进行权重更新。通常将K设置在10到100的范围内，以适应模型的上下文窗口。</p><p>• <strong>One-Shot（1S）：</strong> 模型在推理时通过提供一个任务示例作为上下文信息，同时还有任务的自然语言描述。这种方式最接近于人类在解决某些任务时所使用的方式。</p><p>• <strong>Zero-Shot（0S）：</strong> 不提供任何上下文信息，模型只给出一个描述任务的自然语言指令。</p></blockquote><h2 id="_0323" tabindex="-1">0323 <a class="header-anchor" href="#_0323" aria-label="Permalink to &quot;0323&quot;">​</a></h2><h3 id="_1-1-课前准备" tabindex="-1">1.1 课前准备 <a class="header-anchor" href="#_1-1-课前准备" aria-label="Permalink to &quot;1.1 课前准备&quot;">​</a></h3><ul><li><p>T5模型数据集下载并修改代码</p></li><li><p>tumx和终端不后台从训练</p></li></ul><blockquote><p>sh Train.sh &gt; ./xxx.log &amp;</p></blockquote><blockquote><p>tumx</p></blockquote><h3 id="_2-1-scaling-laws" tabindex="-1">2.1 Scaling Laws <a class="header-anchor" href="#_2-1-scaling-laws" aria-label="Permalink to &quot;2.1 Scaling Laws&quot;">​</a></h3><blockquote><p>tip：模型规模搞大可以提高自己的能力？</p></blockquote><ul><li>实验变量：</li></ul><blockquote><p>C, D, N</p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250323103615212.png" alt="image-20250323103615212"></p></blockquote><ul><li>数据规模与模型规模扩大比：5/8</li><li>一些超参数的设定</li></ul><hr><h3 id="_3-1-分布式训练" tabindex="-1">3.1 分布式训练 <a class="header-anchor" href="#_3-1-分布式训练" aria-label="Permalink to &quot;3.1 分布式训练&quot;">​</a></h3><ol><li>并行可以并行哪些？拆哪些？</li><li>多卡并行范式</li></ol><ul><li><p><strong>数据并行性</strong>(DP)：将模型（所有weight）复制到别的Worker中，所以模型大于单个显存的时候使用这种方式无法很好工作</p></li><li><p>模型并行性(MP)，存在Bubble问题</p></li><li><p>MP优化：管线并行性（MP --&gt; PP），又叫<strong>流水线并行</strong></p><blockquote><p>pp传播的是activation（前向）和对应的grad（反向）</p><p>GPipe的不足：最后一个执行完才能backward</p><p>PipeDream：前反向穿插，调度问题很难，工程上=&gt;Pipeline flash。实现one F one B</p></blockquote></li><li><p><strong>张量并行性（TP）</strong></p><blockquote><p>前面是纵向分割，现在提出用横向分割</p><p>将一个算子的Tenser分到多节点计算</p></blockquote></li><li><p><strong>专家混合（EP，MoE）</strong></p><blockquote><p>G shard</p><p>switch Transformer</p></blockquote><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250323144057238.png" alt="image-20250323144057238"></p></li><li><p>后面还有CP，xxxxP</p></li></ul><ol start="3"><li><p>分布式框架</p><blockquote><p>pytorch的</p><p>deepspeed</p></blockquote></li></ol><h3 id="_4-显存占用问题" tabindex="-1">4. 显存占用问题 <a class="header-anchor" href="#_4-显存占用问题" aria-label="Permalink to &quot;4. 显存占用问题&quot;">​</a></h3><h4 id="_4-1-解决方案" tabindex="-1">4.1 解决方案 <a class="header-anchor" href="#_4-1-解决方案" aria-label="Permalink to &quot;4.1 解决方案&quot;">​</a></h4><blockquote><p>之前有多个batch的grad累加</p></blockquote><ol><li>重计算（recompute）</li></ol><p>Pytorch2.6开始更加新的重计算</p><ol start="2"><li><p>offload ：用完就放到CPU</p><p>eg:</p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250323152635318.png" alt="image-20250323152635318" style="zoom:150%;"></li><li><p>gradient accumulate</p></li></ol><h4 id="_4-2-显存分析" tabindex="-1">4.2 显存分析 <a class="header-anchor" href="#_4-2-显存分析" aria-label="Permalink to &quot;4.2 显存分析&quot;">​</a></h4><p>1.API</p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250323152133484.png" alt="image-20250323152133484"><p>2.显存高峰期</p><p>在第一个step不会，理论上是在第二个step的forward之后</p><h3 id="_5-混合精度训练-amp-🌟🌟🌟🌟🌟" tabindex="-1">5. 混合精度训练（AMP）🌟🌟🌟🌟🌟 <a class="header-anchor" href="#_5-混合精度训练-amp-🌟🌟🌟🌟🌟" aria-label="Permalink to &quot;5. 混合精度训练（AMP）🌟🌟🌟🌟🌟&quot;">​</a></h3><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image.png" alt="alt text"></p><p>大模型必用，加速训练</p><blockquote><p>==下一个热点FP8==</p></blockquote><p>1.权重副本fp32</p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250323155428008.png" alt="image-20250323155428008" style="zoom:50%;"><p>2.损失缩放</p><p>3.输出存储到单精度，最终变半精度</p><p>舍入误差（下溢）</p><h3 id="_6-apex" tabindex="-1">6. Apex <a class="header-anchor" href="#_6-apex" aria-label="Permalink to &quot;6. Apex&quot;">​</a></h3><h2 id="_0329-0330" tabindex="-1">0329-0330 <a class="header-anchor" href="#_0329-0330" aria-label="Permalink to &quot;0329-0330&quot;">​</a></h2><h3 id="_1-续分布式训练" tabindex="-1">1. 续分布式训练： <a class="header-anchor" href="#_1-续分布式训练" aria-label="Permalink to &quot;1. 续分布式训练：&quot;">​</a></h3><p>DP</p><p>DDP</p><ul><li>Ring All Reduce</li><li>分桶进行，bucket</li><li>一个进程单独占用一张卡</li><li>通信成本是2P(2*param)</li></ul><p>nccl通信后端</p><ul><li><p>Gather相当于concat吗，reduce相当于进行了point wise操作吗</p><blockquote><p>是的</p><p>B2sixERQSe/b</p></blockquote></li></ul><p>watch -n</p><p>分布式</p><ul><li>数据采样</li><li>ddp要xxx（忘了）</li><li>init group</li></ul><h3 id="_2-zero策略-系列-分布式训练加速-原理和案例" tabindex="-1">2. ==ZeRO策略==系列（分布式训练加速）（原理和案例） <a class="header-anchor" href="#_2-zero策略-系列-分布式训练加速-原理和案例" aria-label="Permalink to &quot;2. ==ZeRO策略==系列（分布式训练加速）（原理和案例）&quot;">​</a></h3><p>beepspeed框架</p><p>ZeRO=&gt;优化内存=》增大batchsize</p><p>==属于数据并行的范畴，优化了optim==</p><p>显存主要被谁占用？==〉optim的state，而且在混合精度训练时候，在optim的时候是fp32</p><p>dp和mp没有解决这一问题</p><blockquote><p>回顾不足</p></blockquote><p>实现了计算和内存双丰收</p><p>==将状态states分区==（有三个级别或者三个优化阶段）</p><ul><li><p>分区：optim状态，梯度分区，prarm分区</p></li><li><p>即：Pos，optim state + g，optim state + g + p</p></li></ul><p>通信量：</p><p>分布式这块还需要讲哪些（如果多的话可以先跳过一些困惑）</p><h3 id="_3-pytorch对zero3的实现——-fsdp" tabindex="-1">3. pytorch对zero3的实现——==FSDP== <a class="header-anchor" href="#_3-pytorch对zero3的实现——-fsdp" aria-label="Permalink to &quot;3. pytorch对zero3的实现——==FSDP==&quot;">​</a></h3><p>FSDP有俩个版本，pytorch2.4之前是v1，24后大更新v2，能够很好的进行3d并行</p><h4 id="magatore" tabindex="-1">magatore <a class="header-anchor" href="#magatore" aria-label="Permalink to &quot;magatore&quot;">​</a></h4><h3 id="_4-tp张量并行" tabindex="-1">4. TP张量并行 <a class="header-anchor" href="#_4-tp张量并行" aria-label="Permalink to &quot;4. TP张量并行&quot;">​</a></h3><ul><li>对param切分</li><li>行并行和列并行</li><li>==megareoa== <ul><li>上面的方法增大了通信量</li><li>模型并行</li><li>列切分和行切分的时候合并再做激活，对MLP的layer1的weight做列切，进行分别激活，（这儿减少了一次通信）然后对layer做行切，最后再做All Reduce</li><li>与Transformer非常适合结合使用</li><li>拆头的时候如果切列数不跟head一样数目，会有点点问题（但实际不可能有这种情况）</li></ul></li></ul><h3 id="_5-pp" tabindex="-1">5. PP <a class="header-anchor" href="#_5-pp" aria-label="Permalink to &quot;5. PP&quot;">​</a></h3><blockquote><p>调度策略做得好就厉害</p></blockquote><h4 id="之前的" tabindex="-1">之前的： <a class="header-anchor" href="#之前的" aria-label="Permalink to &quot;之前的：&quot;">​</a></h4><blockquote><p>setp的batch：group batch</p></blockquote><ul><li>Gpipe：Micro Batch=&gt; 1. 执行完forward才能backward， 2. pipeline flash（backward执行完才参数更新）</li></ul><blockquote><p>Micro Batch分的越细，bubble越小</p></blockquote><ul><li>PipeDream：采用异步更新，一个时间步实现forward后会进行backward，即1f2b，（后面会有两种选择，有b先b，没有就f）</li></ul><blockquote><p>有时候weight可能不一样（有些是已经更新后的），但是多保留了一些冗余weight</p><p>工程上并没有使用</p></blockquote><ul><li><p>PipeDream 2BW</p></li><li><p>==PipeDream Flash==：有明显的一个分界线，来做参数更新</p></li></ul><blockquote><p>工程使用</p></blockquote><ul><li>megator对上面这种做了优化：interleaved 1f1b</li></ul><blockquote><p>拆storage的粒度更小</p><p>通讯次数变多</p></blockquote><h4 id="近两年" tabindex="-1">近两年： <a class="header-anchor" href="#近两年" aria-label="Permalink to &quot;近两年：&quot;">​</a></h4><blockquote><p>上面的还是可用的</p></blockquote><ul><li>zero- bubble</li></ul><blockquote><p>将backward拆成俩（b和w）</p><p>一个计算weight的grad，一个计算activation的grad（backward的目的是啥，过程是啥）</p><p>pipe flash不再是竖线，而是斜线</p></blockquote><ul><li>Dual pipe（DeepSeek ）</li></ul><h3 id="_6-sp-sequence-p" tabindex="-1">6. SP（sequence P） <a class="header-anchor" href="#_6-sp-sequence-p" aria-label="Permalink to &quot;6. SP（sequence P）&quot;">​</a></h3><p>TP做完之后对seq做SP</p><p>对activation做sp</p><p>两个通信g和g‘，</p><blockquote><p>g，这个增加了一点通信量</p><p>g&#39;之前是All Reduce，现在使用Reduce- Scatter，而且通信量减少了一点（后面的参数量即显存减少了）</p><p>总的通信量与TP的AllReduce一样</p></blockquote><h3 id="_7-3d并行" tabindex="-1">7. 3D并行 <a class="header-anchor" href="#_7-3d并行" aria-label="Permalink to &quot;7. 3D并行&quot;">​</a></h3><p>deepspeed的3D</p><ul><li>2个大DP（All Reduce），每个DP里面是4组PP实现（跨节点send和resv），每组PP切成4个TP（All Reduce）</li></ul><h3 id="_8-cp-context-p" tabindex="-1">8. CP（context P） <a class="header-anchor" href="#_8-cp-context-p" aria-label="Permalink to &quot;8. CP（context P）&quot;">​</a></h3><p>==flash Attention==</p><p>长文本，会对Activation大量增加，weight只是很少的一部分</p><p><strong>EP</strong></p><blockquote><p>EP阶段应当不再有zero策略，Attention需要，MLP做成了EP，除了MLP部分，其他参数的并行是一致的</p></blockquote><hr><h3 id="_9-moe-稀疏moe" tabindex="-1">9. MoE（稀疏MoE） <a class="header-anchor" href="#_9-moe-稀疏moe" aria-label="Permalink to &quot;9. MoE（稀疏MoE）&quot;">​</a></h3><ul><li><p><strong>Gshard</strong></p><ul><li><p>专家（expert）</p><p>放在mlp层，也就是mlp</p><p>专家主要是区分token，而不是sentence</p></li><li><p>路由（router）</p><p>哪个token发到哪个专家</p></li><li><p>最后合并根据index，通信用All to All</p></li><li><p>关键点</p><p>1.跟seq大小无关</p><p>2.位置关系靠All to All</p><p>3.计算参数量会减少，有个分发，所以参数量差不多</p></li><li><p>存在问题：专家拥堵=&gt; 专家分布不均匀（负载不均衡），某些得不到训练</p></li><li><p>keep top K策略</p></li><li><p>token choice</p><ul><li>==辅助损失==（<strong>负载均衡损失</strong>） <ul><li>可能会影响模型性能（因为是一种强制平衡性）</li></ul></li></ul></li><li><p>专家容量</p><p>1</p><p>2</p></li></ul></li><li><p><strong>switch transformer</strong></p></li><li><p><strong>mixtral</strong></p></li><li><p><strong>DeepSeek MoE</strong></p></li></ul><p>如何把MoE和别的并行结合起来</p><h3 id="_10-优化策略总结" tabindex="-1">10. 优化策略总结 <a class="header-anchor" href="#_10-优化策略总结" aria-label="Permalink to &quot;10. 优化策略总结&quot;">​</a></h3><h3 id="_11-监督微调sft" tabindex="-1">11. 监督微调sft <a class="header-anchor" href="#_11-监督微调sft" aria-label="Permalink to &quot;11. 监督微调sft&quot;">​</a></h3><ul><li>指令调优</li></ul><h3 id="_12-rlhf-强化学习" tabindex="-1">12. RLHF（强化学习） <a class="header-anchor" href="#_12-rlhf-强化学习" aria-label="Permalink to &quot;12. RLHF（强化学习）&quot;">​</a></h3><p>==instruct GPT==</p><p>PPO（强化学习算法，奖励模型）</p><p>工程化落地</p><h3 id="_13-微调工具" tabindex="-1">13.微调工具 <a class="header-anchor" href="#_13-微调工具" aria-label="Permalink to &quot;13.微调工具&quot;">​</a></h3><p>LoRA、PEFT</p><p>样本数据+数据加载，</p><h3 id="_14-推理模型-后训练时代" tabindex="-1">14.推理模型（后训练时代） <a class="header-anchor" href="#_14-推理模型-后训练时代" aria-label="Permalink to &quot;14.推理模型（后训练时代）&quot;">​</a></h3><p>Long CoT（长思维链）</p><h3 id="_15-llm发展脉络" tabindex="-1">15. LLM发展脉络 <a class="header-anchor" href="#_15-llm发展脉络" aria-label="Permalink to &quot;15. LLM发展脉络&quot;">​</a></h3><h3 id="_16-deepseek" tabindex="-1">16. DeepSeek <a class="header-anchor" href="#_16-deepseek" aria-label="Permalink to &quot;16. DeepSeek&quot;">​</a></h3><ul><li>V1、v2、v3</li><li>R1 zero、R1：消除了SFT（并没有完全消除），完全采用RLHF（==不使用PPO，采用改进的GRPO==），但是性能有点点慢</li><li>DPO</li><li>PPO</li><li>GRPO</li><li>MLA（前注意）</li><li>MoE</li><li>MTP（多token预测）</li><li>等等等</li><li>MLA（前注意力）</li><li>YaRN（旋转位置编码）</li><li>长思维链</li></ul><p>==补充：60min==</p><hr><p>大模型的工作流通常分为多个模块，每个模块涉及不同的工具和技术栈。以下是 <strong>大模型（LLM）开发与部署的完整工作流</strong>，并标注了 <strong>常见负责模块</strong> 和 <strong>工具链</strong>，方便你明确团队的分工范围。</p><h2 id="一、大模型核心工作流" tabindex="-1"><strong>一、大模型核心工作流</strong> <a class="header-anchor" href="#一、大模型核心工作流" aria-label="Permalink to &quot;**一、大模型核心工作流**&quot;">​</a></h2><h3 id="_1-数据准备-data-preparation" tabindex="-1"><strong>1. 数据准备（Data Preparation）</strong> <a class="header-anchor" href="#_1-数据准备-data-preparation" aria-label="Permalink to &quot;**1. 数据准备（Data Preparation）**&quot;">​</a></h3><ul><li><strong>任务</strong>：数据收集、清洗、标注、预处理。</li><li><strong>工具</strong>： <ul><li><strong>爬虫/API</strong>：Scrapy、BeautifulSoup、Apify。</li><li><strong>清洗/标注</strong>：Pandas、OpenRefine、Prodigy、Label Studio。</li><li><strong>存储</strong>：HDFS、AWS S3、Milvus（向量数据库）。</li></ul></li><li><strong>输出</strong>：高质量训练数据集（如JSONL、Parquet格式）。</li><li><strong>负责模块</strong>：若团队负责数据工程，需重点优化数据质量。</li></ul><hr><h3 id="_2-预训练-pretraining" tabindex="-1"><strong>2. 预训练（Pretraining）</strong> <a class="header-anchor" href="#_2-预训练-pretraining" aria-label="Permalink to &quot;**2. 预训练（Pretraining）**&quot;">​</a></h3><ul><li><strong>任务</strong>：在大规模无监督数据上训练基础模型。</li><li><strong>工具</strong>： <ul><li><strong>框架</strong>：Megatron-LM（NVIDIA）、DeepSpeed（微软）、ColossalAI。</li><li><strong>分布式训练</strong>：PyTorch + FSDP（全分片数据并行）、NCCL（GPU通信）。</li><li><strong>硬件</strong>：NVIDIA A100/H100集群、TPU Pods。</li></ul></li><li><strong>输出</strong>：基础模型（如GPT-3架构的Checkpoint）。</li><li><strong>负责模块</strong>：通常由大厂/研究团队完成，中小团队可直接用开源模型（如LLaMA、Falcon）。</li></ul><hr><h3 id="_3-微调-fine-tuning" tabindex="-1"><strong>3. 微调（Fine-tuning）</strong> <a class="header-anchor" href="#_3-微调-fine-tuning" aria-label="Permalink to &quot;**3. 微调（Fine-tuning）**&quot;">​</a></h3><ul><li><strong>任务</strong>：在领域数据上调整模型。</li><li><strong>方法</strong>： <ul><li><strong>全参数微调</strong>：适合数据充足场景。</li><li><strong>高效微调</strong>：LoRA、QLoRA（节省显存）。</li></ul></li><li><strong>工具</strong>： <ul><li><strong>框架</strong>：Hugging Face Transformers、Axolotl。</li><li><strong>库</strong>：PEFT（参数高效微调）、trl（RLHF）。</li></ul></li><li><strong>输出</strong>：领域适配模型（如医疗、法律垂直模型）。</li><li><strong>负责模块</strong>：若团队专注垂直领域，这是核心模块。</li></ul><hr><h3 id="_4-对齐与强化学习-alignment-rlhf" tabindex="-1"><strong>4. 对齐与强化学习（Alignment &amp; RLHF）</strong> <a class="header-anchor" href="#_4-对齐与强化学习-alignment-rlhf" aria-label="Permalink to &quot;**4. 对齐与强化学习（Alignment &amp; RLHF）**&quot;">​</a></h3><ul><li><strong>任务</strong>：让模型符合人类偏好。</li><li><strong>方法</strong>： <ul><li><strong>RLHF</strong>：基于奖励模型（Reward Model）的PPO训练。</li><li><strong>DPO</strong>：直接偏好优化（更简单高效）。</li></ul></li><li><strong>工具</strong>： <ul><li><strong>标注</strong>：Scale AI、Amazon Mechanical Turk。</li><li><strong>训练</strong>：trl（Transformer Reinforcement Learning）。</li></ul></li><li><strong>输出</strong>：对齐后的模型（如ChatGPT风格）。</li><li><strong>负责模块</strong>：若需产品化，对齐是关键。</li></ul><hr><h3 id="_5-模型评估-evaluation" tabindex="-1"><strong>5. 模型评估（Evaluation）</strong> <a class="header-anchor" href="#_5-模型评估-evaluation" aria-label="Permalink to &quot;**5. 模型评估（Evaluation）**&quot;">​</a></h3><ul><li><strong>任务</strong>：评测模型性能。</li><li><strong>指标</strong>： <ul><li><strong>通用能力</strong>：MMLU、Big-Bench。</li><li><strong>垂直领域</strong>：自定义评估集（如代码生成、问答准确率）。</li></ul></li><li><strong>工具</strong>： <ul><li><strong>评估库</strong>：lm-evaluation-harness、HELM。</li><li><strong>自动化测试</strong>：Pytest + CI/CD（GitHub Actions）。</li></ul></li><li><strong>输出</strong>：模型性能报告。</li><li><strong>负责模块</strong>：所有团队需参与，但可自动化。</li></ul><hr><h3 id="_6-推理部署-inference-deployment" tabindex="-1"><strong>6. 推理部署（Inference &amp; Deployment）</strong> <a class="header-anchor" href="#_6-推理部署-inference-deployment" aria-label="Permalink to &quot;**6. 推理部署（Inference &amp; Deployment）**&quot;">​</a></h3><ul><li><strong>任务</strong>：高效部署模型提供服务。</li><li><strong>优化技术</strong>： <ul><li><strong>量化</strong>：GGUF（llama.cpp）、AWQ。</li><li><strong>推理框架</strong>：vLLM、TensorRT-LLM、TGI（Hugging Face）。</li><li><strong>硬件</strong>：NVIDIA T4/A10G（低成本推理）。</li></ul></li><li><strong>部署方式</strong>： <ul><li><strong>API服务</strong>：FastAPI + Docker + Kubernetes。</li><li><strong>边缘设备</strong>：ONNX Runtime（移动端）。</li></ul></li><li><strong>负责模块</strong>：若团队负责落地，这是重点。</li></ul><hr><h3 id="_7-应用开发-application-integration" tabindex="-1"><strong>7. 应用开发（Application Integration）</strong> <a class="header-anchor" href="#_7-应用开发-application-integration" aria-label="Permalink to &quot;**7. 应用开发（Application Integration）**&quot;">​</a></h3><ul><li><strong>任务</strong>：将模型集成到产品中。</li><li><strong>场景</strong>： <ul><li><strong>聊天机器人</strong>：LangChain、LlamaIndex。</li><li><strong>RAG</strong>：Elasticsearch + 向量数据库（Weaviate）。</li></ul></li><li><strong>工具</strong>： <ul><li><strong>后端</strong>：Flask/FastAPI。</li><li><strong>前端</strong>：Gradio、Streamlit。</li></ul></li><li><strong>负责模块</strong>：应用团队核心工作。</li></ul><hr><h2 id="二、团队常见分工与工具链" tabindex="-1"><strong>二、团队常见分工与工具链</strong> <a class="header-anchor" href="#二、团队常见分工与工具链" aria-label="Permalink to &quot;**二、团队常见分工与工具链**&quot;">​</a></h2><table tabindex="0"><thead><tr><th><strong>模块</strong></th><th><strong>负责团队</strong></th><th><strong>核心工具/技术</strong></th></tr></thead><tbody><tr><td>数据准备</td><td>数据工程团队</td><td>Pandas、Prodigy、AWS S3</td></tr><tr><td>预训练</td><td>大模型研究团队</td><td>Megatron-LM、DeepSpeed</td></tr><tr><td>微调</td><td>算法团队</td><td>Hugging Face、PEFT、QLoRA</td></tr><tr><td>对齐（RLHF/DPO）</td><td>算法+产品团队</td><td>trl、Scale AI</td></tr><tr><td>评估</td><td>算法+QA团队</td><td>lm-evaluation-harness</td></tr><tr><td>推理部署</td><td>工程/DevOps团队</td><td>vLLM、TensorRT-LLM、Kubernetes</td></tr><tr><td>应用开发</td><td>产品+全栈团队</td><td>LangChain、FastAPI、Gradio</td></tr></tbody></table><hr><h2 id="三、如何选择负责模块" tabindex="-1"><strong>三、如何选择负责模块？</strong> <a class="header-anchor" href="#三、如何选择负责模块" aria-label="Permalink to &quot;**三、如何选择负责模块？**&quot;">​</a></h2><ol><li><p><strong>资源较少团队</strong>：</p><ul><li>聚焦 <strong>微调+应用开发</strong>（如基于LLaMA做垂直领域模型）。</li><li>使用开源工具（Hugging Face + vLLM）。</li></ul></li><li><p><strong>研究型团队</strong>：</p><ul><li>参与 <strong>预训练/对齐</strong>（需大量算力）。</li><li>工具：Megatron-LM + DeepSpeed。</li></ul></li><li><p><strong>工程化团队</strong>：</p><ul><li>主攻 <strong>推理优化与部署</strong>（如量化、低延迟API）。</li><li>工具：TensorRT-LLM + Triton Inference Server。</li></ul></li></ol><hr><h2 id="四、典型案例" tabindex="-1"><strong>四、典型案例</strong> <a class="header-anchor" href="#四、典型案例" aria-label="Permalink to &quot;**四、典型案例**&quot;">​</a></h2><ul><li><strong>ChatGPT类产品</strong>：<div class="language-mermaid vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mermaid</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">graph LR</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  A[数据收集] --&gt; B[预训练] --&gt; C[RLHF对齐] --&gt; D[API部署] --&gt; E[应用集成]</span></span></code></pre></div></li><li><strong>企业知识库</strong>：<div class="language-mermaid vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mermaid</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">graph LR</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  A[内部数据清洗] --&gt; B[LoRA微调] --&gt; C[RAG系统] --&gt; D[FastAPI服务]</span></span></code></pre></div></li></ul><hr><h2 id="五、关键挑战" tabindex="-1"><strong>五、关键挑战</strong> <a class="header-anchor" href="#五、关键挑战" aria-label="Permalink to &quot;**五、关键挑战**&quot;">​</a></h2><ul><li><strong>算力需求</strong>：预训练需千卡GPU集群（可考虑云服务如AWS/AutoDL）。</li><li><strong>数据质量</strong>：清洗和标注成本高（可借助合成数据工具）。</li><li><strong>部署延迟</strong>：需优化推理框架（如vLLM的PagedAttention）。</li></ul><p>根据团队目标（研究/产品/工程），选择核心模块并匹配工具链。如果需要具体模块的深入方案（如RLHF或量化），可以进一步探讨！ 🚀</p>`,250)]))}const u=e(r,[["render",i]]);export{g as __pageData,u as default};
