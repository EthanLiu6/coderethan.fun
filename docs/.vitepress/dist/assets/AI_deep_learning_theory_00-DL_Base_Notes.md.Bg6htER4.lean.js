import{_ as s,c as e,o as t,a2 as n}from"./chunks/framework.DA-Pb-tg.js";const h=JSON.parse('{"title":"一. DL_Base_Notes","description":"","frontmatter":{},"headers":[],"relativePath":"AI/deep_learning_theory/00-DL_Base_Notes.md","filePath":"AI/deep_learning_theory/00-DL_Base_Notes.md","lastUpdated":1742639200000}'),i={name:"AI/deep_learning_theory/00-DL_Base_Notes.md"};function r(p,a,m,o,l,c){return t(),e("div",null,a[0]||(a[0]=[n('<h1 id="一-dl-base-notes" tabindex="-1">一. DL_Base_Notes <a class="header-anchor" href="#一-dl-base-notes" aria-label="Permalink to &quot;一. DL_Base_Notes&quot;">​</a></h1><h2 id="_1-normalization🌟🌟🌟🌟🌟" tabindex="-1">1. Normalization🌟🌟🌟🌟🌟 <a class="header-anchor" href="#_1-normalization🌟🌟🌟🌟🌟" aria-label="Permalink to &quot;1. Normalization🌟🌟🌟🌟🌟&quot;">​</a></h2><p>Batch Norm，Layer Norm，Instance Norm，Group Norm</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi><mo>=</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mrow><mi mathvariant="normal">E</mi></mrow><mo>[</mo><mi>x</mi><mo>]</mo></mrow><mrow><msqrt><mrow><mrow><mi mathvariant="normal">V</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi></mrow><mo>[</mo><mi>x</mi><mo>]</mo><mo>+</mo><mi>ϵ</mi></mrow></msqrt></mrow></mfrac><mo>∗</mo><mi>γ</mi><mo>+</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.427em;"></span><span class="strut bottom" style="height:2.55701em;vertical-align:-1.13001em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mrel">=</span><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.825005em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="sqrt mord"><span class="sqrt-sign" style="top:-0.04500500000000007em;"><span class="style-wrap reset-textstyle textstyle uncramped"><span class="delimsizing size1">√</span></span></span><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="mord textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathrm" style="margin-right:0.01389em;">V</span><span class="mord mathrm">a</span><span class="mord mathrm">r</span></span><span class="mopen">[</span><span class="mord mathit">x</span><span class="mclose">]</span><span class="mbin">+</span><span class="mord mathit">ϵ</span></span></span><span style="top:-0.855005em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="reset-textstyle textstyle uncramped sqrt-line"></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span>​</span></span></span></span></span></span><span style="top:-0.22999999999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.677em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathit">x</span><span class="mbin">−</span><span class="mord textstyle uncramped"><span class="mord mathrm">E</span></span><span class="mopen">[</span><span class="mord mathit">x</span><span class="mclose">]</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mbin">∗</span><span class="mord mathit" style="margin-right:0.05556em;">γ</span><span class="mbin">+</span><span class="mord mathit" style="margin-right:0.05278em;">β</span></span></span></span></span></p><p>LayerNorm有助于稳定训练过程并提高收敛性。它的工作原理是对输入的各个特征进行归一化，确保激活的均值和方差一致。普遍认为这种归一化有助于缓解与内部协变量偏移相关的问题，使模型能够更有效地学习并降低对初始权重的敏感性。从架构图上看，LayerNorm在每个Transformer 块中应用两次，一次在自注意力机制之后，一次在FFN层之后，但是在实际工作中不一定如此。</p><p>其他资料：<a href="https://blog.csdn.net/LoseInVain/article/details/86476010" target="_blank" rel="noreferrer">Batch Norm的技术博客</a></p><p><strong>思考：在训练和推理时有何不同？？？</strong></p><h2 id="_2-activation🌟🌟🌟" tabindex="-1">2. Activation🌟🌟🌟 <a class="header-anchor" href="#_2-activation🌟🌟🌟" aria-label="Permalink to &quot;2. Activation🌟🌟🌟&quot;">​</a></h2><h3 id="_2-1-non-linear-activations的两种类型" tabindex="-1">2.1 Non-linear Activations的两种类型 <a class="header-anchor" href="#_2-1-non-linear-activations的两种类型" aria-label="Permalink to &quot;2.1 Non-linear Activations的两种类型&quot;">​</a></h3><p>一种是逐元素操作（Element wise 或者Point wise），eg:ReLU,Sigmoid,Tanh,等，另一种是操作对象（元素）之间具有相关性，eg.Softmax</p><h3 id="_2-2" tabindex="-1">2.2 <a class="header-anchor" href="#_2-2" aria-label="Permalink to &quot;2.2&quot;">​</a></h3><p>······</p><h2 id="_3-loss-function🌟" tabindex="-1">3. Loss Function🌟 <a class="header-anchor" href="#_3-loss-function🌟" aria-label="Permalink to &quot;3. Loss Function🌟&quot;">​</a></h2><h2 id="_4-optimizer🌟🌟🌟🌟" tabindex="-1">4. Optimizer🌟🌟🌟🌟 <a class="header-anchor" href="#_4-optimizer🌟🌟🌟🌟" aria-label="Permalink to &quot;4. Optimizer🌟🌟🌟🌟&quot;">​</a></h2><blockquote><p>动量后面的Admw那些据估计忘了</p></blockquote><h2 id="_5-transformer🌟🌟🌟🌟🌟" tabindex="-1">5. Transformer🌟🌟🌟🌟🌟 <a class="header-anchor" href="#_5-transformer🌟🌟🌟🌟🌟" aria-label="Permalink to &quot;5. Transformer🌟🌟🌟🌟🌟&quot;">​</a></h2><p>深入理解请阅读Transformer系列文章<a href="/AI/Transformer/">Transformer</a></p><h3 id="_5-1-为啥attention的时候要除以" tabindex="-1">5.1 为啥Attention的时候要除以<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msqrt><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow></msqrt></mrow><annotation encoding="application/x-tex">\\sqrt{d_k}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8572200000000001em;"></span><span class="strut bottom" style="height:1.04em;vertical-align:-0.18278em;"></span><span class="base textstyle uncramped"><span class="sqrt mord"><span class="sqrt-sign" style="top:-0.017220000000000013em;"><span class="style-wrap reset-textstyle textstyle uncramped">√</span></span><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="mord textstyle cramped"><span class="mord"><span class="mord mathit">d</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span><span style="top:-0.77722em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="reset-textstyle textstyle uncramped sqrt-line"></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span>​</span></span></span></span></span></span>？ <a class="header-anchor" href="#_5-1-为啥attention的时候要除以" aria-label="Permalink to &quot;5.1 为啥Attention的时候要除以$\\sqrt{d_k}$？&quot;">​</a></h3> Attention(Q, K, V ) = softmax(\\frac{Q·K^T}{\\sqrt{d_k}})·V <p>当 dk<em>d**k</em> 的值比较小的时候，两种点积机制(additive 和 Dot-Product)的性能相差相近，当 dk<em>d**k</em> 比较大时，additive attention 比不带scale 的点积attention性能好。 我们怀疑，对于很大的 dk<em>d**k</em> 值，点积大幅度增长，将softmax函数推向具有极小梯度的区域。 为了抵消这种影响，我们缩小点积 1dk√<em>d**k</em>1 倍。</p><h3 id="_5-2-为啥拆多头-为啥效果好了" tabindex="-1">5.2 为啥拆多头？为啥效果好了？ <a class="header-anchor" href="#_5-2-为啥拆多头-为啥效果好了" aria-label="Permalink to &quot;5.2 为啥拆多头？为啥效果好了？&quot;">​</a></h3><ul><li>提取到了更多的信息（类似CNN的multi- kernel），数据分布组与组之间独立</li><li>减少计算量（应该可以在这一层减少原来的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mn>1</mn></mrow><mrow><mi>N</mi><mi>u</mi><msub><mi>m</mi><mrow><mi>h</mi><mi>e</mi><mi>a</mi><mi>d</mi></mrow></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\\frac{1}{Num_{head}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.845108em;"></span><span class="strut bottom" style="height:1.2959679999999998em;vertical-align:-0.4508599999999999em;"></span><span class="base textstyle uncramped"><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.345em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.10903em;">N</span><span class="mord mathit">u</span><span class="mord"><span class="mord mathit">m</span><span class="vlist"><span style="top:0.15122857142857138em;margin-right:0.07142857142857144em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord scriptscriptstyle cramped"><span class="mord mathit">h</span><span class="mord mathit">e</span><span class="mord mathit">a</span><span class="mord mathit">d</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span><span style="top:-0.22999999999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.394em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span>倍）</li></ul><h3 id="_5-3-cross-multi-head-attention" tabindex="-1">5.3 Cross Multi-Head Attention？ <a class="header-anchor" href="#_5-3-cross-multi-head-attention" aria-label="Permalink to &quot;5.3 Cross Multi-Head Attention？&quot;">​</a></h3><p>首先，Self- Attention与传统的Attention机制非常的不同：传统的Attention是基于source端和target端的隐变量（hidden state）计算Attention的，得到的结果是源端（source端）的每个词与目标端（target端）每个词之间的依赖关系。 其次，Self-Attention首先分别在source端和target端进行自身的attention，仅与source input或者target input自身相关的Self -Attention，以捕捉source端或target端自身的词与词之间的依赖关系；然后再把source端的得到的self -Attention加入到target端得到的Attention中，称作为<strong>Cross-Attention</strong>，以捕捉source端和target端词与词之间的依赖关系。</p><h3 id="_5-4-mask-multi-head-attention" tabindex="-1">5.4 Mask Multi-Head Attention <a class="header-anchor" href="#_5-4-mask-multi-head-attention" aria-label="Permalink to &quot;5.4 Mask Multi-Head Attention&quot;">​</a></h3><p>​ 与Encoder的Multi-Head Attention计算原理一样，只是多加了一个mask码。mask 表示掩码，它对某些值进行掩盖，使其在参数更新时不产生效果。Transformer 模型里面涉及两种 mask，分别是 padding mask 和 sequence mask。</p><h3 id="_5-5-masking实现机理" tabindex="-1">5.5 Masking实现机理 <a class="header-anchor" href="#_5-5-masking实现机理" aria-label="Permalink to &quot;5.5 Masking实现机理&quot;">​</a></h3><p>具体的做法是，把<strong>这些位置</strong>的值<strong>加上一个非常大的负数(负无穷)</strong>，这样的话，经过 softmax，这些位置的概率就会接近0！</p><h3 id="_5-6-mqa和gqa" tabindex="-1">5.6 MQA和GQA <a class="header-anchor" href="#_5-6-mqa和gqa" aria-label="Permalink to &quot;5.6 MQA和GQA&quot;">​</a></h3><p>MQA多头共用K，V</p><p>GQA将头分组，组内共用KV</p><h3 id="_5-x-其他" tabindex="-1">5.x 其他 <a class="header-anchor" href="#_5-x-其他" aria-label="Permalink to &quot;5.x 其他&quot;">​</a></h3><ul><li><p>工程中将QKV的权重矩阵直接放在一块，shape就是原来<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mi>e</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>d</mi><mi>d</mi><mi>i</mi><mi>n</mi><mi>g</mi><mi mathvariant="normal">_</mi><mi>d</mi><mi>i</mi><mi>m</mi><mo separator="true">,</mo><mi>e</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>d</mi><mi>d</mi><mi>i</mi><mi>n</mi><mi>g</mi><mi mathvariant="normal">_</mi><mi>d</mi><mi>i</mi><mi>m</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">(embedding\\_dim, embedding\\_dim)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1.06em;vertical-align:-0.31em;"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord mathit">e</span><span class="mord mathit">m</span><span class="mord mathit">b</span><span class="mord mathit">e</span><span class="mord mathit">d</span><span class="mord mathit">d</span><span class="mord mathit">i</span><span class="mord mathit">n</span><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="mord mathrm" style="margin-right:0.02778em;">_</span><span class="mord mathit">d</span><span class="mord mathit">i</span><span class="mord mathit">m</span><span class="mpunct">,</span><span class="mord mathit">e</span><span class="mord mathit">m</span><span class="mord mathit">b</span><span class="mord mathit">e</span><span class="mord mathit">d</span><span class="mord mathit">d</span><span class="mord mathit">i</span><span class="mord mathit">n</span><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="mord mathrm" style="margin-right:0.02778em;">_</span><span class="mord mathit">d</span><span class="mord mathit">i</span><span class="mord mathit">m</span><span class="mclose">)</span></span></span></span>到 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mi>e</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>d</mi><mi>d</mi><mi>i</mi><mi>n</mi><mi>g</mi><mi mathvariant="normal">_</mi><mi>d</mi><mi>i</mi><mi>m</mi><mo separator="true">,</mo><mi>e</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>d</mi><mi>d</mi><mi>i</mi><mi>n</mi><mi>g</mi><mi mathvariant="normal">_</mi><mi>d</mi><mi>i</mi><mi>m</mi><mo>×</mo><mn>3</mn><mo>)</mo></mrow><annotation encoding="application/x-tex">(embedding\\_dim, embedding\\_dim \\times 3)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1.06em;vertical-align:-0.31em;"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord mathit">e</span><span class="mord mathit">m</span><span class="mord mathit">b</span><span class="mord mathit">e</span><span class="mord mathit">d</span><span class="mord mathit">d</span><span class="mord mathit">i</span><span class="mord mathit">n</span><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="mord mathrm" style="margin-right:0.02778em;">_</span><span class="mord mathit">d</span><span class="mord mathit">i</span><span class="mord mathit">m</span><span class="mpunct">,</span><span class="mord mathit">e</span><span class="mord mathit">m</span><span class="mord mathit">b</span><span class="mord mathit">e</span><span class="mord mathit">d</span><span class="mord mathit">d</span><span class="mord mathit">i</span><span class="mord mathit">n</span><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="mord mathrm" style="margin-right:0.02778em;">_</span><span class="mord mathit">d</span><span class="mord mathit">i</span><span class="mord mathit">m</span><span class="mbin">×</span><span class="mord mathrm">3</span><span class="mclose">)</span></span></span></span></p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/1850883-20250209144655019-1620461538.jpg" alt="img"></p></li><li><p>Attention的时候是否需要对自身做？自回归的时候应当下一次token尽可能不是上一个词，所以矩阵对角线是否应当是趋于零的？</p></li></ul><h2 id="_6-🌟🌟🌟k-v-cache" tabindex="-1">6. 🌟🌟🌟K-V Cache <a class="header-anchor" href="#_6-🌟🌟🌟k-v-cache" aria-label="Permalink to &quot;6. 🌟🌟🌟K-V Cache&quot;">​</a></h2><h2 id="_7-🌟🌟🌟常见的正则化方法" tabindex="-1">7. 🌟🌟🌟常见的正则化方法 <a class="header-anchor" href="#_7-🌟🌟🌟常见的正则化方法" aria-label="Permalink to &quot;7. 🌟🌟🌟常见的正则化方法&quot;">​</a></h2><ul><li>Dropout</li><li></li></ul><h1 id="二-课堂记录" tabindex="-1">二. 课堂记录 <a class="header-anchor" href="#二-课堂记录" aria-label="Permalink to &quot;二. 课堂记录&quot;">​</a></h1><h2 id="_0301-0302-transformer🌟🌟🌟" tabindex="-1">0301-0302 Transformer🌟🌟🌟 <a class="header-anchor" href="#_0301-0302-transformer🌟🌟🌟" aria-label="Permalink to &quot;0301-0302 Transformer🌟🌟🌟&quot;">​</a></h2><ul><li>0301:input序列长度大于embedding时候的seq_len时, input的输入序列会按照seq_len进行切割拼接到batch上吗? (老师讲了encoder时候input不足seq_len时候使用mask然后想问的另一个问题)</li><li>0302:<code>K-V cache</code>时候当预测下一个时间步的时候与之前的做Attention的时候, 中途会取出cache里的K—V吗还是只取出里面的K还是只在最后一个结束后才整体取一次 (我想问的也就是在一个batch或者一个seq的访存情况, 每一个时间步都需要访问cache一次吗)</li><li>是直接使用缓存的填充矩阵还是需要拿出缓存数据(读还是取)</li></ul><h2 id="_0308-0309——pytorch" tabindex="-1">0308-0309——PyTorch <a class="header-anchor" href="#_0308-0309——pytorch" aria-label="Permalink to &quot;0308-0309——PyTorch&quot;">​</a></h2><blockquote><p>提及: 混合精度训练</p></blockquote><h3 id="_1-1-tensor-中数据的连续性" tabindex="-1">1.1 Tensor 中数据的连续性 <a class="header-anchor" href="#_1-1-tensor-中数据的连续性" aria-label="Permalink to &quot;1.1 Tensor 中数据的连续性&quot;">​</a></h3><p>reshape, transpose, view, T(转置), permute</p><p>transpose会让raw data不变(共用), mata data的stride和shape等属性就变了 is_contiguous()不连续, 但reshape和permute这些是不会变的,因为他们会发生data copy, contiguous()会发生copy raw data数据</p><p>view和reshape的区别</p><p>view更加安全, 不会重新拷贝数据, 但数据不连续不能使用view,也就是stride不协调, reshape不会错误, 会重新拷贝数据, 数据也连续</p><p>permute和transpose会让stride属性改变, 从而发生数据不连续, 通常使用后要加一个contiguous()让数据连续</p><h3 id="_1-2-pytorch-autograd" tabindex="-1">1.2 pytorch autograd <a class="header-anchor" href="#_1-2-pytorch-autograd" aria-label="Permalink to &quot;1.2 pytorch autograd&quot;">​</a></h3><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250308163009045.png" alt="image-20250308163009045" style="zoom:36%;"><p>……………………</p><p>叶子结点+requests_grad=True才有最终的grad, 非叶子结点中途可能会计算grad, 但用了就会丢弃(requests_grad=True的)</p><p>梯度累加也有可能, 多个step的梯度累加, 隐式增加batch</p><p>若没进行xxx.grad.zero_()或者xxx.grad = None, 则会进行accumulate()累加grad, 这两种方法有一点区别, zero__()会置零,会占用显存, 但=None的话会释放显存, 两者各有好坏</p><h3 id="_1-3-inplace-op" tabindex="-1">1.3 inplace-op <a class="header-anchor" href="#_1-3-inplace-op" aria-label="Permalink to &quot;1.3 inplace-op&quot;">​</a></h3><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250309102559857.png" alt="image-20250309102559857"></p><p>叶子结点的Tensor变量不能进行in-place操作, 因为要更新梯度的时候要用叶子结点</p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250309113212792.png" alt="image-20250309113212792" style="zoom:50%;"><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250309114217344.png" alt="image-20250309114217344" style="zoom:50%;"><p>no_grad()底层是基于set_grad_enable(Flase)的</p><h3 id="_1-4-自动微分机制-auto-grad-重点" tabindex="-1">1.4 自动微分机制(auto grad) 重点： <a class="header-anchor" href="#_1-4-自动微分机制-auto-grad-重点" aria-label="Permalink to &quot;1.4 自动微分机制(auto grad) 重点：&quot;">​</a></h3><ul><li>pytorch中 正向forward 对我们用户是可见的，但是backward对我们用户是不可见的；</li><li>一般情况下，每一个正向的函数，都对应一个反向的函数（grad_fn--&gt; Tensor中）；</li><li>tensor：requires_grad = True</li><li>tensor: grad --&gt; tensor 中存储grad的地方；</li><li>tensor: grad_fn --&gt; 存储我们反向函数的地方</li><li>tesnor: is_leaf --&gt; 这个tensor 是不是 叶子节点；</li><li>net::all weight --&gt; 都是leaf</li><li>叶子节点的梯度会自动保存下来的（weight）；</li><li>中间的 activation 的梯度会计算，但是不保留；</li><li>pytorch 动态图 vs tensorflow 静态图；</li><li>我们不能改变一个非叶子节点的 requires_grad;</li><li>非叶子（一个函数的output）节点它的 requires_grad 自动推导的；</li><li>非叶子节点对应函数的inputs 中只要有一个 requires_grad = True, 那么这个非叶子节点的requires_grad = True;</li><li>torch.no_grad() 会使得里面的新的tensor requires_grad = False</li><li>inplace的操作，非常大的风险：覆盖了原来的值，导致反向传播时计算不准确；</li><li>标量的梯度才能被隐式创建，隐式创建（.backward(1)）；</li><li>一般情况下，.backward(gradient)是有输入的: ;</li></ul><h3 id="_2-1-torch-nn-module" tabindex="-1">2.1 torch.nn.Module <a class="header-anchor" href="#_2-1-torch-nn-module" aria-label="Permalink to &quot;2.1 torch.nn.Module&quot;">​</a></h3><p>train模式和veal模式不会对grad的情况做修改,只是对训练和推理的对应的算子做不同的处理(等价处理)</p><p>常用算子dropout和BachNorm</p><p>xxx.cuda()的时候搬迁的是_parameters到cuda, 还有buffer也搬迁到cuda, 并没有将模型结构进行搬迁.</p><p>按照深度优先遍历sub module,将里面的_parameters和buffer到cuda, 数据类型转换也是一样的操作</p><p>c++底层实现了一个dispather分发机制,按照device属性分发, 对应device会调用对应的fn算子, 计算部分才执行</p><p>_parameters()送参数给优化器的时候将所有的parameters送到optim, 但数据共用, 同时更新</p><p>钩子函数(没太懂)</p><hr><h2 id="_0315-0316-续pytorch" tabindex="-1">0315-0316（续PyTorch） <a class="header-anchor" href="#_0315-0316-续pytorch" aria-label="Permalink to &quot;0315-0316（续PyTorch）&quot;">​</a></h2><h3 id="_1-1-回顾" tabindex="-1">1.1 回顾 <a class="header-anchor" href="#_1-1-回顾" aria-label="Permalink to &quot;1.1 回顾&quot;">​</a></h3><p>1.Tensor类和重要属性 2.autograd，动态图 3.Module以及属性和方法</p><blockquote><p>training,_parameters,_buffers,_modules(hooks是主要用二次开发等情况)</p></blockquote><p>子模块啥时候定义的呢？</p><p>_parameters,_buffers哪些有哪些没有</p><p>将module里的parameters传给optim，会通过调用parameters()进行</p><p>一系列方法具体情况</p><h3 id="_1-2-问题合集" tabindex="-1">1.2 问题合集 <a class="header-anchor" href="#_1-2-问题合集" aria-label="Permalink to &quot;1.2 问题合集&quot;">​</a></h3><ol><li>在讲transformer的padding mask的时候想到，如果输入seq_len大于了定义的seq_len，会直接截断还是截断再拼接到下一个batch</li><li>在sequence mask的时候，忘了要问啥了</li><li>在normalization层的时候不是有两个学习的参数吗，这俩参数是一次forward训练一次还是单独有自己的训练？还有，这俩参数是咋更新的？</li><li>dataset会迭代的将所有数据加载到内存吗，然后dataloader再一批次的提取吗</li></ol><p>with torch.no_grad(): eval时候用，计算图不再进行，对require_grads=True的不进行梯度计算，显存占用量会减少，activation的就会丢弃</p><p>dataset会迭代的将所有数据加载到内存吗，然后dataloader再一批次一批次的提取吗？还是说dataloader准备拿一个batch，然后dataset根据batch_size迭代获取size条。</p><blockquote><p>是后者，也就是I/O的时候，batch_size太小的话会增加I/O负担</p></blockquote><h3 id="_2-1-torch-optim" tabindex="-1">2.1 torch.optim <a class="header-anchor" href="#_2-1-torch-optim" aria-label="Permalink to &quot;2.1 torch.optim&quot;">​</a></h3><p>参数传param的时候的传递和打包方式</p><p>self.param_groups</p><p>==self.state==：训练时候显存消耗的主要项（优化器的动量项有关） 他是一个dict，keys是tensor，values也是 模型</p><blockquote><p>移动指数平均是啥忘了</p></blockquote><p>def load_state_dict</p><h3 id="_2-2-learning-rate-调整方案" tabindex="-1">2.2 learning rate 调整方案 <a class="header-anchor" href="#_2-2-learning-rate-调整方案" aria-label="Permalink to &quot;2.2 learning rate 调整方案&quot;">​</a></h3><p>Torch.optim.lr_scheduler</p><p>震荡类型的学习率调整是减少进入局部最优解的情况</p><p>==状态字典==，三个地方见过，都类似，模型保存时候需要有</p><h3 id="_2-3-模型保存和加载" tabindex="-1">2.3 模型保存和加载 <a class="header-anchor" href="#_2-3-模型保存和加载" aria-label="Permalink to &quot;2.3 模型保存和加载&quot;">​</a></h3><p>==动态图==</p><p>1.save state_dict的时候只有参数，save model的时候无法直接保存整个网络，但是他的材料（init）的那些会保存，模型加载的时候能通过，但runing time时候，forward并没有，必须导入或者自己实现，需要原来Net的签名（具体定义可以不一致，会放入_modules）</p><p>2.如果是自己写的算子，在init时候也放入_modules吗？</p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/%7B6fbad3cc-1899-4404-b3b3-d91f7da5cb95%7D.png" alt="img"></p><p>3.==onnx==模型保存必须输入对应的input，自己run一遍，是一个静态图</p><p>4.训练中的保存和加载（check point）==模型保存的几种参数类型==）</p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250316113313785.png" alt="image-20250316113313785"></p><h3 id="_3-1-dataset-and-dataloader" tabindex="-1">3.1 Dataset and Dataloader <a class="header-anchor" href="#_3-1-dataset-and-dataloader" aria-label="Permalink to &quot;3.1 Dataset and Dataloader&quot;">​</a></h3><blockquote><p>只学习pytorch的，后续自己补hf的那些</p></blockquote><h3 id="_4-1-nlp" tabindex="-1">4.1 NLP <a class="header-anchor" href="#_4-1-nlp" aria-label="Permalink to &quot;4.1 NLP&quot;">​</a></h3><p>GPT：自监督训练得到预训练模型（采用迁移学习）</p><p>Bert：完形填空</p><p>迁移学习：预训练+微调（微调的数据集就是专业领域的数据集）</p><h3 id="_4-2-bert" tabindex="-1">4.2 Bert <a class="header-anchor" href="#_4-2-bert" aria-label="Permalink to &quot;4.2 Bert&quot;">​</a></h3><p>1.两个任务：MLM和NSP</p><p>2.Embedding，词嵌入</p><p>词，句子（分段），位置 嵌入</p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250316165604714.png" alt="image-20250316165604714"></p><blockquote><p>transformer的词嵌入式用三角位置嵌入</p></blockquote><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250316175009793.png" alt="image-20250316175009793"></p><p>未讲知识：分词器tokenizer</p><h2 id="_0322-0323" tabindex="-1">0322-0323 <a class="header-anchor" href="#_0322-0323" aria-label="Permalink to &quot;0322-0323&quot;">​</a></h2><h3 id="_1-1-回顾-1" tabindex="-1">1.1 回顾 <a class="header-anchor" href="#_1-1-回顾-1" aria-label="Permalink to &quot;1.1 回顾&quot;">​</a></h3><p><code>bert4torch</code>的ner项目讲解和debug</p><h3 id="_2-1-t5讲解" tabindex="-1">2.1 T5讲解 <a class="header-anchor" href="#_2-1-t5讲解" aria-label="Permalink to &quot;2.1 T5讲解&quot;">​</a></h3><h3 id="_2-2-position-embedding🌟🌟🌟🌟🌟" tabindex="-1">2.2 position embedding🌟🌟🌟🌟🌟 <a class="header-anchor" href="#_2-2-position-embedding🌟🌟🌟🌟🌟" aria-label="Permalink to &quot;2.2 position embedding🌟🌟🌟🌟🌟&quot;">​</a></h3><p>绝对位置编码</p><ul><li>三角函数式(Sinusoidal)</li><li>可学习(Learnable)</li></ul><p>相对位置编码</p><ul><li><strong>是在Attention的时候才位置编码</strong></li><li>只对q和k做位置编码，对value不做，value是结果或者说是token本身的特征信息</li><li>T5的分桶思想</li></ul><p>==旋转位置编码==（大模型使用的方法）</p><ul><li>根据数学原理推导</li></ul><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250322144241104.png" alt="image-20250322144241104"></p><ul><li>想要得到的效果=&gt;反推</li></ul><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250322144600625.png" alt="image-20250322144600625"></p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250322145503812.png" alt="image-20250322145503812"></p><h3 id="_3-1-gpt" tabindex="-1">3.1 GPT <a class="header-anchor" href="#_3-1-gpt" aria-label="Permalink to &quot;3.1 GPT&quot;">​</a></h3><p>GPT-1 已经出现zero-shot迹象，层归一化还是之前的post-norm</p><p>GPT-2 零样本学习，即zero-shot，层归一化有点变化，改成per-Norm</p><blockquote><p>相当于纯预训练</p></blockquote><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250322170432945.png" alt="image-20250322170432945"></p><p>GPT-3 few-shot（给案例），发现模型规模可以提高能力，最后实现了无需微调到达一些较好的任务处理，架构基本和GPT-2一致，但加了一个新的‘交替的稠密和稀疏的’Attention，余弦衰减的学习率策略，batch-size从小变大，再加上0.1的权重衰减正则化</p><blockquote><p>Few-shot, one-shot, zero-shot</p><p>• <strong>Few-Shot（FS）：</strong> 模型在推理时给出K个任务示例作为上下文信息，同时提供任务的自然语言描述，但不允许模型进行权重更新。通常将K设置在10到100的范围内，以适应模型的上下文窗口。</p><p>• <strong>One-Shot（1S）：</strong> 模型在推理时通过提供一个任务示例作为上下文信息，同时还有任务的自然语言描述。这种方式最接近于人类在解决某些任务时所使用的方式。</p><p>• <strong>Zero-Shot（0S）：</strong> 不提供任何上下文信息，模型只给出一个描述任务的自然语言指令。</p></blockquote>',137)]))}const u=s(i,[["render",r]]);export{h as __pageData,u as default};
