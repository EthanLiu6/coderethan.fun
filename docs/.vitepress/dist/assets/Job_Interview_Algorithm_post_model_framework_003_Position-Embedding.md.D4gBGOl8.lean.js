import{_ as a,c as n,o as e,a2 as t}from"./chunks/framework.DA-Pb-tg.js";const d=JSON.parse('{"title":"","description":"","frontmatter":{},"headers":[],"relativePath":"Job_Interview/Algorithm_post/model_framework/003_Position-Embedding.md","filePath":"Job_Interview/Algorithm_post/model_framework/003_Position-Embedding.md","lastUpdated":1746438847000}'),i={name:"Job_Interview/Algorithm_post/model_framework/003_Position-Embedding.md"};function p(l,s,m,r,o,h){return e(),n("div",null,s[0]||(s[0]=[t(`<h1 align="center"><p>003_Position-Embedding</p></h1><h2 id="_1-为什么引入位置编码" tabindex="-1">1. 为什么引入位置编码 <a class="header-anchor" href="#_1-为什么引入位置编码" aria-label="Permalink to &quot;1. 为什么引入位置编码&quot;">​</a></h2><p>现在的大模型架构基本都是基于Tansformer的Decoder架构的，之前讲Tansformer并没有提出一个问题，就是：<strong>输入序列内各元素之间的位置关系</strong>是无法捕捉到的，为什么呢？</p><p>其实，在RNN（递归神经网络）和CNN（Conv卷积神经网络）中，对于输入的位置关系是有所捕获的，想想RNN，不就是一个元素一个元素的预测，那他本身就带有了一定的位置关系（好吧，我当时也没想到有这层概念。或许，我应当再仔细分析分析，可能理解有误），在Tansformer论文里也提出了这一点：</p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250504142149764.png" alt="image-20250504142149764"></p><p>那为啥Transformer本身不具有位置编码信息呢？整体上就是<strong>Attention并不会获取到输入序列（token与token之间）的位置信息，只有俩俩的相关性。</strong></p><p>想想Attention机制就清晰了，假设只有一个长度为4的序列（batch_szie=1，seq_len=4），词嵌入维度大小为10，在草稿纸上画一画就可以知道，进行<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>q</mi><mo>⋅</mo><msup><mi>k</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">q \\cdot k^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8413309999999999em;"></span><span class="strut bottom" style="height:1.035771em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">q</span><span class="mbin">⋅</span><span class="mord"><span class="mord mathit" style="margin-right:0.03148em;">k</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">T</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>的操作只是一个点积操作，<strong>基本的意义就是对元素与元素之间的相似度计算结果</strong>（这是笔者自己尝试和思考之后的理解，不一定对），而在后续的整个Attention操作当中，都没有进行位置关系的操作，所以对序列的元素顺序进行打乱，依然可以得到对应相同的结果（<strong>我说的是相对</strong>），所以，为了解决这一问题，原论文作者就引入了位置编码向量了，好了，接下来我们看看有哪些位置编码以及引入方式（后续对位置编码用PE表示）。</p><p><strong>思考</strong>：</p><ul><li><p>位置编码向量应该长啥样？如何引入？</p><blockquote><p>&quot;The positional encodings have the same dimension <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">d_{model}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">d</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">m</span><span class="mord mathit">o</span><span class="mord mathit">d</span><span class="mord mathit">e</span><span class="mord mathit" style="margin-right:0.01968em;">l</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> as the embeddings, so that the two can be summed.&quot;</p></blockquote></li><li><p>引入位置编码信息和不引入会有什么区别和影响？</p><blockquote><p>简单理解就是使模型能区分 <code>&quot;从 我家 出门 到 你家&quot;</code> 和 <code>&quot;从 你家 出门 到 我家&quot;</code></p></blockquote></li><li><p>如果是你，你会采取哪种方式进行位置信息的嵌入？</p></li></ul><h2 id="_2-基本的类型" tabindex="-1">2. 基本的类型 <a class="header-anchor" href="#_2-基本的类型" aria-label="Permalink to &quot;2. 基本的类型&quot;">​</a></h2><h3 id="_2-1-三角函数式的pe" tabindex="-1">2.1 三角函数式的PE <a class="header-anchor" href="#_2-1-三角函数式的pe" aria-label="Permalink to &quot;2.1 三角函数式的PE&quot;">​</a></h3><p>中英文对照论文：<a href="https://yiyibooks.cn/arxiv/1706.03762v7/index.html" target="_blank" rel="noreferrer">Attention Is All You Need</a></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><msubsup><mi>E</mi><mrow><mn>2</mn><mi>i</mi></mrow><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow></msubsup><mo>=</mo><mi>s</mi><mi>i</mi><mi>n</mi><mo>(</mo><mfrac><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow><mrow><mn>1</mn><mn>0</mn><mn>0</mn><mn>0</mn><msup><mn>0</mn><mrow><mn>2</mn><mi>i</mi><mi mathvariant="normal">/</mi><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow></msup></mrow></mfrac><mo>)</mo></mrow><annotation encoding="application/x-tex">PE^{pos}_{2i} = sin(\\frac{pos}{10000^{2i/d_{model}}}) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.10756em;"></span><span class="strut bottom" style="height:1.81156em;vertical-align:-0.704em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mord"><span class="mord mathit" style="margin-right:0.05764em;">E</span><span class="vlist"><span style="top:0.276864em;margin-left:-0.05764em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathrm">2</span><span class="mord mathit">i</span></span></span></span><span style="top:-0.4809079999999999em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">p</span><span class="mord mathit">o</span><span class="mord mathit">s</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mord mathit">s</span><span class="mord mathit">i</span><span class="mord mathit">n</span><span class="mopen">(</span><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.704em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathrm">1</span><span class="mord mathrm">0</span><span class="mord mathrm">0</span><span class="mord mathrm">0</span><span class="mord"><span class="mord mathrm">0</span><span class="vlist"><span style="top:-0.289em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathrm">2</span><span class="mord mathit">i</span><span class="mord mathrm">/</span><span class="mord"><span class="mord mathit">d</span><span class="vlist"><span style="top:0.15122857142857138em;margin-right:0.07142857142857144em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord scriptscriptstyle cramped"><span class="mord mathit">m</span><span class="mord mathit">o</span><span class="mord mathit">d</span><span class="mord mathit">e</span><span class="mord mathit" style="margin-right:0.01968em;">l</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span><span style="top:-0.22999999999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.677em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathit">p</span><span class="mord mathit">o</span><span class="mord mathit">s</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mclose">)</span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><msubsup><mi>E</mi><mrow><mn>2</mn><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow></msubsup><mo>=</mo><mi>c</mi><mi>o</mi><mi>s</mi><mo>(</mo><mfrac><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow><mrow><mn>1</mn><mn>0</mn><mn>0</mn><mn>0</mn><msup><mn>0</mn><mrow><mn>2</mn><mi>i</mi><mi mathvariant="normal">/</mi><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow></msup></mrow></mfrac><mo>)</mo></mrow><annotation encoding="application/x-tex">PE^{pos}_{2i+1} = cos(\\frac{pos}{10000^{2i/d_{model}}}) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.10756em;"></span><span class="strut bottom" style="height:1.81156em;vertical-align:-0.704em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mord"><span class="mord mathit" style="margin-right:0.05764em;">E</span><span class="vlist"><span style="top:0.276864em;margin-left:-0.05764em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathrm">2</span><span class="mord mathit">i</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span style="top:-0.480908em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">p</span><span class="mord mathit">o</span><span class="mord mathit">s</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mord mathit">c</span><span class="mord mathit">o</span><span class="mord mathit">s</span><span class="mopen">(</span><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.704em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathrm">1</span><span class="mord mathrm">0</span><span class="mord mathrm">0</span><span class="mord mathrm">0</span><span class="mord"><span class="mord mathrm">0</span><span class="vlist"><span style="top:-0.289em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathrm">2</span><span class="mord mathit">i</span><span class="mord mathrm">/</span><span class="mord"><span class="mord mathit">d</span><span class="vlist"><span style="top:0.15122857142857138em;margin-right:0.07142857142857144em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord scriptscriptstyle cramped"><span class="mord mathit">m</span><span class="mord mathit">o</span><span class="mord mathit">d</span><span class="mord mathit">e</span><span class="mord mathit" style="margin-right:0.01968em;">l</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span><span style="top:-0.22999999999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.677em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathit">p</span><span class="mord mathit">o</span><span class="mord mathit">s</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mclose">)</span></span></span></span></span></p><ul><li><p>这是一种绝对位置编码方式</p><blockquote><p>直接对word embedding和position embedding相加送入到模型</p></blockquote></li><li><p>是Transformer架构最初的编码方式</p><p>首先，论文里面说 &quot;The positional encodings have the same dimension <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">d_{model}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">d</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">m</span><span class="mord mathit">o</span><span class="mord mathit">d</span><span class="mord mathit">e</span><span class="mord mathit" style="margin-right:0.01968em;">l</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> as the embeddings, so that the two can be summed.&quot;，那位置编码向量也就跟embedding之后的大小是一致的，然后将PE矩阵和Embedding后的矩阵直接相加得到结果，这就是作者的引入方式。我们所有最初的测试都按照batch_size=1的数据进行分析，也就是现在只考虑一个样本序列，假设他的shape为[seq_len, embedding_dim]，OK，我们继续详读论文。</p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250504161302347.png" alt="image-20250504161302347"></p></li><li><p>就上面公式，其中：</p><p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow><annotation encoding="application/x-tex">pos</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit">p</span><span class="mord mathit">o</span><span class="mord mathit">s</span></span></span></span>表示第<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow><annotation encoding="application/x-tex">pos</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit">p</span><span class="mord mathit">o</span><span class="mord mathit">s</span></span></span></span>个token，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>2</mn><mi>i</mi></mrow><annotation encoding="application/x-tex">2i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.65952em;"></span><span class="strut bottom" style="height:0.65952em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">2</span><span class="mord mathit">i</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>2</mn><mi>i</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">2i+1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.65952em;"></span><span class="strut bottom" style="height:0.74285em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">2</span><span class="mord mathit">i</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span>表示token对应的embedding（第奇数个和偶数个分别采用cos和sin）</p></li><li><p>ransformer的作者表示：</p><p>他们还尝试使用学习的位置嵌入 ，发现这两个版本产生了几乎相同的结果。他们选择正弦版本，因为它可能允许模型预测到时候处理比训练期间遇到的序列长度更长的句子。</p></li></ul><p><strong>思考：</strong></p><ul><li><p>公式中对于2i+1个分量的分母指数那块为啥是2i？</p></li><li><p>那哪里体现了位置信息呢？</p><blockquote><p>想想公式，pos</p></blockquote></li><li><p>公式到底是怎样来的呢？是怎样的原理哇？</p></li><li><p>在Attention之前进行了qkv矩阵映射，那位置编码是对哪些做哪些不做？</p><blockquote><p>只对q和k做位置编码，对value不做，因为value是token本身的特征信息</p></blockquote></li></ul><h3 id="_2-2-可学习的pe" tabindex="-1">2.2 可学习的PE <a class="header-anchor" href="#_2-2-可学习的pe" aria-label="Permalink to &quot;2.2 可学习的PE&quot;">​</a></h3><p>很显然，绝对位置编码的一个最朴素方案是不特意去设计什么，而是直接将位置编码当作可训练参数，比如最大长度为512，编码维度为768，那么就初始化一个512×768的矩阵作为位置向量，让它随着训练过程更新。现在的BERT、GPT3等模型所用的就是这种位置编码（后续会专门整理Bert这块的内容）。</p><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250504181716301.png" alt="image-20250504181716301"></p><p>​ 对于这种训练式的绝对位置编码，一般的认为它的缺点是没有<strong>外推性</strong>（推理阶段确保模型能处理远超预训练时的文本长度），即如果预训练最大长度为512的话，那么最多就只能处理长度为512的句子，再长就处理不了了。当然，也可以将超过512的位置向量随机初始化，然后继续微调。</p><h3 id="_2-3-rope-旋转位置编码" tabindex="-1">2.3 RoPE（旋转位置编码） <a class="header-anchor" href="#_2-3-rope-旋转位置编码" aria-label="Permalink to &quot;2.3 RoPE（旋转位置编码）&quot;">​</a></h3><blockquote><p>使用绝对位置编码的方式实现相对位置信息编码</p><p>&quot;Specifically, the proposed RoPE <strong>encodes the absolute position with a rotation matrix</strong> and meanwhile <strong>incorporates the explicit relative position dependency</strong> in self-attention formulation&quot;</p></blockquote> \\left\\langle f_{q}\\left(\\boldsymbol{x}_{m}, m\\right), f_{k}\\left(\\boldsymbol{x}_{n}, n\\right)\\right\\rangle=g\\left(\\boldsymbol{x}_{m}, \\boldsymbol{x}_{n}, m-n\\right) <ul><li><p><strong>大模型主流位置编码方式</strong></p><p>现在的LLama系列、GPT4系列、DeepSeek系列、Qwen系列等都用的是这种方式。（代码实现可能有所变化）</p></li><li><p><strong>为什么要找到这样一个函数呢？</strong>（摘抄自参考博客）</p><p>因为我们希望 fq 和 fk 进行内积操作,受到他们相对位置的影响。（符合自然语言的习惯）</p><p>1.两个词相对位置近的时候(m-n小),内积可以大一点。</p><p>2.两个词相对位置远的时候(m-n大),内积可以小一点。（长度衰减）</p></li><li><p>直观理解</p><p><a href="https://www.bilibili.com/video/BV1CQoaY2EU2?spm_id_from=333.788.player.switch&amp;vd_source=d0891b7353b29ec2c50b1ea1f7004bfa" target="_blank" rel="noreferrer">RoPE理解</a></p><p><a href="https://www.bilibili.com/video/BV1iuoYYNEcZ?spm_id_from=333.788.videopod.sections&amp;vd_source=d0891b7353b29ec2c50b1ea1f7004bfa" target="_blank" rel="noreferrer">RoPE的远程衰减性</a></p></li></ul><h3 id="_2-4-alibi" tabindex="-1">2.4 ALiBi <a class="header-anchor" href="#_2-4-alibi" aria-label="Permalink to &quot;2.4 ALiBi&quot;">​</a></h3><h2 id="_3-代码实现" tabindex="-1">3. 代码实现 <a class="header-anchor" href="#_3-代码实现" aria-label="Permalink to &quot;3. 代码实现&quot;">​</a></h2><h3 id="_3-1-三角位置编码" tabindex="-1">3.1 三角位置编码 <a class="header-anchor" href="#_3-1-三角位置编码" aria-label="Permalink to &quot;3.1 三角位置编码&quot;">​</a></h3><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">embedding_dim </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 1024</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">seq_len </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 32</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.randn(seq_len, embedding_dim)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">pe </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.ones_like(x)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> seq_idx </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> range</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(seq_len):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    pos </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> seq_idx</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> emb_idx </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> range</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(embedding_dim):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        tmp </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pos </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10000</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> **</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (emb_idx </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> embedding_dim))  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 这里跟论文的2i有点不一样</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> emb_idx </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&amp;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> ==</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            pe[seq_idx][emb_idx] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.sin(torch.tensor(tmp))</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        else</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            pe[seq_idx][emb_idx] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.cos(torch.tensor(tmp))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            </span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.imshow(pe.numpy().T, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">cmap</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;hot&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">aspect</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;auto&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.xlabel(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Position&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.ylabel(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Embedding Dim&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.colorbar()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.show()</span></span></code></pre></div><p><img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/Figure_1.png" alt="Figure_1"></p><h2 id="_4-长文本外推技术" tabindex="-1">4. 长文本外推技术 <a class="header-anchor" href="#_4-长文本外推技术" aria-label="Permalink to &quot;4. 长文本外推技术&quot;">​</a></h2><blockquote><p>随机位置法、线性插值法</p></blockquote><h3 id="_4-1-位置内插-pi" tabindex="-1">4.1 位置内插(PI) <a class="header-anchor" href="#_4-1-位置内插-pi" aria-label="Permalink to &quot;4.1 位置内插(PI)&quot;">​</a></h3><h3 id="_4-2-ntk-aware" tabindex="-1">4.2 NTK-Aware <a class="header-anchor" href="#_4-2-ntk-aware" aria-label="Permalink to &quot;4.2 NTK-Aware&quot;">​</a></h3><h2 id="_5-模型上下文能力" tabindex="-1">5. 模型上下文能力 <a class="header-anchor" href="#_5-模型上下文能力" aria-label="Permalink to &quot;5. 模型上下文能力&quot;">​</a></h2><p>通过位置编码，我们可以进一步优化如下效果</p><ul><li>位置信息</li><li>注意力稀释问题</li></ul><h2 id="参考资料" tabindex="-1">参考资料 <a class="header-anchor" href="#参考资料" aria-label="Permalink to &quot;参考资料&quot;">​</a></h2><ul><li><p><a href="https://mp.weixin.qq.com/s/LOQxDGH3zB5Tly7S0kL-Ag" target="_blank" rel="noreferrer">https://mp.weixin.qq.com/s/LOQxDGH3zB5Tly7S0kL-Ag</a></p></li><li><p>中英文对照论文：<a href="https://yiyibooks.cn/arxiv/1706.03762v7/index.html" target="_blank" rel="noreferrer">Attention Is All You Need</a></p></li><li><p><a href="https://cloud.tencent.com/developer/article/2336891" target="_blank" rel="noreferrer">https://cloud.tencent.com/developer/article/2336891</a></p></li><li><p><a href="https://blog.csdn.net/panwang666/article/details/62883261" target="_blank" rel="noreferrer">https://blog.csdn.net/panwang666/article/details/62883261</a></p></li><li><p><a href="https://blog.csdn.net/qq_35962520/article/details/131480903" target="_blank" rel="noreferrer">https://blog.csdn.net/qq_35962520/article/details/131480903</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/675243992" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/675243992</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/8306958113" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/8306958113</a></p></li><li><p><a href="https://zh.wikipedia.org/wiki/%E6%AC%A7%E6%8B%89%E5%85%AC%E5%BC%8F" target="_blank" rel="noreferrer">https://zh.wikipedia.org/wiki/欧拉公式</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/863378538" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/863378538</a></p></li></ul>`,39)]))}const k=a(i,[["render",p]]);export{d as __pageData,k as default};
