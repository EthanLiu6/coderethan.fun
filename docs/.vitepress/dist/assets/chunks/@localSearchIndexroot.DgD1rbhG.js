const e='{"documentCount":2158,"nextId":2158,"documentIds":{"0":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#探秘transformer系列之-12-多头自注意力","1":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#_0x00-概述","2":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#_0x01-研究背景","3":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#_1-1-问题","4":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#_1-2-根源","5":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#_1-3-解决方案","6":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#_0x02-原理","7":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#_2-1-架构图","8":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#偏置","9":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#权重矩阵","10":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#wowow-o矩阵","11":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#_2-2-设计思路","12":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#子空间-分治","13":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#ensemble-融合","14":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#缓解稀疏","15":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#_2-3-计算","16":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#计算流程","17":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#计算强度","18":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#_2-4-效果","19":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#_2-5-融合方式","20":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#_2-6-分析","21":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#_2-7-优点","22":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#_0x03-实现","23":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#_3-1-定义","24":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#_3-2-运算逻辑","25":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#输入","26":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#投影","27":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#切分数据","28":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#逻辑角度","29":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#物理角度","30":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#小结","31":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#调整维度","32":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#为每个头计算注意力","33":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#单独分组","34":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#并行","35":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#融合每个头的z","36":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#forward-函数","37":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#_3-3-调用","38":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#编码器","39":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#解码器","40":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#_0x04-改进","41":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#_4-1-mohsa","42":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#_4-2-moh","43":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#_4-3-dcmha","44":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#研究背景","45":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#动机","46":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#思路","47":"/AI/Transformer/探秘Transformer系列之（12）--- 多头自注意力.html#_0xff-参考","48":"/AI/Transformer/#transformer系列","49":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#探秘transformer系列之-11-掩码","50":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#_0x00-概述","51":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#_0x01-需求","52":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#_1-1-避免偏差","53":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#实际情况","54":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#问题所在","55":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#解决方案","56":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#_1-2-防止偷看","57":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#实际情况-1","58":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#问题所在-1","59":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#解决方案-1","60":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#_0x02-padding-mask","61":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#_2-1-逻辑","62":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#掩码矩阵","63":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#计算注意力步骤","64":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#_2-2-实现","65":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#设置填充符号","66":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#建立mask","67":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#实施mask","68":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#_0x03-sequence-mask","69":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#_3-1-逻辑","70":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#掩码矩阵-1","71":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#掩码自注意力","72":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#交叉注意力","73":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#_3-2-实现","74":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#生成掩码","75":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#施加掩码","76":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#_3-3-transformer","77":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#_0x04-数据流","78":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#_4-1-如何应用于注意力","79":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#_4-2-变量说明","80":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#encoder数据流","81":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#_4-3-使用","82":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#训练","83":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#推理","84":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#_4-4-pytorch","85":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#_4-5-小结","86":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#_0x05-进阶","87":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#_5-1-sample-packing和mask","88":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#定义","89":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#attention-mask","90":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#策略","91":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#_5-2-功用","92":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#创新点","93":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#掩码注意力","94":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#带layernorm的掩码注意力","95":"/AI/Transformer/探秘Transformer系列之（11）--- 掩码.html#_0xff-参考","96":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#探秘transformer系列之-13-ffn","97":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#_0x00-概述","98":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#_0x01-网络结构","99":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#_1-1-数学表示","100":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#_1-2-中间层比率","101":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#_1-3-position-wise","102":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#_1-4-激活函数","103":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#常见函数","104":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#relu","105":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#glu","106":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#gelu","107":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#swiglu","108":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#swish函数","109":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#swiglu激活函数","110":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#实现","111":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#drelu","112":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#_0x02-实现","113":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#_2-1-哈佛代码","114":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#_2-2-llama3","115":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#_0x03-ffn的作用","116":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#_3-1-提取更多语义信息","117":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#_3-2-增加表达能力","118":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#_3-3-存储知识","119":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#_3-4-增加参数量","120":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#_3-5-小结","121":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#_0x04-知识利用","122":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#_4-1-提取步骤","123":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#_4-2-知识记忆","124":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#键值对形式","125":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#记忆网络","126":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#key-value","127":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#key模式","128":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#值向量表示的是分布","129":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#分布式存储和记忆聚合","130":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#知识回路","131":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#注意力模块","132":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#_4-3-知识的定位","133":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#事实的定位","134":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#知识归因-knowledge-attribution","135":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#精炼神经元-knowledge-neuron-refining","136":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#关系的定位","137":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#字典学习和稀疏自编码器","138":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#_4-3-修改知识","139":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#相关路线","140":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#功能","141":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#分类","142":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#内在知识编辑","143":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#ffn","144":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#注意力头","145":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#rome","146":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#_4-4-学习知识","147":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#前向传播","148":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#反向传播","149":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#_0x05-优化与演进","150":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#_5-1-moe","151":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#_5-2-memoryformer","152":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#动机与挑战","153":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#原理与创新","154":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#_5-3-memory-layers-at-scale","155":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#_5-4-kan","156":"/AI/Transformer/探秘Transformer系列之（13）--- FFN.html#_0xff-参考","157":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#探秘transformer系列之-10-自注意力","158":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#_0x00-概述","159":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#_0x01-原理","160":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#_1-1-设计思路","161":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#_1-2-输入","162":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#_1-3-qkv解析","163":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#心理学角度","164":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#数据库角度","165":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#seq2seq角度","166":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#重构词向量角度","167":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#相互操作","168":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#提取特征","169":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#加权求和","170":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#_1-4-小结","171":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#_0x02-实现","172":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#_2-1-权重矩阵","173":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#_2-2-计算过程","174":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#_2-3-点积注意力函数","175":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#方案选择","176":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#解读","177":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#_2-4-softmax","178":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#定义","179":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#算法","180":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#必要性","181":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#缺点","182":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#改进","183":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#log-softmax","184":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#hierarchical-softmax-h-softmax","185":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#adaptive-softmax","186":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#_2-5-缩放","187":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#结论","188":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#问题推导","189":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#方差变大","190":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#元素间差值变大","191":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#softmax退化","192":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#梯度消失","193":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#如何降低方差","194":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#熵的作用","195":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#_2-6-小结","196":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#_0x03-实现","197":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#_3-1-哈佛代码","198":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#输入-输出","199":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#图例-代码","200":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#再分析注意力","201":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#_3-2-llama3","202":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#_0x04-优化","203":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#_4-1-优化策略","204":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#从序列角度优化","205":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#从多头角度优化","206":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#从软硬件层面优化-mha","207":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#从其它角度优化","208":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#_4-2-案例","209":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#注意力权重细化","210":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#线性注意力","211":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#polaformer","212":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#研究背景","213":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#思路","214":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#minimax-01","215":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#模型架构","216":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#lightning-attention","217":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#hybrid-lightning","218":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#transformer2","219":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#研究背景-1","220":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#自适应性","221":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#svd","222":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#研究动机","223":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#思路-1","224":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#奇异值微调-svf","225":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#自适应性-1","226":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#titans","227":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#研究背景和动机","228":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#核心创新","229":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#titans架构","230":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#长期记忆","231":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#融合记忆","232":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#sana","233":"/AI/Transformer/探秘Transformer系列之（10）--- 自注意力.html#_0xff-参考","234":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#探秘transformer系列之-1-注意力机制","235":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#_0x00-概述","236":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#_0x01-背景知识","237":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#_1-1-seq2seq","238":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#_1-2-文本生成机制","239":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#_1-3-自回归模型","240":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#_1-4-隐变量自回归模型","241":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#_1-5-编码器-解码器模型","242":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#_1-6-如何压缩","243":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#_0x02-cnn和rnn方案","244":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#_2-1-技术挑战","245":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#对齐","246":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#长依赖","247":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#_2-2-cnn方案","248":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#_2-3-rnn方案","249":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#思路","250":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#优点","251":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#缺点","252":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#表达能力缺失","253":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#信息遗失","254":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#难以并行","255":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#难以训练","256":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#_2-4-当前问题","257":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#_0x03-注意力机制","258":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#_3-1-原理","259":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#上下文决定一切","260":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#资源分配","261":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#信息交换","262":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#_3-2-通用结构","263":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#任务模型","264":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#注意力模型","265":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#qkv","266":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#_3-3-计算流程","267":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#思路-1","268":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#注意力分数","269":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#注意力权重","270":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#加权求和","271":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#小结","272":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#_3-4-问题解决","273":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#增大信息含量","274":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#缩减单词间距","275":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#选择性处理信息","276":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#加权求和-1","277":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#对齐机制","278":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#同时前瞻和回顾","279":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#_3-5-优劣","280":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#_0x04-注意力发展历史","281":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#_4-1-rctm","282":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#_4-2-rnn-编码器-解码器","283":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#_4-3-sequence-to-sequence-learning-with-neural-networks","284":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#_4-4-bahdanau-attention","285":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#_4-5-luong-attention","286":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#_4-6-resnet","287":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#_4-7-self-attention","288":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#_4-8-qkv-attention","289":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#_4-9-multihead-self-attention","290":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#_4-10-multi-step-attention","291":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#_4-11-小结","292":"/AI/Transformer/探秘Transformer系列之（1）--- 注意力机制.html#_0xff-参考","293":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#探秘transformer系列之-14-残差网络和归一化","294":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#_0x00-概述","295":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#_0x01-残差连接","296":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#_1-1-问题","297":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#_1-2-相关知识","298":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#shortcut-connections","299":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#恒等映射","300":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#_1-3-网络结构","301":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#论文v1","302":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#论文v2","303":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#_1-4-功用","304":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#梯度消失","305":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#缓解退化","306":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#层间修正","307":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#掩码-vs-残差","308":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#_0x02-归一化","309":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#_2-1-问题","310":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#_2-2-定义","311":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#_2-3-类型","312":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#_0x03-batchnorm","313":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#_3-1-公式","314":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#_3-2-作用","315":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#_3-3-pytorch使用","316":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#_3-4-问题","317":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#_0x04-layernorm","318":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#_4-1-解决方案","319":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#_4-2-公式","320":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#_4-3-作用","321":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#_4-4-ln和bn的差异","322":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#作用对象","323":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#作用方向","324":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#业务选择","325":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#cv","326":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#nlp","327":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#具体实现","328":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#_4-5-post-norm-vs-pre-norm","329":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#概念","330":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#论文实现","331":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#post-norm","332":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#难以训练","333":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#需要热身","334":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#pre-norm","335":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#小结","336":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#_0x05-扩展比对","337":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#_5-1-instance-norm","338":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#_5-2-groupnorm","339":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#_5-3-比对","340":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#类比","341":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#细节","342":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#_0x06-实现","343":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#_6-1-layernorm","344":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#_6-2-残差","345":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#_0x07-优化与演进","346":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#_7-1-rmsnorm","347":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#_7-2-deep-norm","348":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#_7-3-prepbn","349":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#_7-4-realformer","350":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#_7-5-ngpt","351":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#研究背景-动机","352":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#核心贡献","353":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#对比","354":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#层和块-layers-and-blocks","355":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#自注意力块","356":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#mlp-块-mlp-block","357":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#切换","358":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#_7-6-dyt","359":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#动机","360":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#实现","361":"/AI/Transformer/探秘Transformer系列之（14）--- 残差网络和归一化.html#_0xff-参考","362":"/AI/Transformer/探秘Transformer系列之（3）---数据处理.html#探秘transformer系列之-3-数据处理","363":"/AI/Transformer/探秘Transformer系列之（3）---数据处理.html#_0x00-概要","364":"/AI/Transformer/探秘Transformer系列之（3）---数据处理.html#_0x01-总体流程","365":"/AI/Transformer/探秘Transformer系列之（3）---数据处理.html#_0x02-数据集","366":"/AI/Transformer/探秘Transformer系列之（3）---数据处理.html#_2-1-行业做法","367":"/AI/Transformer/探秘Transformer系列之（3）---数据处理.html#常见数据集","368":"/AI/Transformer/探秘Transformer系列之（3）---数据处理.html#数据源比率","369":"/AI/Transformer/探秘Transformer系列之（3）---数据处理.html#数据治理","370":"/AI/Transformer/探秘Transformer系列之（3）---数据处理.html#_2-2-哈佛数据集","371":"/AI/Transformer/探秘Transformer系列之（3）---数据处理.html#_0x03-加载功能模块","372":"/AI/Transformer/探秘Transformer系列之（3）---数据处理.html#_3-1-加载模型","373":"/AI/Transformer/探秘Transformer系列之（3）---数据处理.html#_3-2-加载分词器","374":"/AI/Transformer/探秘Transformer系列之（3）---数据处理.html#_3-3-加载词表","375":"/AI/Transformer/探秘Transformer系列之（3）---数据处理.html#_0x04-加载数据","376":"/AI/Transformer/探秘Transformer系列之（3）---数据处理.html#_4-1-填充-padding","377":"/AI/Transformer/探秘Transformer系列之（3）---数据处理.html#改进","378":"/AI/Transformer/探秘Transformer系列之（3）---数据处理.html#左填充","379":"/AI/Transformer/探秘Transformer系列之（3）---数据处理.html#_4-2-batch类","380":"/AI/Transformer/探秘Transformer系列之（3）---数据处理.html#成员变量","381":"/AI/Transformer/探秘Transformer系列之（3）---数据处理.html#目标语句","382":"/AI/Transformer/探秘Transformer系列之（3）---数据处理.html#生成掩码","383":"/AI/Transformer/探秘Transformer系列之（3）---数据处理.html#构建batch","384":"/AI/Transformer/探秘Transformer系列之（3）---数据处理.html#_4-3-加载batch","385":"/AI/Transformer/探秘Transformer系列之（3）---数据处理.html#_4-3-训练使用","386":"/AI/Transformer/探秘Transformer系列之（3）---数据处理.html#小结","387":"/AI/Transformer/探秘Transformer系列之（3）---数据处理.html#_0xff-参考","388":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#探秘transformer系列之-5-训练-推理","389":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#_0x00-概述","390":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#_0x01-训练","391":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#_1-1-输入","392":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#_1-2-dropout","393":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#原理","394":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#位置","395":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#源码","396":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#发展","397":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#_1-3-损失函数","398":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#交叉熵","399":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#label-smoothing","400":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#_1-4-学习率","401":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#warmup","402":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#noam","403":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#_1-5-初始化","404":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#_1-6-teacher-forcing","405":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#问题","406":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#概念","407":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#示例","408":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#原理-1","409":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#掩码","410":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#实现","411":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#优劣","412":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#小结","413":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#_1-7-并行","414":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#逻辑维度","415":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#编码器","416":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#解码器","417":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#模型维度","418":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#自注意力","419":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#ffn","420":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#张量维度","421":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#_1-7-代码","422":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#训练方式","423":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#单机训练代码","424":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#总体代码","425":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#_0x02-推理","426":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#_2-1-输入输出","427":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#_2-2-流程","428":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#_2-3-代码","429":"/AI/Transformer/探秘Transformer系列之（5）--- 训练&推理.html#_0xff-参考","430":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#_0x00-概述","431":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#_0-1-流程","432":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#_0-2-说明","433":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#_0x01-总体架构","434":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#_1-1-设计动机","435":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#_1-2-模型结构","436":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#主体模块","437":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#多层","438":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#_1-3-注意力模块","439":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#分类","440":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#位置","441":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#作用","442":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#全局自注意力层","443":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#掩码自注意力","444":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#交叉注意力层","445":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#_1-4-执行流程","446":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#_1-6-小结","447":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#_0x02-构建","448":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#_2-1-参数","449":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#_2-2-构建逻辑","450":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#_2-3-主体类","451":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#_2-4-如何调用","452":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#_0x03-输入","453":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#_3-1-输入分类","454":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#_3-2-输入模块","455":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#_3-3-文字转换","456":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#分词","457":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#embedding化","458":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#token-embedding","459":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#positional-encoding","460":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#word-embedding","461":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#_0x04-transformer-layer","462":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#_4-1-多头自注意力机制","463":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#第-1-步-根据原始嵌入计算查询、键和值矩阵","464":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#第-2-步-掩码自我注意力","465":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#第3步-拼接","466":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#_4-2-ffn层","467":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#_4-3-辅助架构","468":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#layernorm","469":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#dropout","470":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#残差连接","471":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#_0x05-概率输出-output-probabilities","472":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#_5-1-解码器结果","473":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#_5-2-转换","474":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#_0x06-解释","475":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#_6-1-机械可解释性","476":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#causal-tracing","477":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#稀疏探测","478":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#输入归因","479":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#模型组件归因","480":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#circuit-analysis","481":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#_6-2-机器学习角度","482":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#前向传播角度","483":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#反向传播角度","484":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#动机","485":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#方案","486":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#_6-3-生物学角度","487":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#星形胶质细胞","488":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#动机-1","489":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#方案-1","490":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#海马体","491":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#_6-4-数学角度","492":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#ode视角","493":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#神经常微分方程","494":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#推导","495":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#优势","496":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#示例","497":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#do-residual-neural-networks-discretize-neural-ordinary-differential-equations","498":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#多粒子动态系统视角","499":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#流映射视角","500":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#差分角度","501":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#总体架构","502":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#differential-attention-差分注意力","503":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#多头","504":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#图灵完备性质","505":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#范畴论","506":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#_6-4-物理学角度","507":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#基本动力学特性","508":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#物理自旋系统的结构","509":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#受力角度","510":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#_0x07-总结","511":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#_7-1-效果","512":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#_7-2-优劣","513":"/AI/Transformer/探秘Transformer系列之（2）---总体架构.html#_0xff-参考","514":"/AI/Transformer/探秘Transformer系列之（4）--- 编码器 & 解码器.html#探秘transformer系列之-4-编码器-解码器","515":"/AI/Transformer/探秘Transformer系列之（4）--- 编码器 & 解码器.html#_0x00-摘要","516":"/AI/Transformer/探秘Transformer系列之（4）--- 编码器 & 解码器.html#_0x01-编码器","517":"/AI/Transformer/探秘Transformer系列之（4）--- 编码器 & 解码器.html#_1-1-结构","518":"/AI/Transformer/探秘Transformer系列之（4）--- 编码器 & 解码器.html#_1-2-输入和输出","519":"/AI/Transformer/探秘Transformer系列之（4）--- 编码器 & 解码器.html#_1-3-流程","520":"/AI/Transformer/探秘Transformer系列之（4）--- 编码器 & 解码器.html#_1-4-张量形状变化","521":"/AI/Transformer/探秘Transformer系列之（4）--- 编码器 & 解码器.html#_1-6-实现","522":"/AI/Transformer/探秘Transformer系列之（4）--- 编码器 & 解码器.html#encoder","523":"/AI/Transformer/探秘Transformer系列之（4）--- 编码器 & 解码器.html#encoderlayer","524":"/AI/Transformer/探秘Transformer系列之（4）--- 编码器 & 解码器.html#_0x02-解码器","525":"/AI/Transformer/探秘Transformer系列之（4）--- 编码器 & 解码器.html#_2-1-结构","526":"/AI/Transformer/探秘Transformer系列之（4）--- 编码器 & 解码器.html#_2-2-输入和输出","527":"/AI/Transformer/探秘Transformer系列之（4）--- 编码器 & 解码器.html#_2-3-流程","528":"/AI/Transformer/探秘Transformer系列之（4）--- 编码器 & 解码器.html#训练","529":"/AI/Transformer/探秘Transformer系列之（4）--- 编码器 & 解码器.html#推理","530":"/AI/Transformer/探秘Transformer系列之（4）--- 编码器 & 解码器.html#_2-4-张量形状变化","531":"/AI/Transformer/探秘Transformer系列之（4）--- 编码器 & 解码器.html#_2-5-实现","532":"/AI/Transformer/探秘Transformer系列之（4）--- 编码器 & 解码器.html#decoder","533":"/AI/Transformer/探秘Transformer系列之（4）--- 编码器 & 解码器.html#decoderlayer","534":"/AI/Transformer/探秘Transformer系列之（4）--- 编码器 & 解码器.html#_0x03-交叉注意力深入","535":"/AI/Transformer/探秘Transformer系列之（4）--- 编码器 & 解码器.html#_3-1-分类","536":"/AI/Transformer/探秘Transformer系列之（4）--- 编码器 & 解码器.html#_3-2-业务逻辑","537":"/AI/Transformer/探秘Transformer系列之（4）--- 编码器 & 解码器.html#_3-3-业务流程","538":"/AI/Transformer/探秘Transformer系列之（4）--- 编码器 & 解码器.html#_3-4-代码逻辑","539":"/AI/Transformer/探秘Transformer系列之（4）--- 编码器 & 解码器.html#_0x04-decoder-only","540":"/AI/Transformer/探秘Transformer系列之（4）--- 编码器 & 解码器.html#_4-1-分类","541":"/AI/Transformer/探秘Transformer系列之（4）--- 编码器 & 解码器.html#_4-2-decoder-only","542":"/AI/Transformer/探秘Transformer系列之（4）--- 编码器 & 解码器.html#_4-3-架构选择","543":"/AI/Transformer/探秘Transformer系列之（4）--- 编码器 & 解码器.html#_0xff-参考","544":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#探秘transformer系列之-6-token","545":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#_0x00-概述","546":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#_0x01-基础概念","547":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#_1-1-分词","548":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#_1-2-token","549":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#_1-3-tokenizer","550":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#_1-4-词表","551":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#_1-6-分词流程","552":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#规范化","553":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#预分词","554":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#模型处理","555":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#后处理","556":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#_0x02-词表","557":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#_2-1-构建词表","558":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#_2-2-使用词表","559":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#_2-3-词表大小","560":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#任务相关","561":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#优势","562":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#劣势","563":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#_0x03-tokenizer","564":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#_3-1-分词粒度","565":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#按单词粒度","566":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#按字符粒度","567":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#按子词粒度","568":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#如何选择","569":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#_3-2-常见tokenizer","570":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#_3-3-llama3示例","571":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#定义","572":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#编码","573":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#解码","574":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#_0x04-bpe","575":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#_4-1-思路","576":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#_4-2-算法","577":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#_4-3-示例剖析","578":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#统计频率","579":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#初始分割","580":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#构建初始词表","581":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#循环迭代学习结合规则","582":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#第一次迭代","583":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#第二次迭代","584":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#后续迭代","585":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#小结","586":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#_4-4-使用","587":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#编码-1","588":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#解码-1","589":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#_4-5-minbpe","590":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#基础函数","591":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#tokenizer","592":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#bpe-tokenizer","593":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#_4-5-优劣","594":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#优点","595":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#劣势-1","596":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#_0x05-其它算法","597":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#_5-1-wordpiece","598":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#思想","599":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#算法","600":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#优势与劣势","601":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#_5-2-unilm","602":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#算法-1","603":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#优势与劣势-1","604":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#比对","605":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#_5-3-bbpe","606":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#动机","607":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#思想-1","608":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#优劣","609":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#_0x06-发展","610":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#_6-1-better-than-tokens","611":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#主要贡献","612":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#动机-1","613":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#patch化","614":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#blt架构","615":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#交互","616":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#_6-2-tokenformer","617":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#主要贡献-1","618":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#动机-2","619":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#架构","620":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#对比","621":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#pattention机制","622":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#ffn的革新","623":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#复用","624":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#总结","625":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#_6-3-lcm","626":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#问题","627":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#动机-3","628":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#思路","629":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#总体架构","630":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#细节","631":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#sonar嵌入空间","632":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#base-lcm","633":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#基于扩散的lcm-diffusion-based-lcm","634":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#单塔扩散lcm-one-tower-diffusion-lcm","635":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#双塔扩散lcm-two-tower-diffusion-lcm","636":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#quant-lcm","637":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#_6-4-动作tokenizer","638":"/AI/Transformer/探秘Transformer系列之（6）--- token.html#_0xff-参考","639":"/AI/deep_learning_theory/00-DL_Base_Notes.html#一-dl-base-notes","640":"/AI/deep_learning_theory/00-DL_Base_Notes.html#_1-🌟🌟🌟🌟🌟normalization","641":"/AI/deep_learning_theory/00-DL_Base_Notes.html#_2-🌟🌟🌟activation","642":"/AI/deep_learning_theory/00-DL_Base_Notes.html#_2-1-non-linear-activations-的两种类型","643":"/AI/deep_learning_theory/00-DL_Base_Notes.html#_2-2","644":"/AI/deep_learning_theory/00-DL_Base_Notes.html#_3-🌟loss-function","645":"/AI/deep_learning_theory/00-DL_Base_Notes.html#_4-🌟🌟🌟🌟optimizer","646":"/AI/deep_learning_theory/00-DL_Base_Notes.html#_5-🌟🌟🌟🌟🌟回顾attention","647":"/AI/deep_learning_theory/00-DL_Base_Notes.html#_5-1-为啥attention的时候要除以","648":"/AI/deep_learning_theory/00-DL_Base_Notes.html#_5-2-为啥拆多头-为啥效果好了","649":"/AI/deep_learning_theory/00-DL_Base_Notes.html#_5-3-cross-multi-head-attention","650":"/AI/deep_learning_theory/00-DL_Base_Notes.html#_5-4-mask-multi-head-attention","651":"/AI/deep_learning_theory/00-DL_Base_Notes.html#_5-5-masking实现机理","652":"/AI/deep_learning_theory/00-DL_Base_Notes.html#_5-6-mqa和gqa","653":"/AI/deep_learning_theory/00-DL_Base_Notes.html#_6-🌟🌟🌟k-v-cache","654":"/AI/deep_learning_theory/00-DL_Base_Notes.html#_7-🌟🌟🌟常见的正则化方法","655":"/AI/deep_learning_theory/00-DL_Base_Notes.html#二-课堂记录","656":"/AI/deep_learning_theory/00-DL_Base_Notes.html#🌟0301-0302","657":"/AI/deep_learning_theory/00-DL_Base_Notes.html#_0308-0309——pytorch","658":"/AI/deep_learning_theory/00-DL_Base_Notes.html#_1-1-tensor-中数据的连续性","659":"/AI/deep_learning_theory/00-DL_Base_Notes.html#_1-2-pytorch-autograd","660":"/AI/deep_learning_theory/00-DL_Base_Notes.html#_1-3-inplace-op","661":"/AI/deep_learning_theory/00-DL_Base_Notes.html#_1-4-自动微分机制-auto-grad-重点","662":"/AI/deep_learning_theory/00-DL_Base_Notes.html#_2-1-torch-nn-module","663":"/AI/deep_learning_theory/00-DL_Base_Notes.html#_0315-0316-续pytorch","664":"/AI/deep_learning_theory/00-DL_Base_Notes.html#_1-1-回顾","665":"/AI/deep_learning_theory/00-DL_Base_Notes.html#_1-2-问题合集","666":"/AI/deep_learning_theory/00-DL_Base_Notes.html#_2-1-torch-optim","667":"/AI/deep_learning_theory/00-DL_Base_Notes.html#_2-2-learning-rate-调整方案","668":"/AI/deep_learning_theory/00-DL_Base_Notes.html#_2-3-模型保存和加载","669":"/AI/deep_learning_theory/00-DL_Base_Notes.html#_3-1-dataset-and-dataloader","670":"/AI/deep_learning_theory/00-DL_Base_Notes.html#_4-1-nlp","671":"/AI/deep_learning_theory/00-DL_Base_Notes.html#_4-2-bert","672":"/AI/Transformer/探秘Transformer系列之（8）--- 位置编码.html#探秘transformer之-8-位置编码","673":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#探秘transformer系列之-7-embedding","674":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#_0x00-概要","675":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#_0x01-演进思路","676":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#_1-1-概念","677":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#_1-2-需求","678":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#_1-3-文本","679":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#_1-4-数字","680":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#_1-5-向量","681":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#独热编码","682":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#改进诉求","683":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#增维","684":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#降维","685":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#语义相似性","686":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#可训练","687":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#小结","688":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#_1-5-embedding","689":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#表征概念","690":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#样例","691":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#相似性","692":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#相似度计算","693":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#_1-6-小结","694":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#对比","695":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#流转过程","696":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#优势","697":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#_0x02-transformer嵌入层实现","698":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#_2-1-流程-架构","699":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#_2-2-实现","700":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#嵌入矩阵","701":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#哈佛代码","702":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#pytorch-embedding","703":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#调用","704":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#输入输出","705":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#_2-3-训练","706":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#embedding的来源","707":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#为何要再训练","708":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#让模型自己设计","709":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#训练流程","710":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#_0x03-文本嵌入","711":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#_3-1-历史","712":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#_3-2-word2vec","713":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#思路","714":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#架构","715":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#问题","716":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#_3-3-elmo","717":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#思路-1","718":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#训练","719":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#_3-4-bert","720":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#动机","721":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#思路-2","722":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#embedding","723":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#代码","724":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#_3-5-bge","725":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#数据集","726":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#训练-1","727":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#retromae","728":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#_3-6-llm-as-embedding","729":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#backbone选择","730":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#架构改进","731":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#pooling策略","732":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#注意力架构","733":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#额外投影层-additional-projector","734":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#_3-7-llm2vec","735":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#_3-8-nv-embedding","736":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#_3-9-通过提示工程的方法","737":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#_3-10-使用moe进行embedding","738":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#研究背景","739":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#动机-1","740":"/AI/Transformer/探秘Transformer系列之（7）--- embedding.html#_0xff-参考","741":"/AI/Transformer/探秘Transformer系列之（9）--- 位置编码分类.html#探秘transformer系列之-9-位置编码分类","742":"/AI/Transformer/探秘Transformer系列之（9）--- 位置编码分类.html#_0x00-概述","743":"/AI/Transformer/探秘Transformer系列之（9）--- 位置编码分类.html#_0x01-区别","744":"/AI/Transformer/探秘Transformer系列之（9）--- 位置编码分类.html#_1-1-从直观角度来看","745":"/AI/Transformer/探秘Transformer系列之（9）--- 位置编码分类.html#_1-2-从模型处理角度来看","746":"/AI/Transformer/探秘Transformer系列之（9）--- 位置编码分类.html#_1-3-优劣","747":"/AI/Transformer/探秘Transformer系列之（9）--- 位置编码分类.html#_0x02-绝对位置编码","748":"/AI/Transformer/探秘Transformer系列之（9）--- 位置编码分类.html#_2-1-基础方案","749":"/AI/Transformer/探秘Transformer系列之（9）--- 位置编码分类.html#_2-2-训练式","750":"/AI/Transformer/探秘Transformer系列之（9）--- 位置编码分类.html#_2-3-三角函数式","751":"/AI/Transformer/探秘Transformer系列之（9）--- 位置编码分类.html#_2-4-其它","752":"/AI/Transformer/探秘Transformer系列之（9）--- 位置编码分类.html#_0x03-相对位置编码","753":"/AI/Transformer/探秘Transformer系列之（9）--- 位置编码分类.html#_3-1-意义","754":"/AI/Transformer/探秘Transformer系列之（9）--- 位置编码分类.html#大脑中的参考系","755":"/AI/Transformer/探秘Transformer系列之（9）--- 位置编码分类.html#语义影响","756":"/AI/Transformer/探秘Transformer系列之（9）--- 位置编码分类.html#长度外推","757":"/AI/Transformer/探秘Transformer系列之（9）--- 位置编码分类.html#_3-2-绝对位置编码的位置","758":"/AI/Transformer/探秘Transformer系列之（9）--- 位置编码分类.html#_3-3-绝对位置编码的公式","759":"/AI/Transformer/探秘Transformer系列之（9）--- 位置编码分类.html#_3-4-经典式","760":"/AI/Transformer/探秘Transformer系列之（9）--- 位置编码分类.html#_3-5-xlnet","761":"/AI/Transformer/探秘Transformer系列之（9）--- 位置编码分类.html#_3-6-tener","762":"/AI/Transformer/探秘Transformer系列之（9）--- 位置编码分类.html#_3-7-t5","763":"/AI/Transformer/探秘Transformer系列之（9）--- 位置编码分类.html#_3-8-deberta式","764":"/AI/Transformer/探秘Transformer系列之（9）--- 位置编码分类.html#_3-9-tupe","765":"/AI/Transformer/探秘Transformer系列之（9）--- 位置编码分类.html#_3-10-alibi","766":"/AI/Transformer/探秘Transformer系列之（9）--- 位置编码分类.html#_3-11-偏置编码-上下文模式","767":"/AI/Transformer/探秘Transformer系列之（9）--- 位置编码分类.html#_3-12-小结","768":"/AI/Transformer/探秘Transformer系列之（9）--- 位置编码分类.html#_0xff-参考","769":"/AI/deep_learning_theory/04-convolution_neural_network.html#_1-概念","770":"/AI/deep_learning_theory/04-convolution_neural_network.html#_2-卷积运算","771":"/AI/deep_learning_theory/04-convolution_neural_network.html#_3-体会卷积的作用","772":"/AI/deep_learning_theory/04-convolution_neural_network.html#_4-卷积-和-前馈神经网络的关系","773":"/AI/deep_learning_theory/04-convolution_neural_network.html#_5-工程上标准的卷积","774":"/AI/deep_learning_theory/04-convolution_neural_network.html#_6-1x1-卷积","775":"/AI/deep_learning_theory/04-convolution_neural_network.html#_7-分组卷积-group-convolution","776":"/AI/deep_learning_theory/04-convolution_neural_network.html#_8-深度可分离卷积-deepwise-convolution","777":"/AI/deep_learning_theory/04-convolution_neural_network.html#_9-空间可分离卷积-spatially-separable-convolutions","778":"/AI/deep_learning_theory/04-convolution_neural_network.html#_10-空洞卷积-膨胀卷积-dilated-convolution-atrous-convolution","779":"/AI/deep_learning_theory/04-convolution_neural_network.html#_11-反卷积-转置卷积-deconvolution-transposed-convolution","780":"/AI/deep_learning_theory/04-convolution_neural_network.html#_12-可变形卷积-deformable-convolution","781":"/AI/deep_learning_theory/04-convolution_neural_network.html#_12-1-原理","782":"/AI/deep_learning_theory/04-convolution_neural_network.html#_12-2-过程","783":"/AI/deep_learning_theory/04-convolution_neural_network.html#_13-3d-卷积","784":"/AI/deep_learning_theory/04-convolution_neural_network.html#_14-参考链接","785":"/AI/deep_learning_theory/05-deep_learning_model.html#_1-什么是深度学习模型","786":"/AI/deep_learning_theory/05-deep_learning_model.html#_2-下载一个预训练好的深度学习模型","787":"/AI/deep_learning_theory/05-deep_learning_model.html#_3-可视化这个深度学习模型","788":"/AI/deep_learning_theory/06-pytorch_install.html#_1-pytorch-官网","789":"/AI/deep_learning_theory/06-pytorch_install.html#_2-pytorch-简介","790":"/AI/deep_learning_theory/06-pytorch_install.html#_2-1-认识pytorch","791":"/AI/deep_learning_theory/06-pytorch_install.html#_2-2-pytorch-软件栈","792":"/AI/deep_learning_theory/06-pytorch_install.html#_3-pytorch-install","793":"/AI/deep_learning_theory/06-pytorch_install.html#_4-nvidia-相关软件库","794":"/AI/deep_learning_theory/06-pytorch_install.html#_4-1-显卡驱动","795":"/AI/deep_learning_theory/06-pytorch_install.html#_4-2-cuda","796":"/AI/deep_learning_theory/06-pytorch_install.html#_4-3-cudnn","797":"/AI/deep_learning_theory/06-pytorch_install.html#_5-gpu","798":"/AI/deep_learning_theory/06-pytorch_install.html#_5-1-gpu-加速原理","799":"/AI/deep_learning_theory/06-pytorch_install.html#_5-2-最先进的gpu","800":"/AI/deep_learning_theory/07-operators.html#_1-convolution","801":"/AI/deep_learning_theory/07-operators.html#_1-1-conv2d","802":"/AI/deep_learning_theory/07-operators.html#_1-2-convtranspose2d","803":"/AI/deep_learning_theory/07-operators.html#_2-线性变换层","804":"/AI/deep_learning_theory/07-operators.html#_2-1-linear-gemm","805":"/AI/deep_learning_theory/07-operators.html#_2-2-matmul-相关","806":"/AI/deep_learning_theory/07-operators.html#_3-normalization","807":"/AI/deep_learning_theory/07-operators.html#_3-1-batchnorm2d","808":"/AI/deep_learning_theory/07-operators.html#_3-2-layernorm","809":"/AI/deep_learning_theory/07-operators.html#_3-3-instance-normalization","810":"/AI/deep_learning_theory/07-operators.html#_3-4-group-normalization","811":"/AI/deep_learning_theory/07-operators.html#_3-5-switch-norm","812":"/AI/deep_learning_theory/07-operators.html#_3-6-rms-norm","813":"/AI/deep_learning_theory/07-operators.html#_4-pooling","814":"/AI/deep_learning_theory/07-operators.html#_4-1-max-pooling","815":"/AI/deep_learning_theory/07-operators.html#_4-2-averagepooling","816":"/AI/deep_learning_theory/07-operators.html#_4-3-global-average-pooling","817":"/AI/deep_learning_theory/07-operators.html#_5-activation-functions","818":"/AI/deep_learning_theory/07-operators.html#_6-reshape、-view、-permute、transpose","819":"/AI/deep_learning_theory/07-operators.html#_6-1-reshape","820":"/AI/deep_learning_theory/07-operators.html#_6-2-view","821":"/AI/deep_learning_theory/07-operators.html#_6-3-transpose","822":"/AI/deep_learning_theory/07-operators.html#_6-4-permute","823":"/AI/deep_learning_theory/07-operators.html#_7-sequenze-和-unequenze","824":"/AI/deep_learning_theory/07-operators.html#_8-concat、stack、expand-和-flatten","825":"/AI/deep_learning_theory/07-operators.html#_8-1-concat","826":"/AI/deep_learning_theory/07-operators.html#_8-2-stack","827":"/AI/deep_learning_theory/07-operators.html#_8-3-expand","828":"/AI/deep_learning_theory/07-operators.html#_8-4-flatten","829":"/AI/deep_learning_theory/07-operators.html#_9-pointwise","830":"/AI/deep_learning_theory/07-operators.html#_10-split-和-slice","831":"/AI/deep_learning_theory/07-operators.html#_10-1-split","832":"/AI/deep_learning_theory/07-operators.html#_10-2-slice","833":"/AI/deep_learning_theory/07-operators.html#_11-reduce-规约类算子","834":"/AI/deep_learning_theory/07-operators.html#_12-embedding","835":"/AI/deep_learning_theory/07-operators.html#_13-dropout","836":"/AI/deep_learning_theory/07-operators.html#_14-附录","837":"/AI/deep_learning_theory/07-operators.html#_15-参考链接","838":"/AI/deep_learning_theory/08-activation_functions.html#_0-activation-整体介绍","839":"/AI/deep_learning_theory/08-activation_functions.html#_1-s-型激活函数","840":"/AI/deep_learning_theory/08-activation_functions.html#_2-relu-激活函数","841":"/AI/deep_learning_theory/08-activation_functions.html#_3-relu6","842":"/AI/deep_learning_theory/08-activation_functions.html#_4-其它relu-相关-激活函数","843":"/AI/deep_learning_theory/08-activation_functions.html#_5-elu-exponential-linear-units-和-selu-scaled-elu","844":"/AI/deep_learning_theory/08-activation_functions.html#_6-gelu-gaussian-error-linear-unit","845":"/AI/deep_learning_theory/08-activation_functions.html#_7-swish-与-hardswish","846":"/AI/deep_learning_theory/08-activation_functions.html#_8-mish","847":"/AI/deep_learning_theory/08-activation_functions.html#_9-softmax","848":"/AI/deep_learning_theory/08-activation_functions.html#_10-总结-好的激活函数应有的性质","849":"/AI/deep_learning_theory/08-activation_functions.html#_11-参考链接","850":"/AI/deep_learning_theory/09-recurrent_neural_network.html#_0-循环神经网络-recurrent-neural-network-rnn","851":"/AI/deep_learning_theory/09-recurrent_neural_network.html#_2-典型的rnn网络","852":"/AI/deep_learning_theory/09-recurrent_neural_network.html#_3-rnn-结构详解","853":"/AI/deep_learning_theory/09-recurrent_neural_network.html#_3-1-rnn-循环过程如下图所示","854":"/AI/deep_learning_theory/09-recurrent_neural_network.html#_3-2-按时间步展开如下","855":"/AI/deep_learning_theory/09-recurrent_neural_network.html#_3-3-经典rnn的计算图如下","856":"/AI/deep_learning_theory/09-recurrent_neural_network.html#_3-4-rnn具体计算公式为","857":"/AI/deep_learning_theory/09-recurrent_neural_network.html#_3-5-rnn-工程图展示","858":"/AI/deep_learning_theory/09-recurrent_neural_network.html#_3-6-rnn可扩展到双向的情况-其结构如下","859":"/AI/deep_learning_theory/09-recurrent_neural_network.html#_3-7-rnn扩展到多层构成循环神经网络-结构如下","860":"/AI/deep_learning_theory/09-recurrent_neural_network.html#_4-rnn-应用案例-意图识别","861":"/AI/deep_learning_theory/09-recurrent_neural_network.html#_5-经典rnn-存在的问题","862":"/AI/deep_learning_theory/09-recurrent_neural_network.html#_6-lstm-long-short-term-memory-长短期记忆网络","863":"/AI/deep_learning_theory/09-recurrent_neural_network.html#_6-1-lstm-整体结构","864":"/AI/deep_learning_theory/09-recurrent_neural_network.html#_6-2-lstm-cell-详解","865":"/AI/deep_learning_theory/09-recurrent_neural_network.html#_6-2-1-遗忘门","866":"/AI/deep_learning_theory/09-recurrent_neural_network.html#_6-2-2-输入门","867":"/AI/deep_learning_theory/09-recurrent_neural_network.html#_6-2-3-cell-state","868":"/AI/deep_learning_theory/09-recurrent_neural_network.html#_6-2-4-输出门","869":"/AI/deep_learning_theory/09-recurrent_neural_network.html#_6-2-5-总结","870":"/AI/deep_learning_theory/09-recurrent_neural_network.html#_6-3-lstm-cell-具体计算","871":"/AI/deep_learning_theory/09-recurrent_neural_network.html#_6-4-与rnn-类似-lstm-也有双向的","872":"/AI/deep_learning_theory/09-recurrent_neural_network.html#_7-gru-门控循环单元-gated-recurrent-unit","873":"/AI/deep_learning_theory/09-recurrent_neural_network.html#_7-1-lstm-和-gru-对比","874":"/AI/deep_learning_theory/09-recurrent_neural_network.html#_7-2-原理","875":"/AI/deep_learning_theory/09-recurrent_neural_network.html#_7-3-计算过程","876":"/AI/deep_learning_theory/09-recurrent_neural_network.html#_8-参考链接","877":"/AI/deep_learning_theory/10-seq2seq.html#_1-从rnn-到-seq2seq","878":"/AI/deep_learning_theory/10-seq2seq.html#_1-1-rnn-简述","879":"/AI/deep_learning_theory/10-seq2seq.html#_1-2-rnn-应用场景","880":"/AI/deep_learning_theory/10-seq2seq.html#_1-2-1-rnn-解决-n-vs-n-问题","881":"/AI/deep_learning_theory/10-seq2seq.html#_1-2-2-rnn-解决-n-versus-1-问题","882":"/AI/deep_learning_theory/10-seq2seq.html#_1-2-3-rnn-解决-1-vs-n-问题","883":"/AI/deep_learning_theory/10-seq2seq.html#_1-3-n-vs-m-型任务","884":"/AI/deep_learning_theory/10-seq2seq.html#_2-seq2seq-模型","885":"/AI/deep_learning_theory/10-seq2seq.html#_2-1-seq2seq-定义","886":"/AI/deep_learning_theory/10-seq2seq.html#_2-2-seq2seq-模型结构","887":"/AI/deep_learning_theory/10-seq2seq.html#_2-2-1-encoder-decoder-架构","888":"/AI/deep_learning_theory/10-seq2seq.html#_2-2-2-encoder-部分","889":"/AI/deep_learning_theory/10-seq2seq.html#_2-2-3-decoder-部分","890":"/AI/deep_learning_theory/10-seq2seq.html#_2-3-seq2seq-实现举例","891":"/AI/deep_learning_theory/10-seq2seq.html#_3-seq2seq-中的-attention-机制","892":"/AI/deep_learning_theory/10-seq2seq.html#_3-1-seq2seq-中的-attention-机制","893":"/AI/deep_learning_theory/10-seq2seq.html#_4-seq2seq-的工作流程","894":"/AI/deep_learning_theory/10-seq2seq.html#_4-1-预测时流程","895":"/AI/deep_learning_theory/10-seq2seq.html#_4-2-训练时流程","896":"/AI/deep_learning_theory/10-seq2seq.html#_4-2-1-teacher-forcing","897":"/AI/deep_learning_theory/10-seq2seq.html#_4-2-2-scheduled-sampling","898":"/AI/deep_learning_theory/10-seq2seq.html#_4-3-decoder的预训练","899":"/AI/deep_learning_theory/10-seq2seq.html#_5-seq2seq-的损失函数","900":"/AI/deep_learning_theory/10-seq2seq.html#_6-decoding-中的-beam-search","901":"/AI/deep_learning_theory/10-seq2seq.html#_6-1-贪心decoding","902":"/AI/deep_learning_theory/10-seq2seq.html#_6-2-beam-search-原理","903":"/AI/deep_learning_theory/10-seq2seq.html#_6-3-公式分析","904":"/AI/deep_learning_theory/10-seq2seq.html#_6-4-beam-search-分析","905":"/AI/deep_learning_theory/10-seq2seq.html#_7-nlp-从机器学习到深度学习","906":"/AI/deep_learning_theory/10-seq2seq.html#_7-1-nlp-中常见任务","907":"/AI/deep_learning_theory/10-seq2seq.html#_7-2-机器翻译的发展历程","908":"/AI/deep_learning_theory/10-seq2seq.html#_7-3-smt-方法简介","909":"/AI/deep_learning_theory/10-seq2seq.html#_7-4-nmt","910":"/AI/deep_learning_theory/10-seq2seq.html#_8-参考文献","911":"/AI/deep_learning_theory/11-1attentions.html#_1-attention-is-all-you-need","912":"/AI/deep_learning_theory/11-1attentions.html#_2-transformer-model-architecture","913":"/AI/deep_learning_theory/11-1attentions.html#_3-编码器和解码器堆栈","914":"/AI/deep_learning_theory/11-1attentions.html#_3-1-编码器","915":"/AI/deep_learning_theory/11-1attentions.html#_3-2-解码器","916":"/AI/deep_learning_theory/11-1attentions.html#_4-scaled-dot-product-attention-缩放版本的点积注意力","917":"/AI/deep_learning_theory/11-1attentions.html#_4-1-模型结构图","918":"/AI/deep_learning_theory/11-1attentions.html#_4-2-数学公式为","919":"/AI/deep_learning_theory/11-1attentions.html#_4-3-推导过程详解","920":"/AI/deep_learning_theory/11-1attentions.html#_4-2-1-self-attention-的思想","921":"/AI/deep_learning_theory/11-1attentions.html#_4-2-2-自注意的思想","922":"/AI/deep_learning_theory/11-1attentions.html#_4-2-3-自注意机制运算过程","923":"/AI/deep_learning_theory/11-1attentions.html#_4-2-4-写成矩阵的形式","924":"/AI/deep_learning_theory/11-1attentions.html#_4-4-为什么要进行缩放","925":"/AI/deep_learning_theory/11-1attentions.html#_5-multi-head-self-attention","926":"/AI/deep_learning_theory/11-1attentions.html#_5-1-原理简介","927":"/AI/deep_learning_theory/11-1attentions.html#_5-2-公式表达","928":"/AI/deep_learning_theory/11-1attentions.html#_5-3-底层原理","929":"/AI/deep_learning_theory/11-1attentions.html#_5-4-多头的实现细节展示","930":"/AI/deep_learning_theory/11-1attentions.html#_6-实际工程上的-multi-head-attention-详解","931":"/AI/deep_learning_theory/11-1attentions.html#_7-cross-multi-head-attention","932":"/AI/deep_learning_theory/11-1attentions.html#_8-mask-multi-head-attention","933":"/AI/deep_learning_theory/11-1attentions.html#_8-1-padding-mask","934":"/AI/deep_learning_theory/11-1attentions.html#_8-2-sequence-mask","935":"/AI/deep_learning_theory/11-1attentions.html#_9-mqa-multi-query-attention","936":"/AI/deep_learning_theory/11-1attentions.html#_10-大模型神器-gqa-grouped-query-attention","937":"/AI/deep_learning_theory/11-1attentions.html#_10-1-gqa-structure","938":"/AI/deep_learning_theory/11-1attentions.html#_10-2-精度改进-converting-the-checkpoint-and-uptraining","939":"/AI/deep_learning_theory/11-1attentions.html#_11-大模型加速利器-flashattention","940":"/AI/deep_learning_theory/11-1attentions.html#_11-1-原理介绍","941":"/AI/deep_learning_theory/11-1attentions.html#_11-2-标准attention机制的算法实现","942":"/AI/deep_learning_theory/11-1attentions.html#_11-3-flash-attention-算法思想","943":"/AI/deep_learning_theory/11-1attentions.html#_11-4-准备-切片的方式计算softmax","944":"/AI/deep_learning_theory/11-1attentions.html#_11-5-具体flashattention的算法","945":"/AI/deep_learning_theory/11-1attentions.html#flash-attention-效果","946":"/AI/deep_learning_theory/11-1attentions.html#_11-6-重计算-recompute","947":"/AI/deep_learning_theory/11-1attentions.html#_12-flash-attention-2","948":"/AI/deep_learning_theory/11-1attentions.html#_13-大模型推理加速利器-kv-cache","949":"/AI/deep_learning_theory/11-1attentions.html#_14-大模型推理加速利器-page-attention","950":"/AI/deep_learning_theory/11-1attentions.html#_15-参考链接","951":"/AI/deep_learning_theory/11-2attention-extension.html#_1-mqa-multi-query-attention","952":"/AI/deep_learning_theory/11-2attention-extension.html#_2-大模型神器-gqa-grouped-query-attention","953":"/AI/deep_learning_theory/11-2attention-extension.html#_2-1-gqa-structure","954":"/AI/deep_learning_theory/11-2attention-extension.html#_2-2-精度改进-converting-the-checkpoint-and-uptraining","955":"/AI/deep_learning_theory/11-2attention-extension.html#_3-mla-multi-head-latent-attention-boosting-inference-efficiency","956":"/AI/deep_learning_theory/11-2attention-extension.html#_3-1-mla-原理","957":"/AI/deep_learning_theory/11-2attention-extension.html#_3-2-mla-实现逻辑","958":"/AI/deep_learning_theory/11-2attention-extension.html#_4-大模型加速利器-flashattention","959":"/AI/deep_learning_theory/11-2attention-extension.html#_4-1-原理及思想介绍","960":"/AI/deep_learning_theory/11-2attention-extension.html#_4-2-标准attention机制的算法实现","961":"/AI/deep_learning_theory/11-2attention-extension.html#_4-3-准备-切片的方式计算softmax","962":"/AI/deep_learning_theory/11-2attention-extension.html#_4-4-flash-attention-1-算法图解","963":"/AI/deep_learning_theory/11-2attention-extension.html#_4-5-flashattention1-forward-伪代码","964":"/AI/deep_learning_theory/11-2attention-extension.html#_4-6-flashattention1-backward-伪代码","965":"/AI/deep_learning_theory/11-2attention-extension.html#_4-7-flash-attention-效果","966":"/AI/deep_learning_theory/11-2attention-extension.html#_4-8-重计算-recompute","967":"/AI/deep_learning_theory/11-2attention-extension.html#_4-9-flashattention1-的不足之处","968":"/AI/deep_learning_theory/11-2attention-extension.html#_5-flashattention-2-faster-attention-with-better-parallelism-and-work-partitioning","969":"/AI/deep_learning_theory/11-2attention-extension.html#_5-1-softmax-trick-v1-vs-v2","970":"/AI/deep_learning_theory/11-2attention-extension.html#_5-2-forward-pass","971":"/AI/deep_learning_theory/11-2attention-extension.html#_5-3-backward-pass","972":"/AI/deep_learning_theory/11-2attention-extension.html#_5-4-v2-相对于-v1-的改进","973":"/AI/deep_learning_theory/11-2attention-extension.html#_6-flashattention3-fast-and-accurate-attention-with-asynchrony-and-low-precision","974":"/AI/deep_learning_theory/11-2attention-extension.html#_7-ringattention","975":"/AI/deep_learning_theory/11-2attention-extension.html#_7-1-具体实现原理","976":"/AI/deep_learning_theory/11-2attention-extension.html#_8-从ring-attention-到-context-parallel","977":"/AI/deep_learning_theory/11-2attention-extension.html#_9-从context-parallel-到-chunked-pipeline-parallelism","978":"/AI/deep_learning_theory/11-2attention-extension.html#_10-大模型推理加速利器-kv-cache","979":"/AI/deep_learning_theory/11-2attention-extension.html#_11-大模型推理加速利器-page-attention-and-vllm","980":"/AI/deep_learning_theory/11-2attention-extension.html#_11-1-vllm-简介","981":"/AI/deep_learning_theory/11-2attention-extension.html#_11-2-秘密武器-pagedattention","982":"/AI/deep_learning_theory/11-2attention-extension.html#_11-3-优势1-block-无需连续","983":"/AI/deep_learning_theory/11-2attention-extension.html#_11-4-优势2-内存共享","984":"/AI/deep_learning_theory/11-2attention-extension.html#_12-radixattention","985":"/AI/deep_learning_theory/11-2attention-extension.html#_12-1-当前kv-cache","986":"/AI/deep_learning_theory/11-2attention-extension.html#_12-2-redisattention-strategy","987":"/AI/deep_learning_theory/11-2attention-extension.html#_13-参考链接","988":"/AI/deep_learning_theory/12-weight-initialization.html#_1-参数初始化概念-parameters-initialization","989":"/AI/deep_learning_theory/12-weight-initialization.html#_2-参数初始化的重要性","990":"/AI/deep_learning_theory/12-weight-initialization.html#_2-1-为什么参数初始化很重要","991":"/AI/deep_learning_theory/12-weight-initialization.html#_2-1-不合理初始化的问题","992":"/AI/deep_learning_theory/12-weight-initialization.html#_3-全0或常量初始化","993":"/AI/deep_learning_theory/12-weight-initialization.html#_4-随机初始化","994":"/AI/deep_learning_theory/12-weight-initialization.html#_4-1-较小随机值时","995":"/AI/deep_learning_theory/12-weight-initialization.html#_4-2-较大随机初始值时","996":"/AI/deep_learning_theory/12-weight-initialization.html#_4-3-结论","997":"/AI/deep_learning_theory/12-weight-initialization.html#_5-理想的参数初始化","998":"/AI/deep_learning_theory/12-weight-initialization.html#_5-1-参数初始化的必要条件","999":"/AI/deep_learning_theory/12-weight-initialization.html#_5-2-glorot-条件","1000":"/AI/deep_learning_theory/12-weight-initialization.html#_6-塞维尔初始化-xavier-initialization","1001":"/AI/deep_learning_theory/12-weight-initialization.html#_7-kaiming-initialization","1002":"/AI/deep_learning_theory/12-weight-initialization.html#_7-1-方差计算数学基础","1003":"/AI/deep_learning_theory/12-weight-initialization.html#_7-2-前向推导过程","1004":"/AI/deep_learning_theory/12-weight-initialization.html#_7-3-反向推导过程","1005":"/AI/deep_learning_theory/12-weight-initialization.html#_7-4-凯明初始化总结","1006":"/AI/deep_learning_theory/12-weight-initialization.html#_7-4-1-服从正态分布时","1007":"/AI/deep_learning_theory/12-weight-initialization.html#_7-4-2-服从均匀分布时","1008":"/AI/deep_learning_theory/12-weight-initialization.html#_8-初始化策略选择","1009":"/AI/deep_learning_theory/12-weight-initialization.html#_9-使用预训练的weight","1010":"/AI/deep_learning_theory/12-weight-initialization.html#_10-参考文献","1011":"/AI/deep_learning_theory/14-regularization.html#_1-正则化概念","1012":"/AI/deep_learning_theory/14-regularization.html#_2-什么情况下容易出现过拟合","1013":"/AI/deep_learning_theory/14-regularization.html#_3-常见的正则化方法","1014":"/AI/deep_learning_theory/14-regularization.html#_3-1-参数范数惩罚","1015":"/AI/deep_learning_theory/14-regularization.html#_3-2-数据集增强","1016":"/AI/deep_learning_theory/14-regularization.html#_3-3-标签平滑-label-smoothing","1017":"/AI/deep_learning_theory/14-regularization.html#_3-4-droupout","1018":"/AI/deep_learning_theory/14-regularization.html#_3-5-dropconnet","1019":"/AI/deep_learning_theory/14-regularization.html#_3-6-dropblock","1020":"/AI/deep_learning_theory/14-regularization.html#_3-7-其它正则化方法","1021":"/AI/deep_learning_theory/13-optimizers.html#optimizer-概述","1022":"/AI/deep_learning_theory/13-optimizers.html#_1-gradient-descend","1023":"/AI/deep_learning_theory/13-optimizers.html#_1-1-梯度下降法概念","1024":"/AI/deep_learning_theory/13-optimizers.html#_1-2-梯度下降法三个变种","1025":"/AI/deep_learning_theory/13-optimizers.html#_1-2-1-bgd-batch-gradient-descend","1026":"/AI/deep_learning_theory/13-optimizers.html#_1-2-2-sgd-stochastic-gradient-descend","1027":"/AI/deep_learning_theory/13-optimizers.html#_1-2-3-mini-bgd","1028":"/AI/deep_learning_theory/13-optimizers.html#_2-sgd-with-momentum","1029":"/AI/deep_learning_theory/13-optimizers.html#_2-1-算法过程","1030":"/AI/deep_learning_theory/13-optimizers.html#_2-2-算法图示","1031":"/AI/deep_learning_theory/13-optimizers.html#_2-2-特点","1032":"/AI/deep_learning_theory/13-optimizers.html#_2-3-作用","1033":"/AI/deep_learning_theory/13-optimizers.html#_3-nag-nesterov-accelerated-gradient","1034":"/AI/deep_learning_theory/13-optimizers.html#_3-1-算法原理","1035":"/AI/deep_learning_theory/13-optimizers.html#_3-2-算法原理图","1036":"/AI/deep_learning_theory/13-optimizers.html#_3-3-算法详述","1037":"/AI/deep_learning_theory/13-optimizers.html#_4-pytorch-中实现-sgd","1038":"/AI/deep_learning_theory/13-optimizers.html#_4-1-算法过程","1039":"/AI/deep_learning_theory/13-optimizers.html#_4-2-代码实现","1040":"/AI/deep_learning_theory/13-optimizers.html#_5-adagrad-优化算法","1041":"/AI/deep_learning_theory/13-optimizers.html#_5-1-自适应学习率的概念","1042":"/AI/deep_learning_theory/13-optimizers.html#_5-2-adagrad-算法原理","1043":"/AI/deep_learning_theory/13-optimizers.html#_5-3-adagrad-算法","1044":"/AI/deep_learning_theory/13-optimizers.html#_5-4-特点","1045":"/AI/deep_learning_theory/13-optimizers.html#_5-5-缺点","1046":"/AI/deep_learning_theory/13-optimizers.html#_5-6-pytorch-实现","1047":"/AI/deep_learning_theory/13-optimizers.html#_6-rmsprop-优化算法","1048":"/AI/deep_learning_theory/13-optimizers.html#_6-1-理论基础","1049":"/AI/deep_learning_theory/13-optimizers.html#_6-2-算法流程","1050":"/AI/deep_learning_theory/13-optimizers.html#_6-3-pytorch-实现","1051":"/AI/deep_learning_theory/13-optimizers.html#_7-adadelta","1052":"/AI/deep_learning_theory/13-optimizers.html#_7-1-概述","1053":"/AI/deep_learning_theory/13-optimizers.html#_7-2-算法流程","1054":"/AI/deep_learning_theory/13-optimizers.html#_7-3-pytorch-实现","1055":"/AI/deep_learning_theory/13-optimizers.html#_8-不同优化算法效果对比","1056":"/AI/deep_learning_theory/13-optimizers.html#_8-1-loss-对比图","1057":"/AI/deep_learning_theory/13-optimizers.html#_8-2-收敛过程对比","1058":"/AI/deep_learning_theory/13-optimizers.html#_9-adam-优化器","1059":"/AI/deep_learning_theory/13-optimizers.html#_9-1-原理概述","1060":"/AI/deep_learning_theory/13-optimizers.html#_9-2-算法实现流程","1061":"/AI/deep_learning_theory/13-optimizers.html#_9-3-pytorch-实现","1062":"/AI/deep_learning_theory/13-optimizers.html#_9-4-效果展示","1063":"/AI/deep_learning_theory/13-optimizers.html#_10-adamw","1064":"/AI/deep_learning_theory/13-optimizers.html#_10-1-算法原理","1065":"/AI/deep_learning_theory/13-optimizers.html#_10-2-pytorch实现","1066":"/AI/deep_learning_theory/13-optimizers.html#_11-optimizer-收敛趋势对比图","1067":"/AI/deep_learning_theory/13-optimizers.html#_12-参考文献","1068":"/AI/deep_learning_theory/20-pytorch-tensor.html#_1-创建pytorch-tensor","1069":"/AI/deep_learning_theory/20-pytorch-tensor.html#_1-1-用torch-tensor-创建","1070":"/AI/deep_learning_theory/20-pytorch-tensor.html#_1-2-直接生成特殊的tensor","1071":"/AI/deep_learning_theory/20-pytorch-tensor.html#_1-3-仿照其它tensor生成","1072":"/AI/deep_learning_theory/20-pytorch-tensor.html#_1-4-从numpy生成","1073":"/AI/deep_learning_theory/20-pytorch-tensor.html#_2-工程实践","1074":"/AI/deep_learning_theory/20-pytorch-tensor.html#_3-tensor-中的-to-方法","1075":"/AI/deep_learning_theory/20-pytorch-tensor.html#_3-1-数据类型转化","1076":"/AI/deep_learning_theory/20-pytorch-tensor.html#_3-2-device-转化","1077":"/AI/deep_learning_theory/20-pytorch-tensor.html#_4-tensor-讲解","1078":"/AI/deep_learning_theory/20-pytorch-tensor.html#_4-1-两个角度认识-tensor","1079":"/AI/deep_learning_theory/20-pytorch-tensor.html#_4-2-代码实践之-视图到底是什么","1080":"/AI/deep_learning_theory/20-pytorch-tensor.html#_4-3-代码实践之-tensor-中数据的连续性","1081":"/AI/deep_learning_theory/20-pytorch-tensor.html#_5-tensor-运算的几种主要类型","1082":"/AI/deep_learning_theory/20-pytorch-tensor.html#_6-tensor-的属性全解","1083":"/AI/deep_learning_theory/20-pytorch-tensor.html#_7-外层-tensor-方法汇总","1084":"/AI/deep_learning_theory/20-pytorch-tensor.html#_8-tensorbase-方法汇总","1085":"/AI/deep_learning_theory/20-pytorch-tensor.html#_8-1-魔术方法-基本运算符-构造函数-索引","1086":"/AI/deep_learning_theory/20-pytorch-tensor.html#_8-2-私有方法","1087":"/AI/deep_learning_theory/20-pytorch-tensor.html#_8-3-tensor-的-对外api接口","1088":"/AI/deep_learning_theory/21-pytorch-autograd.html#_1-pytorch-autograd-原理概述","1089":"/AI/deep_learning_theory/21-pytorch-autograd.html#_1-1-原理概述","1090":"/AI/deep_learning_theory/21-pytorch-autograd.html#_1-2-实现细节","1091":"/AI/deep_learning_theory/21-pytorch-autograd.html#_2-pytorch-代码实现","1092":"/AI/deep_learning_theory/21-pytorch-autograd.html#_2-1-pytorch-autograd-展示","1093":"/AI/deep_learning_theory/21-pytorch-autograd.html#_2-2-require-grad-的自动推理机制","1094":"/AI/deep_learning_theory/21-pytorch-autograd.html#_2-3-detach-隔离功能","1095":"/AI/deep_learning_theory/21-pytorch-autograd.html#_2-4-控制梯度计算","1096":"/AI/deep_learning_theory/21-pytorch-autograd.html#_2-5-梯度累加和清0","1097":"/AI/deep_learning_theory/21-pytorch-autograd.html#_2-6-小心-inplace-op","1098":"/AI/deep_learning_theory/21-pytorch-autograd.html#_2-7-pytorch-autograd-解方程","1099":"/AI/deep_learning_theory/21-pytorch-autograd.html#_2-8-保存中间-activation-tensor-的梯度","1100":"/AI/deep_learning_theory/21-pytorch-autograd.html#_2-9-customer-自定义自己的反向传播函数","1101":"/AI/deep_learning_theory/21-pytorch-autograd.html#_2-10-多维tensor-如何backward","1102":"/AI/deep_learning_theory/21-pytorch-autograd.html#_2-11-example-train-a-model-with-two-mlp-layers","1103":"/AI/deep_learning_theory/21-pytorch-autograd.html#_3-要点总结","1104":"/AI/deep_learning_theory/21-pytorch-autograd.html#_3-1-自动微分机制-auto-grad-重点","1105":"/AI/deep_learning_theory/21-pytorch-autograd.html#_3-2-反向传播算法","1106":"/AI/deep_learning_theory/21-pytorch-autograd.html#_3-3-tensor-的梯度","1107":"/AI/deep_learning_theory/21-pytorch-autograd.html#_3-4-反向求导原理","1108":"/AI/deep_learning_theory/21-pytorch-autograd.html#_3-5-动态图机制","1109":"/AI/deep_learning_theory/21-pytorch-autograd.html#_3-6-auto-grad-机制不足","1110":"/AI/deep_learning_theory/21-pytorch-autograd.html#_3-7-autograd是什么","1111":"/AI/deep_learning_theory/21-pytorch-autograd.html#_3-8-grad-fun","1112":"/AI/deep_learning_theory/21-pytorch-autograd.html#_4-pytorch-autograd-自动微分机制-extension-了解即可-不需要掌握","1113":"/AI/deep_learning_theory/21-pytorch-autograd.html#_4-1-自动微分如何编码历史记录","1114":"/AI/deep_learning_theory/21-pytorch-autograd.html#_4-2-saved-tensors","1115":"/AI/deep_learning_theory/21-pytorch-autograd.html#_4-3-对于不可微分的函数的梯度计算","1116":"/AI/deep_learning_theory/21-pytorch-autograd.html#_4-4-局部禁用梯度计算","1117":"/AI/deep_learning_theory/21-pytorch-autograd.html#_4-5-设置-requires-grad","1118":"/AI/deep_learning_theory/21-pytorch-autograd.html#_4-6-梯度模式","1119":"/AI/deep_learning_theory/21-pytorch-autograd.html#_4-6-1-默认模式-grad-mode","1120":"/AI/deep_learning_theory/21-pytorch-autograd.html#_4-6-2-无梯度模式","1121":"/AI/deep_learning_theory/21-pytorch-autograd.html#_4-6-3-推断模式-inference-mode","1122":"/AI/deep_learning_theory/21-pytorch-autograd.html#_4-6-4-评估模式-nn-module-eval","1123":"/AI/deep_learning_theory/21-pytorch-autograd.html#_4-7-in-place-operations-with-autograd","1124":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#深度学习调优指南中文版","1125":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#目录","1126":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#这份手册是为谁准备的","1127":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#为什么需要这份调优手册","1128":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#开始新项目的指南","1129":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#选择模型架构","1130":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#选择优化器","1131":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#选择batchsize","1132":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#确定可行的batch-size并估计训练吞吐量","1133":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#选择合适的batch-size以最小化训练时间","1134":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#选择合适的batch-size以最小化资源消耗","1135":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#更改batch-size需要重新调整大多数超参数","1136":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#batch-norm会对batch-size的选择造成什么影响","1137":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#选择初始配置","1138":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#提高模型性能的科学方法","1139":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#增量调整策略","1140":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#探索与利用","1141":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#选择下一轮实验的目标","1142":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#设计下一轮实验","1143":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#识别目标超参数、冗余超参数和固定超参数","1144":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#创建一组研究","1145":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#平衡实验的信息量和成本","1146":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#从实验结果中获取经验","1147":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#识别错误的搜索空间边界","1148":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#没有在搜索空间中采样足够的点","1149":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#检查训练曲线","1150":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#使用isolation图检测更改是否有用","1151":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#自动化常用的绘图","1152":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#确定是否采用此训练工作流更改或超参数配置","1153":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#探索结束后","1154":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#确定每次训练运行的步数","1155":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#当训练不受计算限制时如何决定该训练多久","1156":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#使用学习率搜索算法来确定-max-train-steps-的初始值","1157":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#当训练受计算限制时如何决定该训练多久","1158":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#第一轮","1159":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#第二轮","1160":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#关于训练管道的额外补充","1161":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#优化输入管道","1162":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#评估模型性能","1163":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#评估设置","1164":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#设置定期评估","1165":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#选择样本进行定期评估","1166":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#保存检查点并追溯选择最佳检查点","1167":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#设置实验跟踪","1168":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#batchnorm的实现细节","1169":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#多主机管道的考虑因素","1170":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#常见问题的回答","1171":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#最好的学习率衰减方案是什么","1172":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#我应该使用哪种学习率衰减方案作为默认值","1173":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#为什么有些论文有复杂的学习率衰减方案","1174":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#adam-的超参数应该如何调整","1175":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#为什么在优化的探索阶段使用quasi-random-search而不是更复杂的黑盒优化算法","1176":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#在哪里可以找到quasi-random-search的实现","1177":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#需要多少次试验才能通过quasi-random-search获得较好的结果","1178":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#如何调试和缓解优化失败","1179":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#识别不稳定的训练任务","1180":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#常见不稳定模式的潜在修复方式","1181":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#学习率预热","1182":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#何时对学习率进行预热","1183":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#如何对学习率进行预热","1184":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#梯度截断","1185":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#为什么将学习率和其他优化参数称为超参数-它们不是任何先验分布的参数。","1186":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#为什么不应该调整batch-size来直接提高验证集性能","1187":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#所有流行的优化算法的更新规则是什么","1188":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#stochastic-gradient-descent-sgd","1189":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#momentum","1190":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#nesterov","1191":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#rmsprop","1192":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#adam","1193":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#nadam","1194":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#致谢","1195":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#引用","1196":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#关于贡献","1197":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#贡献者许可协议","1198":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#代码审核","1199":"/AI/deep_learning_theory/15-deep-learning-tuning-guide.html#社区指南","1200":"/AI/deep_learning_theory/22-pytorch-module.html#torch-nn-module","1201":"/AI/deep_learning_theory/22-pytorch-module.html#_1-pytorch-自带的-torch-nn-layer","1202":"/AI/deep_learning_theory/22-pytorch-module.html#_1-1-用-torch-nn-解决之前的问题","1203":"/AI/deep_learning_theory/22-pytorch-module.html#_1-2-tensor-和-parameter-的区别","1204":"/AI/deep_learning_theory/22-pytorch-module.html#_2-定义我们自己的module","1205":"/AI/deep_learning_theory/22-pytorch-module.html#_2-1-代码案例","1206":"/AI/deep_learning_theory/22-pytorch-module.html#_2-2-customer-layer-要点","1207":"/AI/deep_learning_theory/22-pytorch-module.html#_3-nn-module-中的容器","1208":"/AI/deep_learning_theory/22-pytorch-module.html#_4-nn-module-属性详解","1209":"/AI/deep_learning_theory/22-pytorch-module.html#_5-torch-nn-module-常用功能","1210":"/AI/deep_learning_theory/22-pytorch-module.html#_5-1-parameters-设置机制","1211":"/AI/deep_learning_theory/22-pytorch-module.html#_5-2-buffers-功能展示","1212":"/AI/deep_learning_theory/22-pytorch-module.html#_5-3-前向钩子函数展示","1213":"/AI/deep_learning_theory/22-pytorch-module.html#_5-4-反向钩子函数展示","1214":"/AI/deep_learning_theory/22-pytorch-module.html#_6-nn-module-方法全解","1215":"/AI/deep_learning_theory/23-1training-example-1.html#_1-端到端训练一个深度学习模型","1216":"/AI/deep_learning_theory/23-2decoder.html#decoder","1217":"/AI/deep_learning_theory/23-3encoder.html#encoder-layer","1218":"/AI/deep_learning_theory/23-4transformer.html#transformer-demo","1219":"/AI/deep_learning_theory/24-pytorch-optimizer.html#_0-torch-optim","1220":"/AI/deep_learning_theory/24-pytorch-optimizer.html#_1-如何使用torch-optim","1221":"/AI/deep_learning_theory/24-pytorch-optimizer.html#_1-1-创建一个优化器对象","1222":"/AI/deep_learning_theory/24-pytorch-optimizer.html#_1-2-逐参数选项-per-parameter-options","1223":"/AI/deep_learning_theory/24-pytorch-optimizer.html#_1-3-进行优化步骤","1224":"/AI/deep_learning_theory/24-pytorch-optimizer.html#_2-torch-optim-base-class-introduce","1225":"/AI/deep_learning_theory/24-pytorch-optimizer.html#_2-1-torch-optim-optimizer-的输入参数","1226":"/AI/deep_learning_theory/24-pytorch-optimizer.html#_2-2-torch-optim-optimizer-属性","1227":"/AI/deep_learning_theory/24-pytorch-optimizer.html#_2-3-torch-optim-optimizer-方法","1228":"/AI/deep_learning_theory/24-pytorch-optimizer.html#_3-不同实现与性能优化","1229":"/AI/deep_learning_theory/25-pytorch-lr-scheduler.html#learning-rate-调整方案","1230":"/AI/deep_learning_theory/25-pytorch-lr-scheduler.html#_2-pytorch中-torch-optim-lr-scheduler-使用方法","1231":"/AI/deep_learning_theory/25-pytorch-lr-scheduler.html#_2-1-使用方法","1232":"/AI/deep_learning_theory/25-pytorch-lr-scheduler.html#_3-学习率调度器-策略全解-learning-rate-scheduler","1233":"/AI/deep_learning_theory/25-pytorch-lr-scheduler.html#_3-1-lr-scheduler-lambdalr","1234":"/AI/deep_learning_theory/25-pytorch-lr-scheduler.html#_3-2-lr-scheduler-multiplicativelr","1235":"/AI/deep_learning_theory/25-pytorch-lr-scheduler.html#_3-3-lr-scheduler-steplr","1236":"/AI/deep_learning_theory/25-pytorch-lr-scheduler.html#_3-4-lr-scheduler-multisteplr","1237":"/AI/deep_learning_theory/25-pytorch-lr-scheduler.html#_3-5-lr-scheduler-constantlr","1238":"/AI/deep_learning_theory/25-pytorch-lr-scheduler.html#_3-6-lr-scheduler-linearlr","1239":"/AI/deep_learning_theory/25-pytorch-lr-scheduler.html#_3-7-lr-scheduler-exponentiallr","1240":"/AI/deep_learning_theory/25-pytorch-lr-scheduler.html#_3-8-lr-scheduler-polynomiallr","1241":"/AI/deep_learning_theory/25-pytorch-lr-scheduler.html#_3-9-lr-scheduler-cycliclr","1242":"/AI/deep_learning_theory/25-pytorch-lr-scheduler.html#_3-10-lr-scheduler-onecyclelr","1243":"/AI/deep_learning_theory/25-pytorch-lr-scheduler.html#_3-11-lr-scheduler-cosineannealinglr","1244":"/AI/deep_learning_theory/25-pytorch-lr-scheduler.html#_3-12-lr-scheduler-cosineannealingwarmrestarts","1245":"/AI/deep_learning_theory/25-pytorch-lr-scheduler.html#_3-13-reducelronplateau","1246":"/AI/deep_learning_theory/25-pytorch-lr-scheduler.html#_3-14-lr-scheduler-chainedscheduler","1247":"/AI/deep_learning_theory/25-pytorch-lr-scheduler.html#_3-15-lr-scheduler-sequentiallr","1248":"/AI/deep_learning_theory/25-pytorch-lr-scheduler.html#_4-探索源码","1249":"/AI/deep_learning_theory/26-pytorch-dataloader.html#_1-dataset","1250":"/AI/deep_learning_theory/26-pytorch-dataloader.html#_2-定义自己的数据集","1251":"/AI/deep_learning_theory/26-pytorch-dataloader.html#_3-torch-中的-dataloader","1252":"/AI/deep_learning_theory/26-pytorch-dataloader.html#_4-torchvision","1253":"/AI/deep_learning_theory/26-pytorch-dataloader.html#_4-1-torchvision-中的dataset","1254":"/AI/deep_learning_theory/26-pytorch-dataloader.html#torchvision-中的-transforms","1255":"/AI/deep_learning_theory/27-pytorch-model-save.html#_1-tensor-的保存和加载","1256":"/AI/deep_learning_theory/27-pytorch-model-save.html#_2-模型状态的保存","1257":"/AI/deep_learning_theory/27-pytorch-model-save.html#_2-1-定义一个模型","1258":"/AI/deep_learning_theory/27-pytorch-model-save.html#_2-2-保存模型的状态","1259":"/AI/deep_learning_theory/27-pytorch-model-save.html#_2-3-加载模型的状态","1260":"/AI/deep_learning_theory/27-pytorch-model-save.html#_2-4-思考与尝试","1261":"/AI/deep_learning_theory/27-pytorch-model-save.html#_3-保存与加载模型","1262":"/AI/deep_learning_theory/27-pytorch-model-save.html#_3-1-保存模型","1263":"/AI/deep_learning_theory/27-pytorch-model-save.html#_3-2-加载模型","1264":"/AI/deep_learning_theory/27-pytorch-model-save.html#_3-3-思考与尝试","1265":"/AI/deep_learning_theory/27-pytorch-model-save.html#_4-训练中的保存和加载","1266":"/AI/deep_learning_theory/27-pytorch-model-save.html#_4-1-保存训练中的状态","1267":"/AI/deep_learning_theory/27-pytorch-model-save.html#_4-2-加载训练中的状态","1268":"/AI/deep_learning_theory/27-pytorch-model-save.html#_5-保存和加载模型的静态图","1269":"/AI/deep_learning_theory/27-pytorch-model-save.html#_5-1-保存模型静态图","1270":"/AI/deep_learning_theory/27-pytorch-model-save.html#_5-2-加载模型静态图","1271":"/AI/deep_learning_theory/27-pytorch-model-save.html#_6-通用格式onnx的保存","1272":"/AI/deep_learning_theory/27-pytorch-model-save.html#_6-1-保存onnx-静态图模型","1273":"/AI/deep_learning_theory/27-pytorch-model-save.html#_6-2-运行onnx-模型","1274":"/AI/deep_learning_theory/27-pytorch-model-save.html#_6-3-shape-infer","1275":"/AI/deep_learning_theory/28-pytorch-tensorboard.html#_1-tensorboard-介绍","1276":"/AI/deep_learning_theory/28-pytorch-tensorboard.html#_2-安装方式","1277":"/AI/deep_learning_theory/28-pytorch-tensorboard.html#_3-抓取log","1278":"/AI/deep_learning_theory/28-pytorch-tensorboard.html#_3-1-import-summarywriter","1279":"/AI/deep_learning_theory/28-pytorch-tensorboard.html#_3-2-plot-scalar","1280":"/AI/deep_learning_theory/28-pytorch-tensorboard.html#_3-3-plot-loss-and-accuracy","1281":"/AI/deep_learning_theory/28-pytorch-tensorboard.html#_4-执行方式","1282":"/AI/deep_learning_theory/28-pytorch-tensorboard.html#_5-查看graph","1283":"/AI/deep_learning_theory/28-pytorch-tensorboard.html#_6-查看特征图","1284":"/AI/deep_learning_theory/28-pytorch-tensorboard.html#_7-性能分析profiler","1285":"/AI/deep_learning_theory/29-pytorch-graph-mode.html#_1-pytorch-几种模式概览","1286":"/AI/deep_learning_theory/29-pytorch-graph-mode.html#_1-1-pytorch-不仅仅是动态图","1287":"/AI/deep_learning_theory/29-pytorch-graph-mode.html#_1-2-理解动态图和静态图","1288":"/AI/deep_learning_theory/29-pytorch-graph-mode.html#_1-3-静态图的优势","1289":"/AI/deep_learning_theory/29-pytorch-graph-mode.html#_2-几种模式简介","1290":"/AI/deep_learning_theory/29-pytorch-graph-mode.html#_2-1-fx-图","1291":"/AI/deep_learning_theory/29-pytorch-graph-mode.html#_2-2-torch-jit-script","1292":"/AI/deep_learning_theory/29-pytorch-graph-mode.html#_2-3-torch-jit-trace","1293":"/AI/deep_learning_theory/29-pytorch-graph-mode.html#_2-4-torch-compile","1294":"/AI/deep_learning_theory/29-pytorch-graph-mode.html#_3-案例","1295":"/AI/deep_learning_theory/29-pytorch-graph-mode.html#_3-1-模型准备","1296":"/AI/deep_learning_theory/29-pytorch-graph-mode.html#_3-2-jit-script-代码展示","1297":"/AI/deep_learning_theory/29-pytorch-graph-mode.html#_3-3-jit-traced-代码展示","1298":"/AI/deep_learning_theory/29-pytorch-graph-mode.html#_4-export-to-onnx","1299":"/AI/deep_learning_theory/29-pytorch-graph-mode.html#_5-compile-to-graph-dynamo","1300":"/AI/deep_learning_theory/30-training-example-2.html#_1-imagenet-training-in-pytorch","1301":"/AI/deep_learning_theory/30-training-example-2.html#_2-requirements","1302":"/AI/deep_learning_theory/30-training-example-2.html#_3-数据集下载","1303":"/AI/deep_learning_theory/30-training-example-2.html#_4-training","1304":"/AI/deep_learning_theory/30-training-example-2.html#use-dummy-data","1305":"/AI/deep_learning_theory/30-training-example-2.html#multi-processing-distributed-data-parallel-training","1306":"/AI/deep_learning_theory/30-training-example-2.html#single-node-multiple-gpus","1307":"/AI/deep_learning_theory/30-training-example-2.html#multiple-nodes","1308":"/AI/deep_learning_theory/30-training-example-2.html#usage","1309":"/AI/deep_learning_theory/40-ner.html#_1-模型跑通","1310":"/AI/deep_learning_theory/40-ner.html#_2-bert-介绍","1311":"/AI/deep_learning_theory/40-ner.html#_3-transformer-发展脉络","1312":"/AI/deep_learning_theory/40-ner.html#_3-1-transformer-概述","1313":"/AI/deep_learning_theory/40-ner.html#_3-2-迁移学习","1314":"/AI/deep_learning_theory/40-ner.html#_3-3-transformer-家族","1315":"/AI/deep_learning_theory/40-ner.html#_3-4-encoder-分支","1316":"/AI/deep_learning_theory/40-ner.html#_3-5-decoder-分支","1317":"/AI/deep_learning_theory/40-ner.html#_3-6-encoder-decoder-分支","1318":"/AI/deep_learning_theory/40-ner.html#_3-7-大模型的爆发","1319":"/AI/deep_learning_theory/40-ner.html#_4-bert-crf-conditional-random-field-实现命名实体识别-ner-任务","1320":"/AI/deep_learning_theory/40-ner.html#_4-1-任务概述","1321":"/AI/deep_learning_theory/40-ner.html#_4-2-crf-原理详解","1322":"/AI/deep_learning_theory/40-ner.html#_4-2-1-线性crf的定义","1323":"/AI/deep_learning_theory/40-ner.html#_4-2-2-发射分数","1324":"/AI/deep_learning_theory/40-ner.html#_4-2-3-转移分数","1325":"/AI/deep_learning_theory/40-ner.html#_4-2-4-crf-的损失函数计算","1326":"/AI/deep_learning_theory/40-ner.html#_4-2-5-crf的viterbi解码","1327":"/AI/deep_learning_theory/40-ner.html#_5-代码详解","1328":"/AI/deep_learning_theory/40-ner.html#_5-1-真实路径得分计算","1329":"/AI/deep_learning_theory/40-ner.html#_5-2-总路径得分计算","1330":"/AI/deep_learning_theory/40-ner.html#_5-3-viterbi-解码过程","1331":"/AI/deep_learning_theory/40-ner.html#_5-4-f1-score-的计算","1332":"/AI/deep_learning_theory/41-question-answering.html#_1-模型跑通","1333":"/AI/deep_learning_theory/41-question-answering.html#_2-t5-介绍","1334":"/AI/deep_learning_theory/41-question-answering.html#_3-position-embedding-总结","1335":"/AI/deep_learning_theory/41-question-answering.html#_3-1-绝对位置编码","1336":"/AI/deep_learning_theory/41-question-answering.html#_3-1-1-三角函数式-sinusoidal-位置编码","1337":"/AI/deep_learning_theory/41-question-answering.html#_3-1-2-可学习-learnable-的位置编码","1338":"/AI/deep_learning_theory/41-question-answering.html#_3-2-相对位置编码","1339":"/AI/deep_learning_theory/41-question-answering.html#_3-2-1-经典的相对位置编码","1340":"/AI/deep_learning_theory/41-question-answering.html#_3-2-2-t5-中的相对位置编码","1341":"/AI/deep_learning_theory/41-question-answering.html#_3-3-旋转位置编码","1342":"/AI/deep_learning_theory/41-question-answering.html#_3-3-1-rope-原理","1343":"/AI/deep_learning_theory/41-question-answering.html#_3-3-2-2-维扩展到多维","1344":"/AI/deep_learning_theory/41-question-answering.html#_3-3-3-rope-的高效计算","1345":"/AI/deep_learning_theory/41-question-answering.html#_3-3-4-llama-中的rope-代码实现","1346":"/AI/deep_learning_theory/42-1stable-diffusion.html#_1-代码介绍","1347":"/AI/deep_learning_theory/42-1stable-diffusion.html#_2-代码复现步骤","1348":"/AI/deep_learning_theory/42-1stable-diffusion.html#_3-stable-diffusion-整体结构","1349":"/AI/deep_learning_theory/42-1stable-diffusion.html#_3-1-整体流程图","1350":"/AI/deep_learning_theory/42-1stable-diffusion.html#_3-2-sd-训练流程图","1351":"/AI/deep_learning_theory/42-1stable-diffusion.html#_3-3-sd-推理流程图","1352":"/AI/deep_learning_theory/42-1stable-diffusion.html#_3-4-clip-原理图","1353":"/AI/deep_learning_theory/42-1stable-diffusion.html#_3-5-latent-space","1354":"/AI/deep_learning_theory/42-1stable-diffusion.html#_3-6-noiser-and-denoiser","1355":"/AI/deep_learning_theory/42-1stable-diffusion.html#_4-stable-diffusion-具体模型结构","1356":"/AI/deep_learning_theory/42-1stable-diffusion.html#_4-1-clip-结构","1357":"/AI/deep_learning_theory/42-1stable-diffusion.html#_4-2-vae-模型结构","1358":"/AI/deep_learning_theory/42-1stable-diffusion.html#_4-2-unet-base-模型结构图","1359":"/AI/deep_learning_theory/42-1stable-diffusion.html#_5-评价指标","1360":"/AI/deep_learning_theory/42-1stable-diffusion.html#_5-1-clip-score","1361":"/AI/deep_learning_theory/42-1stable-diffusion.html#_5-2-fid","1362":"/AI/deep_learning_theory/42-1stable-diffusion.html#_6-sd-进阶","1363":"/AI/deep_learning_theory/42-1stable-diffusion.html#_6-1-sd2-之前版本异同","1364":"/AI/deep_learning_theory/42-1stable-diffusion.html#_6-2-从-sd-到-sdxl","1365":"/AI/deep_learning_theory/42-1stable-diffusion.html#_6-参考链接","1366":"/AI/deep_learning_theory/42-2SDXL.html#sdxl","1367":"/AI/deep_learning_theory/42-2SDXL.html#参考链接","1368":"/AI/deep_learning_theory/42-3VAE.html#vae","1369":"/AI/deep_learning_theory/42-3VAE.html#_1-vae-的作用-数据压缩和数据生成","1370":"/AI/deep_learning_theory/42-3VAE.html#_1-1-数据压缩","1371":"/AI/deep_learning_theory/42-3VAE.html#_1-2-数据生成","1372":"/AI/deep_learning_theory/42-3VAE.html#_1-3-数据压缩与数据生成的关系","1373":"/AI/deep_learning_theory/42-3VAE.html#_1-4-example","1374":"/AI/deep_learning_theory/42-3VAE.html#_1-5-可能出现的问题","1375":"/AI/deep_learning_theory/42-3VAE.html#_1-6-vae-要点总结","1376":"/AI/deep_learning_theory/42-3VAE.html#_2-理论推导vae","1377":"/AI/deep_learning_theory/42-3VAE.html#_2-1-引入变分","1378":"/AI/deep_learning_theory/42-3VAE.html#_4-参考文献","1379":"/AI/deep_learning_theory/44-scaling-law.html#scaling-laws-for-neural-language-models","1380":"/AI/deep_learning_theory/45-distribute-training.html#how-to-training-realy-large-model","1381":"/AI/deep_learning_theory/46-nlp-llama.html#_1-llama-v1","1382":"/AI/deep_learning_theory/46-nlp-llama.html#_2-llama-v2","1383":"/AI/deep_learning_theory/46-nlp-llama.html#_3-llama-v3","1384":"/AI/deep_learning_theory/46-nlp-llama.html#_4-llama-code-implement","1385":"/AI/deep_learning_theory/03-bp_example_demo.html#神经网络案例展示","1386":"/AI/deep_learning_theory/03-bp_example_demo.html#_1-题目","1387":"/AI/deep_learning_theory/03-bp_example_demo.html#_2-前向传播过程-feedforward","1388":"/AI/deep_learning_theory/03-bp_example_demo.html#_2-1-第一层求解","1389":"/AI/deep_learning_theory/03-bp_example_demo.html#_2-2-第二层计算","1390":"/AI/deep_learning_theory/03-bp_example_demo.html#_3-反向传播过程-back-propagation","1391":"/AI/deep_learning_theory/03-bp_example_demo.html#_3-1-末层权重梯度计算","1392":"/AI/deep_learning_theory/03-bp_example_demo.html#_3-1-1-计算流程概述","1393":"/AI/deep_learning_theory/03-bp_example_demo.html#_3-1-2-具体计算过程","1394":"/AI/deep_learning_theory/03-bp_example_demo.html#_3-2-前一层权重梯度计算-以-梯度计算为例","1395":"/AI/deep_learning_theory/03-bp_example_demo.html#_4-权重更新","1396":"/AI/deep_learning_theory/03-bp_example_demo.html#_5-迭代训练","1397":"/AI/deep_learning_theory/03-bp_example_demo.html#_6-将前馈网络写成矩阵形式","1398":"/AI/deep_learning_theory/03-bp_example_demo.html#_7-代码展示","1399":"/AI/deep_learning_theory/47-nlp-deepseek.html#deepseek-v2","1400":"/AI/deep_learning_theory/47-nlp-deepseek.html#deepseek-moe","1401":"/AI/deep_learning_theory/47-nlp-deepseek.html#deepseek-v3","1402":"/AI/deep_learning_theory/47-nlp-deepseek.html#deepseek-r1","1403":"/AI/#ai时代的算法学习","1404":"/AI/deep_learning_theory/#deep-learning-theroy","1405":"/IT-learning/408知识/OS-4.1 进程同步.html#_4-1-进程同步","1406":"/IT-learning/408知识/OS-4.1 进程同步.html#_1-基本概念","1407":"/IT-learning/408知识/OS-4.1 进程同步.html#_1-1为什么要提出","1408":"/IT-learning/408知识/OS-4.1 进程同步.html#_1-2-同步是什么","1409":"/IT-learning/408知识/OS-4.1 进程同步.html#_1-3-什么又是互斥","1410":"/IT-learning/408知识/OS-4.1 进程同步.html#_1-4-临界资源是啥","1411":"/IT-learning/408知识/OS-4.1 进程同步.html#_1-4-1-系统资源","1412":"/IT-learning/408知识/OS-4.1 进程同步.html#_1-4-2-临界资源-共享资源","1413":"/IT-learning/408知识/OS-4.1 进程同步.html#_1-4-3-临界区","1414":"/IT-learning/408知识/OS-4.1 进程同步.html#_2-同步如何实现","1415":"/IT-learning/408知识/OS-4.1 进程同步.html#_2-1-访问原则","1416":"/IT-learning/408知识/OS-4.1 进程同步.html#_2-2-软件实现-后续补充","1417":"/IT-learning/408知识/OS-4.1 进程同步.html#_2-2-1-单标志法","1418":"/IT-learning/408知识/OS-4.1 进程同步.html#_2-2-2-双标志先检查法","1419":"/IT-learning/408知识/OS-4.1 进程同步.html#_2-2-3-双标志后检查法","1420":"/IT-learning/408知识/OS-4.1 进程同步.html#_2-3-硬件实现-后续补充","1421":"/IT-learning/408知识/OS-4.1 进程同步.html#_2-3-1-中断屏蔽方法","1422":"/IT-learning/408知识/OS-4.1 进程同步.html#_2-3-2-test-and-set-ts指令-tsl指令","1423":"/IT-learning/408知识/OS-4.1 进程同步.html#_2-3-3-swap指令-exchange-xchg指令","1424":"/IT-learning/408知识/OS-4.1 进程同步.html#_2-3-4-信号量机制-重点-下一节详细讲解","1425":"/IT-learning/408知识/OS-4.1 进程同步.html#_3-参考资料","1426":"/IT-learning/408知识/OS-4.4 信号量机制pv操作之“可见”.html#_4-4-信号量机制pv操作之-可见","1427":"/IT-learning/408知识/#_408知识","1428":"/IT-learning/408知识/#ds-数据结构-data-structures","1429":"/IT-learning/408知识/#cn-计算机网络-computer-networks","1430":"/IT-learning/408知识/#co-计算机组成原理-computer-organization","1431":"/IT-learning/408知识/#os-操作系统-operating-systems","1432":"/IT-learning/408知识/#or-碎片知识-other","1433":"/IT-learning/Java/01.java-se.html#javase-简介","1434":"/IT-learning/Java/01.java-se.html#javase-核心概念","1435":"/IT-learning/Java/01.java-se.html#_1-jdk、jre、jvm","1436":"/IT-learning/Java/01.java-se.html#_2-java-基础语法","1437":"/AI/deep_learning_theory/02-back_propagation.html#_1-概念理解","1438":"/AI/deep_learning_theory/02-back_propagation.html#_1-1-神经网络训练流程概述","1439":"/AI/deep_learning_theory/02-back_propagation.html#_1-2-反向传播的定义","1440":"/AI/deep_learning_theory/02-back_propagation.html#_2-梯度下降算法简述","1441":"/AI/deep_learning_theory/02-back_propagation.html#_3-bp-或-深度神经网络训练需要明确的几个概念","1442":"/AI/deep_learning_theory/02-back_propagation.html#_4-链式求导法则","1443":"/AI/deep_learning_theory/02-back_propagation.html#_5-bp-流程图示","1444":"/AI/deep_learning_theory/02-back_propagation.html#_6-反向传播数学推导","1445":"/AI/deep_learning_theory/02-back_propagation.html#_6-1-反向传播目的确认","1446":"/AI/deep_learning_theory/02-back_propagation.html#_6-2-线性连接层-weight-的梯度","1447":"/AI/deep_learning_theory/02-back_propagation.html#_6-3-激活函数-input-的梯度","1448":"/AI/deep_learning_theory/02-back_propagation.html#_6-4-激活函数-output-的梯度","1449":"/AI/deep_learning_theory/02-back_propagation.html#_6-5-下层激活-input-z-and-z-梯度求解","1450":"/AI/deep_learning_theory/02-back_propagation.html#_7-反向传播总结","1451":"/AI/deep_learning_theory/01-feedforward_network.html#前馈神经网络-feedforward-neural-network","1452":"/AI/deep_learning_theory/01-feedforward_network.html#_1-相关概念","1453":"/AI/deep_learning_theory/01-feedforward_network.html#_1-1-人工智能是什么","1454":"/AI/deep_learning_theory/01-feedforward_network.html#_1-2-深度学习与人工智能的关系","1455":"/AI/deep_learning_theory/01-feedforward_network.html#_1-3-深度学习的概念","1456":"/AI/deep_learning_theory/01-feedforward_network.html#_1-4-什么是人工神经网络","1457":"/AI/deep_learning_theory/01-feedforward_network.html#_1-5-前馈神经网络的概念","1458":"/AI/deep_learning_theory/01-feedforward_network.html#_2-神经元模型","1459":"/AI/deep_learning_theory/01-feedforward_network.html#_2-1-m-p-神经元","1460":"/AI/deep_learning_theory/01-feedforward_network.html#_2-2-经典激活函数","1461":"/AI/deep_learning_theory/01-feedforward_network.html#_3-从神经元到感知机","1462":"/AI/deep_learning_theory/01-feedforward_network.html#_3-1-使用感知机解决线性可分问题","1463":"/AI/deep_learning_theory/01-feedforward_network.html#_3-2-如何解决异或问题","1464":"/AI/deep_learning_theory/01-feedforward_network.html#_4-从感知机到深度神经网络","1465":"/AI/deep_learning_theory/01-feedforward_network.html#_4-1-为何要用深度神经网络","1466":"/AI/deep_learning_theory/01-feedforward_network.html#_4-2-深度神经网络解决问题案例","1467":"/AI/deep_learning_theory/01-feedforward_network.html#_5-前馈神经网络计算流程","1468":"/AI/deep_learning_theory/01-feedforward_network.html#_6-深度学习与传统机器学习","1469":"/AI/deep_learning_theory/01-feedforward_network.html#_6-1-相同点","1470":"/AI/deep_learning_theory/01-feedforward_network.html#_6-2-不同点","1471":"/AI/deep_learning_theory/01-feedforward_network.html#_7-深度学习的特点","1472":"/AI/deep_learning_theory/01-feedforward_network.html#_8-深度学习的典型算法","1473":"/AI/deep_learning_theory/01-feedforward_network.html#_9-参考文献","1474":"/IT-learning/Java/05.MyBatis.html#mybatis框架","1475":"/IT-learning/Java/05.MyBatis.html#_01、mybatis简介","1476":"/IT-learning/Java/05.MyBatis.html#_1-1、什么是mybatis","1477":"/IT-learning/Java/05.MyBatis.html#_1-2、持久化","1478":"/IT-learning/Java/05.MyBatis.html#_1-3、持久层","1479":"/IT-learning/Java/05.MyBatis.html#_1-4、为什么需要mybatis","1480":"/IT-learning/Java/05.MyBatis.html#_02、mybatis第一个程序","1481":"/IT-learning/Java/05.MyBatis.html#_2-1、代码演示","1482":"/IT-learning/Java/05.MyBatis.html#_2-2、问题说明","1483":"/IT-learning/Java/05.MyBatis.html#_03、crud操作","1484":"/IT-learning/Java/05.MyBatis.html#_3-1、namespace","1485":"/IT-learning/Java/05.MyBatis.html#_3-2、select","1486":"/IT-learning/Java/05.MyBatis.html#_3-3、insert","1487":"/IT-learning/Java/05.MyBatis.html#_3-4、update","1488":"/IT-learning/Java/05.MyBatis.html#_3-5、delete","1489":"/IT-learning/Java/05.MyBatis.html#_3-6、思考题","1490":"/IT-learning/Java/#java-后端学习框架","1491":"/IT-learning/Java/#_1-java-基础","1492":"/IT-learning/Java/#_2-数据库基础","1493":"/IT-learning/Java/#_3-java-web-开发","1494":"/IT-learning/Java/#_4-mybatis","1495":"/IT-learning/Java/#_5-spring","1496":"/IT-learning/Java/#_6-spring-mvc","1497":"/IT-learning/Java/#_7-sprintboot","1498":"/IT-learning/Java/#_8-消息中间件","1499":"/IT-learning/Java/#_9-部署与监控","1500":"/IT-learning/Java/#_10-进阶主题","1501":"/IT-learning/Java/#参考资料","1502":"/IT-learning/Linux/01.Linux基础.html#linux基础部分","1503":"/IT-learning/Linux/01.Linux基础.html#一、基本命令使用","1504":"/IT-learning/Linux/01.Linux基础.html#_1-linux-文件系统结构","1505":"/IT-learning/Linux/01.Linux基础.html#简介","1506":"/IT-learning/Linux/01.Linux基础.html#常用目录","1507":"/IT-learning/Linux/01.Linux基础.html#示例","1508":"/IT-learning/Linux/01.Linux基础.html#_2-基本命令操作","1509":"/IT-learning/Linux/01.Linux基础.html#文件和目录管理","1510":"/IT-learning/Linux/01.Linux基础.html#文件操作","1511":"/IT-learning/Linux/01.Linux基础.html#_3-文件权限管理","1512":"/IT-learning/Linux/01.Linux基础.html#权限表示","1513":"/IT-learning/Linux/01.Linux基础.html#查看和修改权限","1514":"/IT-learning/Linux/01.Linux基础.html#_4-文本查看","1515":"/IT-learning/Linux/01.Linux基础.html#查看文本文件","1516":"/IT-learning/Linux/01.Linux基础.html#查找内容","1517":"/IT-learning/Linux/01.Linux基础.html#_5-vim-编辑器基础","1518":"/IT-learning/Linux/01.Linux基础.html#_5-1-进入退出","1519":"/IT-learning/Linux/01.Linux基础.html#_5-2-模式","1520":"/IT-learning/Linux/01.Linux基础.html#_5-3-基本操作","1521":"/IT-learning/Linux/01.Linux基础.html#_6-进程管理","1522":"/IT-learning/Linux/01.Linux基础.html#查看进程","1523":"/IT-learning/Linux/01.Linux基础.html#管理进程","1524":"/IT-learning/Linux/01.Linux基础.html#示例-1","1525":"/IT-learning/Linux/01.Linux基础.html#_7-网络管理","1526":"/IT-learning/Linux/01.Linux基础.html#查看网络配置","1527":"/IT-learning/Linux/01.Linux基础.html#查看网络端口","1528":"/IT-learning/Linux/01.Linux基础.html#抓取网页内容","1529":"/IT-learning/Linux/01.Linux基础.html#_8-用户和组管理","1530":"/IT-learning/Linux/01.Linux基础.html#用户管理","1531":"/IT-learning/Linux/01.Linux基础.html#_9-文件查找","1532":"/IT-learning/Linux/01.Linux基础.html#查找文件","1533":"/IT-learning/Linux/01.Linux基础.html#查找可执行文件","1534":"/IT-learning/Linux/01.Linux基础.html#_10-归档与压缩","1535":"/IT-learning/Linux/01.Linux基础.html#打包和解压","1536":"/IT-learning/Linux/01.Linux基础.html#_11-系统更新与软件管理","1537":"/IT-learning/Linux/01.Linux基础.html#更新系统和安装软件包","1538":"/IT-learning/Linux/01.Linux基础.html#_12-日志管理","1539":"/IT-learning/Linux/01.Linux基础.html#查看日志","1540":"/IT-learning/Linux/01.Linux基础.html#二、vim操作命令","1541":"/IT-learning/Linux/01.Linux基础.html#_1-vim-模式简介","1542":"/IT-learning/Linux/01.Linux基础.html#_2-启动和退出-vim","1543":"/IT-learning/Linux/01.Linux基础.html#启动","1544":"/IT-learning/Linux/01.Linux基础.html#退出","1545":"/IT-learning/Linux/01.Linux基础.html#_3-插入模式操作","1546":"/IT-learning/Linux/01.Linux基础.html#_4-普通模式基础操作","1547":"/IT-learning/Linux/01.Linux基础.html#光标移动","1548":"/IT-learning/Linux/01.Linux基础.html#删除操作","1549":"/IT-learning/Linux/01.Linux基础.html#复制和粘贴","1550":"/IT-learning/Linux/01.Linux基础.html#撤销与重做","1551":"/IT-learning/Linux/01.Linux基础.html#_5-可视模式-选择操作","1552":"/IT-learning/Linux/01.Linux基础.html#_6-查找与替换","1553":"/IT-learning/Linux/01.Linux基础.html#查找","1554":"/IT-learning/Linux/01.Linux基础.html#替换","1555":"/IT-learning/Linux/01.Linux基础.html#_7-多文件和多窗口操作","1556":"/IT-learning/Linux/01.Linux基础.html#打开多个文件","1557":"/IT-learning/Linux/01.Linux基础.html#分屏操作","1558":"/IT-learning/Linux/01.Linux基础.html#_8-文本缩进和格式调整","1559":"/IT-learning/Linux/01.Linux基础.html#自动缩进","1560":"/IT-learning/Linux/01.Linux基础.html#格式化代码","1561":"/IT-learning/Linux/03.MPI并行计算.html#mpi并行计算","1562":"/IT-learning/Linux/03.MPI并行计算.html#一、并行的引入","1563":"/IT-learning/Linux/03.MPI并行计算.html#_1-1-基本概念","1564":"/IT-learning/Linux/03.MPI并行计算.html#_1-2-什么是并行","1565":"/IT-learning/Linux/03.MPI并行计算.html#_1-3-并行有啥用","1566":"/IT-learning/Linux/03.MPI并行计算.html#_1-4-并行的实际案例","1567":"/IT-learning/Linux/03.MPI并行计算.html#二、并行的类型","1568":"/IT-learning/Linux/03.MPI并行计算.html#_2-1-按照处理机划分","1569":"/IT-learning/Linux/03.MPI并行计算.html#_2-2-按照实现方式划分","1570":"/IT-learning/Linux/03.MPI并行计算.html#三、mpi的基本原理","1571":"/IT-learning/Linux/03.MPI并行计算.html#_3-1-基本原理","1572":"/IT-learning/Linux/03.MPI并行计算.html#_1-mpi的架构","1573":"/IT-learning/Linux/03.MPI并行计算.html#_2-进程和通信","1574":"/IT-learning/Linux/03.MPI并行计算.html#_3-mpi的通信模式","1575":"/IT-learning/Linux/03.MPI并行计算.html#_4-mpi的基本函数","1576":"/IT-learning/Linux/03.MPI并行计算.html#_5-mpi的数据类型和消息标签","1577":"/IT-learning/Linux/03.MPI并行计算.html#_6-通信域-communicator","1578":"/IT-learning/Linux/03.MPI并行计算.html#_7-mpi中的常见通信模式","1579":"/IT-learning/Linux/03.MPI并行计算.html#_8-mpi的优势和劣势","1580":"/IT-learning/Linux/03.MPI并行计算.html#_3-2-模型演示","1581":"/IT-learning/Linux/03.MPI并行计算.html#四、基本环境配置-简略","1582":"/IT-learning/Linux/03.MPI并行计算.html#_4-1-linux环境","1583":"/IT-learning/Linux/03.MPI并行计算.html#_4-2-ssh工具","1584":"/IT-learning/Linux/03.MPI并行计算.html#_4-3-vim编辑器","1585":"/IT-learning/Linux/03.MPI并行计算.html#_4-4-mpi环境","1586":"/IT-learning/Linux/03.MPI并行计算.html#_4-4-1-安装","1587":"/IT-learning/Linux/03.MPI并行计算.html#_4-4-2-编译","1588":"/IT-learning/Linux/03.MPI并行计算.html#_4-4-3-配置","1589":"/IT-learning/Linux/03.MPI并行计算.html#_4-4-4-了解","1590":"/IT-learning/Linux/03.MPI并行计算.html#五、mpi的基本使用","1591":"/IT-learning/Linux/03.MPI并行计算.html#_5-1-快速使用","1592":"/IT-learning/Linux/03.MPI并行计算.html#_5-2-源码了解","1593":"/IT-learning/Linux/03.MPI并行计算.html#_5-3-进阶","1594":"/IT-learning/Linux/03.MPI并行计算.html#_5-3-1-分布式实现","1595":"/IT-learning/Linux/03.MPI并行计算.html#六、学习策略与建议","1596":"/IT-learning/Linux/03.MPI并行计算.html#_6-1-新东西学习","1597":"/IT-learning/Linux/03.MPI并行计算.html#_6-2-遇事不决","1598":"/IT-learning/Linux/03.MPI并行计算.html#_6-3-知识体系构建","1599":"/IT-learning/Linux/#linux模块","1600":"/IT-learning/c++基础/01_开发环境搭建与基础数据类型.html#c-编程语言-01-从-python-到-c-的启航","1601":"/IT-learning/c++基础/01_开发环境搭建与基础数据类型.html#引言-欢迎来到-c-的世界","1602":"/IT-learning/c++基础/01_开发环境搭建与基础数据类型.html#一、-c-与-python-的对比-理解差异-扬长避短","1603":"/IT-learning/c++基础/01_开发环境搭建与基础数据类型.html#二、-c-的发展与平台-历史的脉络与选择","1604":"/IT-learning/c++基础/01_开发环境搭建与基础数据类型.html#三、-c-编译原理-从代码到执行","1605":"/IT-learning/c++基础/01_开发环境搭建与基础数据类型.html#四、-c-开发环境搭建-工欲善其事-必先利其器","1606":"/IT-learning/c++基础/01_开发环境搭建与基础数据类型.html#五、-第一个-c-程序-迈出-c-编程的第一步","1607":"/IT-learning/c++基础/01_开发环境搭建与基础数据类型.html#六、c-数据类型","1608":"/IT-learning/c++基础/01_开发环境搭建与基础数据类型.html#作业","1609":"/IT-learning/c++基础/03_指针与引用.html#c-教学课件-第三次课","1610":"/IT-learning/c++基础/03_指针与引用.html#c-第三节课-深入理解内存与地址——指针与引用","1611":"/IT-learning/c++基础/03_指针与引用.html#一、指针-pointer","1612":"/IT-learning/c++基础/03_指针与引用.html#二、引用-reference","1613":"/IT-learning/c++基础/03_指针与引用.html#三、变量与常量-variables-and-constants","1614":"/IT-learning/c++基础/03_指针与引用.html#四、常量指针和指针常量-const-pointers-and-pointer-constants","1615":"/IT-learning/c++基础/03_指针与引用.html#五、auto-自动类型推断-automatic-type-deduction","1616":"/IT-learning/c++基础/03_指针与引用.html#六、常用转义字符-escape-sequences","1617":"/IT-learning/c++基础/02_控制流语句与复合数据类型.html#c-编程语言-02","1618":"/IT-learning/c++基础/02_控制流语句与复合数据类型.html#第一部分-流程控制语句","1619":"/IT-learning/c++基础/02_控制流语句与复合数据类型.html#_1-if-语句-让程序做出判断","1620":"/IT-learning/c++基础/02_控制流语句与复合数据类型.html#_2-while-语句-重复执行直到条件不满足","1621":"/IT-learning/c++基础/02_控制流语句与复合数据类型.html#_3-for-循环-结构化的循环","1622":"/IT-learning/c++基础/02_控制流语句与复合数据类型.html#第二部分-复合数据类型","1623":"/IT-learning/c++基础/02_控制流语句与复合数据类型.html#_1-数组-存储相同类型数据的集合","1624":"/IT-learning/c++基础/02_控制流语句与复合数据类型.html#_2-字符串-处理文本数据","1625":"/IT-learning/c++基础/02_控制流语句与复合数据类型.html#课后作业-课后完成","1626":"/IT-learning/c++基础/05_头文件与指针的算术运算.html#c-第五课","1627":"/IT-learning/c++基础/05_头文件与指针的算术运算.html#课程目标","1628":"/IT-learning/c++基础/05_头文件与指针的算术运算.html#_0-头文件的原理与使用","1629":"/IT-learning/c++基础/05_头文件与指针的算术运算.html#_1-赋值总结","1630":"/IT-learning/c++基础/05_头文件与指针的算术运算.html#_2-运算符总结","1631":"/IT-learning/c++基础/05_头文件与指针的算术运算.html#_3-控制流语句总结","1632":"/IT-learning/c++基础/05_头文件与指针的算术运算.html#_4-宏定义","1633":"/IT-learning/c++基础/05_头文件与指针的算术运算.html#_5-指针的算术运算","1634":"/IT-learning/c++基础/05_头文件与指针的算术运算.html#_6-数组-回顾与补充","1635":"/IT-learning/c++基础/08_运算符优先级表.html#运算符优先级表","1636":"/IT-learning/c++基础/10_深入类和对象.html#c-第十课-深入类和对象","1637":"/IT-learning/c++基础/10_深入类和对象.html#一、回顾与引入-1","1638":"/IT-learning/c++基础/10_深入类和对象.html#二、隐藏的-this-指针-4","1639":"/IT-learning/c++基础/10_深入类和对象.html#三、静态成员-6","1640":"/IT-learning/c++基础/10_深入类和对象.html#四、常量成员-6","1641":"/IT-learning/c++基础/10_深入类和对象.html#五、构造函数参数初始化列表-4","1642":"/IT-learning/c++基础/10_深入类和对象.html#作业","1643":"/IT-learning/c++基础/07_函数进阶与内存管理.html#c-第七节课-函数进阶与内存管理","1644":"/IT-learning/c++基础/07_函数进阶与内存管理.html#引言","1645":"/IT-learning/c++基础/07_函数进阶与内存管理.html#一、回调函数-callback-functions","1646":"/IT-learning/c++基础/07_函数进阶与内存管理.html#二、函数的递归调用-recursive-function-calls","1647":"/IT-learning/c++基础/07_函数进阶与内存管理.html#三、new-和-delete-new-delete","1648":"/IT-learning/c++基础/07_函数进阶与内存管理.html#四、内存中的栈区和堆区-stack-and-heap","1649":"/IT-learning/c++基础/07_函数进阶与内存管理.html#五、全局变量、局部变量、static-静态变量","1650":"/IT-learning/c++基础/07_函数进阶与内存管理.html#六、函数的指针传参和引用传参","1651":"/IT-learning/c++基础/07_函数进阶与内存管理.html#作业","1652":"/IT-learning/c++基础/11_类的大小、继承与权限控制.html#c-第10课-类的大小、继承与权限控制","1653":"/IT-learning/c++基础/11_类的大小、继承与权限控制.html#_1-sizeof-自定义类","1654":"/IT-learning/c++基础/11_类的大小、继承与权限控制.html#_2-类的继承","1655":"/IT-learning/c++基础/11_类的大小、继承与权限控制.html#_3-protected-访问权限","1656":"/IT-learning/c++基础/11_类的大小、继承与权限控制.html#_4-final-关键字","1657":"/IT-learning/c++基础/11_类的大小、继承与权限控制.html#课后作业-设计一个简单的员工管理系统","1658":"/IT-learning/c++基础/12_继承进阶.html#c-第11次课-继承进阶","1659":"/IT-learning/c++基础/12_继承进阶.html#_1-多重继承","1660":"/IT-learning/c++基础/12_继承进阶.html#_2-名词歧义","1661":"/IT-learning/c++基础/12_继承进阶.html#_3-菱形继承","1662":"/IT-learning/c++基础/12_继承进阶.html#_4-虚继承","1663":"/IT-learning/c++基础/12_继承进阶.html#_5-继承中函数的重载和覆盖","1664":"/IT-learning/c++基础/12_继承进阶.html#课后作业","1665":"/IT-learning/c++基础/12_继承进阶.html#参考代码","1666":"/IT-learning/c++基础/09_指针、内存管理和类的基础.html#c-课程第九课-指针、内存管理和类的基础","1667":"/IT-learning/c++基础/09_指针、内存管理和类的基础.html#_1-数组传参的本质-指针","1668":"/IT-learning/c++基础/09_指针、内存管理和类的基础.html#_2-动态内存分配-new-和-new","1669":"/IT-learning/c++基础/09_指针、内存管理和类的基础.html#_3-动态内存释放-delete-和-delete","1670":"/IT-learning/c++基础/09_指针、内存管理和类的基础.html#_4-内存越界及其危害","1671":"/IT-learning/c++基础/09_指针、内存管理和类的基础.html#_5-内存泄漏及其危害","1672":"/IT-learning/c++基础/09_指针、内存管理和类的基础.html#_6-指针的安全使用原则","1673":"/IT-learning/c++基础/09_指针、内存管理和类的基础.html#_7-c-输入输出流-std-cin-和-std-cout","1674":"/IT-learning/c++基础/09_指针、内存管理和类的基础.html#_8-初探类-封装数据和行为","1675":"/IT-learning/c++基础/09_指针、内存管理和类的基础.html#_9-构造函数-对象的初始化","1676":"/IT-learning/c++基础/09_指针、内存管理和类的基础.html#_10-析构函数-对象的清理","1677":"/IT-learning/c++基础/09_指针、内存管理和类的基础.html#_11-公有成员和私有成员-访问控制","1678":"/IT-learning/c++基础/09_指针、内存管理和类的基础.html#sizeof-std-cout-的深入理解","1679":"/IT-learning/c++基础/09_指针、内存管理和类的基础.html#作业","1680":"/IT-learning/c++基础/09_指针、内存管理和类的基础.html#_1-封装一个-student-类","1681":"/IT-learning/c++基础/13_类型转换和多态与虚函数.html#c-课程-第12讲-类型转换、多态与虚函数","1682":"/IT-learning/c++基础/13_类型转换和多态与虚函数.html#一、类型转换-type-conversion","1683":"/IT-learning/c++基础/13_类型转换和多态与虚函数.html#_1-显示转换-explicit-conversion","1684":"/IT-learning/c++基础/13_类型转换和多态与虚函数.html#_2-隐式转换-implicit-conversion","1685":"/IT-learning/c++基础/13_类型转换和多态与虚函数.html#二、自定义类型转换","1686":"/IT-learning/c++基础/13_类型转换和多态与虚函数.html#三、多态-polymorphism","1687":"/IT-learning/c++基础/13_类型转换和多态与虚函数.html#_1-静态多态-static-polymorphism","1688":"/IT-learning/c++基础/13_类型转换和多态与虚函数.html#_2-动态多态-dynamic-polymorphism","1689":"/IT-learning/c++基础/13_类型转换和多态与虚函数.html#作业描述","1690":"/IT-learning/c++基础/13_类型转换和多态与虚函数.html#任务要求","1691":"/IT-learning/c++基础/13_类型转换和多态与虚函数.html#代码示例","1692":"/IT-learning/c++基础/14_纯虚函数、抽象类、深浅拷贝及智能指针.html#c-第-13-节课-纯虚函数、抽象类、深浅拷贝及智能指针","1693":"/IT-learning/c++基础/14_纯虚函数、抽象类、深浅拷贝及智能指针.html#一、纯虚函数与抽象类-9","1694":"/IT-learning/c++基础/14_纯虚函数、抽象类、深浅拷贝及智能指针.html#二、浅拷贝和深拷贝-9","1695":"/IT-learning/c++基础/14_纯虚函数、抽象类、深浅拷贝及智能指针.html#三、智能指针和裸指针-8","1696":"/IT-learning/c++基础/14_纯虚函数、抽象类、深浅拷贝及智能指针.html#作业","1697":"/IT-learning/c++基础/17_模板.html#c-模板-第-16-节课","1698":"/IT-learning/c++基础/17_模板.html#_1-模板-实现泛型编程","1699":"/IT-learning/c++基础/17_模板.html#_2-函数模板","1700":"/IT-learning/c++基础/17_模板.html#_3-类模板","1701":"/IT-learning/c++基础/17_模板.html#_4-成员函数模板","1702":"/IT-learning/c++基础/17_模板.html#作业","1703":"/IT-learning/c++基础/06_字符串、数组、指针与函数.html#c-课程-6-深入理解字符串、数组、指针与函数","1704":"/IT-learning/c++基础/06_字符串、数组、指针与函数.html#一、深入理解字符串常量","1705":"/IT-learning/c++基础/06_字符串、数组、指针与函数.html#二、深入理解二维数组与行指针","1706":"/IT-learning/c++基础/06_字符串、数组、指针与函数.html#三、函数与指针的深度应用","1707":"/IT-learning/c++基础/06_字符串、数组、指针与函数.html#四、函数的重载-提高代码的灵活性","1708":"/IT-learning/c++基础/06_字符串、数组、指针与函数.html#五、函数的默认参数值-简化函数调用","1709":"/IT-learning/c++基础/06_字符串、数组、指针与函数.html#六、内联函数-提升程序性能","1710":"/IT-learning/c++基础/06_字符串、数组、指针与函数.html#作业","1711":"/IT-learning/c++基础/15_运算符重载与 String 类详解.html#c-第-14-课-运算符重载与-string-类详解","1712":"/IT-learning/c++基础/15_运算符重载与 String 类详解.html#_1-运算符重载","1713":"/IT-learning/c++基础/15_运算符重载与 String 类详解.html#_2-string-类详解","1714":"/IT-learning/c++基础/15_运算符重载与 String 类详解.html#_3-静态数组和动态数组","1715":"/IT-learning/c++基础/15_运算符重载与 String 类详解.html#_4-补充-c-风格字符串","1716":"/IT-learning/c++基础/15_运算符重载与 String 类详解.html#作业","1717":"/IT-learning/c++基础/18_迭代器与其应用.html#c-第-17-次课-迭代器与容器的应用","1718":"/IT-learning/c++基础/18_迭代器与其应用.html#_1-迭代器简介","1719":"/IT-learning/c++基础/18_迭代器与其应用.html#_2-vector","1720":"/IT-learning/c++基础/18_迭代器与其应用.html#_3-list","1721":"/IT-learning/c++基础/18_迭代器与其应用.html#_4-forward-list","1722":"/IT-learning/c++基础/18_迭代器与其应用.html#_5-deque","1723":"/IT-learning/c++基础/18_迭代器与其应用.html#_6-queue","1724":"/IT-learning/c++基础/18_迭代器与其应用.html#_7-set","1725":"/IT-learning/c++基础/18_迭代器与其应用.html#_8-map","1726":"/IT-learning/c++基础/18_迭代器与其应用.html#作业","1727":"/IT-learning/c++基础/04_自定义数据类型与函数.html#c-第四课-自定义数据类型与函数","1728":"/IT-learning/c++基础/04_自定义数据类型与函数.html#第一部分-自定义数据类型的魅力","1729":"/IT-learning/c++基础/04_自定义数据类型与函数.html#第二部分-函数的魔力-代码的组织者和复用者","1730":"/IT-learning/c++基础/19_C++ 标准库常用算法.html#c-18-c-标准库常用算法","1731":"/IT-learning/c++基础/19_C++ 标准库常用算法.html#课程目标","1732":"/IT-learning/c++基础/19_C++ 标准库常用算法.html#课程内容","1733":"/IT-learning/c++基础/19_C++ 标准库常用算法.html#_1-引言与简介-1","1734":"/IT-learning/c++基础/19_C++ 标准库常用算法.html#什么是-algorithm-库","1735":"/IT-learning/c++基础/19_C++ 标准库常用算法.html#_2-常用的非修改算法-4","1736":"/IT-learning/c++基础/19_C++ 标准库常用算法.html#_2-1-std-all-of","1737":"/IT-learning/c++基础/19_C++ 标准库常用算法.html#_2-2-std-any-of","1738":"/IT-learning/c++基础/19_C++ 标准库常用算法.html#_2-3-std-none-of","1739":"/IT-learning/c++基础/19_C++ 标准库常用算法.html#_2-4-std-for-each","1740":"/IT-learning/c++基础/19_C++ 标准库常用算法.html#_3-常用的修改算法-4","1741":"/IT-learning/c++基础/19_C++ 标准库常用算法.html#_3-1-std-transform","1742":"/IT-learning/c++基础/19_C++ 标准库常用算法.html#_3-2-std-copy","1743":"/IT-learning/c++基础/19_C++ 标准库常用算法.html#_3-3-std-replace","1744":"/IT-learning/c++基础/19_C++ 标准库常用算法.html#_3-4-std-fill","1745":"/IT-learning/c++基础/19_C++ 标准库常用算法.html#_4-数值算法-3","1746":"/IT-learning/c++基础/19_C++ 标准库常用算法.html#_4-1-std-accumulate","1747":"/IT-learning/c++基础/19_C++ 标准库常用算法.html#_4-2-std-iota","1748":"/IT-learning/c++基础/19_C++ 标准库常用算法.html#_5-排序算法-6","1749":"/IT-learning/c++基础/19_C++ 标准库常用算法.html#_5-1-std-sort","1750":"/IT-learning/c++基础/19_C++ 标准库常用算法.html#_5-2-std-stable-sort","1751":"/IT-learning/c++基础/19_C++ 标准库常用算法.html#_5-3-std-partial-sort","1752":"/IT-learning/c++基础/19_C++ 标准库常用算法.html#_5-4-std-nth-element","1753":"/IT-learning/c++基础/19_C++ 标准库常用算法.html#_6-查找算法-3","1754":"/IT-learning/c++基础/19_C++ 标准库常用算法.html#_6-1-std-find","1755":"/IT-learning/c++基础/19_C++ 标准库常用算法.html#_6-2-std-binary-search","1756":"/IT-learning/c++基础/19_C++ 标准库常用算法.html#_6-3-std-equal-range","1757":"/IT-learning/c++基础/19_C++ 标准库常用算法.html#_7-总结与习题-3","1758":"/IT-learning/c++基础/19_C++ 标准库常用算法.html#总结","1759":"/IT-learning/c++基础/19_C++ 标准库常用算法.html#作业","1760":"/IT-learning/c++基础/20_C++ 异常处理 - 第19次课.html#c-异常处理-第19次课","1761":"/IT-learning/c++基础/20_C++ 异常处理 - 第19次课.html#第一部分-引言-为什么需要异常处理","1762":"/IT-learning/c++基础/20_C++ 异常处理 - 第19次课.html#第二部分-c-异常处理机制详解","1763":"/IT-learning/c++基础/20_C++ 异常处理 - 第19次课.html#第三部分-自定义异常类型","1764":"/IT-learning/c++基础/20_C++ 异常处理 - 第19次课.html#第四部分-异常处理的最佳实践","1765":"/IT-learning/c++基础/20_C++ 异常处理 - 第19次课.html#第五部分-总结","1766":"/IT-learning/c++基础/20_C++ 异常处理 - 第19次课.html#课后作业","1767":"/IT-learning/c++基础/21_友元及友元相关内容.html#第-20-节-课件-友元及友元相关内容","1768":"/IT-learning/c++基础/21_友元及友元相关内容.html#目录","1769":"/IT-learning/c++基础/21_友元及友元相关内容.html#友元简介","1770":"/IT-learning/c++基础/21_友元及友元相关内容.html#关键字-friend","1771":"/IT-learning/c++基础/21_友元及友元相关内容.html#_1-友元函数","1772":"/IT-learning/c++基础/21_友元及友元相关内容.html#_1-1-定义友元函数","1773":"/IT-learning/c++基础/21_友元及友元相关内容.html#_1-2-友元函数的权限","1774":"/IT-learning/c++基础/21_友元及友元相关内容.html#_1-3-友元函数的特性与示例","1775":"/IT-learning/c++基础/21_友元及友元相关内容.html#_1-4-友元函数要点","1776":"/IT-learning/c++基础/21_友元及友元相关内容.html#_1-5-友元函数的优缺点","1777":"/IT-learning/c++基础/21_友元及友元相关内容.html#_2-友元类","1778":"/IT-learning/c++基础/21_友元及友元相关内容.html#_2-1-定义友元类","1779":"/IT-learning/c++基础/21_友元及友元相关内容.html#_2-2-友元类的特性与示例","1780":"/IT-learning/c++基础/21_友元及友元相关内容.html#_2-3-友元类强调","1781":"/IT-learning/c++基础/21_友元及友元相关内容.html#友元关系的特性","1782":"/IT-learning/c++基础/21_友元及友元相关内容.html#_3-友元的继承","1783":"/IT-learning/c++基础/21_友元及友元相关内容.html#_3-1-友元的访问权限继承","1784":"/IT-learning/c++基础/21_友元及友元相关内容.html#_3-2-友元继承示例","1785":"/IT-learning/c++基础/21_友元及友元相关内容.html#输出结果","1786":"/IT-learning/c++基础/21_友元及友元相关内容.html#_3-3-友元与继承的关系","1787":"/IT-learning/c++基础/21_友元及友元相关内容.html#_4-运算符重载友元","1788":"/IT-learning/c++基础/21_友元及友元相关内容.html#_4-1-友元作为运算符重载函数","1789":"/IT-learning/c++基础/21_友元及友元相关内容.html#_4-2-示例-友元运算符重载","1790":"/IT-learning/c++基础/21_友元及友元相关内容.html#作业","1791":"/IT-learning/c++基础/21_友元及友元相关内容.html#作业-1-自定义立方体类-用友元函数访问它的私有成员","1792":"/IT-learning/c++基础/21_友元及友元相关内容.html#作业-2-自定义立方体类-用友元函数实现立方体相加","1793":"/IT-learning/c++基础/21_友元及友元相关内容.html#总结与注意事项","1794":"/IT-learning/c++基础/16_有序容器与无序容器.html#c-第-15次授课-有序容器与关联容器","1795":"/IT-learning/c++基础/16_有序容器与无序容器.html#课程目录","1796":"/IT-learning/c++基础/16_有序容器与无序容器.html#_1-有序容器","1797":"/IT-learning/c++基础/16_有序容器与无序容器.html#_1-1-vector","1798":"/IT-learning/c++基础/16_有序容器与无序容器.html#_1-2-其他常用的有序容器","1799":"/IT-learning/c++基础/16_有序容器与无序容器.html#_1-2-1-list","1800":"/IT-learning/c++基础/16_有序容器与无序容器.html#_1-2-2-deque","1801":"/IT-learning/c++基础/16_有序容器与无序容器.html#_1-2-3-forward-list","1802":"/IT-learning/c++基础/16_有序容器与无序容器.html#_1-2-4-array","1803":"/IT-learning/c++基础/16_有序容器与无序容器.html#_1-2-5-string","1804":"/IT-learning/c++基础/16_有序容器与无序容器.html#_2-关联容器","1805":"/IT-learning/c++基础/16_有序容器与无序容器.html#_2-1-pair","1806":"/IT-learning/c++基础/16_有序容器与无序容器.html#_2-2-set","1807":"/IT-learning/c++基础/16_有序容器与无序容器.html#_2-3-map","1808":"/IT-learning/c++基础/16_有序容器与无序容器.html#作业","1809":"/IT-learning/c++基础/22_C++ IO 流详解-feadbc607d7f.html#第-21-课-c-io-流详解","1810":"/IT-learning/c++基础/22_C++ IO 流详解-feadbc607d7f.html#_1-什么是-io-流","1811":"/IT-learning/c++基础/22_C++ IO 流详解-feadbc607d7f.html#_2-c-标准-io-流对象","1812":"/IT-learning/c++基础/22_C++ IO 流详解-feadbc607d7f.html#_3-输入流-input-stream","1813":"/IT-learning/c++基础/22_C++ IO 流详解-feadbc607d7f.html#_3-1-cin-常用方法","1814":"/IT-learning/c++基础/22_C++ IO 流详解-feadbc607d7f.html#_3-2-输入流的状态","1815":"/IT-learning/c++基础/22_C++ IO 流详解-feadbc607d7f.html#_4-输出流-output-stream","1816":"/IT-learning/c++基础/22_C++ IO 流详解-feadbc607d7f.html#_4-1-cout-常用方法","1817":"/IT-learning/c++基础/22_C++ IO 流详解-feadbc607d7f.html#_4-2-格式化输出","1818":"/IT-learning/c++基础/22_C++ IO 流详解-feadbc607d7f.html#_5-文件-io","1819":"/IT-learning/c++基础/22_C++ IO 流详解-feadbc607d7f.html#_5-1-文件流类","1820":"/IT-learning/c++基础/22_C++ IO 流详解-feadbc607d7f.html#_5-2-文件操作步骤","1821":"/IT-learning/c++基础/22_C++ IO 流详解-feadbc607d7f.html#_5-3-文件的随机访问","1822":"/IT-learning/c++基础/22_C++ IO 流详解-feadbc607d7f.html#_6-字符串流-string-stream","1823":"/IT-learning/c++基础/22_C++ IO 流详解-feadbc607d7f.html#_6-1-字符串流类","1824":"/IT-learning/c++基础/22_C++ IO 流详解-feadbc607d7f.html#_6-2-字符串流用法","1825":"/IT-learning/c++基础/22_C++ IO 流详解-feadbc607d7f.html#_7、作业","1826":"/IT-learning/c++基础/22_C++ IO 流详解-feadbc607d7f.html#_8-总结","1827":"/IT-learning/c++基础/23_C++ IO 流详解.html#第-21-课-c-io-流详解","1828":"/IT-learning/c++基础/23_C++ IO 流详解.html#_1-什么是-io-流","1829":"/IT-learning/c++基础/23_C++ IO 流详解.html#_2-c-标准-io-流对象","1830":"/IT-learning/c++基础/23_C++ IO 流详解.html#_3-输入流-input-stream","1831":"/IT-learning/c++基础/23_C++ IO 流详解.html#_3-1-cin-常用方法","1832":"/IT-learning/c++基础/23_C++ IO 流详解.html#_3-2-输入流的状态","1833":"/IT-learning/c++基础/23_C++ IO 流详解.html#_4-输出流-output-stream","1834":"/IT-learning/c++基础/23_C++ IO 流详解.html#_4-1-cout-常用方法","1835":"/IT-learning/c++基础/23_C++ IO 流详解.html#_4-2-格式化输出","1836":"/IT-learning/c++基础/23_C++ IO 流详解.html#_5-文件-io","1837":"/IT-learning/c++基础/23_C++ IO 流详解.html#_5-1-文件流类","1838":"/IT-learning/c++基础/23_C++ IO 流详解.html#_5-2-文件操作步骤","1839":"/IT-learning/c++基础/23_C++ IO 流详解.html#_5-3-文件的随机访问","1840":"/IT-learning/c++基础/23_C++ IO 流详解.html#_6-字符串流-string-stream","1841":"/IT-learning/c++基础/23_C++ IO 流详解.html#_6-1-字符串流类","1842":"/IT-learning/c++基础/23_C++ IO 流详解.html#_6-2-字符串流用法","1843":"/IT-learning/c++基础/23_C++ IO 流详解.html#_7、作业","1844":"/IT-learning/c++基础/23_C++ IO 流详解.html#_8-总结","1845":"/IT-learning/c++基础/25_C++三种继承方式.html#c-继承方式-public-protected-private","1846":"/IT-learning/c++基础/25_C++三种继承方式.html#_1-课件介绍","1847":"/IT-learning/c++基础/25_C++三种继承方式.html#_2-继承的基本概念","1848":"/IT-learning/c++基础/25_C++三种继承方式.html#_2-1-继承的定义","1849":"/IT-learning/c++基础/25_C++三种继承方式.html#_2-2-示例代码","1850":"/IT-learning/c++基础/25_C++三种继承方式.html#_3-public-继承","1851":"/IT-learning/c++基础/25_C++三种继承方式.html#_3-1-public-继承的定义","1852":"/IT-learning/c++基础/25_C++三种继承方式.html#_3-2-public-继承的特点","1853":"/IT-learning/c++基础/25_C++三种继承方式.html#_3-3-示例代码","1854":"/IT-learning/c++基础/25_C++三种继承方式.html#_4-protected-继承","1855":"/IT-learning/c++基础/25_C++三种继承方式.html#_4-1-protected-继承的定义","1856":"/IT-learning/c++基础/25_C++三种继承方式.html#_4-2-protected-继承的特点","1857":"/IT-learning/c++基础/25_C++三种继承方式.html#_4-3-示例代码","1858":"/IT-learning/c++基础/25_C++三种继承方式.html#_5-private-继承","1859":"/IT-learning/c++基础/25_C++三种继承方式.html#_5-1-private-继承的定义","1860":"/IT-learning/c++基础/25_C++三种继承方式.html#_5-2-private-继承的特点","1861":"/IT-learning/c++基础/25_C++三种继承方式.html#_5-3-示例代码","1862":"/IT-learning/c++基础/25_C++三种继承方式.html#_6-三种继承方式的对比","1863":"/IT-learning/c++基础/25_C++三种继承方式.html#_6-1-访问控制对比","1864":"/IT-learning/c++基础/25_C++三种继承方式.html#_6-2-使用场景总结","1865":"/IT-learning/c++基础/25_C++三种继承方式.html#_6-4-实际应用场景讨论","1866":"/IT-learning/c++基础/25_C++三种继承方式.html#_1-接口继承-public-继承","1867":"/IT-learning/c++基础/25_C++三种继承方式.html#_2-部分封闭设计-protected-继承","1868":"/IT-learning/c++基础/25_C++三种继承方式.html#_3-完全封闭继承-private-继承","1869":"/IT-learning/c++基础/25_C++三种继承方式.html#_4-多继承与菱形问题","1870":"/IT-learning/c++基础/25_C++三种继承方式.html#课后作业-c-继承方式应用","1871":"/IT-learning/c++基础/25_C++三种继承方式.html#作业目标","1872":"/IT-learning/c++基础/25_C++三种继承方式.html#作业内容","1873":"/IT-learning/c++基础/25_C++三种继承方式.html#任务描述","1874":"/IT-learning/c++基础/25_C++三种继承方式.html#提示","1875":"/IT-learning/c++基础/26_C++11 高级特性.html#c-11-新特性概述","1876":"/IT-learning/c++基础/26_C++11 高级特性.html#课程目标","1877":"/IT-learning/c++基础/26_C++11 高级特性.html#_1-自动类型推导-auto","1878":"/IT-learning/c++基础/26_C++11 高级特性.html#_1-1-简介","1879":"/IT-learning/c++基础/26_C++11 高级特性.html#_1-2-示例","1880":"/IT-learning/c++基础/26_C++11 高级特性.html#_1-3-意义","1881":"/IT-learning/c++基础/26_C++11 高级特性.html#_2-lambda-表达式","1882":"/IT-learning/c++基础/26_C++11 高级特性.html#_2-1-简介","1883":"/IT-learning/c++基础/26_C++11 高级特性.html#_2-2-示例","1884":"/IT-learning/c++基础/26_C++11 高级特性.html#_2-3-意义","1885":"/IT-learning/c++基础/26_C++11 高级特性.html#_3-右值引用与移动语义","1886":"/IT-learning/c++基础/26_C++11 高级特性.html#_3-1-简介","1887":"/IT-learning/c++基础/26_C++11 高级特性.html#_3-2-示例","1888":"/IT-learning/c++基础/26_C++11 高级特性.html#_3-3-意义","1889":"/IT-learning/c++基础/26_C++11 高级特性.html#_4-标准库的改进","1890":"/IT-learning/c++基础/26_C++11 高级特性.html#_4-1-智能指针","1891":"/IT-learning/c++基础/26_C++11 高级特性.html#_4-2-示例","1892":"/IT-learning/c++基础/26_C++11 高级特性.html#_4-3-意义","1893":"/IT-learning/c++基础/26_C++11 高级特性.html#_5-多线程支持","1894":"/IT-learning/c++基础/26_C++11 高级特性.html#_5-1-简介","1895":"/IT-learning/c++基础/26_C++11 高级特性.html#_5-2-示例","1896":"/IT-learning/c++基础/26_C++11 高级特性.html#_5-3-意义","1897":"/IT-learning/c++基础/26_C++11 高级特性.html#_6-其他重要特性","1898":"/IT-learning/c++基础/26_C++11 高级特性.html#_6-1-示例-range-based-for-loop","1899":"/IT-learning/c++基础/26_C++11 高级特性.html#_6-2-意义","1900":"/IT-learning/c++基础/26_C++11 高级特性.html#课堂练习","1901":"/IT-learning/c++基础/26_C++11 高级特性.html#总结回顾","1902":"/IT-learning/c++基础/26_C++11 高级特性.html#作业","1903":"/IT-learning/c++基础/27_C++14 新特性.html#c-14-新特性","1904":"/IT-learning/c++基础/27_C++14 新特性.html#目录","1905":"/IT-learning/c++基础/27_C++14 新特性.html#_1-函数返回类型推导","1906":"/IT-learning/c++基础/27_C++14 新特性.html#_2-泛型-lambda-表达式-generic-lambdas","1907":"/IT-learning/c++基础/27_C++14 新特性.html#_3-lambda-捕获表达式-lambda-capture-expressions","1908":"/IT-learning/c++基础/27_C++14 新特性.html#_4-变量模板-variable-templates","1909":"/IT-learning/c++基础/27_C++14 新特性.html#_5-deprecated-属性","1910":"/IT-learning/c++基础/27_C++14 新特性.html#_6-二进制字面量和数字分隔符","1911":"/IT-learning/c++基础/27_C++14 新特性.html#_7-std-make-unique","1912":"/IT-learning/c++基础/27_C++14 新特性.html#_8-编译期整数序列-std-integer-sequence","1913":"/IT-learning/c++基础/27_C++14 新特性.html#_9-总结","1914":"/IT-learning/c++基础/27_C++14 新特性.html#_10-作业","1915":"/IT-learning/c++基础/29_多文件和 Makefile工程管理.html#_27-课-多文件和-makefile工程管理","1916":"/IT-learning/c++基础/29_多文件和 Makefile工程管理.html#一、c-多文件编程-9","1917":"/IT-learning/c++基础/29_多文件和 Makefile工程管理.html#二、makefile-15","1918":"/IT-learning/c++基础/29_多文件和 Makefile工程管理.html#作业","1919":"/IT-learning/c++基础/28_C++17 新特性.html#c-17-新特性","1920":"/IT-learning/c++基础/28_C++17 新特性.html#目录","1921":"/IT-learning/c++基础/28_C++17 新特性.html#_1-结构化绑定-structured-bindings","1922":"/IT-learning/c++基础/28_C++17 新特性.html#_2-if-和-switch-语句初始化","1923":"/IT-learning/c++基础/28_C++17 新特性.html#_3-内联变量-inline-variables","1924":"/IT-learning/c++基础/28_C++17 新特性.html#_4-constexpr-lambda-表达式","1925":"/IT-learning/c++基础/28_C++17 新特性.html#_5-类模板参数推导-class-template-argument-deduction","1926":"/IT-learning/c++基础/28_C++17 新特性.html#_6-std-variant","1927":"/IT-learning/c++基础/28_C++17 新特性.html#_7-std-optional","1928":"/IT-learning/c++基础/28_C++17 新特性.html#_8-std-any","1929":"/IT-learning/c++基础/28_C++17 新特性.html#_9-std-string-view","1930":"/IT-learning/c++基础/28_C++17 新特性.html#_10-文件系统库-file-system-library","1931":"/IT-learning/c++基础/28_C++17 新特性.html#_11-其他特性","1932":"/IT-learning/c++基础/28_C++17 新特性.html#_12-总结","1933":"/IT-learning/c++基础/28_C++17 新特性.html#课后作业-统计单词频率","1934":"/IT-learning/c++基础/31_C++ 主要就业方向与技术能力分析报告.html#c-主要就业方向与技术能力分析报告","1935":"/IT-learning/c++基础/31_C++ 主要就业方向与技术能力分析报告.html#_1-游戏开发领域","1936":"/IT-learning/c++基础/31_C++ 主要就业方向与技术能力分析报告.html#核心方向","1937":"/IT-learning/c++基础/31_C++ 主要就业方向与技术能力分析报告.html#核心技能要求","1938":"/IT-learning/c++基础/31_C++ 主要就业方向与技术能力分析报告.html#典型岗位","1939":"/IT-learning/c++基础/31_C++ 主要就业方向与技术能力分析报告.html#代表企业","1940":"/IT-learning/c++基础/31_C++ 主要就业方向与技术能力分析报告.html#_2-系统级软件开发","1941":"/IT-learning/c++基础/31_C++ 主要就业方向与技术能力分析报告.html#核心方向-1","1942":"/IT-learning/c++基础/31_C++ 主要就业方向与技术能力分析报告.html#核心技能要求-1","1943":"/IT-learning/c++基础/31_C++ 主要就业方向与技术能力分析报告.html#典型岗位-1","1944":"/IT-learning/c++基础/31_C++ 主要就业方向与技术能力分析报告.html#代表企业-1","1945":"/IT-learning/c++基础/31_C++ 主要就业方向与技术能力分析报告.html#_3-金融科技领域","1946":"/IT-learning/c++基础/31_C++ 主要就业方向与技术能力分析报告.html#核心方向-2","1947":"/IT-learning/c++基础/31_C++ 主要就业方向与技术能力分析报告.html#核心技能要求-2","1948":"/IT-learning/c++基础/31_C++ 主要就业方向与技术能力分析报告.html#典型岗位-2","1949":"/IT-learning/c++基础/31_C++ 主要就业方向与技术能力分析报告.html#代表企业-2","1950":"/IT-learning/c++基础/31_C++ 主要就业方向与技术能力分析报告.html#_4-基础设施开发","1951":"/IT-learning/c++基础/31_C++ 主要就业方向与技术能力分析报告.html#核心方向-3","1952":"/IT-learning/c++基础/31_C++ 主要就业方向与技术能力分析报告.html#核心技能要求-3","1953":"/IT-learning/c++基础/31_C++ 主要就业方向与技术能力分析报告.html#典型岗位-3","1954":"/IT-learning/c++基础/31_C++ 主要就业方向与技术能力分析报告.html#代表企业-3","1955":"/IT-learning/c++基础/31_C++ 主要就业方向与技术能力分析报告.html#_5-工业软件领域","1956":"/IT-learning/c++基础/31_C++ 主要就业方向与技术能力分析报告.html#核心方向-4","1957":"/IT-learning/c++基础/31_C++ 主要就业方向与技术能力分析报告.html#核心技能要求-4","1958":"/IT-learning/c++基础/31_C++ 主要就业方向与技术能力分析报告.html#典型岗位-4","1959":"/IT-learning/c++基础/31_C++ 主要就业方向与技术能力分析报告.html#代表企业-4","1960":"/IT-learning/c++基础/31_C++ 主要就业方向与技术能力分析报告.html#技术趋势分析","1961":"/IT-learning/c++基础/31_C++ 主要就业方向与技术能力分析报告.html#职业发展建议","1962":"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html#c-28课-高性能部署之cmake工程管理","1963":"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html#_1-什么是-cmake","1964":"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html#_1-1-为什么选择-cmake","1965":"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html#_2-基本结构","1966":"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html#_2-1-最简单的-cmakelists-txt-示例","1967":"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html#_2-2-生成与构建","1968":"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html#_2-2-1-linux-macos","1969":"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html#_2-2-2-windows","1970":"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html#_3-cmake-基本命令详解","1971":"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html#_3-1-cmake-minimum-required","1972":"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html#_3-2-project","1973":"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html#_3-3-add-executable","1974":"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html#_3-4-add-library","1975":"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html#_3-5-target-link-libraries","1976":"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html#_3-6-include-directories","1977":"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html#_3-7-find-package","1978":"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html#_4-组织大型项目","1979":"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html#_4-1-子目录和子模块","1980":"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html#_4-1-1-主目录的-cmakelists-txt","1981":"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html#_4-1-2-子模块的-cmakelists-txt-例如-modulea","1982":"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html#_4-2-设置全局属性","1983":"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html#_5-cmake-高级用法","1984":"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html#_5-1-定义编译选项","1985":"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html#_5-2-条件编译","1986":"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html#_5-3-测试支持","1987":"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html#_5-4-安装目标","1988":"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html#_6-cmake-与第三方依赖管理","1989":"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html#_6-1-使用-externalproject-add","1990":"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html#_6-2-使用-fetchcontent","1991":"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html#_6-3-使用-find-package","1992":"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html#_7-cmake-与自定义命令","1993":"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html#_7-1-添加自定义命令","1994":"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html#_7-2-自定义目标","1995":"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html#_8-生成与构建","1996":"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html#_8-1-常见命令总结","1997":"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html#cmake-课后作业","1998":"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html#作业","1999":"/IT-learning/c++基础/30_C++大型项目CMake工程管理.html#cmake-课后作业参考答案","2000":"/IT-learning/c++基础/#c-基础笔记","2001":"/IT-learning/#it学习指南","2002":"/IT-learning/c++基础/32_C++ 基础知识回顾.html#c-基础知识回顾","2003":"/IT-learning/c++基础/32_C++ 基础知识回顾.html#作用域与变量生命周期","2004":"/IT-learning/c++基础/32_C++ 基础知识回顾.html#函数重载与类型转换","2005":"/IT-learning/c++基础/32_C++ 基础知识回顾.html#指针与引用的区别","2006":"/IT-learning/c++基础/32_C++ 基础知识回顾.html#数组名的本质","2007":"/IT-learning/c++基础/32_C++ 基础知识回顾.html#右值引用与移动语义","2008":"/IT-learning/c++基础/32_C++ 基础知识回顾.html#静态局部变量","2009":"/IT-learning/计算机图形学/#计算机图形学的主要内容","2010":"/IT-learning/计算机图形学/#为什么要学习计算机图形学","2011":"/IT-learning/计算机图形学/#最新研究内容和成果","2012":"/IT-learning/计算机图形学/02.直线光栅化.html#yi直线光栅化","2013":"/IT-learning/计算机图形学/02.直线光栅化.html#_1-引入","2014":"/IT-learning/计算机图形学/02.直线光栅化.html#_2-实现","2015":"/IT-learning/计算机图形学/02.直线光栅化.html#_2-1-基本实现思路","2016":"/IT-learning/计算机图形学/02.直线光栅化.html#_2-2-数值微分法-dda算法","2017":"/IT-learning/计算机图形学/02.直线光栅化.html#_2-2-1-dda-算法概述","2018":"/IT-learning/计算机图形学/02.直线光栅化.html#_2-2-2-算法步骤","2019":"/IT-learning/计算机图形学/02.直线光栅化.html#_2-2-3-伪代码演示","2020":"/IT-learning/计算机图形学/02.直线光栅化.html#_2-2-4-优化方向","2021":"/IT-learning/计算机图形学/02.直线光栅化.html#_2-3-中心点画线法","2022":"/IT-learning/计算机图形学/02.直线光栅化.html#_2-3-1-算法思想","2023":"/IT-learning/计算机图形学/02.直线光栅化.html#_2-3-2-算法步骤","2024":"/IT-learning/计算机图形学/02.直线光栅化.html#_2-3-3-举例","2025":"/IT-learning/计算机图形学/02.直线光栅化.html#_2-3-4-伪代码演示","2026":"/IT-learning/计算机图形学/02.直线光栅化.html#_2-3-bresenham算法","2027":"/IT-learning/计算机图形学/02.直线光栅化.html#_2-3-1-基本思想","2028":"/IT-learning/计算机图形学/02.直线光栅化.html#_2-3-2-改进策略","2029":"/IT-learning/计算机图形学/02.直线光栅化.html#_2-3-3-伪代码实现","2030":"/IT-learning/计算机图形学/02.直线光栅化.html#_2-3-4-举例","2031":"/IT-learning/计算机图形学/02.直线光栅化.html#_3-思考","2032":"/README.html#码医森","2033":"/README.html#主页","2034":"/README.html#站点大纲","2035":"/guide/#整体该博客-站点-指南","2036":"/guide/博客结构.html#该站点的整体结构","2037":"/guide/博客结构.html#主页","2038":"/guide/博客结构.html#站点大纲","2039":"/improve/#个人提升","2040":"/update/更新日志.html#更新日志","2041":"/update/更新日志.html#_2024-10","2042":"/update/更新日志.html#大更新","2043":"/update/更新日志.html#小更新","2044":"/update/更新日志.html#_2024-11","2045":"/update/更新日志.html#大更新-1","2046":"/update/更新日志.html#小更新-1","2047":"/update/更新日志.html#_2025-03","2048":"/update/更新日志.html#大更新-2","2049":"/update/更新日志.html#小更新-2","2050":"/我的感悟/2024/#更新日志","2051":"/我的感悟/2024/不同商家的视野.html#不同商家之间的视野——带室友修电脑","2052":"/我的感悟/2024/学而篇.html#论语之悟——学而篇","2053":"/我的感悟/2024/学而篇.html#_1-学而时习之","2054":"/我的感悟/2024/学而篇.html#_1-1-个人参悟","2055":"/我的感悟/2024/学而篇.html#_1-2-不解之言","2056":"/我的感悟/2024/重温士兵突击.html#士兵突击告诉让我明白的那些事","2057":"/IT-learning/c++基础/24_位运算符总结.html#第-22-节-位运算","2058":"/IT-learning/c++基础/24_位运算符总结.html#_1-课程导入-1","2059":"/IT-learning/c++基础/24_位运算符总结.html#_2-位运算符介绍-3","2060":"/IT-learning/c++基础/24_位运算符总结.html#_3-应用示例与实践-9","2061":"/IT-learning/c++基础/24_位运算符总结.html#_4-深入探讨-位运算的高级用法-4","2062":"/IT-learning/c++基础/24_位运算符总结.html#_6-std-bitset-的使用-3","2063":"/IT-learning/c++基础/24_位运算符总结.html#课后作业-4","2064":"/技术问题清单/doccano账户管理.html#docker下的doccano添加普通用户","2065":"/技术问题清单/doccano账户管理.html#_1-操作步骤","2066":"/技术问题清单/doccano账户管理.html#_1-1-启动docker的doccano","2067":"/技术问题清单/doccano账户管理.html#_1-2-进入doccano容器","2068":"/技术问题清单/doccano账户管理.html#_1-3-配置-添加-超级管理员或者普通管理员账号","2069":"/技术问题清单/doccano账户管理.html#_1-3-1-管理员账户","2070":"/技术问题清单/doccano账户管理.html#_1-3-2-普通用户账户","2071":"/技术问题清单/#各类技术问题和踩坑","2072":"/技术问题清单/专英翻转课堂—PyTorch.html#introduction-to-pytorch","2073":"/技术问题清单/专英翻转课堂—PyTorch.html#_1-what-is-pytorch","2074":"/技术问题清单/专英翻转课堂—PyTorch.html#_2-key-features-of-pytorch","2075":"/技术问题清单/专英翻转课堂—PyTorch.html#_2-1-dynamic-computation-graphs","2076":"/技术问题清单/专英翻转课堂—PyTorch.html#_2-2-autograd-automatic-gradient-calculation","2077":"/技术问题清单/专英翻转课堂—PyTorch.html#_2-3-tensors","2078":"/技术问题清单/专英翻转课堂—PyTorch.html#_2-4-easy-to-use-with-python","2079":"/技术问题清单/专英翻转课堂—PyTorch.html#_2-5-rich-ecosystem-of-libraries","2080":"/技术问题清单/专英翻转课堂—PyTorch.html#_3-the-application-domains-of-pytorch","2081":"/技术问题清单/专英翻转课堂—PyTorch.html#_3-1-research","2082":"/技术问题清单/专英翻转课堂—PyTorch.html#_3-2-industry","2083":"/技术问题清单/专英翻转课堂—PyTorch.html#_3-3-education","2084":"/技术问题清单/专英翻转课堂—PyTorch.html#_4-how-to-use-pytorch","2085":"/技术问题清单/专英翻转课堂—PyTorch.html#_4-1-official-tutorials","2086":"/技术问题清单/专英翻转课堂—PyTorch.html#_4-2-basic-framework","2087":"/技术问题清单/专英翻转课堂—PyTorch.html#_5-conclusion","2088":"/技术问题清单/虚拟机网络问题.html#有关虚拟机创建的网络问题","2089":"/技术问题清单/虚拟机网络问题.html#_1-基础环境","2090":"/技术问题清单/虚拟机网络问题.html#_2-相关知识","2091":"/技术问题清单/虚拟机网络问题.html#_3-常见网络问题","2092":"/技术问题清单/虚拟机网络问题.html#_4-解决方案","2093":"/技术问题清单/虚拟机网络问题.html#_4-1-法一-在物理主机开启nat和dhcp服务","2094":"/技术问题清单/虚拟机网络问题.html#_4-2-法二-恢复默认网络配置","2095":"/技术问题清单/虚拟机网络问题.html#_5-固定虚拟机网络ip-自动ip分配的可以不用操作","2096":"/生活与算法/#生活与算法","2097":"/生活与算法/#_1-什么是算法","2098":"/生活与算法/#_2-生活中常常有算法","2099":"/生活与算法/#_2-1-选择顺序-排队和优先处理","2100":"/生活与算法/#_2-2-找东西-生活中的搜索","2101":"/生活与算法/#_2-3-每次都选最佳-贪心法则","2102":"/生活与算法/#_2-4-长远规划-逐步优化的过程","2103":"/生活与算法/#_3-生活离不开算法思维","2104":"/生活与算法/#_3-1-化繁为简-一步步来","2105":"/生活与算法/#_3-2-避免错误-逻辑推理","2106":"/生活与算法/#_3-3-时间管理-高效完成任务","2107":"/生活与算法/#_4-算法交织于生活","2108":"/生活与算法/#_5-简单总结一下","2109":"/我的感悟/#站长随笔","2110":"/我的感悟/#更新进度","2111":"/生活与算法/贪心算法/1.人的本性——贪心！.html#人的本性——贪心","2112":"/生活与算法/贪心算法/1.人的本性——贪心！.html#_1-贪心与人生","2113":"/生活与算法/贪心算法/1.人的本性——贪心！.html#_2-贪心算法基本概念","2114":"/生活与算法/贪心算法/1.人的本性——贪心！.html#_2-1-概念","2115":"/生活与算法/贪心算法/1.人的本性——贪心！.html#_2-2-通俗讲解","2116":"/生活与算法/贪心算法/1.人的本性——贪心！.html#_3-贪心算法原理","2117":"/生活与算法/贪心算法/1.人的本性——贪心！.html#_3-1-我想说","2118":"/生活与算法/贪心算法/1.人的本性——贪心！.html#_3-2-贪心策略","2119":"/生活与算法/贪心算法/1.人的本性——贪心！.html#_3-3-基本原理","2120":"/生活与算法/贪心算法/1.人的本性——贪心！.html#_4-总结","2121":"/生活与算法/贪心算法/2.初步感受贪心.html#让我们简单步入-贪心-叭","2122":"/生活与算法/贪心算法/2.初步感受贪心.html#_1-最大四位数的例子-从数字中选最大","2123":"/生活与算法/贪心算法/2.初步感受贪心.html#_1-1-问题","2124":"/生活与算法/贪心算法/2.初步感受贪心.html#_1-2-贪心思维","2125":"/生活与算法/贪心算法/2.初步感受贪心.html#_1-3-数学过程","2126":"/生活与算法/贪心算法/2.初步感受贪心.html#_1-4-结论","2127":"/生活与算法/贪心算法/2.初步感受贪心.html#大家可能会觉得这不是显而易见的吗-但这只能帮你解决简单的问题-复杂的问题就需要我们认真去分析了。","2128":"/生活与算法/贪心算法/2.初步感受贪心.html#_2-选择最便宜的出行方案","2129":"/生活与算法/贪心算法/2.初步感受贪心.html#_2-1-问题","2130":"/生活与算法/贪心算法/2.初步感受贪心.html#_2-2-贪心思维","2131":"/生活与算法/贪心算法/2.初步感受贪心.html#_2-3-数学过程","2132":"/生活与算法/贪心算法/2.初步感受贪心.html#_3-如何分配时间-优先完成最紧急的任务","2133":"/生活与算法/贪心算法/2.初步感受贪心.html#_3-1-问题","2134":"/生活与算法/贪心算法/2.初步感受贪心.html#_3-2-贪心思维","2135":"/生活与算法/贪心算法/2.初步感受贪心.html#_3-3-数学过程","2136":"/生活与算法/贪心算法/2.初步感受贪心.html#_4-找零问题-使用最少的硬币","2137":"/生活与算法/贪心算法/2.初步感受贪心.html#_4-1-问题","2138":"/生活与算法/贪心算法/2.初步感受贪心.html#_4-2-贪心思维","2139":"/生活与算法/贪心算法/2.初步感受贪心.html#_4-3-数学过程","2140":"/生活与算法/贪心算法/2.初步感受贪心.html#_5-我们已经步入-贪心算法-啦","2141":"/面试求职/Java面经/#这年代-不吃面经咋活","2142":"/面试求职/场景问题/#场景问题内容","2143":"/面试求职/算法岗/#前任铺路后人走","2144":"/面试求职/算法岗/#nlp面经","2145":"/面试求职/经验分享/#前任铺路后人走","2146":"/生活与算法/贪心算法/3. 分发饼干问题.html#分发饼干问题","2147":"/生活与算法/贪心算法/3. 分发饼干问题.html#_1-问题描述","2148":"/生活与算法/贪心算法/3. 分发饼干问题.html#_2-目标","2149":"/生活与算法/贪心算法/3. 分发饼干问题.html#_3-输入","2150":"/生活与算法/贪心算法/3. 分发饼干问题.html#_4-输出","2151":"/生活与算法/贪心算法/3. 分发饼干问题.html#_5-贪心算法的解法思路","2152":"/生活与算法/贪心算法/3. 分发饼干问题.html#_6-解题步骤","2153":"/生活与算法/贪心算法/3. 分发饼干问题.html#_7-代码实现-python","2154":"/生活与算法/贪心算法/3. 分发饼干问题.html#_8-示例","2155":"/生活与算法/贪心算法/3. 分发饼干问题.html#_9-时间复杂度","2156":"/生活与算法/贪心算法/3. 分发饼干问题.html#_10-贪心策略的解释","2157":"/生活与算法/贪心算法/3. 分发饼干问题.html#_11-思考"},"fieldIds":{"title":0,"titles":1,"text":2},"fieldLength":{"0":[3,1,68],"1":[2,3,43],"2":[2,3,1],"3":[2,5,44],"4":[3,5,32],"5":[3,5,108],"6":[2,3,1],"7":[3,5,43],"8":[1,8,90],"9":[1,8,88],"10":[1,8,70],"11":[2,5,5],"12":[3,7,56],"13":[3,7,32],"14":[1,7,18],"15":[3,5,1],"16":[1,8,52],"17":[1,8,74],"18":[3,5,24],"19":[3,5,46],"20":[3,5,141],"21":[3,5,24],"22":[2,3,1],"23":[3,5,88],"24":[3,5,25],"25":[1,8,14],"26":[1,8,25],"27":[1,8,9],"28":[1,9,60],"29":[1,9,54],"30":[1,9,28],"31":[1,8,22],"32":[1,8,15],"33":[1,9,27],"34":[1,9,59],"35":[1,8,42],"36":[2,8,211],"37":[2,5,11],"38":[1,7,45],"39":[1,7,60],"40":[2,3,7],"41":[3,5,67],"42":[3,5,48],"43":[3,5,25],"44":[1,8,10],"45":[1,8,30],"46":[1,8,29],"47":[2,3,130],"48":[1,1,12],"49":[3,1,61],"50":[2,3,57],"51":[2,3,2],"52":[2,5,1],"53":[1,7,23],"54":[1,7,46],"55":[1,7,11],"56":[3,5,1],"57":[1,8,65],"58":[1,8,62],"59":[1,8,37],"60":[3,3,3],"61":[3,6,2],"62":[1,9,25],"63":[1,9,27],"64":[2,6,4],"65":[1,8,48],"66":[1,8,74],"67":[1,8,58],"68":[3,3,1],"69":[3,6,18],"70":[1,9,63],"71":[1,9,137],"72":[1,9,29],"73":[3,6,1],"74":[1,9,125],"75":[1,9,4],"76":[2,6,82],"77":[2,3,40],"78":[3,5,64],"79":[3,5,78],"80":[1,8,31],"81":[3,5,19],"82":[1,8,85],"83":[1,8,177],"84":[2,5,88],"85":[3,5,10],"86":[2,3,1],"87":[4,5,27],"88":[1,9,32],"89":[2,9,113],"90":[1,9,98],"91":[3,5,33],"92":[1,8,14],"93":[1,8,63],"94":[1,8,23],"95":[2,3,83],"96":[3,1,84],"97":[2,3,24],"98":[2,3,20],"99":[2,5,93],"100":[3,5,20],"101":[4,5,93],"102":[3,5,8],"103":[1,8,52],"104":[1,8,38],"105":[1,8,55],"106":[1,8,47],"107":[1,8,23],"108":[1,9,59],"109":[1,9,66],"110":[1,9,84],"111":[1,8,20],"112":[2,3,1],"113":[3,5,79],"114":[2,5,49],"115":[2,3,36],"116":[3,5,64],"117":[3,5,41],"118":[2,5,57],"119":[3,5,74],"120":[3,5,18],"121":[2,3,59],"122":[3,5,198],"123":[3,5,34],"124":[1,8,3],"125":[1,9,81],"126":[2,9,71],"127":[1,9,41],"128":[1,9,66],"129":[1,9,29],"130":[1,8,98],"131":[1,8,52],"132":[3,5,3],"133":[1,8,8],"134":[4,9,37],"135":[5,9,80],"136":[1,8,57],"137":[1,8,104],"138":[3,5,32],"139":[1,8,42],"140":[1,8,58],"141":[1,8,84],"142":[1,8,36],"143":[1,9,78],"144":[1,9,40],"145":[1,9,158],"146":[2,5,3],"147":[1,7,69],"148":[1,7,114],"149":[2,3,2],"150":[3,5,26],"151":[3,5,31],"152":[1,8,19],"153":[1,8,52],"154":[6,5,96],"155":[3,5,66],"156":[2,3,302],"157":[3,1,97],"158":[2,3,48],"159":[2,3,1],"160":[2,5,65],"161":[3,5,84],"162":[3,5,47],"163":[1,8,51],"164":[1,8,87],"165":[1,8,20],"166":[1,8,9],"167":[1,9,77],"168":[1,9,38],"169":[1,9,76],"170":[3,5,87],"171":[2,3,1],"172":[3,5,192],"173":[2,5,102],"174":[3,5,8],"175":[1,8,91],"176":[1,8,76],"177":[3,5,1],"178":[1,8,58],"179":[1,8,10],"180":[1,8,108],"181":[1,8,35],"182":[1,8,2],"183":[2,9,42],"184":[4,9,54],"185":[2,9,77],"186":[3,5,26],"187":[1,8,42],"188":[1,8,26],"189":[1,9,70],"190":[1,9,6],"191":[1,9,69],"192":[1,9,37],"193":[2,9,15],"194":[1,8,78],"195":[3,5,3],"196":[2,3,1],"197":[3,5,17],"198":[3,8,88],"199":[3,8,133],"200":[1,8,49],"201":[3,5,263],"202":[2,3,1],"203":[3,5,8],"204":[1,8,183],"205":[1,8,5],"206":[2,8,20],"207":[1,8,4],"208":[3,5,2],"209":[1,8,86],"210":[1,8,103],"211":[1,8,26],"212":[1,9,44],"213":[1,9,69],"214":[2,8,16],"215":[1,10,9],"216":[2,10,54],"217":[2,10,70],"218":[1,8,20],"219":[1,9,1],"220":[1,10,24],"221":[1,10,39],"222":[1,9,94],"223":[1,9,1],"224":[3,10,80],"225":[1,10,46],"226":[1,8,24],"227":[1,9,10],"228":[1,9,37],"229":[1,9,32],"230":[1,9,114],"231":[1,9,73],"232":[1,8,30],"233":[2,3,264],"234":[3,1,1],"235":[2,3,93],"236":[2,3,2],"237":[2,5,22],"238":[3,5,12],"239":[3,5,57],"240":[3,5,42],"241":[4,5,89],"242":[3,5,44],"243":[2,3,8],"244":[3,5,8],"245":[1,7,53],"246":[1,7,57],"247":[2,5,50],"248":[3,5,20],"249":[1,8,55],"250":[1,8,22],"251":[1,8,9],"252":[1,9,23],"253":[1,9,20],"254":[1,9,7],"255":[1,9,25],"256":[3,5,50],"257":[2,3,39],"258":[3,4,11],"259":[1,6,70],"260":[1,6,58],"261":[1,6,50],"262":[3,4,12],"263":[1,7,51],"264":[1,7,18],"265":[1,7,89],"266":[2,4,2],"267":[1,6,127],"268":[1,6,24],"269":[1,6,6],"270":[1,6,31],"271":[1,6,54],"272":[3,4,19],"273":[1,7,55],"274":[1,7,49],"275":[1,7,4],"276":[1,8,33],"277":[1,8,59],"278":[1,7,6],"279":[3,4,34],"280":[2,3,21],"281":[3,5,17],"282":[5,5,25],"283":[8,5,13],"284":[3,5,93],"285":[4,5,59],"286":[3,5,7],"287":[4,5,73],"288":[4,5,58],"289":[5,5,34],"290":[5,5,22],"291":[3,5,32],"292":[2,3,129],"293":[3,1,92],"294":[2,3,53],"295":[2,3,9],"296":[2,5,73],"297":[3,5,1],"298":[2,8,36],"299":[1,8,15],"300":[3,5,22],"301":[1,8,110],"302":[1,8,32],"303":[3,5,1],"304":[1,8,33],"305":[1,8,56],"306":[1,8,33],"307":[3,8,12],"308":[2,3,1],"309":[3,5,59],"310":[2,5,37],"311":[3,5,21],"312":[2,3,15],"313":[3,5,104],"314":[3,5,71],"315":[2,5,119],"316":[3,5,140],"317":[2,3,4],"318":[3,5,55],"319":[3,5,13],"320":[3,5,101],"321":[2,5,27],"322":[1,7,43],"323":[1,7,50],"324":[1,7,3],"325":[1,8,58],"326":[1,8,145],"327":[1,7,27],"328":[6,5,1],"329":[1,11,45],"330":[1,11,51],"331":[2,11,17],"332":[1,11,61],"333":[1,11,77],"334":[2,11,184],"335":[1,11,62],"336":[2,3,1],"337":[4,5,54],"338":[3,5,54],"339":[3,5,7],"340":[1,8,50],"341":[1,8,97],"342":[2,3,1],"343":[3,5,182],"344":[3,5,187],"345":[2,3,1],"346":[3,5,129],"347":[4,5,40],"348":[3,5,26],"349":[3,5,41],"350":[3,5,14],"351":[3,8,21],"352":[1,8,40],"353":[1,8,1],"354":[5,9,30],"355":[1,9,35],"356":[4,9,22],"357":[1,8,17],"358":[3,5,11],"359":[1,8,41],"360":[1,8,32],"361":[2,3,175],"362":[3,1,4],"363":[2,3,49],"364":[2,3,82],"365":[2,3,2],"366":[3,5,1],"367":[1,8,24],"368":[1,8,12],"369":[1,8,29],"370":[2,5,126],"371":[2,3,19],"372":[3,5,63],"373":[3,5,32],"374":[2,5,26],"375":[2,3,92],"376":[5,5,23],"377":[1,10,13],"378":[1,10,23],"379":[3,5,16],"380":[1,8,126],"381":[1,8,70],"382":[1,8,124],"383":[1,8,47],"384":[3,5,93],"385":[3,5,144],"386":[1,5,15],"387":[2,3,70],"388":[5,1,1],"389":[2,5,7],"390":[2,5,10],"391":[2,6,21],"392":[3,6,14],"393":[1,9,58],"394":[1,9,121],"395":[1,9,76],"396":[1,9,76],"397":[3,6,25],"398":[1,9,99],"399":[2,9,298],"400":[3,6,20],"401":[1,9,19],"402":[1,9,150],"403":[3,6,25],"404":[4,6,7],"405":[1,10,57],"406":[1,10,57],"407":[1,10,75],"408":[1,10,36],"409":[1,10,56],"410":[1,10,41],"411":[1,10,50],"412":[1,10,10],"413":[3,6,16],"414":[1,9,7],"415":[1,10,28],"416":[1,10,17],"417":[1,9,26],"418":[1,10,24],"419":[1,10,48],"420":[1,9,71],"421":[3,6,1],"422":[1,9,54],"423":[1,9,169],"424":[1,9,80],"425":[2,5,8],"426":[3,6,33],"427":[2,6,70],"428":[3,6,154],"429":[2,5,220],"430":[2,1,1],"431":[3,2,65],"432":[3,2,47],"433":[2,1,1],"434":[2,2,26],"435":[3,2,3],"436":[1,5,31],"437":[1,5,114],"438":[3,2,3],"439":[1,5,11],"440":[1,5,2],"441":[1,5,4],"442":[1,6,30],"443":[1,6,28],"444":[1,6,30],"445":[3,2,69],"446":[3,2,66],"447":[2,1,5],"448":[3,2,31],"449":[2,2,182],"450":[3,2,147],"451":[3,2,37],"452":[2,1,1],"453":[3,2,57],"454":[3,2,18],"455":[2,2,28],"456":[1,4,30],"457":[1,4,36],"458":[2,5,18],"459":[2,5,9],"460":[2,5,64],"461":[3,1,43],"462":[3,3,17],"463":[5,7,41],"464":[4,7,24],"465":[2,7,4],"466":[3,3,25],"467":[3,3,9],"468":[1,6,12],"469":[1,6,7],"470":[1,6,22],"471":[5,1,11],"472":[3,5,92],"473":[3,5,69],"474":[2,1,28],"475":[3,2,47],"476":[2,5,6],"477":[1,7,51],"478":[1,7,14],"479":[1,7,21],"480":[2,5,46],"481":[3,2,1],"482":[1,5,23],"483":[1,5,14],"484":[1,6,30],"485":[1,6,150],"486":[3,2,4],"487":[1,5,18],"488":[1,6,29],"489":[1,6,31],"490":[1,5,51],"491":[3,2,1],"492":[1,5,11],"493":[1,6,11],"494":[1,7,17],"495":[1,7,35],"496":[1,7,26],"497":[9,6,32],"498":[1,5,113],"499":[1,5,68],"500":[1,5,28],"501":[1,6,41],"502":[4,6,29],"503":[1,6,151],"504":[1,5,116],"505":[1,5,67],"506":[3,2,22],"507":[1,5,76],"508":[1,5,24],"509":[1,5,9],"510":[2,1,89],"511":[3,2,44],"512":[3,2,79],"513":[2,1,308],"514":[5,1,1],"515":[2,5,73],"516":[2,5,46],"517":[2,6,79],"518":[3,6,34],"519":[3,6,73],"520":[3,6,49],"521":[3,6,2],"522":[1,9,95],"523":[1,9,101],"524":[2,5,27],"525":[3,6,118],"526":[2,6,28],"527":[3,6,3],"528":[1,9,94],"529":[1,9,119],"530":[3,6,68],"531":[3,6,1],"532":[1,9,77],"533":[1,9,128],"534":[2,5,3],"535":[3,7,60],"536":[3,7,81],"537":[2,7,59],"538":[3,7,89],"539":[3,5,10],"540":[3,8,73],"541":[4,8,69],"542":[3,8,205],"543":[2,5,111],"544":[3,1,1],"545":[2,3,87],"546":[2,3,1],"547":[2,5,30],"548":[3,5,11],"549":[3,5,13],"550":[3,5,14],"551":[3,5,6],"552":[1,7,41],"553":[1,7,37],"554":[1,7,13],"555":[1,7,17],"556":[2,3,6],"557":[3,5,303],"558":[2,5,138],"559":[3,5,20],"560":[1,8,76],"561":[1,8,73],"562":[1,8,45],"563":[2,3,29],"564":[3,5,49],"565":[1,8,38],"566":[1,8,23],"567":[1,8,84],"568":[1,8,26],"569":[3,5,29],"570":[2,5,3],"571":[1,7,107],"572":[1,7,116],"573":[1,7,40],"574":[2,3,13],"575":[3,5,45],"576":[3,5,93],"577":[3,5,14],"578":[1,8,12],"579":[1,8,61],"580":[1,8,39],"581":[1,8,20],"582":[1,9,69],"583":[1,9,27],"584":[1,9,28],"585":[1,8,24],"586":[2,5,1],"587":[1,7,53],"588":[1,7,26],"589":[3,5,2],"590":[1,8,78],"591":[1,8,190],"592":[2,8,171],"593":[3,5,1],"594":[1,8,17],"595":[1,8,88],"596":[2,3,7],"597":[3,5,12],"598":[1,8,29],"599":[1,8,16],"600":[1,8,7],"601":[3,5,42],"602":[1,8,24],"603":[1,8,17],"604":[1,8,16],"605":[3,5,16],"606":[1,8,22],"607":[1,8,23],"608":[1,8,29],"609":[2,3,2],"610":[5,5,31],"611":[1,9,32],"612":[1,9,107],"613":[1,9,92],"614":[1,9,103],"615":[1,9,13],"616":[3,5,29],"617":[1,7,12],"618":[1,7,19],"619":[1,7,1],"620":[1,8,67],"621":[1,8,81],"622":[1,8,5],"623":[1,7,63],"624":[1,7,51],"625":[3,5,19],"626":[1,7,26],"627":[1,7,53],"628":[1,7,68],"629":[1,7,53],"630":[1,7,1],"631":[1,8,8],"632":[2,8,26],"633":[5,8,12],"634":[6,8,11],"635":[6,8,16],"636":[2,8,26],"637":[3,5,47],"638":[2,3,225],"639":[4,1,1],"640":[2,4,25],"641":[2,4,1],"642":[6,6,14],"643":[1,6,1],"644":[3,4,1],"645":[2,4,2],"646":[2,4,12],"647":[3,6,25],"648":[5,6,1],"649":[7,6,24],"650":[6,6,16],"651":[2,6,8],"652":[3,6,5],"653":[4,4,1],"654":[2,4,1],"655":[2,1,1],"656":[2,2,18],"657":[3,2,3],"658":[3,5,33],"659":[4,5,23],"660":[4,5,10],"661":[7,5,60],"662":[5,5,26],"663":[4,2,1],"664":[2,6,19],"665":[3,6,36],"666":[4,6,18],"667":[4,6,10],"668":[3,6,32],"669":[5,6,3],"670":[3,6,9],"671":[3,6,13],"672":[3,1,3],"673":[3,1,1],"674":[2,3,35],"675":[2,3,1],"676":[2,5,121],"677":[3,5,11],"678":[3,5,30],"679":[3,5,88],"680":[3,5,45],"681":[1,8,64],"682":[1,8,36],"683":[1,9,48],"684":[1,9,45],"685":[1,9,39],"686":[1,9,6],"687":[1,9,18],"688":[3,5,63],"689":[1,7,116],"690":[1,7,21],"691":[1,7,46],"692":[1,7,208],"693":[3,5,1],"694":[1,8,85],"695":[1,8,4],"696":[1,8,55],"697":[2,3,5],"698":[5,5,153],"699":[2,5,18],"700":[1,7,193],"701":[1,7,78],"702":[2,7,149],"703":[1,7,76],"704":[1,7,44],"705":[3,5,1],"706":[1,8,30],"707":[2,8,17],"708":[1,8,22],"709":[1,8,118],"710":[2,3,15],"711":[3,5,85],"712":[3,5,37],"713":[1,8,139],"714":[1,8,89],"715":[1,8,67],"716":[2,5,9],"717":[1,7,40],"718":[1,7,45],"719":[3,5,15],"720":[1,8,14],"721":[1,8,67],"722":[1,8,60],"723":[1,8,109],"724":[3,5,26],"725":[1,8,17],"726":[1,8,54],"727":[1,8,79],"728":[5,5,3],"729":[1,10,19],"730":[1,10,8],"731":[1,11,75],"732":[1,11,30],"733":[4,11,31],"734":[3,5,86],"735":[4,5,87],"736":[3,5,68],"737":[3,5,18],"738":[1,8,11],"739":[1,8,88],"740":[2,3,209],"741":[3,1,50],"742":[2,3,23],"743":[2,3,6],"744":[2,5,41],"745":[3,5,76],"746":[3,5,62],"747":[2,3,34],"748":[3,5,28],"749":[2,5,31],"750":[3,5,9],"751":[3,5,35],"752":[2,3,1],"753":[3,5,2],"754":[1,8,40],"755":[1,8,16],"756":[1,8,32],"757":[3,5,55],"758":[2,5,40],"759":[3,5,65],"760":[3,5,79],"761":[3,5,31],"762":[3,5,59],"763":[3,5,47],"764":[3,5,83],"765":[3,5,146],"766":[5,5,73],"767":[3,5,24],"768":[2,3,225],"769":[2,1,38],"770":[2,1,38],"771":[2,1,2],"772":[4,1,29],"773":[2,1,44],"774":[3,1,9],"775":[5,1,26],"776":[5,1,27],"777":[6,1,15],"778":[7,1,25],"779":[7,1,22],"780":[5,1,1],"781":[3,5,17],"782":[3,5,4],"783":[3,1,17],"784":[2,1,2],"785":[2,1,33],"786":[2,1,2],"787":[2,1,2],"788":[3,1,1],"789":[3,1,1],"790":[3,3,3],"791":[3,3,1],"792":[3,1,9],"793":[3,1,1],"794":[3,3,9],"795":[3,3,24],"796":[3,3,20],"797":[2,1,1],"798":[4,2,1],"799":[3,2,3],"800":[2,1,1],"801":[2,2,40],"802":[3,2,53],"803":[2,1,1],"804":[4,2,26],"805":[3,2,28],"806":[2,1,3],"807":[3,2,92],"808":[3,2,93],"809":[3,2,61],"810":[4,2,75],"811":[4,2,5],"812":[4,2,22],"813":[2,1,19],"814":[4,2,37],"815":[3,2,33],"816":[5,2,63],"817":[3,1,2],"818":[5,1,1],"819":[3,5,26],"820":[3,5,43],"821":[3,5,15],"822":[3,5,18],"823":[4,1,4],"824":[6,1,1],"825":[3,6,22],"826":[3,6,14],"827":[3,6,27],"828":[3,6,28],"829":[2,1,29],"830":[4,1,1],"831":[3,4,21],"832":[3,4,30],"833":[3,1,40],"834":[2,1,57],"835":[2,1,25],"836":[2,1,4],"837":[2,1,5],"838":[3,1,128],"839":[3,3,79],"840":[3,3,135],"841":[2,1,46],"842":[4,1,37],"843":[9,1,51],"844":[7,1,95],"845":[4,1,66],"846":[2,1,43],"847":[2,1,40],"848":[3,1,15],"849":[2,1,5],"850":[7,1,23],"851":[2,1,9],"852":[3,1,1],"853":[5,3,6],"854":[4,3,3],"855":[3,3,19],"856":[4,3,21],"857":[5,3,3],"858":[5,3,6],"859":[5,3,1],"860":[5,1,15],"861":[3,1,22],"862":[7,1,14],"863":[4,7,23],"864":[5,7,12],"865":[4,10,18],"866":[3,10,29],"867":[5,10,13],"868":[4,10,23],"869":[4,10,4],"870":[5,7,7],"871":[6,7,1],"872":[7,1,1],"873":[6,7,1],"874":[3,7,28],"875":[3,7,5],"876":[2,1,4],"877":[4,1,1],"878":[3,4,33],"879":[4,4,16],"880":[7,7,4],"881":[7,7,6],"882":[8,7,15],"883":[6,4,18],"884":[3,1,1],"885":[4,1,38],"886":[3,1,20],"887":[5,3,5],"888":[3,3,17],"889":[4,3,23],"890":[4,3,19],"891":[5,1,17],"892":[6,5,19],"893":[3,1,1],"894":[3,3,31],"895":[3,3,29],"896":[5,5,17],"897":[4,5,20],"898":[3,3,51],"899":[3,1,69],"900":[5,1,1],"901":[3,5,28],"902":[5,5,30],"903":[3,5,41],"904":[5,5,56],"905":[3,1,1],"906":[4,3,86],"907":[3,3,24],"908":[4,3,121],"909":[3,3,16],"910":[2,1,9],"911":[6,1,35],"912":[4,1,28],"913":[2,1,1],"914":[4,2,25],"915":[4,2,22],"916":[7,1,13],"917":[3,7,1],"918":[3,7,15],"919":[3,7,1],"920":[6,8,8],"921":[3,8,13],"922":[4,8,8],"923":[3,8,17],"924":[2,7,31],"925":[5,1,1],"926":[3,5,42],"927":[3,5,53],"928":[3,5,7],"929":[3,5,3],"930":[6,1,9],"931":[5,1,26],"932":[5,1,18],"933":[4,5,73],"934":[4,5,35],"935":[6,1,77],"936":[7,1,1],"937":[4,7,50],"938":[8,1,49],"939":[4,1,1],"940":[3,4,49],"941":[3,4,81],"942":[5,4,28],"943":[4,4,83],"944":[3,4,175],"945":[3,4,65],"946":[5,4,31],"947":[4,1,13],"948":[4,1,13],"949":[4,1,6],"950":[2,1,7],"951":[6,1,77],"952":[7,1,1],"953":[4,7,50],"954":[7,1,49],"955":[9,1,4],"956":[4,9,35],"957":[4,9,44],"958":[4,1,4],"959":[3,4,41],"960":[3,4,86],"961":[4,4,83],"962":[5,4,36],"963":[5,4,51],"964":[5,4,31],"965":[5,4,65],"966":[5,4,28],"967":[4,4,16],"968":[11,1,26],"969":[7,11,10],"970":[4,11,48],"971":[4,11,42],"972":[6,11,17],"973":[10,1,58],"974":[2,1,66],"975":[3,2,107],"976":[6,1,148],"977":[7,1,114],"978":[4,1,13],"979":[6,1,6],"980":[4,6,51],"981":[4,6,66],"982":[5,6,28],"983":[4,6,47],"984":[2,1,2],"985":[4,2,85],"986":[4,2,175],"987":[2,1,12],"988":[5,1,19],"989":[2,1,1],"990":[3,2,15],"991":[3,2,20],"992":[2,1,13],"993":[2,1,7],"994":[3,2,25],"995":[3,2,26],"996":[3,2,7],"997":[2,1,1],"998":[3,2,12],"999":[4,2,48],"1000":[5,1,89],"1001":[3,1,8],"1002":[3,3,33],"1003":[3,3,171],"1004":[3,3,101],"1005":[3,3,1],"1006":[4,5,17],"1007":[5,5,34],"1008":[2,1,1],"1009":[2,1,16],"1010":[2,1,14],"1011":[2,1,15],"1012":[2,1,33],"1013":[2,1,1],"1014":[3,2,16],"1015":[3,2,38],"1016":[5,2,34],"1017":[3,2,22],"1018":[3,2,15],"1019":[3,2,21],"1020":[3,2,1],"1021":[2,1,10],"1022":[3,1,1],"1023":[2,3,25],"1024":[3,3,1],"1025":[7,5,29],"1026":[7,5,30],"1027":[5,5,43],"1028":[4,1,13],"1029":[3,4,9],"1030":[2,4,2],"1031":[2,4,19],"1032":[3,4,17],"1033":[6,1,1],"1034":[3,6,18],"1035":[3,6,7],"1036":[2,6,24],"1037":[4,1,2],"1038":[3,4,1],"1039":[3,4,23],"1040":[3,1,1],"1041":[3,3,13],"1042":[4,3,21],"1043":[4,3,1],"1044":[3,3,10],"1045":[2,3,16],"1046":[4,3,16],"1047":[3,1,13],"1048":[3,3,23],"1049":[3,3,14],"1050":[4,3,4],"1051":[2,1,1],"1052":[3,2,18],"1053":[3,2,1],"1054":[4,2,10],"1055":[2,1,1],"1056":[4,2,1],"1057":[3,2,11],"1058":[3,1,1],"1059":[3,1,49],"1060":[3,3,1],"1061":[4,3,3],"1062":[3,3,1],"1063":[2,1,1],"1064":[3,2,7],"1065":[3,2,3],"1066":[3,1,1],"1067":[2,1,21],"1068":[3,1,1],"1069":[4,3,19],"1070":[3,3,22],"1071":[3,3,17],"1072":[3,3,27],"1073":[2,1,15],"1074":[5,1,1],"1075":[3,5,27],"1076":[4,5,21],"1077":[3,1,4],"1078":[4,3,55],"1079":[4,3,13],"1080":[5,3,5],"1081":[3,3,13],"1082":[3,1,108],"1083":[4,1,338],"1084":[3,1,6],"1085":[8,3,112],"1086":[3,3,218],"1087":[5,3,607],"1088":[4,1,15],"1089":[2,4,23],"1090":[3,4,18],"1091":[3,1,1],"1092":[5,3,32],"1093":[4,3,22],"1094":[4,3,29],"1095":[3,3,32],"1096":[3,3,50],"1097":[5,3,33],"1098":[5,3,81],"1099":[6,3,70],"1100":[4,3,29],"1101":[4,3,32],"1102":[10,3,62],"1103":[2,1,1],"1104":[7,2,60],"1105":[3,2,18],"1106":[3,2,31],"1107":[3,2,18],"1108":[3,2,9],"1109":[5,2,14],"1110":[3,2,51],"1111":[4,2,11],"1112":[7,1,12],"1113":[3,7,41],"1114":[4,7,81],"1115":[3,7,54],"1116":[2,7,46],"1117":[5,7,61],"1118":[3,7,22],"1119":[7,9,18],"1120":[4,9,32],"1121":[7,9,32],"1122":[7,9,34],"1123":[7,7,26],"1124":[1,1,19],"1125":[1,1,29],"1126":[2,1,12],"1127":[2,1,67],"1128":[1,1,14],"1129":[1,2,18],"1130":[1,2,59],"1131":[1,2,35],"1132":[2,3,59],"1133":[2,3,63],"1134":[2,3,56],"1135":[2,3,21],"1136":[4,3,14],"1137":[1,3,49],"1138":[1,1,11],"1139":[1,2,48],"1140":[1,2,33],"1141":[1,2,17],"1142":[1,2,10],"1143":[2,3,163],"1144":[1,3,72],"1145":[1,3,38],"1146":[1,2,50],"1147":[1,3,37],"1148":[1,3,13],"1149":[1,3,98],"1150":[1,3,41],"1151":[1,3,20],"1152":[1,2,60],"1153":[1,2,39],"1154":[1,1,50],"1155":[1,2,75],"1156":[5,3,31],"1157":[1,2,82],"1158":[1,3,51],"1159":[1,3,34],"1160":[1,1,1],"1161":[1,2,41],"1162":[1,2,11],"1163":[1,3,24],"1164":[1,3,50],"1165":[1,3,31],"1166":[1,2,18],"1167":[1,2,18],"1168":[1,2,36],"1169":[1,2,19],"1170":[1,1,1],"1171":[1,2,12],"1172":[2,2,6],"1173":[2,2,34],"1174":[3,2,27],"1175":[4,2,123],"1176":[4,2,28],"1177":[4,2,31],"1178":[1,2,23],"1179":[1,3,57],"1180":[1,3,46],"1181":[1,3,8],"1182":[1,4,21],"1183":[1,4,75],"1184":[1,3,67],"1185":[3,2,34],"1186":[3,2,42],"1187":[2,2,2],"1188":[5,4,13],"1189":[1,4,21],"1190":[1,4,23],"1191":[1,4,37],"1192":[1,4,45],"1193":[1,4,47],"1194":[1,1,22],"1195":[1,1,37],"1196":[1,1,31],"1197":[1,2,20],"1198":[1,2,13],"1199":[1,2,3],"1200":[3,1,9],"1201":[6,1,8],"1202":[5,6,61],"1203":[6,6,4],"1204":[2,1,1],"1205":[3,2,97],"1206":[4,2,19],"1207":[4,1,22],"1208":[4,1,106],"1209":[5,1,1],"1210":[4,5,33],"1211":[4,5,86],"1212":[3,5,40],"1213":[3,5,52],"1214":[4,5,384],"1215":[2,1,276],"1216":[1,1,131],"1217":[2,1,123],"1218":[2,1,179],"1219":[3,1,10],"1220":[3,1,1],"1221":[2,3,41],"1222":[7,3,47],"1223":[3,3,41],"1224":[6,1,6],"1225":[6,6,21],"1226":[5,6,77],"1227":[6,6,217],"1228":[2,1,51],"1229":[3,1,13],"1230":[7,1,10],"1231":[3,7,51],"1232":[7,7,1],"1233":[5,7,39],"1234":[5,7,28],"1235":[4,7,49],"1236":[5,7,49],"1237":[5,7,49],"1238":[5,7,54],"1239":[5,7,55],"1240":[5,7,60],"1241":[5,7,67],"1242":[5,7,102],"1243":[5,7,85],"1244":[5,7,68],"1245":[3,7,47],"1246":[5,7,51],"1247":[5,7,49],"1248":[2,7,7],"1249":[2,1,3],"1250":[2,1,61],"1251":[4,1,22],"1252":[2,1,1],"1253":[4,2,39],"1254":[3,2,237],"1255":[3,1,28],"1256":[2,1,1],"1257":[3,2,52],"1258":[2,2,13],"1259":[3,2,22],"1260":[3,2,8],"1261":[2,1,1],"1262":[3,2,19],"1263":[3,2,23],"1264":[2,2,5],"1265":[2,1,5],"1266":[3,2,27],"1267":[3,2,28],"1268":[2,1,3],"1269":[3,2,19],"1270":[3,2,17],"1271":[2,1,1],"1272":[4,2,17],"1273":[4,2,40],"1274":[4,2,14],"1275":[3,1,21],"1276":[2,1,8],"1277":[2,1,1],"1278":[4,2,16],"1279":[4,2,26],"1280":[5,2,28],"1281":[3,1,11],"1282":[2,1,22],"1283":[2,1,90],"1284":[2,1,107],"1285":[3,1,1],"1286":[3,3,1],"1287":[3,3,36],"1288":[3,3,18],"1289":[2,1,1],"1290":[4,2,15],"1291":[4,2,25],"1292":[5,2,13],"1293":[4,2,9],"1294":[3,1,1],"1295":[3,3,79],"1296":[5,3,44],"1297":[4,3,24],"1298":[4,1,39],"1299":[6,1,42],"1300":[5,1,18],"1301":[2,1,10],"1302":[3,1,40],"1303":[2,1,60],"1304":[3,2,43],"1305":[6,2,19],"1306":[5,7,27],"1307":[3,7,31],"1308":[1,2,201],"1309":[2,1,51],"1310":[3,1,4],"1311":[3,1,1],"1312":[4,3,118],"1313":[3,3,53],"1314":[3,3,6],"1315":[4,3,156],"1316":[4,3,82],"1317":[5,3,87],"1318":[3,3,28],"1319":[10,1,1],"1320":[3,10,34],"1321":[4,10,1],"1322":[4,12,78],"1323":[3,12,59],"1324":[4,12,52],"1325":[4,10,2],"1326":[4,12,15],"1327":[2,1,1],"1328":[3,2,69],"1329":[3,2,102],"1330":[4,2,191],"1331":[5,2,67],"1332":[2,1,67],"1333":[3,1,4],"1334":[4,1,14],"1335":[3,4,20],"1336":[5,6,49],"1337":[6,6,23],"1338":[3,4,10],"1339":[4,6,110],"1340":[4,6,90],"1341":[2,4,40],"1342":[4,5,63],"1343":[3,5,87],"1344":[3,5,83],"1345":[5,5,99],"1346":[2,1,2],"1347":[2,1,9],"1348":[4,1,1],"1349":[3,4,1],"1350":[4,4,53],"1351":[3,4,1],"1352":[4,4,1],"1353":[4,4,1],"1354":[5,4,1],"1355":[4,1,1],"1356":[4,4,1],"1357":[4,4,1],"1358":[5,4,1],"1359":[2,1,1],"1360":[4,2,34],"1361":[3,2,67],"1362":[3,1,1],"1363":[4,3,83],"1364":[6,3,38],"1365":[2,1,13],"1366":[1,1,1],"1367":[1,1,7],"1368":[1,1,2],"1369":[5,1,1],"1370":[2,5,18],"1371":[3,5,18],"1372":[3,5,18],"1373":[3,5,30],"1374":[3,5,20],"1375":[4,5,20],"1376":[2,1,4],"1377":[3,2,68],"1378":[2,1,5],"1379":[6,1,3],"1380":[6,1,2],"1381":[3,1,3],"1382":[3,1,4],"1383":[3,1,4],"1384":[4,1,2],"1385":[1,1,1],"1386":[3,1,21],"1387":[4,1,1],"1388":[3,4,49],"1389":[2,4,69],"1390":[5,1,1],"1391":[3,5,6],"1392":[3,5,42],"1393":[4,5,96],"1394":[5,5,72],"1395":[2,1,84],"1396":[2,1,37],"1397":[2,1,3],"1398":[2,1,120],"1399":[2,1,4],"1400":[2,1,4],"1401":[2,1,4],"1402":[2,1,4],"1403":[1,1,6],"1404":[3,1,92],"1405":[3,1,7],"1406":[2,3,1],"1407":[3,4,16],"1408":[4,4,14],"1409":[4,4,8],"1410":[4,4,1],"1411":[3,6,23],"1412":[6,6,26],"1413":[4,6,17],"1414":[2,3,5],"1415":[3,5,1],"1416":[4,5,1],"1417":[3,8,1],"1418":[2,8,1],"1419":[3,8,1],"1420":[5,5,1],"1421":[4,9,1],"1422":[8,9,1],"1423":[6,9,1],"1424":[7,9,1],"1425":[2,3,15],"1426":[4,1,11],"1427":[1,1,2],"1428":[5,1,1],"1429":[5,1,1],"1430":[5,1,1],"1431":[5,1,1],"1432":[4,1,1],"1433":[2,1,12],"1434":[2,2,1],"1435":[4,3,24],"1436":[3,3,41],"1437":[2,1,1],"1438":[2,2,31],"1439":[3,2,10],"1440":[2,1,47],"1441":[4,1,29],"1442":[2,1,29],"1443":[3,1,41],"1444":[2,1,1],"1445":[3,2,1],"1446":[5,2,1],"1447":[5,2,1],"1448":[5,2,4],"1449":[7,2,3],"1450":[2,1,12],"1451":[5,1,1],"1452":[2,1,1],"1453":[2,2,1],"1454":[3,2,1],"1455":[3,2,28],"1456":[3,2,33],"1457":[3,2,13],"1458":[2,1,1],"1459":[5,2,27],"1460":[2,2,25],"1461":[2,1,19],"1462":[3,2,32],"1463":[4,2,5],"1464":[2,1,34],"1465":[4,2,18],"1466":[3,2,20],"1467":[2,1,4],"1468":[2,1,1],"1469":[3,2,11],"1470":[3,2,9],"1471":[2,1,1],"1472":[2,1,33],"1473":[2,1,4],"1474":[1,1,18],"1475":[2,1,1],"1476":[2,3,38],"1477":[3,3,31],"1478":[3,3,37],"1479":[3,3,50],"1480":[2,1,7],"1481":[3,3,205],"1482":[2,3,17],"1483":[2,1,1],"1484":[3,3,8],"1485":[3,3,114],"1486":[2,3,60],"1487":[3,3,58],"1488":[3,3,59],"1489":[3,3,27],"1490":[2,1,5],"1491":[3,2,18],"1492":[2,2,14],"1493":[4,2,7],"1494":[2,2,1],"1495":[2,2,1],"1496":[3,2,1],"1497":[2,2,1],"1498":[2,2,6],"1499":[2,2,11],"1500":[2,2,13],"1501":[1,2,9],"1502":[1,1,7],"1503":[2,1,1],"1504":[3,3,1],"1505":[1,6,6],"1506":[1,6,22],"1507":[1,6,8],"1508":[2,3,1],"1509":[1,5,29],"1510":[1,5,27],"1511":[2,3,1],"1512":[1,5,10],"1513":[1,5,25],"1514":[2,3,1],"1515":[1,5,13],"1516":[1,5,10],"1517":[3,3,2],"1518":[3,6,11],"1519":[3,6,14],"1520":[3,6,25],"1521":[2,3,1],"1522":[1,5,9],"1523":[1,5,9],"1524":[1,5,13],"1525":[2,3,1],"1526":[1,5,10],"1527":[1,5,7],"1528":[1,5,7],"1529":[2,3,1],"1530":[1,5,15],"1531":[2,3,1],"1532":[1,5,17],"1533":[1,5,7],"1534":[2,3,1],"1535":[1,5,15],"1536":[2,3,1],"1537":[1,5,15],"1538":[2,3,1],"1539":[1,5,14],"1540":[2,1,1],"1541":[3,3,24],"1542":[3,3,1],"1543":[1,6,5],"1544":[1,6,15],"1545":[2,3,16],"1546":[2,3,1],"1547":[1,5,29],"1548":[1,5,15],"1549":[1,5,13],"1550":[1,5,9],"1551":[4,3,22],"1552":[2,3,1],"1553":[1,5,12],"1554":[1,5,16],"1555":[2,3,4],"1556":[1,5,14],"1557":[1,5,24],"1558":[2,3,1],"1559":[1,5,18],"1560":[1,5,4],"1561":[1,1,16],"1562":[2,1,3],"1563":[2,3,36],"1564":[4,3,16],"1565":[4,3,9],"1566":[3,3,95],"1567":[2,1,1],"1568":[3,3,29],"1569":[2,3,62],"1570":[2,1,4],"1571":[3,3,1],"1572":[2,6,8],"1573":[2,6,39],"1574":[2,6,20],"1575":[2,6,40],"1576":[2,6,19],"1577":[4,6,8],"1578":[2,6,20],"1579":[2,6,12],"1580":[3,3,2],"1581":[4,1,1],"1582":[3,5,9],"1583":[3,5,13],"1584":[3,5,21],"1585":[2,5,3],"1586":[3,6,1],"1587":[3,6,1],"1588":[3,6,1],"1589":[2,6,105],"1590":[2,1,62],"1591":[3,3,1],"1592":[3,3,1],"1593":[3,3,1],"1594":[4,6,140],"1595":[2,1,1],"1596":[3,3,13],"1597":[3,3,7],"1598":[3,3,23],"1599":[1,1,10],"1600":[7,1,1],"1601":[5,7,15],"1602":[7,7,74],"1603":[4,7,74],"1604":[4,7,63],"1605":[5,7,93],"1606":[6,7,36],"1607":[3,7,248],"1608":[2,7,70],"1609":[3,1,1],"1610":[4,3,18],"1611":[4,3,226],"1612":[4,3,120],"1613":[6,3,28],"1614":[8,3,77],"1615":[7,3,58],"1616":[5,3,65],"1617":[2,1,3],"1618":[2,2,7],"1619":[4,4,113],"1620":[4,4,83],"1621":[4,4,90],"1622":[2,2,11],"1623":[3,4,115],"1624":[3,4,88],"1625":[3,2,36],"1626":[2,1,1],"1627":[1,2,15],"1628":[2,2,86],"1629":[2,2,92],"1630":[2,2,138],"1631":[2,2,50],"1632":[2,2,77],"1633":[2,2,103],"1634":[4,2,81],"1635":[1,1,80],"1636":[3,1,1],"1637":[3,3,5],"1638":[5,3,42],"1639":[3,3,53],"1640":[3,3,45],"1641":[3,3,31],"1642":[1,1,27],"1643":[3,1,1],"1644":[1,3,15],"1645":[5,3,128],"1646":[6,3,100],"1647":[5,3,145],"1648":[6,3,123],"1649":[5,3,117],"1650":[2,3,118],"1651":[1,1,30],"1652":[4,1,13],"1653":[4,1,49],"1654":[2,1,53],"1655":[3,1,33],"1656":[3,1,25],"1657":[2,1,43],"1658":[3,1,7],"1659":[2,3,31],"1660":[2,3,31],"1661":[2,3,39],"1662":[2,3,39],"1663":[2,3,44],"1664":[1,1,61],"1665":[1,1,66],"1666":[4,1,34],"1667":[3,4,68],"1668":[5,4,74],"1669":[5,4,21],"1670":[2,4,46],"1671":[2,4,65],"1672":[2,4,63],"1673":[7,4,67],"1674":[3,4,94],"1675":[3,4,62],"1676":[3,4,54],"1677":[3,4,67],"1678":[4,4,84],"1679":[1,1,1],"1680":[4,1,42],"1681":[4,1,1],"1682":[5,4,1],"1683":[5,8,61],"1684":[5,8,43],"1685":[2,4,53],"1686":[4,4,1],"1687":[5,8,36],"1688":[5,8,45],"1689":[2,1,9],"1690":[2,5,43],"1691":[2,5,56],"1692":[7,1,1],"1693":[3,7,81],"1694":[3,7,62],"1695":[3,7,98],"1696":[2,1,24],"1697":[6,1,2],"1698":[3,6,73],"1699":[2,6,69],"1700":[2,6,80],"1701":[2,6,64],"1702":[2,1,18],"1703":[5,1,11],"1704":[2,5,123],"1705":[2,5,102],"1706":[2,5,79],"1707":[3,5,47],"1708":[3,5,64],"1709":[3,5,92],"1710":[2,5,43],"1711":[7,1,16],"1712":[2,7,142],"1713":[3,7,231],"1714":[2,7,121],"1715":[4,7,125],"1716":[2,1,14],"1717":[5,1,1],"1718":[2,5,35],"1719":[2,5,64],"1720":[2,5,69],"1721":[3,5,67],"1722":[2,5,65],"1723":[2,5,43],"1724":[2,5,59],"1725":[2,5,69],"1726":[1,1,37],"1727":[3,1,25],"1728":[2,3,306],"1729":[3,3,370],"1730":[3,1,1],"1731":[1,3,7],"1732":[1,3,36],"1733":[2,3,1],"1734":[4,5,11],"1735":[3,3,1],"1736":[5,6,37],"1737":[4,6,45],"1738":[5,6,43],"1739":[5,6,33],"1740":[3,3,1],"1741":[4,6,36],"1742":[4,6,35],"1743":[3,6,34],"1744":[4,6,29],"1745":[3,3,1],"1746":[4,6,30],"1747":[4,6,29],"1748":[3,3,1],"1749":[4,6,36],"1750":[5,6,47],"1751":[5,6,44],"1752":[5,6,48],"1753":[3,3,1],"1754":[4,6,39],"1755":[5,6,34],"1756":[5,6,39],"1757":[3,3,1],"1758":[1,6,27],"1759":[2,3,12],"1760":[3,1,1],"1761":[4,3,95],"1762":[3,3,130],"1763":[2,3,73],"1764":[2,3,66],"1765":[2,3,14],"1766":[2,3,33],"1767":[5,1,1],"1768":[1,5,28],"1769":[1,5,22],"1770":[2,5,14],"1771":[2,5,1],"1772":[2,7,34],"1773":[3,7,6],"1774":[3,7,47],"1775":[3,7,10],"1776":[3,7,12],"1777":[2,5,1],"1778":[3,7,32],"1779":[2,7,47],"1780":[3,7,1],"1781":[1,9,16],"1782":[2,5,1],"1783":[3,7,3],"1784":[3,7,45],"1785":[1,7,9],"1786":[2,7,12],"1787":[2,5,1],"1788":[3,7,49],"1789":[4,7,59],"1790":[1,5,1],"1791":[4,6,40],"1792":[4,6,45],"1793":[1,5,19],"1794":[4,1,1],"1795":[1,4,18],"1796":[2,5,1],"1797":[2,7,81],"1798":[3,7,6],"1799":[3,9,40],"1800":[3,9,44],"1801":[5,9,40],"1802":[4,9,39],"1803":[4,9,32],"1804":[2,5,1],"1805":[3,7,40],"1806":[2,7,53],"1807":[3,7,73],"1808":[2,4,15],"1809":[6,1,1],"1810":[5,6,39],"1811":[5,6,46],"1812":[5,6,7],"1813":[4,11,67],"1814":[3,11,70],"1815":[5,6,7],"1816":[4,11,32],"1817":[3,11,76],"1818":[3,6,5],"1819":[3,8,14],"1820":[3,8,97],"1821":[3,8,94],"1822":[5,6,5],"1823":[3,11,15],"1824":[3,11,60],"1825":[2,6,115],"1826":[2,6,32],"1827":[6,1,1],"1828":[5,6,39],"1829":[5,6,46],"1830":[5,6,7],"1831":[4,11,67],"1832":[3,11,70],"1833":[5,6,7],"1834":[4,11,32],"1835":[3,11,76],"1836":[3,6,5],"1837":[3,8,14],"1838":[3,8,97],"1839":[3,8,94],"1840":[5,6,5],"1841":[3,11,15],"1842":[3,11,60],"1843":[2,6,115],"1844":[2,6,32],"1845":[5,1,1],"1846":[2,5,9],"1847":[2,5,1],"1848":[3,7,13],"1849":[2,7,26],"1850":[3,5,1],"1851":[4,8,9],"1852":[4,8,8],"1853":[2,8,44],"1854":[3,5,1],"1855":[4,8,9],"1856":[4,8,8],"1857":[3,8,50],"1858":[3,5,1],"1859":[4,8,10],"1860":[4,8,8],"1861":[3,8,50],"1862":[2,5,1],"1863":[3,7,11],"1864":[3,7,10],"1865":[3,5,1],"1866":[5,8,43],"1867":[5,8,46],"1868":[5,8,43],"1869":[2,8,37],"1870":[3,1,1],"1871":[1,3,8],"1872":[1,3,1],"1873":[1,4,52],"1874":[1,4,71],"1875":[2,1,1],"1876":[1,2,3],"1877":[3,2,1],"1878":[2,5,2],"1879":[3,5,23],"1880":[3,5,4],"1881":[3,2,1],"1882":[3,5,3],"1883":[2,5,69],"1884":[3,5,4],"1885":[2,2,1],"1886":[3,4,5],"1887":[3,4,82],"1888":[2,4,3],"1889":[2,2,1],"1890":[3,4,6],"1891":[3,4,76],"1892":[3,4,3],"1893":[2,2,1],"1894":[3,4,8],"1895":[3,4,22],"1896":[3,4,3],"1897":[2,2,47],"1898":[8,4,19],"1899":[3,4,2],"1900":[1,2,4],"1901":[1,2,5],"1902":[2,2,96],"1903":[2,1,1],"1904":[1,2,23],"1905":[2,2,50],"1906":[7,2,48],"1907":[6,2,51],"1908":[5,2,36],"1909":[3,2,34],"1910":[2,2,53],"1911":[4,2,62],"1912":[6,2,66],"1913":[2,2,31],"1914":[2,2,101],"1915":[4,1,1],"1916":[4,4,79],"1917":[3,4,97],"1918":[1,1,26],"1919":[2,1,1],"1920":[1,2,34],"1921":[5,2,58],"1922":[5,2,69],"1923":[5,2,46],"1924":[4,2,57],"1925":[7,2,60],"1926":[3,2,63],"1927":[3,2,50],"1928":[3,2,48],"1929":[4,2,66],"1930":[6,2,87],"1931":[2,2,42],"1932":[2,2,43],"1933":[2,4,153],"1934":[2,1,1],"1935":[2,2,1],"1936":[1,4,13],"1937":[1,4,14],"1938":[1,4,10],"1939":[1,4,11],"1940":[2,2,1],"1941":[1,4,9],"1942":[1,4,7],"1943":[1,4,7],"1944":[1,4,5],"1945":[2,2,1],"1946":[1,4,7],"1947":[1,4,7],"1948":[1,4,7],"1949":[1,4,8],"1950":[2,2,1],"1951":[1,4,10],"1952":[1,4,12],"1953":[1,4,7],"1954":[1,4,6],"1955":[2,2,1],"1956":[1,4,10],"1957":[1,4,10],"1958":[1,4,7],"1959":[1,4,5],"1960":[1,2,15],"1961":[1,2,24],"1962":[2,1,1],"1963":[4,2,14],"1964":[4,6,7],"1965":[2,2,6],"1966":[6,4,19],"1967":[2,4,1],"1968":[4,5,6],"1969":[2,5,15],"1970":[3,2,1],"1971":[5,5,10],"1972":[3,5,7],"1973":[3,5,9],"1974":[4,5,10],"1975":[5,5,9],"1976":[4,5,9],"1977":[4,5,9],"1978":[2,2,1],"1979":[3,4,4],"1980":[5,6,21],"1981":[9,6,11],"1982":[3,4,13],"1983":[3,2,1],"1984":[3,5,11],"1985":[3,5,16],"1986":[3,5,15],"1987":[3,5,19],"1988":[3,2,1],"1989":[5,5,19],"1990":[4,5,18],"1991":[5,5,17],"1992":[3,2,1],"1993":[3,5,19],"1994":[3,5,12],"1995":[2,2,1],"1996":[3,4,11],"1997":[2,2,41],"1998":[2,2,1],"1999":[2,2,86],"2000":[2,1,1],"2001":[1,1,7],"2002":[2,1,1],"2003":[1,2,23],"2004":[1,2,24],"2005":[1,2,28],"2006":[1,2,59],"2007":[1,2,24],"2008":[1,2,21],"2009":[1,1,94],"2010":[1,1,31],"2011":[1,1,27],"2012":[1,1,1],"2013":[2,1,3],"2014":[2,1,6],"2015":[3,3,1],"2016":[4,3,11],"2017":[4,6,15],"2018":[2,6,91],"2019":[3,6,17],"2020":[3,6,1],"2021":[3,3,26],"2022":[4,5,15],"2023":[3,5,19],"2024":[3,5,1],"2025":[4,5,1],"2026":[3,3,4],"2027":[4,5,8],"2028":[3,5,1],"2029":[3,5,1],"2030":[4,5,1],"2031":[2,1,14],"2032":[1,1,2],"2033":[2,1,1],"2034":[2,1,9],"2035":[3,1,4],"2036":[1,1,4],"2037":[1,1,1],"2038":[1,1,9],"2039":[1,1,4],"2040":[1,1,1],"2041":[2,1,1],"2042":[2,3,14],"2043":[2,3,74],"2044":[2,1,1],"2045":[2,3,12],"2046":[2,3,17],"2047":[2,1,1],"2048":[2,3,16],"2049":[2,3,22],"2050":[1,1,10],"2051":[2,1,69],"2052":[2,1,15],"2053":[2,2,20],"2054":[2,4,244],"2055":[3,4,3],"2056":[1,1,96],"2057":[4,1,5],"2058":[2,4,4],"2059":[3,4,64],"2060":[3,4,40],"2061":[3,4,32],"2062":[5,4,89],"2063":[2,1,61],"2064":[1,1,8],"2065":[2,1,1],"2066":[2,1,7],"2067":[3,1,6],"2068":[5,1,6],"2069":[3,6,17],"2070":[4,6,111],"2071":[1,1,1],"2072":[3,1,4],"2073":[5,3,59],"2074":[5,3,1],"2075":[5,7,37],"2076":[6,7,31],"2077":[3,7,43],"2078":[7,7,31],"2079":[6,7,72],"2080":[6,3,1],"2081":[3,8,35],"2082":[3,8,19],"2083":[2,8,25],"2084":[6,3,1],"2085":[4,8,6],"2086":[4,8,145],"2087":[2,3,53],"2088":[1,1,10],"2089":[2,1,12],"2090":[2,1,14],"2091":[2,1,3],"2092":[2,1,1],"2093":[4,3,10],"2094":[4,3,8],"2095":[4,1,10],"2096":[1,1,20],"2097":[3,1,35],"2098":[2,1,1],"2099":[4,3,14],"2100":[3,3,13],"2101":[4,3,19],"2102":[4,3,13],"2103":[2,1,9],"2104":[4,3,16],"2105":[4,3,15],"2106":[3,3,12],"2107":[2,1,21],"2108":[2,1,21],"2109":[1,1,12],"2110":[1,1,1],"2111":[3,1,9],"2112":[2,3,21],"2113":[2,3,1],"2114":[3,4,6],"2115":[2,4,21],"2116":[2,3,1],"2117":[3,4,17],"2118":[3,4,12],"2119":[2,4,13],"2120":[2,3,11],"2121":[3,1,19],"2122":[3,3,1],"2123":[3,6,7],"2124":[4,6,7],"2125":[4,6,16],"2126":[4,6,5],"2127":[4,3,1],"2128":[2,3,1],"2129":[4,5,6],"2130":[3,5,7],"2131":[4,5,21],"2132":[3,3,1],"2133":[4,6,4],"2134":[4,6,9],"2135":[3,6,19],"2136":[3,3,1],"2137":[4,6,10],"2138":[4,6,4],"2139":[4,6,21],"2140":[4,3,12],"2141":[3,1,1],"2142":[1,1,1],"2143":[1,1,1],"2144":[1,1,1],"2145":[1,1,1],"2146":[1,1,10],"2147":[2,1,9],"2148":[2,1,3],"2149":[3,1,10],"2150":[3,1,3],"2151":[2,1,9],"2152":[2,1,19],"2153":[4,1,31],"2154":[2,1,15],"2155":[2,1,12],"2156":[2,1,5],"2157":[2,1,8]},"averageFieldLength":[2.7187210379981472,4.4017608897126905,37.01529193697868],"storedFields":{"0":{"title":"探秘Transformer系列之（12）--- 多头自注意力","titles":[]},"1":{"title":"0x00 概述","titles":["探秘Transformer系列之（12）--- 多头自注意力"]},"2":{"title":"0x01 研究背景","titles":["探秘Transformer系列之（12）--- 多头自注意力"]},"3":{"title":"1.1 问题","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x01 研究背景"]},"4":{"title":"1.2 根源","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x01 研究背景"]},"5":{"title":"1.3 解决方案","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x01 研究背景"]},"6":{"title":"0x02 原理","titles":["探秘Transformer系列之（12）--- 多头自注意力"]},"7":{"title":"2.1 架构图","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x02 原理"]},"8":{"title":"偏置","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x02 原理","2.1 架构图"]},"9":{"title":"权重矩阵","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x02 原理","2.1 架构图"]},"10":{"title":"WOWOW^O矩阵","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x02 原理","2.1 架构图"]},"11":{"title":"2.2 设计思路","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x02 原理"]},"12":{"title":"子空间&amp;分治","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x02 原理","2.2 设计思路"]},"13":{"title":"ensemble&amp;融合","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x02 原理","2.2 设计思路"]},"14":{"title":"缓解稀疏","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x02 原理","2.2 设计思路"]},"15":{"title":"2.3 计算","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x02 原理"]},"16":{"title":"计算流程","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x02 原理","2.3 计算"]},"17":{"title":"计算强度","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x02 原理","2.3 计算"]},"18":{"title":"2.4 效果","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x02 原理"]},"19":{"title":"2.5 融合方式","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x02 原理"]},"20":{"title":"2.6 分析","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x02 原理"]},"21":{"title":"2.7 优点","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x02 原理"]},"22":{"title":"0x03 实现","titles":["探秘Transformer系列之（12）--- 多头自注意力"]},"23":{"title":"3.1 定义","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x03 实现"]},"24":{"title":"3.2 运算逻辑","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x03 实现"]},"25":{"title":"输入","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x03 实现","3.2 运算逻辑"]},"26":{"title":"投影","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x03 实现","3.2 运算逻辑"]},"27":{"title":"切分数据","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x03 实现","3.2 运算逻辑"]},"28":{"title":"逻辑角度","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x03 实现","3.2 运算逻辑","切分数据"]},"29":{"title":"物理角度","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x03 实现","3.2 运算逻辑","切分数据"]},"30":{"title":"小结","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x03 实现","3.2 运算逻辑","切分数据"]},"31":{"title":"调整维度","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x03 实现","3.2 运算逻辑"]},"32":{"title":"为每个头计算注意力","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x03 实现","3.2 运算逻辑"]},"33":{"title":"单独分组","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x03 实现","3.2 运算逻辑","为每个头计算注意力"]},"34":{"title":"并行","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x03 实现","3.2 运算逻辑","为每个头计算注意力"]},"35":{"title":"融合每个头的Z","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x03 实现","3.2 运算逻辑"]},"36":{"title":"forward()函数","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x03 实现","3.2 运算逻辑"]},"37":{"title":"3.3 调用","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x03 实现"]},"38":{"title":"编码器","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x03 实现","3.3 调用"]},"39":{"title":"解码器","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x03 实现","3.3 调用"]},"40":{"title":"0x04 改进","titles":["探秘Transformer系列之（12）--- 多头自注意力"]},"41":{"title":"4.1 MOHSA","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x04 改进"]},"42":{"title":"4.2 MoH","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x04 改进"]},"43":{"title":"4.3 DCMHA","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x04 改进"]},"44":{"title":"研究背景","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x04 改进","4.3 DCMHA"]},"45":{"title":"动机","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x04 改进","4.3 DCMHA"]},"46":{"title":"思路","titles":["探秘Transformer系列之（12）--- 多头自注意力","0x04 改进","4.3 DCMHA"]},"47":{"title":"0xFF 参考","titles":["探秘Transformer系列之（12）--- 多头自注意力"]},"48":{"title":"Transformer系列","titles":[]},"49":{"title":"探秘Transformer系列之（11）--- 掩码","titles":[]},"50":{"title":"0x00 概述","titles":["探秘Transformer系列之（11）--- 掩码"]},"51":{"title":"0x01 需求","titles":["探秘Transformer系列之（11）--- 掩码"]},"52":{"title":"1.1 避免偏差","titles":["探秘Transformer系列之（11）--- 掩码","0x01 需求"]},"53":{"title":"实际情况","titles":["探秘Transformer系列之（11）--- 掩码","0x01 需求","1.1 避免偏差"]},"54":{"title":"问题所在","titles":["探秘Transformer系列之（11）--- 掩码","0x01 需求","1.1 避免偏差"]},"55":{"title":"解决方案","titles":["探秘Transformer系列之（11）--- 掩码","0x01 需求","1.1 避免偏差"]},"56":{"title":"1.2 防止偷看","titles":["探秘Transformer系列之（11）--- 掩码","0x01 需求"]},"57":{"title":"实际情况","titles":["探秘Transformer系列之（11）--- 掩码","0x01 需求","1.2 防止偷看"]},"58":{"title":"问题所在","titles":["探秘Transformer系列之（11）--- 掩码","0x01 需求","1.2 防止偷看"]},"59":{"title":"解决方案","titles":["探秘Transformer系列之（11）--- 掩码","0x01 需求","1.2 防止偷看"]},"60":{"title":"0x02 Padding Mask","titles":["探秘Transformer系列之（11）--- 掩码"]},"61":{"title":"2.1 逻辑","titles":["探秘Transformer系列之（11）--- 掩码","0x02 Padding Mask"]},"62":{"title":"掩码矩阵","titles":["探秘Transformer系列之（11）--- 掩码","0x02 Padding Mask","2.1 逻辑"]},"63":{"title":"计算注意力步骤","titles":["探秘Transformer系列之（11）--- 掩码","0x02 Padding Mask","2.1 逻辑"]},"64":{"title":"2.2 实现","titles":["探秘Transformer系列之（11）--- 掩码","0x02 Padding Mask"]},"65":{"title":"设置填充符号","titles":["探秘Transformer系列之（11）--- 掩码","0x02 Padding Mask","2.2 实现"]},"66":{"title":"建立mask","titles":["探秘Transformer系列之（11）--- 掩码","0x02 Padding Mask","2.2 实现"]},"67":{"title":"实施mask","titles":["探秘Transformer系列之（11）--- 掩码","0x02 Padding Mask","2.2 实现"]},"68":{"title":"0x03 Sequence mask","titles":["探秘Transformer系列之（11）--- 掩码"]},"69":{"title":"3.1 逻辑","titles":["探秘Transformer系列之（11）--- 掩码","0x03 Sequence mask"]},"70":{"title":"掩码矩阵","titles":["探秘Transformer系列之（11）--- 掩码","0x03 Sequence mask","3.1 逻辑"]},"71":{"title":"掩码自注意力","titles":["探秘Transformer系列之（11）--- 掩码","0x03 Sequence mask","3.1 逻辑"]},"72":{"title":"交叉注意力","titles":["探秘Transformer系列之（11）--- 掩码","0x03 Sequence mask","3.1 逻辑"]},"73":{"title":"3.2 实现","titles":["探秘Transformer系列之（11）--- 掩码","0x03 Sequence mask"]},"74":{"title":"生成掩码","titles":["探秘Transformer系列之（11）--- 掩码","0x03 Sequence mask","3.2 实现"]},"75":{"title":"施加掩码","titles":["探秘Transformer系列之（11）--- 掩码","0x03 Sequence mask","3.2 实现"]},"76":{"title":"3.3 Transformer","titles":["探秘Transformer系列之（11）--- 掩码","0x03 Sequence mask"]},"77":{"title":"0x04 数据流","titles":["探秘Transformer系列之（11）--- 掩码"]},"78":{"title":"4.1 如何应用于注意力","titles":["探秘Transformer系列之（11）--- 掩码","0x04 数据流"]},"79":{"title":"4.2 变量说明","titles":["探秘Transformer系列之（11）--- 掩码","0x04 数据流"]},"80":{"title":"Encoder数据流","titles":["探秘Transformer系列之（11）--- 掩码","0x04 数据流","4.2 变量说明"]},"81":{"title":"4.3 使用","titles":["探秘Transformer系列之（11）--- 掩码","0x04 数据流"]},"82":{"title":"训练","titles":["探秘Transformer系列之（11）--- 掩码","0x04 数据流","4.3 使用"]},"83":{"title":"推理","titles":["探秘Transformer系列之（11）--- 掩码","0x04 数据流","4.3 使用"]},"84":{"title":"4.4 PyTorch","titles":["探秘Transformer系列之（11）--- 掩码","0x04 数据流"]},"85":{"title":"4.5 小结","titles":["探秘Transformer系列之（11）--- 掩码","0x04 数据流"]},"86":{"title":"0x05 进阶","titles":["探秘Transformer系列之（11）--- 掩码"]},"87":{"title":"5.1 Sample Packing和mask","titles":["探秘Transformer系列之（11）--- 掩码","0x05 进阶"]},"88":{"title":"定义","titles":["探秘Transformer系列之（11）--- 掩码","0x05 进阶","5.1 Sample Packing和mask"]},"89":{"title":"Attention mask","titles":["探秘Transformer系列之（11）--- 掩码","0x05 进阶","5.1 Sample Packing和mask"]},"90":{"title":"策略","titles":["探秘Transformer系列之（11）--- 掩码","0x05 进阶","5.1 Sample Packing和mask"]},"91":{"title":"5.2 功用","titles":["探秘Transformer系列之（11）--- 掩码","0x05 进阶"]},"92":{"title":"创新点","titles":["探秘Transformer系列之（11）--- 掩码","0x05 进阶","5.2 功用"]},"93":{"title":"掩码注意力","titles":["探秘Transformer系列之（11）--- 掩码","0x05 进阶","5.2 功用"]},"94":{"title":"带LayerNorm的掩码注意力","titles":["探秘Transformer系列之（11）--- 掩码","0x05 进阶","5.2 功用"]},"95":{"title":"0xFF 参考","titles":["探秘Transformer系列之（11）--- 掩码"]},"96":{"title":"探秘Transformer系列之（13）--- FFN","titles":[]},"97":{"title":"0x00 概述","titles":["探秘Transformer系列之（13）--- FFN"]},"98":{"title":"0x01 网络结构","titles":["探秘Transformer系列之（13）--- FFN"]},"99":{"title":"1.1 数学表示","titles":["探秘Transformer系列之（13）--- FFN","0x01 网络结构"]},"100":{"title":"1.2 中间层比率","titles":["探秘Transformer系列之（13）--- FFN","0x01 网络结构"]},"101":{"title":"1.3 position-wise","titles":["探秘Transformer系列之（13）--- FFN","0x01 网络结构"]},"102":{"title":"1.4 激活函数","titles":["探秘Transformer系列之（13）--- FFN","0x01 网络结构"]},"103":{"title":"常见函数","titles":["探秘Transformer系列之（13）--- FFN","0x01 网络结构","1.4 激活函数"]},"104":{"title":"ReLU","titles":["探秘Transformer系列之（13）--- FFN","0x01 网络结构","1.4 激活函数"]},"105":{"title":"GLU","titles":["探秘Transformer系列之（13）--- FFN","0x01 网络结构","1.4 激活函数"]},"106":{"title":"GELU","titles":["探秘Transformer系列之（13）--- FFN","0x01 网络结构","1.4 激活函数"]},"107":{"title":"SwiGLU","titles":["探秘Transformer系列之（13）--- FFN","0x01 网络结构","1.4 激活函数"]},"108":{"title":"Swish函数","titles":["探秘Transformer系列之（13）--- FFN","0x01 网络结构","1.4 激活函数","SwiGLU"]},"109":{"title":"SwiGLU激活函数","titles":["探秘Transformer系列之（13）--- FFN","0x01 网络结构","1.4 激活函数","SwiGLU"]},"110":{"title":"实现","titles":["探秘Transformer系列之（13）--- FFN","0x01 网络结构","1.4 激活函数","SwiGLU"]},"111":{"title":"dReLU","titles":["探秘Transformer系列之（13）--- FFN","0x01 网络结构","1.4 激活函数"]},"112":{"title":"0x02 实现","titles":["探秘Transformer系列之（13）--- FFN"]},"113":{"title":"2.1 哈佛代码","titles":["探秘Transformer系列之（13）--- FFN","0x02 实现"]},"114":{"title":"2.2 llama3","titles":["探秘Transformer系列之（13）--- FFN","0x02 实现"]},"115":{"title":"0x03 FFN的作用","titles":["探秘Transformer系列之（13）--- FFN"]},"116":{"title":"3.1 提取更多语义信息","titles":["探秘Transformer系列之（13）--- FFN","0x03 FFN的作用"]},"117":{"title":"3.2 增加表达能力","titles":["探秘Transformer系列之（13）--- FFN","0x03 FFN的作用"]},"118":{"title":"3.3 存储知识","titles":["探秘Transformer系列之（13）--- FFN","0x03 FFN的作用"]},"119":{"title":"3.4 增加参数量","titles":["探秘Transformer系列之（13）--- FFN","0x03 FFN的作用"]},"120":{"title":"3.5 小结","titles":["探秘Transformer系列之（13）--- FFN","0x03 FFN的作用"]},"121":{"title":"0x04 知识利用","titles":["探秘Transformer系列之（13）--- FFN"]},"122":{"title":"4.1 提取步骤","titles":["探秘Transformer系列之（13）--- FFN","0x04 知识利用"]},"123":{"title":"4.2 知识记忆","titles":["探秘Transformer系列之（13）--- FFN","0x04 知识利用"]},"124":{"title":"键值对形式","titles":["探秘Transformer系列之（13）--- FFN","0x04 知识利用","4.2 知识记忆"]},"125":{"title":"记忆网络","titles":["探秘Transformer系列之（13）--- FFN","0x04 知识利用","4.2 知识记忆","键值对形式"]},"126":{"title":"Key-Value","titles":["探秘Transformer系列之（13）--- FFN","0x04 知识利用","4.2 知识记忆","键值对形式"]},"127":{"title":"Key模式","titles":["探秘Transformer系列之（13）--- FFN","0x04 知识利用","4.2 知识记忆","键值对形式"]},"128":{"title":"值向量表示的是分布","titles":["探秘Transformer系列之（13）--- FFN","0x04 知识利用","4.2 知识记忆","键值对形式"]},"129":{"title":"分布式存储和记忆聚合","titles":["探秘Transformer系列之（13）--- FFN","0x04 知识利用","4.2 知识记忆","键值对形式"]},"130":{"title":"知识回路","titles":["探秘Transformer系列之（13）--- FFN","0x04 知识利用","4.2 知识记忆"]},"131":{"title":"注意力模块","titles":["探秘Transformer系列之（13）--- FFN","0x04 知识利用","4.2 知识记忆"]},"132":{"title":"4.3 知识的定位","titles":["探秘Transformer系列之（13）--- FFN","0x04 知识利用"]},"133":{"title":"事实的定位","titles":["探秘Transformer系列之（13）--- FFN","0x04 知识利用","4.3 知识的定位"]},"134":{"title":"知识归因 (Knowledge Attribution)","titles":["探秘Transformer系列之（13）--- FFN","0x04 知识利用","4.3 知识的定位","事实的定位"]},"135":{"title":"精炼神经元 (Knowledge Neuron Refining)","titles":["探秘Transformer系列之（13）--- FFN","0x04 知识利用","4.3 知识的定位","事实的定位"]},"136":{"title":"关系的定位","titles":["探秘Transformer系列之（13）--- FFN","0x04 知识利用","4.3 知识的定位"]},"137":{"title":"字典学习和稀疏自编码器","titles":["探秘Transformer系列之（13）--- FFN","0x04 知识利用","4.3 知识的定位"]},"138":{"title":"4.3 修改知识","titles":["探秘Transformer系列之（13）--- FFN","0x04 知识利用"]},"139":{"title":"相关路线","titles":["探秘Transformer系列之（13）--- FFN","0x04 知识利用","4.3 修改知识"]},"140":{"title":"功能","titles":["探秘Transformer系列之（13）--- FFN","0x04 知识利用","4.3 修改知识"]},"141":{"title":"分类","titles":["探秘Transformer系列之（13）--- FFN","0x04 知识利用","4.3 修改知识"]},"142":{"title":"内在知识编辑","titles":["探秘Transformer系列之（13）--- FFN","0x04 知识利用","4.3 修改知识"]},"143":{"title":"FFN","titles":["探秘Transformer系列之（13）--- FFN","0x04 知识利用","4.3 修改知识","内在知识编辑"]},"144":{"title":"注意力头","titles":["探秘Transformer系列之（13）--- FFN","0x04 知识利用","4.3 修改知识","内在知识编辑"]},"145":{"title":"ROME","titles":["探秘Transformer系列之（13）--- FFN","0x04 知识利用","4.3 修改知识","内在知识编辑"]},"146":{"title":"4.4 学习知识","titles":["探秘Transformer系列之（13）--- FFN","0x04 知识利用"]},"147":{"title":"前向传播","titles":["探秘Transformer系列之（13）--- FFN","0x04 知识利用","4.4 学习知识"]},"148":{"title":"反向传播","titles":["探秘Transformer系列之（13）--- FFN","0x04 知识利用","4.4 学习知识"]},"149":{"title":"0x05 优化与演进","titles":["探秘Transformer系列之（13）--- FFN"]},"150":{"title":"5.1 MoE","titles":["探秘Transformer系列之（13）--- FFN","0x05 优化与演进"]},"151":{"title":"5.2 MemoryFormer","titles":["探秘Transformer系列之（13）--- FFN","0x05 优化与演进"]},"152":{"title":"动机与挑战","titles":["探秘Transformer系列之（13）--- FFN","0x05 优化与演进","5.2 MemoryFormer"]},"153":{"title":"原理与创新","titles":["探秘Transformer系列之（13）--- FFN","0x05 优化与演进","5.2 MemoryFormer"]},"154":{"title":"5.3 Memory Layers at Scale","titles":["探秘Transformer系列之（13）--- FFN","0x05 优化与演进"]},"155":{"title":"5.4 KAN","titles":["探秘Transformer系列之（13）--- FFN","0x05 优化与演进"]},"156":{"title":"0xFF 参考","titles":["探秘Transformer系列之（13）--- FFN"]},"157":{"title":"探秘Transformer系列之（10）--- 自注意力","titles":[]},"158":{"title":"0x00 概述","titles":["探秘Transformer系列之（10）--- 自注意力"]},"159":{"title":"0x01 原理","titles":["探秘Transformer系列之（10）--- 自注意力"]},"160":{"title":"1.1 设计思路","titles":["探秘Transformer系列之（10）--- 自注意力","0x01 原理"]},"161":{"title":"1.2 输入","titles":["探秘Transformer系列之（10）--- 自注意力","0x01 原理"]},"162":{"title":"1.3 QKV解析","titles":["探秘Transformer系列之（10）--- 自注意力","0x01 原理"]},"163":{"title":"心理学角度","titles":["探秘Transformer系列之（10）--- 自注意力","0x01 原理","1.3 QKV解析"]},"164":{"title":"数据库角度","titles":["探秘Transformer系列之（10）--- 自注意力","0x01 原理","1.3 QKV解析"]},"165":{"title":"seq2seq角度","titles":["探秘Transformer系列之（10）--- 自注意力","0x01 原理","1.3 QKV解析"]},"166":{"title":"重构词向量角度","titles":["探秘Transformer系列之（10）--- 自注意力","0x01 原理","1.3 QKV解析"]},"167":{"title":"相互操作","titles":["探秘Transformer系列之（10）--- 自注意力","0x01 原理","1.3 QKV解析","重构词向量角度"]},"168":{"title":"提取特征","titles":["探秘Transformer系列之（10）--- 自注意力","0x01 原理","1.3 QKV解析","重构词向量角度"]},"169":{"title":"加权求和","titles":["探秘Transformer系列之（10）--- 自注意力","0x01 原理","1.3 QKV解析","重构词向量角度"]},"170":{"title":"1.4 小结","titles":["探秘Transformer系列之（10）--- 自注意力","0x01 原理"]},"171":{"title":"0x02 实现","titles":["探秘Transformer系列之（10）--- 自注意力"]},"172":{"title":"2.1 权重矩阵","titles":["探秘Transformer系列之（10）--- 自注意力","0x02 实现"]},"173":{"title":"2.2 计算过程","titles":["探秘Transformer系列之（10）--- 自注意力","0x02 实现"]},"174":{"title":"2.3 点积注意力函数","titles":["探秘Transformer系列之（10）--- 自注意力","0x02 实现"]},"175":{"title":"方案选择","titles":["探秘Transformer系列之（10）--- 自注意力","0x02 实现","2.3 点积注意力函数"]},"176":{"title":"解读","titles":["探秘Transformer系列之（10）--- 自注意力","0x02 实现","2.3 点积注意力函数"]},"177":{"title":"2.4 softmax","titles":["探秘Transformer系列之（10）--- 自注意力","0x02 实现"]},"178":{"title":"定义","titles":["探秘Transformer系列之（10）--- 自注意力","0x02 实现","2.4 softmax"]},"179":{"title":"算法","titles":["探秘Transformer系列之（10）--- 自注意力","0x02 实现","2.4 softmax"]},"180":{"title":"必要性","titles":["探秘Transformer系列之（10）--- 自注意力","0x02 实现","2.4 softmax"]},"181":{"title":"缺点","titles":["探秘Transformer系列之（10）--- 自注意力","0x02 实现","2.4 softmax"]},"182":{"title":"改进","titles":["探秘Transformer系列之（10）--- 自注意力","0x02 实现","2.4 softmax"]},"183":{"title":"Log-Softmax","titles":["探秘Transformer系列之（10）--- 自注意力","0x02 实现","2.4 softmax","改进"]},"184":{"title":"Hierarchical Softmax(H-Softmax)","titles":["探秘Transformer系列之（10）--- 自注意力","0x02 实现","2.4 softmax","改进"]},"185":{"title":"adaptive softmax","titles":["探秘Transformer系列之（10）--- 自注意力","0x02 实现","2.4 softmax","改进"]},"186":{"title":"2.5 缩放","titles":["探秘Transformer系列之（10）--- 自注意力","0x02 实现"]},"187":{"title":"结论","titles":["探秘Transformer系列之（10）--- 自注意力","0x02 实现","2.5 缩放"]},"188":{"title":"问题推导","titles":["探秘Transformer系列之（10）--- 自注意力","0x02 实现","2.5 缩放"]},"189":{"title":"方差变大","titles":["探秘Transformer系列之（10）--- 自注意力","0x02 实现","2.5 缩放","问题推导"]},"190":{"title":"元素间差值变大","titles":["探秘Transformer系列之（10）--- 自注意力","0x02 实现","2.5 缩放","问题推导"]},"191":{"title":"softmax退化","titles":["探秘Transformer系列之（10）--- 自注意力","0x02 实现","2.5 缩放","问题推导"]},"192":{"title":"梯度消失","titles":["探秘Transformer系列之（10）--- 自注意力","0x02 实现","2.5 缩放","问题推导"]},"193":{"title":"如何降低方差？","titles":["探秘Transformer系列之（10）--- 自注意力","0x02 实现","2.5 缩放","问题推导"]},"194":{"title":"熵的作用","titles":["探秘Transformer系列之（10）--- 自注意力","0x02 实现","2.5 缩放"]},"195":{"title":"2.6 小结","titles":["探秘Transformer系列之（10）--- 自注意力","0x02 实现"]},"196":{"title":"0x03 实现","titles":["探秘Transformer系列之（10）--- 自注意力"]},"197":{"title":"3.1 哈佛代码","titles":["探秘Transformer系列之（10）--- 自注意力","0x03 实现"]},"198":{"title":"输入&amp;输出","titles":["探秘Transformer系列之（10）--- 自注意力","0x03 实现","3.1 哈佛代码"]},"199":{"title":"图例&amp;代码","titles":["探秘Transformer系列之（10）--- 自注意力","0x03 实现","3.1 哈佛代码"]},"200":{"title":"再分析注意力","titles":["探秘Transformer系列之（10）--- 自注意力","0x03 实现","3.1 哈佛代码"]},"201":{"title":"3.2 llama3","titles":["探秘Transformer系列之（10）--- 自注意力","0x03 实现"]},"202":{"title":"0x04 优化","titles":["探秘Transformer系列之（10）--- 自注意力"]},"203":{"title":"4.1 优化策略","titles":["探秘Transformer系列之（10）--- 自注意力","0x04 优化"]},"204":{"title":"从序列角度优化","titles":["探秘Transformer系列之（10）--- 自注意力","0x04 优化","4.1 优化策略"]},"205":{"title":"从多头角度优化","titles":["探秘Transformer系列之（10）--- 自注意力","0x04 优化","4.1 优化策略"]},"206":{"title":"从软硬件层面优化 MHA","titles":["探秘Transformer系列之（10）--- 自注意力","0x04 优化","4.1 优化策略"]},"207":{"title":"从其它角度优化","titles":["探秘Transformer系列之（10）--- 自注意力","0x04 优化","4.1 优化策略"]},"208":{"title":"4.2 案例","titles":["探秘Transformer系列之（10）--- 自注意力","0x04 优化"]},"209":{"title":"注意力权重细化","titles":["探秘Transformer系列之（10）--- 自注意力","0x04 优化","4.2 案例"]},"210":{"title":"线性注意力","titles":["探秘Transformer系列之（10）--- 自注意力","0x04 优化","4.2 案例"]},"211":{"title":"PolaFormer","titles":["探秘Transformer系列之（10）--- 自注意力","0x04 优化","4.2 案例"]},"212":{"title":"研究背景","titles":["探秘Transformer系列之（10）--- 自注意力","0x04 优化","4.2 案例","PolaFormer"]},"213":{"title":"思路","titles":["探秘Transformer系列之（10）--- 自注意力","0x04 优化","4.2 案例","PolaFormer"]},"214":{"title":"MiniMax-01","titles":["探秘Transformer系列之（10）--- 自注意力","0x04 优化","4.2 案例"]},"215":{"title":"模型架构","titles":["探秘Transformer系列之（10）--- 自注意力","0x04 优化","4.2 案例","MiniMax-01"]},"216":{"title":"Lightning Attention","titles":["探秘Transformer系列之（10）--- 自注意力","0x04 优化","4.2 案例","MiniMax-01"]},"217":{"title":"Hybrid-lightning","titles":["探秘Transformer系列之（10）--- 自注意力","0x04 优化","4.2 案例","MiniMax-01"]},"218":{"title":"Transformer²","titles":["探秘Transformer系列之（10）--- 自注意力","0x04 优化","4.2 案例"]},"219":{"title":"研究背景","titles":["探秘Transformer系列之（10）--- 自注意力","0x04 优化","4.2 案例","Transformer²"]},"220":{"title":"自适应性","titles":["探秘Transformer系列之（10）--- 自注意力","0x04 优化","4.2 案例","Transformer²","研究背景"]},"221":{"title":"SVD","titles":["探秘Transformer系列之（10）--- 自注意力","0x04 优化","4.2 案例","Transformer²","研究背景"]},"222":{"title":"研究动机","titles":["探秘Transformer系列之（10）--- 自注意力","0x04 优化","4.2 案例","Transformer²"]},"223":{"title":"思路","titles":["探秘Transformer系列之（10）--- 自注意力","0x04 优化","4.2 案例","Transformer²"]},"224":{"title":"奇异值微调（SVF）","titles":["探秘Transformer系列之（10）--- 自注意力","0x04 优化","4.2 案例","Transformer²","思路"]},"225":{"title":"自适应性","titles":["探秘Transformer系列之（10）--- 自注意力","0x04 优化","4.2 案例","Transformer²","思路"]},"226":{"title":"Titans","titles":["探秘Transformer系列之（10）--- 自注意力","0x04 优化","4.2 案例"]},"227":{"title":"研究背景和动机","titles":["探秘Transformer系列之（10）--- 自注意力","0x04 优化","4.2 案例","Titans"]},"228":{"title":"核心创新","titles":["探秘Transformer系列之（10）--- 自注意力","0x04 优化","4.2 案例","Titans"]},"229":{"title":"Titans架构","titles":["探秘Transformer系列之（10）--- 自注意力","0x04 优化","4.2 案例","Titans"]},"230":{"title":"长期记忆","titles":["探秘Transformer系列之（10）--- 自注意力","0x04 优化","4.2 案例","Titans"]},"231":{"title":"融合记忆","titles":["探秘Transformer系列之（10）--- 自注意力","0x04 优化","4.2 案例","Titans"]},"232":{"title":"SANA","titles":["探秘Transformer系列之（10）--- 自注意力","0x04 优化","4.2 案例"]},"233":{"title":"0xFF 参考","titles":["探秘Transformer系列之（10）--- 自注意力"]},"234":{"title":"探秘Transformer系列之（1）：注意力机制","titles":[]},"235":{"title":"0x00 概述","titles":["探秘Transformer系列之（1）：注意力机制"]},"236":{"title":"0x01 背景知识","titles":["探秘Transformer系列之（1）：注意力机制"]},"237":{"title":"1.1 seq2seq","titles":["探秘Transformer系列之（1）：注意力机制","0x01 背景知识"]},"238":{"title":"1.2 文本生成机制","titles":["探秘Transformer系列之（1）：注意力机制","0x01 背景知识"]},"239":{"title":"1.3 自回归模型","titles":["探秘Transformer系列之（1）：注意力机制","0x01 背景知识"]},"240":{"title":"1.4 隐变量自回归模型","titles":["探秘Transformer系列之（1）：注意力机制","0x01 背景知识"]},"241":{"title":"1.5 编码器-解码器模型","titles":["探秘Transformer系列之（1）：注意力机制","0x01 背景知识"]},"242":{"title":"1.6 如何压缩","titles":["探秘Transformer系列之（1）：注意力机制","0x01 背景知识"]},"243":{"title":"0x02 CNN和RNN方案","titles":["探秘Transformer系列之（1）：注意力机制"]},"244":{"title":"2.1 技术挑战","titles":["探秘Transformer系列之（1）：注意力机制","0x02 CNN和RNN方案"]},"245":{"title":"对齐","titles":["探秘Transformer系列之（1）：注意力机制","0x02 CNN和RNN方案","2.1 技术挑战"]},"246":{"title":"长依赖","titles":["探秘Transformer系列之（1）：注意力机制","0x02 CNN和RNN方案","2.1 技术挑战"]},"247":{"title":"2.2 CNN方案","titles":["探秘Transformer系列之（1）：注意力机制","0x02 CNN和RNN方案"]},"248":{"title":"2.3 RNN方案","titles":["探秘Transformer系列之（1）：注意力机制","0x02 CNN和RNN方案"]},"249":{"title":"思路","titles":["探秘Transformer系列之（1）：注意力机制","0x02 CNN和RNN方案","2.3 RNN方案"]},"250":{"title":"优点","titles":["探秘Transformer系列之（1）：注意力机制","0x02 CNN和RNN方案","2.3 RNN方案"]},"251":{"title":"缺点","titles":["探秘Transformer系列之（1）：注意力机制","0x02 CNN和RNN方案","2.3 RNN方案"]},"252":{"title":"表达能力缺失","titles":["探秘Transformer系列之（1）：注意力机制","0x02 CNN和RNN方案","2.3 RNN方案","缺点"]},"253":{"title":"信息遗失","titles":["探秘Transformer系列之（1）：注意力机制","0x02 CNN和RNN方案","2.3 RNN方案","缺点"]},"254":{"title":"难以并行","titles":["探秘Transformer系列之（1）：注意力机制","0x02 CNN和RNN方案","2.3 RNN方案","缺点"]},"255":{"title":"难以训练","titles":["探秘Transformer系列之（1）：注意力机制","0x02 CNN和RNN方案","2.3 RNN方案","缺点"]},"256":{"title":"2.4 当前问题","titles":["探秘Transformer系列之（1）：注意力机制","0x02 CNN和RNN方案"]},"257":{"title":"0x03 注意力机制","titles":["探秘Transformer系列之（1）：注意力机制"]},"258":{"title":"3.1 原理","titles":["探秘Transformer系列之（1）：注意力机制","0x03 注意力机制"]},"259":{"title":"上下文决定一切","titles":["探秘Transformer系列之（1）：注意力机制","0x03 注意力机制","3.1 原理"]},"260":{"title":"资源分配","titles":["探秘Transformer系列之（1）：注意力机制","0x03 注意力机制","3.1 原理"]},"261":{"title":"信息交换","titles":["探秘Transformer系列之（1）：注意力机制","0x03 注意力机制","3.1 原理"]},"262":{"title":"3.2 通用结构","titles":["探秘Transformer系列之（1）：注意力机制","0x03 注意力机制"]},"263":{"title":"任务模型","titles":["探秘Transformer系列之（1）：注意力机制","0x03 注意力机制","3.2 通用结构"]},"264":{"title":"注意力模型","titles":["探秘Transformer系列之（1）：注意力机制","0x03 注意力机制","3.2 通用结构"]},"265":{"title":"QKV","titles":["探秘Transformer系列之（1）：注意力机制","0x03 注意力机制","3.2 通用结构"]},"266":{"title":"3.3 计算流程","titles":["探秘Transformer系列之（1）：注意力机制","0x03 注意力机制"]},"267":{"title":"思路","titles":["探秘Transformer系列之（1）：注意力机制","0x03 注意力机制","3.3 计算流程"]},"268":{"title":"注意力分数","titles":["探秘Transformer系列之（1）：注意力机制","0x03 注意力机制","3.3 计算流程"]},"269":{"title":"注意力权重","titles":["探秘Transformer系列之（1）：注意力机制","0x03 注意力机制","3.3 计算流程"]},"270":{"title":"加权求和","titles":["探秘Transformer系列之（1）：注意力机制","0x03 注意力机制","3.3 计算流程"]},"271":{"title":"小结","titles":["探秘Transformer系列之（1）：注意力机制","0x03 注意力机制","3.3 计算流程"]},"272":{"title":"3.4 问题解决","titles":["探秘Transformer系列之（1）：注意力机制","0x03 注意力机制"]},"273":{"title":"增大信息含量","titles":["探秘Transformer系列之（1）：注意力机制","0x03 注意力机制","3.4 问题解决"]},"274":{"title":"缩减单词间距","titles":["探秘Transformer系列之（1）：注意力机制","0x03 注意力机制","3.4 问题解决"]},"275":{"title":"选择性处理信息","titles":["探秘Transformer系列之（1）：注意力机制","0x03 注意力机制","3.4 问题解决"]},"276":{"title":"加权求和","titles":["探秘Transformer系列之（1）：注意力机制","0x03 注意力机制","3.4 问题解决","选择性处理信息"]},"277":{"title":"对齐机制","titles":["探秘Transformer系列之（1）：注意力机制","0x03 注意力机制","3.4 问题解决","选择性处理信息"]},"278":{"title":"同时前瞻和回顾","titles":["探秘Transformer系列之（1）：注意力机制","0x03 注意力机制","3.4 问题解决"]},"279":{"title":"3.5 优劣","titles":["探秘Transformer系列之（1）：注意力机制","0x03 注意力机制"]},"280":{"title":"0x04 注意力发展历史","titles":["探秘Transformer系列之（1）：注意力机制"]},"281":{"title":"4.1 RCTM","titles":["探秘Transformer系列之（1）：注意力机制","0x04 注意力发展历史"]},"282":{"title":"4.2 RNN 编码器-解码器","titles":["探秘Transformer系列之（1）：注意力机制","0x04 注意力发展历史"]},"283":{"title":"4.3 Sequence to Sequence Learning with Neural Networks","titles":["探秘Transformer系列之（1）：注意力机制","0x04 注意力发展历史"]},"284":{"title":"4.4 Bahdanau Attention","titles":["探秘Transformer系列之（1）：注意力机制","0x04 注意力发展历史"]},"285":{"title":"4.5 Luong Attention","titles":["探秘Transformer系列之（1）：注意力机制","0x04 注意力发展历史"]},"286":{"title":"4.6 ResNet","titles":["探秘Transformer系列之（1）：注意力机制","0x04 注意力发展历史"]},"287":{"title":"4.7 Self Attention","titles":["探秘Transformer系列之（1）：注意力机制","0x04 注意力发展历史"]},"288":{"title":"4.8 QKV-Attention","titles":["探秘Transformer系列之（1）：注意力机制","0x04 注意力发展历史"]},"289":{"title":"4.9 MultiHead Self Attention","titles":["探秘Transformer系列之（1）：注意力机制","0x04 注意力发展历史"]},"290":{"title":"4.10 Multi-step Attention","titles":["探秘Transformer系列之（1）：注意力机制","0x04 注意力发展历史"]},"291":{"title":"4.11 小结","titles":["探秘Transformer系列之（1）：注意力机制","0x04 注意力发展历史"]},"292":{"title":"0xFF 参考","titles":["探秘Transformer系列之（1）：注意力机制"]},"293":{"title":"探秘Transformer系列之（14）--- 残差网络和归一化","titles":[]},"294":{"title":"0x00 概述","titles":["探秘Transformer系列之（14）--- 残差网络和归一化"]},"295":{"title":"0x01 残差连接","titles":["探秘Transformer系列之（14）--- 残差网络和归一化"]},"296":{"title":"1.1 问题","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x01 残差连接"]},"297":{"title":"1.2 相关知识","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x01 残差连接"]},"298":{"title":"shortcut connections","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x01 残差连接","1.2 相关知识"]},"299":{"title":"恒等映射","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x01 残差连接","1.2 相关知识"]},"300":{"title":"1.3 网络结构","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x01 残差连接"]},"301":{"title":"论文V1","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x01 残差连接","1.3 网络结构"]},"302":{"title":"论文V2","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x01 残差连接","1.3 网络结构"]},"303":{"title":"1.4 功用","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x01 残差连接"]},"304":{"title":"梯度消失","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x01 残差连接","1.4 功用"]},"305":{"title":"缓解退化","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x01 残差连接","1.4 功用"]},"306":{"title":"层间修正","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x01 残差连接","1.4 功用"]},"307":{"title":"掩码 VS 残差","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x01 残差连接","1.4 功用"]},"308":{"title":"0x02 归一化","titles":["探秘Transformer系列之（14）--- 残差网络和归一化"]},"309":{"title":"2.1 问题","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x02 归一化"]},"310":{"title":"2.2 定义","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x02 归一化"]},"311":{"title":"2.3 类型","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x02 归一化"]},"312":{"title":"0x03 BatchNorm","titles":["探秘Transformer系列之（14）--- 残差网络和归一化"]},"313":{"title":"3.1 公式","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x03 BatchNorm"]},"314":{"title":"3.2 作用","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x03 BatchNorm"]},"315":{"title":"3.3 PyTorch使用","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x03 BatchNorm"]},"316":{"title":"3.4 问题","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x03 BatchNorm"]},"317":{"title":"0x04 layerNorm","titles":["探秘Transformer系列之（14）--- 残差网络和归一化"]},"318":{"title":"4.1 解决方案","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x04 layerNorm"]},"319":{"title":"4.2 公式","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x04 layerNorm"]},"320":{"title":"4.3 作用","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x04 layerNorm"]},"321":{"title":"4.4 LN和BN的差异","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x04 layerNorm"]},"322":{"title":"作用对象","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x04 layerNorm","4.4 LN和BN的差异"]},"323":{"title":"作用方向","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x04 layerNorm","4.4 LN和BN的差异"]},"324":{"title":"业务选择","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x04 layerNorm","4.4 LN和BN的差异"]},"325":{"title":"CV","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x04 layerNorm","4.4 LN和BN的差异","业务选择"]},"326":{"title":"NLP","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x04 layerNorm","4.4 LN和BN的差异","业务选择"]},"327":{"title":"具体实现","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x04 layerNorm","4.4 LN和BN的差异"]},"328":{"title":"4.5 Post-Norm VS Pre-Norm","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x04 layerNorm"]},"329":{"title":"概念","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x04 layerNorm","4.5 Post-Norm VS Pre-Norm"]},"330":{"title":"论文实现","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x04 layerNorm","4.5 Post-Norm VS Pre-Norm"]},"331":{"title":"Post-Norm","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x04 layerNorm","4.5 Post-Norm VS Pre-Norm"]},"332":{"title":"难以训练","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x04 layerNorm","4.5 Post-Norm VS Pre-Norm","Post-Norm"]},"333":{"title":"需要热身","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x04 layerNorm","4.5 Post-Norm VS Pre-Norm","Post-Norm"]},"334":{"title":"Pre-Norm","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x04 layerNorm","4.5 Post-Norm VS Pre-Norm"]},"335":{"title":"小结","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x04 layerNorm","4.5 Post-Norm VS Pre-Norm"]},"336":{"title":"0x05 扩展比对","titles":["探秘Transformer系列之（14）--- 残差网络和归一化"]},"337":{"title":"5.1 Instance Norm","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x05 扩展比对"]},"338":{"title":"5.2 GroupNorm","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x05 扩展比对"]},"339":{"title":"5.3 比对","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x05 扩展比对"]},"340":{"title":"类比","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x05 扩展比对","5.3 比对"]},"341":{"title":"细节","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x05 扩展比对","5.3 比对"]},"342":{"title":"0x06 实现","titles":["探秘Transformer系列之（14）--- 残差网络和归一化"]},"343":{"title":"6.1 LayerNorm","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x06 实现"]},"344":{"title":"6.2 残差","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x06 实现"]},"345":{"title":"0x07 优化与演进","titles":["探秘Transformer系列之（14）--- 残差网络和归一化"]},"346":{"title":"7.1 RMSNorm","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x07 优化与演进"]},"347":{"title":"7.2 Deep Norm","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x07 优化与演进"]},"348":{"title":"7.3 PRepBN","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x07 优化与演进"]},"349":{"title":"7.4 RealFormer","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x07 优化与演进"]},"350":{"title":"7.5 nGPT","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x07 优化与演进"]},"351":{"title":"研究背景&amp;动机","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x07 优化与演进","7.5 nGPT"]},"352":{"title":"核心贡献","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x07 优化与演进","7.5 nGPT"]},"353":{"title":"对比","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x07 优化与演进","7.5 nGPT"]},"354":{"title":"层和块（LAYERS AND BLOCKS）","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x07 优化与演进","7.5 nGPT","对比"]},"355":{"title":"自注意力块","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x07 优化与演进","7.5 nGPT","对比"]},"356":{"title":"MLP 块（MLP BLOCK）","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x07 优化与演进","7.5 nGPT","对比"]},"357":{"title":"切换","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x07 优化与演进","7.5 nGPT"]},"358":{"title":"7.6 DyT","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x07 优化与演进"]},"359":{"title":"动机","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x07 优化与演进","7.6 DyT"]},"360":{"title":"实现","titles":["探秘Transformer系列之（14）--- 残差网络和归一化","0x07 优化与演进","7.6 DyT"]},"361":{"title":"0xFF 参考","titles":["探秘Transformer系列之（14）--- 残差网络和归一化"]},"362":{"title":"探秘Transformer系列之（3）---数据处理","titles":[]},"363":{"title":"0x00 概要","titles":["探秘Transformer系列之（3）---数据处理"]},"364":{"title":"0x01 总体流程","titles":["探秘Transformer系列之（3）---数据处理"]},"365":{"title":"0x02 数据集","titles":["探秘Transformer系列之（3）---数据处理"]},"366":{"title":"2.1 行业做法","titles":["探秘Transformer系列之（3）---数据处理","0x02 数据集"]},"367":{"title":"常见数据集","titles":["探秘Transformer系列之（3）---数据处理","0x02 数据集","2.1 行业做法"]},"368":{"title":"数据源比率","titles":["探秘Transformer系列之（3）---数据处理","0x02 数据集","2.1 行业做法"]},"369":{"title":"数据治理","titles":["探秘Transformer系列之（3）---数据处理","0x02 数据集","2.1 行业做法"]},"370":{"title":"2.2 哈佛数据集","titles":["探秘Transformer系列之（3）---数据处理","0x02 数据集"]},"371":{"title":"0x03 加载功能模块","titles":["探秘Transformer系列之（3）---数据处理"]},"372":{"title":"3.1 加载模型","titles":["探秘Transformer系列之（3）---数据处理","0x03 加载功能模块"]},"373":{"title":"3.2 加载分词器","titles":["探秘Transformer系列之（3）---数据处理","0x03 加载功能模块"]},"374":{"title":"3.3 加载词表","titles":["探秘Transformer系列之（3）---数据处理","0x03 加载功能模块"]},"375":{"title":"0x04 加载数据","titles":["探秘Transformer系列之（3）---数据处理"]},"376":{"title":"4.1 填充（Padding）","titles":["探秘Transformer系列之（3）---数据处理","0x04 加载数据"]},"377":{"title":"改进","titles":["探秘Transformer系列之（3）---数据处理","0x04 加载数据","4.1 填充（Padding）"]},"378":{"title":"左填充","titles":["探秘Transformer系列之（3）---数据处理","0x04 加载数据","4.1 填充（Padding）"]},"379":{"title":"4.2 Batch类","titles":["探秘Transformer系列之（3）---数据处理","0x04 加载数据"]},"380":{"title":"成员变量","titles":["探秘Transformer系列之（3）---数据处理","0x04 加载数据","4.2 Batch类"]},"381":{"title":"目标语句","titles":["探秘Transformer系列之（3）---数据处理","0x04 加载数据","4.2 Batch类"]},"382":{"title":"生成掩码","titles":["探秘Transformer系列之（3）---数据处理","0x04 加载数据","4.2 Batch类"]},"383":{"title":"构建batch","titles":["探秘Transformer系列之（3）---数据处理","0x04 加载数据","4.2 Batch类"]},"384":{"title":"4.3 加载batch","titles":["探秘Transformer系列之（3）---数据处理","0x04 加载数据"]},"385":{"title":"4.3 训练使用","titles":["探秘Transformer系列之（3）---数据处理","0x04 加载数据"]},"386":{"title":"小结","titles":["探秘Transformer系列之（3）---数据处理","0x04 加载数据"]},"387":{"title":"0xFF 参考","titles":["探秘Transformer系列之（3）---数据处理"]},"388":{"title":"探秘Transformer系列之（5）--- 训练&amp;推理","titles":[]},"389":{"title":"0x00 概述","titles":["探秘Transformer系列之（5）--- 训练&amp;推理"]},"390":{"title":"0x01 训练","titles":["探秘Transformer系列之（5）--- 训练&amp;推理"]},"391":{"title":"1.1 输入","titles":["探秘Transformer系列之（5）--- 训练&amp;推理","0x01 训练"]},"392":{"title":"1.2 Dropout","titles":["探秘Transformer系列之（5）--- 训练&amp;推理","0x01 训练"]},"393":{"title":"原理","titles":["探秘Transformer系列之（5）--- 训练&amp;推理","0x01 训练","1.2 Dropout"]},"394":{"title":"位置","titles":["探秘Transformer系列之（5）--- 训练&amp;推理","0x01 训练","1.2 Dropout"]},"395":{"title":"源码","titles":["探秘Transformer系列之（5）--- 训练&amp;推理","0x01 训练","1.2 Dropout"]},"396":{"title":"发展","titles":["探秘Transformer系列之（5）--- 训练&amp;推理","0x01 训练","1.2 Dropout"]},"397":{"title":"1.3 损失函数","titles":["探秘Transformer系列之（5）--- 训练&amp;推理","0x01 训练"]},"398":{"title":"交叉熵","titles":["探秘Transformer系列之（5）--- 训练&amp;推理","0x01 训练","1.3 损失函数"]},"399":{"title":"Label Smoothing","titles":["探秘Transformer系列之（5）--- 训练&amp;推理","0x01 训练","1.3 损失函数"]},"400":{"title":"1.4 学习率","titles":["探秘Transformer系列之（5）--- 训练&amp;推理","0x01 训练"]},"401":{"title":"Warmup","titles":["探秘Transformer系列之（5）--- 训练&amp;推理","0x01 训练","1.4 学习率"]},"402":{"title":"Noam","titles":["探秘Transformer系列之（5）--- 训练&amp;推理","0x01 训练","1.4 学习率"]},"403":{"title":"1.5 初始化","titles":["探秘Transformer系列之（5）--- 训练&amp;推理","0x01 训练"]},"404":{"title":"1.6 Teacher Forcing","titles":["探秘Transformer系列之（5）--- 训练&amp;推理","0x01 训练"]},"405":{"title":"问题","titles":["探秘Transformer系列之（5）--- 训练&amp;推理","0x01 训练","1.6 Teacher Forcing"]},"406":{"title":"概念","titles":["探秘Transformer系列之（5）--- 训练&amp;推理","0x01 训练","1.6 Teacher Forcing"]},"407":{"title":"示例","titles":["探秘Transformer系列之（5）--- 训练&amp;推理","0x01 训练","1.6 Teacher Forcing"]},"408":{"title":"原理","titles":["探秘Transformer系列之（5）--- 训练&amp;推理","0x01 训练","1.6 Teacher Forcing"]},"409":{"title":"掩码","titles":["探秘Transformer系列之（5）--- 训练&amp;推理","0x01 训练","1.6 Teacher Forcing"]},"410":{"title":"实现","titles":["探秘Transformer系列之（5）--- 训练&amp;推理","0x01 训练","1.6 Teacher Forcing"]},"411":{"title":"优劣","titles":["探秘Transformer系列之（5）--- 训练&amp;推理","0x01 训练","1.6 Teacher Forcing"]},"412":{"title":"小结","titles":["探秘Transformer系列之（5）--- 训练&amp;推理","0x01 训练","1.6 Teacher Forcing"]},"413":{"title":"1.7 并行","titles":["探秘Transformer系列之（5）--- 训练&amp;推理","0x01 训练"]},"414":{"title":"逻辑维度","titles":["探秘Transformer系列之（5）--- 训练&amp;推理","0x01 训练","1.7 并行"]},"415":{"title":"编码器","titles":["探秘Transformer系列之（5）--- 训练&amp;推理","0x01 训练","1.7 并行","逻辑维度"]},"416":{"title":"解码器","titles":["探秘Transformer系列之（5）--- 训练&amp;推理","0x01 训练","1.7 并行","逻辑维度"]},"417":{"title":"模型维度","titles":["探秘Transformer系列之（5）--- 训练&amp;推理","0x01 训练","1.7 并行"]},"418":{"title":"自注意力","titles":["探秘Transformer系列之（5）--- 训练&amp;推理","0x01 训练","1.7 并行","模型维度"]},"419":{"title":"FFN","titles":["探秘Transformer系列之（5）--- 训练&amp;推理","0x01 训练","1.7 并行","模型维度"]},"420":{"title":"张量维度","titles":["探秘Transformer系列之（5）--- 训练&amp;推理","0x01 训练","1.7 并行"]},"421":{"title":"1.7 代码","titles":["探秘Transformer系列之（5）--- 训练&amp;推理","0x01 训练"]},"422":{"title":"训练方式","titles":["探秘Transformer系列之（5）--- 训练&amp;推理","0x01 训练","1.7 代码"]},"423":{"title":"单机训练代码","titles":["探秘Transformer系列之（5）--- 训练&amp;推理","0x01 训练","1.7 代码"]},"424":{"title":"总体代码","titles":["探秘Transformer系列之（5）--- 训练&amp;推理","0x01 训练","1.7 代码"]},"425":{"title":"0x02 推理","titles":["探秘Transformer系列之（5）--- 训练&amp;推理"]},"426":{"title":"2.1 输入输出","titles":["探秘Transformer系列之（5）--- 训练&amp;推理","0x02 推理"]},"427":{"title":"2.2 流程","titles":["探秘Transformer系列之（5）--- 训练&amp;推理","0x02 推理"]},"428":{"title":"2.3 代码","titles":["探秘Transformer系列之（5）--- 训练&amp;推理","0x02 推理"]},"429":{"title":"0xFF 参考","titles":["探秘Transformer系列之（5）--- 训练&amp;推理"]},"430":{"title":"0x00 概述","titles":[]},"431":{"title":"0.1 流程","titles":["0x00 概述"]},"432":{"title":"0.2 说明","titles":["0x00 概述"]},"433":{"title":"0x01 总体架构","titles":[]},"434":{"title":"1.1 设计动机","titles":["0x01 总体架构"]},"435":{"title":"1.2 模型结构","titles":["0x01 总体架构"]},"436":{"title":"主体模块","titles":["0x01 总体架构","1.2 模型结构"]},"437":{"title":"多层","titles":["0x01 总体架构","1.2 模型结构"]},"438":{"title":"1.3 注意力模块","titles":["0x01 总体架构"]},"439":{"title":"分类","titles":["0x01 总体架构","1.3 注意力模块"]},"440":{"title":"位置","titles":["0x01 总体架构","1.3 注意力模块"]},"441":{"title":"作用","titles":["0x01 总体架构","1.3 注意力模块"]},"442":{"title":"全局自注意力层","titles":["0x01 总体架构","1.3 注意力模块","作用"]},"443":{"title":"掩码自注意力","titles":["0x01 总体架构","1.3 注意力模块","作用"]},"444":{"title":"交叉注意力层","titles":["0x01 总体架构","1.3 注意力模块","作用"]},"445":{"title":"1.4 执行流程","titles":["0x01 总体架构"]},"446":{"title":"1.6 小结","titles":["0x01 总体架构"]},"447":{"title":"0x02 构建","titles":[]},"448":{"title":"2.1 参数","titles":["0x02 构建"]},"449":{"title":"2.2 构建逻辑","titles":["0x02 构建"]},"450":{"title":"2.3 主体类","titles":["0x02 构建"]},"451":{"title":"2.4 如何调用","titles":["0x02 构建"]},"452":{"title":"0x03 输入","titles":[]},"453":{"title":"3.1 输入分类","titles":["0x03 输入"]},"454":{"title":"3.2 输入模块","titles":["0x03 输入"]},"455":{"title":"3.3 文字转换","titles":["0x03 输入"]},"456":{"title":"分词","titles":["0x03 输入","3.3 文字转换"]},"457":{"title":"embedding化","titles":["0x03 输入","3.3 文字转换"]},"458":{"title":"Token Embedding","titles":["0x03 输入","3.3 文字转换","embedding化"]},"459":{"title":"Positional Encoding","titles":["0x03 输入","3.3 文字转换","embedding化"]},"460":{"title":"Word Embedding","titles":["0x03 输入","3.3 文字转换","embedding化"]},"461":{"title":"0x04 Transformer Layer","titles":[]},"462":{"title":"4.1 多头自注意力机制","titles":["0x04 Transformer Layer"]},"463":{"title":"第 1 步：根据原始嵌入计算查询、键和值矩阵","titles":["0x04 Transformer Layer","4.1 多头自注意力机制","embedding化"]},"464":{"title":"第 2 步：掩码自我注意力","titles":["0x04 Transformer Layer","4.1 多头自注意力机制","embedding化"]},"465":{"title":"第3步：拼接","titles":["0x04 Transformer Layer","4.1 多头自注意力机制","embedding化"]},"466":{"title":"4.2 FFN层","titles":["0x04 Transformer Layer"]},"467":{"title":"4.3 辅助架构","titles":["0x04 Transformer Layer"]},"468":{"title":"LayerNorm","titles":["0x04 Transformer Layer","4.3 辅助架构"]},"469":{"title":"Dropout","titles":["0x04 Transformer Layer","4.3 辅助架构"]},"470":{"title":"残差连接","titles":["0x04 Transformer Layer","4.3 辅助架构"]},"471":{"title":"0x05 概率输出（Output Probabilities）","titles":[]},"472":{"title":"5.1 解码器结果","titles":["0x05 概率输出（Output Probabilities）"]},"473":{"title":"5.2 转换","titles":["0x05 概率输出（Output Probabilities）"]},"474":{"title":"0x06 解释","titles":[]},"475":{"title":"6.1 机械可解释性","titles":["0x06 解释"]},"476":{"title":"causal tracing","titles":["0x06 解释","6.1 机械可解释性"]},"477":{"title":"稀疏探测","titles":["0x06 解释","6.1 机械可解释性","causal tracing"]},"478":{"title":"输入归因","titles":["0x06 解释","6.1 机械可解释性","causal tracing"]},"479":{"title":"模型组件归因","titles":["0x06 解释","6.1 机械可解释性","causal tracing"]},"480":{"title":"circuit analysis","titles":["0x06 解释","6.1 机械可解释性"]},"481":{"title":"6.2 机器学习角度","titles":["0x06 解释"]},"482":{"title":"前向传播角度","titles":["0x06 解释","6.2 机器学习角度"]},"483":{"title":"反向传播角度","titles":["0x06 解释","6.2 机器学习角度"]},"484":{"title":"动机","titles":["0x06 解释","6.2 机器学习角度","反向传播角度"]},"485":{"title":"方案","titles":["0x06 解释","6.2 机器学习角度","反向传播角度"]},"486":{"title":"6.3 生物学角度","titles":["0x06 解释"]},"487":{"title":"星形胶质细胞","titles":["0x06 解释","6.3 生物学角度"]},"488":{"title":"动机","titles":["0x06 解释","6.3 生物学角度","星形胶质细胞"]},"489":{"title":"方案","titles":["0x06 解释","6.3 生物学角度","星形胶质细胞"]},"490":{"title":"海马体","titles":["0x06 解释","6.3 生物学角度"]},"491":{"title":"6.4 数学角度","titles":["0x06 解释"]},"492":{"title":"ODE视角","titles":["0x06 解释","6.4 数学角度"]},"493":{"title":"神经常微分方程","titles":["0x06 解释","6.4 数学角度","ODE视角"]},"494":{"title":"推导","titles":["0x06 解释","6.4 数学角度","ODE视角","神经常微分方程"]},"495":{"title":"优势","titles":["0x06 解释","6.4 数学角度","ODE视角","神经常微分方程"]},"496":{"title":"示例","titles":["0x06 解释","6.4 数学角度","ODE视角","神经常微分方程"]},"497":{"title":"Do Residual Neural Networks discretize Neural Ordinary Differential Equations?","titles":["0x06 解释","6.4 数学角度","ODE视角"]},"498":{"title":"多粒子动态系统视角","titles":["0x06 解释","6.4 数学角度"]},"499":{"title":"流映射视角","titles":["0x06 解释","6.4 数学角度"]},"500":{"title":"差分角度","titles":["0x06 解释","6.4 数学角度"]},"501":{"title":"总体架构","titles":["0x06 解释","6.4 数学角度","差分角度"]},"502":{"title":"Differential Attention（差分注意力）","titles":["0x06 解释","6.4 数学角度","差分角度"]},"503":{"title":"多头","titles":["0x06 解释","6.4 数学角度","差分角度"]},"504":{"title":"图灵完备性质","titles":["0x06 解释","6.4 数学角度"]},"505":{"title":"范畴论","titles":["0x06 解释","6.4 数学角度"]},"506":{"title":"6.4 物理学角度","titles":["0x06 解释"]},"507":{"title":"基本动力学特性","titles":["0x06 解释","6.4 物理学角度"]},"508":{"title":"物理自旋系统的结构","titles":["0x06 解释","6.4 物理学角度"]},"509":{"title":"受力角度","titles":["0x06 解释","6.4 物理学角度"]},"510":{"title":"0x07 总结","titles":[]},"511":{"title":"7.1 效果","titles":["0x07 总结"]},"512":{"title":"7.2 优劣","titles":["0x07 总结"]},"513":{"title":"0xFF 参考","titles":[]},"514":{"title":"探秘Transformer系列之（4）--- 编码器 &amp; 解码器","titles":[]},"515":{"title":"0x00 摘要","titles":["探秘Transformer系列之（4）--- 编码器 &amp; 解码器"]},"516":{"title":"0x01 编码器","titles":["探秘Transformer系列之（4）--- 编码器 &amp; 解码器"]},"517":{"title":"1.1 结构","titles":["探秘Transformer系列之（4）--- 编码器 &amp; 解码器","0x01 编码器"]},"518":{"title":"1.2 输入和输出","titles":["探秘Transformer系列之（4）--- 编码器 &amp; 解码器","0x01 编码器"]},"519":{"title":"1.3 流程","titles":["探秘Transformer系列之（4）--- 编码器 &amp; 解码器","0x01 编码器"]},"520":{"title":"1.4 张量形状变化","titles":["探秘Transformer系列之（4）--- 编码器 &amp; 解码器","0x01 编码器"]},"521":{"title":"1.6 实现","titles":["探秘Transformer系列之（4）--- 编码器 &amp; 解码器","0x01 编码器"]},"522":{"title":"Encoder","titles":["探秘Transformer系列之（4）--- 编码器 &amp; 解码器","0x01 编码器","1.6 实现"]},"523":{"title":"EncoderLayer","titles":["探秘Transformer系列之（4）--- 编码器 &amp; 解码器","0x01 编码器","1.6 实现"]},"524":{"title":"0x02 解码器","titles":["探秘Transformer系列之（4）--- 编码器 &amp; 解码器"]},"525":{"title":"2.1 结构","titles":["探秘Transformer系列之（4）--- 编码器 &amp; 解码器","0x02 解码器"]},"526":{"title":"2.2 输入和输出","titles":["探秘Transformer系列之（4）--- 编码器 &amp; 解码器","0x02 解码器"]},"527":{"title":"2.3 流程","titles":["探秘Transformer系列之（4）--- 编码器 &amp; 解码器","0x02 解码器"]},"528":{"title":"训练","titles":["探秘Transformer系列之（4）--- 编码器 &amp; 解码器","0x02 解码器","2.3 流程"]},"529":{"title":"推理","titles":["探秘Transformer系列之（4）--- 编码器 &amp; 解码器","0x02 解码器","2.3 流程"]},"530":{"title":"2.4 张量形状变化","titles":["探秘Transformer系列之（4）--- 编码器 &amp; 解码器","0x02 解码器"]},"531":{"title":"2.5 实现","titles":["探秘Transformer系列之（4）--- 编码器 &amp; 解码器","0x02 解码器"]},"532":{"title":"Decoder","titles":["探秘Transformer系列之（4）--- 编码器 &amp; 解码器","0x02 解码器","2.5 实现"]},"533":{"title":"DecoderLayer","titles":["探秘Transformer系列之（4）--- 编码器 &amp; 解码器","0x02 解码器","2.5 实现"]},"534":{"title":"0x03 交叉注意力深入","titles":["探秘Transformer系列之（4）--- 编码器 &amp; 解码器"]},"535":{"title":"3.1 分类","titles":["探秘Transformer系列之（4）--- 编码器 &amp; 解码器","0x03 交叉注意力深入"]},"536":{"title":"3.2 业务逻辑","titles":["探秘Transformer系列之（4）--- 编码器 &amp; 解码器","0x03 交叉注意力深入"]},"537":{"title":"3.3 业务流程","titles":["探秘Transformer系列之（4）--- 编码器 &amp; 解码器","0x03 交叉注意力深入"]},"538":{"title":"3.4 代码逻辑","titles":["探秘Transformer系列之（4）--- 编码器 &amp; 解码器","0x03 交叉注意力深入"]},"539":{"title":"0x04 Decoder Only","titles":["探秘Transformer系列之（4）--- 编码器 &amp; 解码器"]},"540":{"title":"4.1 分类","titles":["探秘Transformer系列之（4）--- 编码器 &amp; 解码器","0x04 Decoder Only"]},"541":{"title":"4.2 Decoder Only","titles":["探秘Transformer系列之（4）--- 编码器 &amp; 解码器","0x04 Decoder Only"]},"542":{"title":"4.3 架构选择","titles":["探秘Transformer系列之（4）--- 编码器 &amp; 解码器","0x04 Decoder Only"]},"543":{"title":"0xFF 参考","titles":["探秘Transformer系列之（4）--- 编码器 &amp; 解码器"]},"544":{"title":"探秘Transformer系列之（6）--- token","titles":[]},"545":{"title":"0x00 概述","titles":["探秘Transformer系列之（6）--- token"]},"546":{"title":"0x01 基础概念","titles":["探秘Transformer系列之（6）--- token"]},"547":{"title":"1.1 分词","titles":["探秘Transformer系列之（6）--- token","0x01 基础概念"]},"548":{"title":"1.2 token","titles":["探秘Transformer系列之（6）--- token","0x01 基础概念"]},"549":{"title":"1.3 tokenizer","titles":["探秘Transformer系列之（6）--- token","0x01 基础概念"]},"550":{"title":"1.4 词表","titles":["探秘Transformer系列之（6）--- token","0x01 基础概念"]},"551":{"title":"1.6 分词流程","titles":["探秘Transformer系列之（6）--- token","0x01 基础概念"]},"552":{"title":"规范化","titles":["探秘Transformer系列之（6）--- token","0x01 基础概念","1.6 分词流程"]},"553":{"title":"预分词","titles":["探秘Transformer系列之（6）--- token","0x01 基础概念","1.6 分词流程"]},"554":{"title":"模型处理","titles":["探秘Transformer系列之（6）--- token","0x01 基础概念","1.6 分词流程"]},"555":{"title":"后处理","titles":["探秘Transformer系列之（6）--- token","0x01 基础概念","1.6 分词流程"]},"556":{"title":"0x02 词表","titles":["探秘Transformer系列之（6）--- token"]},"557":{"title":"2.1 构建词表","titles":["探秘Transformer系列之（6）--- token","0x02 词表"]},"558":{"title":"2.2 使用词表","titles":["探秘Transformer系列之（6）--- token","0x02 词表"]},"559":{"title":"2.3 词表大小","titles":["探秘Transformer系列之（6）--- token","0x02 词表"]},"560":{"title":"任务相关","titles":["探秘Transformer系列之（6）--- token","0x02 词表","2.3 词表大小"]},"561":{"title":"优势","titles":["探秘Transformer系列之（6）--- token","0x02 词表","2.3 词表大小"]},"562":{"title":"劣势","titles":["探秘Transformer系列之（6）--- token","0x02 词表","2.3 词表大小"]},"563":{"title":"0x03 Tokenizer","titles":["探秘Transformer系列之（6）--- token"]},"564":{"title":"3.1 分词粒度","titles":["探秘Transformer系列之（6）--- token","0x03 Tokenizer"]},"565":{"title":"按单词粒度","titles":["探秘Transformer系列之（6）--- token","0x03 Tokenizer","3.1 分词粒度"]},"566":{"title":"按字符粒度","titles":["探秘Transformer系列之（6）--- token","0x03 Tokenizer","3.1 分词粒度"]},"567":{"title":"按子词粒度","titles":["探秘Transformer系列之（6）--- token","0x03 Tokenizer","3.1 分词粒度"]},"568":{"title":"如何选择","titles":["探秘Transformer系列之（6）--- token","0x03 Tokenizer","3.1 分词粒度"]},"569":{"title":"3.2 常见tokenizer","titles":["探秘Transformer系列之（6）--- token","0x03 Tokenizer"]},"570":{"title":"3.3 Llama3示例","titles":["探秘Transformer系列之（6）--- token","0x03 Tokenizer"]},"571":{"title":"定义","titles":["探秘Transformer系列之（6）--- token","0x03 Tokenizer","3.3 Llama3示例"]},"572":{"title":"编码","titles":["探秘Transformer系列之（6）--- token","0x03 Tokenizer","3.3 Llama3示例"]},"573":{"title":"解码","titles":["探秘Transformer系列之（6）--- token","0x03 Tokenizer","3.3 Llama3示例"]},"574":{"title":"0x04 BPE","titles":["探秘Transformer系列之（6）--- token"]},"575":{"title":"4.1 思路","titles":["探秘Transformer系列之（6）--- token","0x04 BPE"]},"576":{"title":"4.2 算法","titles":["探秘Transformer系列之（6）--- token","0x04 BPE"]},"577":{"title":"4.3 示例剖析","titles":["探秘Transformer系列之（6）--- token","0x04 BPE"]},"578":{"title":"统计频率","titles":["探秘Transformer系列之（6）--- token","0x04 BPE","4.3 示例剖析"]},"579":{"title":"初始分割","titles":["探秘Transformer系列之（6）--- token","0x04 BPE","4.3 示例剖析"]},"580":{"title":"构建初始词表","titles":["探秘Transformer系列之（6）--- token","0x04 BPE","4.3 示例剖析"]},"581":{"title":"循环迭代学习结合规则","titles":["探秘Transformer系列之（6）--- token","0x04 BPE","4.3 示例剖析"]},"582":{"title":"第一次迭代","titles":["探秘Transformer系列之（6）--- token","0x04 BPE","4.3 示例剖析","循环迭代学习结合规则"]},"583":{"title":"第二次迭代","titles":["探秘Transformer系列之（6）--- token","0x04 BPE","4.3 示例剖析","循环迭代学习结合规则"]},"584":{"title":"后续迭代","titles":["探秘Transformer系列之（6）--- token","0x04 BPE","4.3 示例剖析","循环迭代学习结合规则"]},"585":{"title":"小结","titles":["探秘Transformer系列之（6）--- token","0x04 BPE","4.3 示例剖析"]},"586":{"title":"4.4 使用","titles":["探秘Transformer系列之（6）--- token","0x04 BPE"]},"587":{"title":"编码","titles":["探秘Transformer系列之（6）--- token","0x04 BPE","4.4 使用"]},"588":{"title":"解码","titles":["探秘Transformer系列之（6）--- token","0x04 BPE","4.4 使用"]},"589":{"title":"4.5 MINBPE","titles":["探秘Transformer系列之（6）--- token","0x04 BPE"]},"590":{"title":"基础函数","titles":["探秘Transformer系列之（6）--- token","0x04 BPE","4.5 MINBPE"]},"591":{"title":"Tokenizer","titles":["探秘Transformer系列之（6）--- token","0x04 BPE","4.5 MINBPE"]},"592":{"title":"BPE Tokenizer","titles":["探秘Transformer系列之（6）--- token","0x04 BPE","4.5 MINBPE"]},"593":{"title":"4.5 优劣","titles":["探秘Transformer系列之（6）--- token","0x04 BPE"]},"594":{"title":"优点","titles":["探秘Transformer系列之（6）--- token","0x04 BPE","4.5 优劣"]},"595":{"title":"劣势","titles":["探秘Transformer系列之（6）--- token","0x04 BPE","4.5 优劣"]},"596":{"title":"0x05 其它算法","titles":["探秘Transformer系列之（6）--- token"]},"597":{"title":"5.1 WordPiece","titles":["探秘Transformer系列之（6）--- token","0x05 其它算法"]},"598":{"title":"思想","titles":["探秘Transformer系列之（6）--- token","0x05 其它算法","5.1 WordPiece"]},"599":{"title":"算法","titles":["探秘Transformer系列之（6）--- token","0x05 其它算法","5.1 WordPiece"]},"600":{"title":"优势与劣势","titles":["探秘Transformer系列之（6）--- token","0x05 其它算法","5.1 WordPiece"]},"601":{"title":"5.2 UniLM","titles":["探秘Transformer系列之（6）--- token","0x05 其它算法"]},"602":{"title":"算法","titles":["探秘Transformer系列之（6）--- token","0x05 其它算法","5.2 UniLM"]},"603":{"title":"优势与劣势","titles":["探秘Transformer系列之（6）--- token","0x05 其它算法","5.2 UniLM"]},"604":{"title":"比对","titles":["探秘Transformer系列之（6）--- token","0x05 其它算法","5.2 UniLM"]},"605":{"title":"5.3 BBPE","titles":["探秘Transformer系列之（6）--- token","0x05 其它算法"]},"606":{"title":"动机","titles":["探秘Transformer系列之（6）--- token","0x05 其它算法","5.3 BBPE"]},"607":{"title":"思想","titles":["探秘Transformer系列之（6）--- token","0x05 其它算法","5.3 BBPE"]},"608":{"title":"优劣","titles":["探秘Transformer系列之（6）--- token","0x05 其它算法","5.3 BBPE"]},"609":{"title":"0x06 发展","titles":["探秘Transformer系列之（6）--- token"]},"610":{"title":"6.1 Better Than Tokens","titles":["探秘Transformer系列之（6）--- token","0x06 发展"]},"611":{"title":"主要贡献","titles":["探秘Transformer系列之（6）--- token","0x06 发展","6.1 Better Than Tokens"]},"612":{"title":"动机","titles":["探秘Transformer系列之（6）--- token","0x06 发展","6.1 Better Than Tokens"]},"613":{"title":"Patch化","titles":["探秘Transformer系列之（6）--- token","0x06 发展","6.1 Better Than Tokens"]},"614":{"title":"BLT架构","titles":["探秘Transformer系列之（6）--- token","0x06 发展","6.1 Better Than Tokens"]},"615":{"title":"交互","titles":["探秘Transformer系列之（6）--- token","0x06 发展","6.1 Better Than Tokens"]},"616":{"title":"6.2 Tokenformer","titles":["探秘Transformer系列之（6）--- token","0x06 发展"]},"617":{"title":"主要贡献","titles":["探秘Transformer系列之（6）--- token","0x06 发展","6.2 Tokenformer"]},"618":{"title":"动机","titles":["探秘Transformer系列之（6）--- token","0x06 发展","6.2 Tokenformer"]},"619":{"title":"架构","titles":["探秘Transformer系列之（6）--- token","0x06 发展","6.2 Tokenformer"]},"620":{"title":"对比","titles":["探秘Transformer系列之（6）--- token","0x06 发展","6.2 Tokenformer","架构"]},"621":{"title":"Pattention机制","titles":["探秘Transformer系列之（6）--- token","0x06 发展","6.2 Tokenformer","架构"]},"622":{"title":"FFN的革新","titles":["探秘Transformer系列之（6）--- token","0x06 发展","6.2 Tokenformer","架构"]},"623":{"title":"复用","titles":["探秘Transformer系列之（6）--- token","0x06 发展","6.2 Tokenformer"]},"624":{"title":"总结","titles":["探秘Transformer系列之（6）--- token","0x06 发展","6.2 Tokenformer"]},"625":{"title":"6.3 LCM","titles":["探秘Transformer系列之（6）--- token","0x06 发展"]},"626":{"title":"问题","titles":["探秘Transformer系列之（6）--- token","0x06 发展","6.3 LCM"]},"627":{"title":"动机","titles":["探秘Transformer系列之（6）--- token","0x06 发展","6.3 LCM"]},"628":{"title":"思路","titles":["探秘Transformer系列之（6）--- token","0x06 发展","6.3 LCM"]},"629":{"title":"总体架构","titles":["探秘Transformer系列之（6）--- token","0x06 发展","6.3 LCM"]},"630":{"title":"细节","titles":["探秘Transformer系列之（6）--- token","0x06 发展","6.3 LCM"]},"631":{"title":"SONAR嵌入空间","titles":["探秘Transformer系列之（6）--- token","0x06 发展","6.3 LCM","细节"]},"632":{"title":"Base-LCM","titles":["探秘Transformer系列之（6）--- token","0x06 发展","6.3 LCM","细节"]},"633":{"title":"基于扩散的LCM（Diffusion-based LCM）","titles":["探秘Transformer系列之（6）--- token","0x06 发展","6.3 LCM","细节"]},"634":{"title":"单塔扩散LCM（One-Tower Diffusion LCM）","titles":["探秘Transformer系列之（6）--- token","0x06 发展","6.3 LCM","细节"]},"635":{"title":"双塔扩散LCM（Two-Tower Diffusion-LCM）","titles":["探秘Transformer系列之（6）--- token","0x06 发展","6.3 LCM","细节"]},"636":{"title":"Quant-LCM","titles":["探秘Transformer系列之（6）--- token","0x06 发展","6.3 LCM","细节"]},"637":{"title":"6.4 动作Tokenizer","titles":["探秘Transformer系列之（6）--- token","0x06 发展"]},"638":{"title":"0xFF 参考","titles":["探秘Transformer系列之（6）--- token"]},"639":{"title":"一. DL_Base_Notes","titles":[]},"640":{"title":"1. 🌟🌟🌟🌟🌟Normalization","titles":["一. DL_Base_Notes"]},"641":{"title":"2. 🌟🌟🌟Activation","titles":["一. DL_Base_Notes"]},"642":{"title":"2.1 Non-linear Activations 的两种类型","titles":["一. DL_Base_Notes","2. 🌟🌟🌟Activation"]},"643":{"title":"2.2","titles":["一. DL_Base_Notes","2. 🌟🌟🌟Activation"]},"644":{"title":"3. 🌟Loss Function","titles":["一. DL_Base_Notes"]},"645":{"title":"4. 🌟🌟🌟🌟Optimizer","titles":["一. DL_Base_Notes"]},"646":{"title":"5. 🌟🌟🌟🌟🌟回顾Attention","titles":["一. DL_Base_Notes"]},"647":{"title":"5.1 为啥Attention的时候要除以dk","titles":["一. DL_Base_Notes","5. 🌟🌟🌟🌟🌟回顾Attention"]},"648":{"title":"5.2 为啥拆多头？为啥效果好了？","titles":["一. DL_Base_Notes","5. 🌟🌟🌟🌟🌟回顾Attention"]},"649":{"title":"5.3 Cross Multi-Head Attention？","titles":["一. DL_Base_Notes","5. 🌟🌟🌟🌟🌟回顾Attention"]},"650":{"title":"5.4 Mask Multi-Head Attention","titles":["一. DL_Base_Notes","5. 🌟🌟🌟🌟🌟回顾Attention"]},"651":{"title":"5.5 Masking实现机理","titles":["一. DL_Base_Notes","5. 🌟🌟🌟🌟🌟回顾Attention"]},"652":{"title":"5.6 MQA和GQA","titles":["一. DL_Base_Notes","5. 🌟🌟🌟🌟🌟回顾Attention"]},"653":{"title":"6. 🌟🌟🌟K-V Cache","titles":["一. DL_Base_Notes"]},"654":{"title":"7. 🌟🌟🌟常见的正则化方法","titles":["一. DL_Base_Notes"]},"655":{"title":"二. 课堂记录","titles":[]},"656":{"title":"🌟0301-0302","titles":["二. 课堂记录"]},"657":{"title":"0308-0309——PyTorch","titles":["二. 课堂记录"]},"658":{"title":"1.1 Tensor 中数据的连续性","titles":["二. 课堂记录","0308-0309——PyTorch"]},"659":{"title":"1.2 pytorch autograd","titles":["二. 课堂记录","0308-0309——PyTorch"]},"660":{"title":"1.3 inplace-op","titles":["二. 课堂记录","0308-0309——PyTorch"]},"661":{"title":"1.4 自动微分机制(auto grad) 重点：","titles":["二. 课堂记录","0308-0309——PyTorch"]},"662":{"title":"2.1 torch.nn.Module","titles":["二. 课堂记录","0308-0309——PyTorch"]},"663":{"title":"0315-0316（续PyTorch）","titles":["二. 课堂记录"]},"664":{"title":"1.1 回顾","titles":["二. 课堂记录","0315-0316（续PyTorch）"]},"665":{"title":"1.2 问题合集","titles":["二. 课堂记录","0315-0316（续PyTorch）"]},"666":{"title":"2.1 torch.optim","titles":["二. 课堂记录","0315-0316（续PyTorch）"]},"667":{"title":"2.2 learning rate 调整方案","titles":["二. 课堂记录","0315-0316（续PyTorch）"]},"668":{"title":"2.3 模型保存和加载","titles":["二. 课堂记录","0315-0316（续PyTorch）"]},"669":{"title":"3.1 Dataset and Dataloader","titles":["二. 课堂记录","0315-0316（续PyTorch）"]},"670":{"title":"4.1 NLP","titles":["二. 课堂记录","0315-0316（续PyTorch）"]},"671":{"title":"4.2 Bert","titles":["二. 课堂记录","0315-0316（续PyTorch）"]},"672":{"title":"探秘Transformer之（8）--- 位置编码","titles":[]},"673":{"title":"探秘Transformer系列之（7）--- embedding","titles":[]},"674":{"title":"0x00 概要","titles":["探秘Transformer系列之（7）--- embedding"]},"675":{"title":"0x01 演进思路","titles":["探秘Transformer系列之（7）--- embedding"]},"676":{"title":"1.1 概念","titles":["探秘Transformer系列之（7）--- embedding","0x01 演进思路"]},"677":{"title":"1.2 需求","titles":["探秘Transformer系列之（7）--- embedding","0x01 演进思路"]},"678":{"title":"1.3 文本","titles":["探秘Transformer系列之（7）--- embedding","0x01 演进思路"]},"679":{"title":"1.4 数字","titles":["探秘Transformer系列之（7）--- embedding","0x01 演进思路"]},"680":{"title":"1.5 向量","titles":["探秘Transformer系列之（7）--- embedding","0x01 演进思路"]},"681":{"title":"独热编码","titles":["探秘Transformer系列之（7）--- embedding","0x01 演进思路","1.5 向量"]},"682":{"title":"改进诉求","titles":["探秘Transformer系列之（7）--- embedding","0x01 演进思路","1.5 向量"]},"683":{"title":"增维","titles":["探秘Transformer系列之（7）--- embedding","0x01 演进思路","1.5 向量","改进诉求"]},"684":{"title":"降维","titles":["探秘Transformer系列之（7）--- embedding","0x01 演进思路","1.5 向量","改进诉求"]},"685":{"title":"语义相似性","titles":["探秘Transformer系列之（7）--- embedding","0x01 演进思路","1.5 向量","改进诉求"]},"686":{"title":"可训练","titles":["探秘Transformer系列之（7）--- embedding","0x01 演进思路","1.5 向量","改进诉求"]},"687":{"title":"小结","titles":["探秘Transformer系列之（7）--- embedding","0x01 演进思路","1.5 向量","改进诉求"]},"688":{"title":"1.5 embedding","titles":["探秘Transformer系列之（7）--- embedding","0x01 演进思路"]},"689":{"title":"表征概念","titles":["探秘Transformer系列之（7）--- embedding","0x01 演进思路","1.5 embedding"]},"690":{"title":"样例","titles":["探秘Transformer系列之（7）--- embedding","0x01 演进思路","1.5 embedding"]},"691":{"title":"相似性","titles":["探秘Transformer系列之（7）--- embedding","0x01 演进思路","1.5 embedding"]},"692":{"title":"相似度计算","titles":["探秘Transformer系列之（7）--- embedding","0x01 演进思路","1.5 embedding"]},"693":{"title":"1.6 小结","titles":["探秘Transformer系列之（7）--- embedding","0x01 演进思路"]},"694":{"title":"对比","titles":["探秘Transformer系列之（7）--- embedding","0x01 演进思路","1.6 小结"]},"695":{"title":"流转过程","titles":["探秘Transformer系列之（7）--- embedding","0x01 演进思路","1.6 小结"]},"696":{"title":"优势","titles":["探秘Transformer系列之（7）--- embedding","0x01 演进思路","1.6 小结"]},"697":{"title":"0x02 Transformer嵌入层实现","titles":["探秘Transformer系列之（7）--- embedding"]},"698":{"title":"2.1 流程&amp;架构","titles":["探秘Transformer系列之（7）--- embedding","0x02 Transformer嵌入层实现"]},"699":{"title":"2.2 实现","titles":["探秘Transformer系列之（7）--- embedding","0x02 Transformer嵌入层实现"]},"700":{"title":"嵌入矩阵","titles":["探秘Transformer系列之（7）--- embedding","0x02 Transformer嵌入层实现","2.2 实现"]},"701":{"title":"哈佛代码","titles":["探秘Transformer系列之（7）--- embedding","0x02 Transformer嵌入层实现","2.2 实现"]},"702":{"title":"PyTorch Embedding","titles":["探秘Transformer系列之（7）--- embedding","0x02 Transformer嵌入层实现","2.2 实现"]},"703":{"title":"调用","titles":["探秘Transformer系列之（7）--- embedding","0x02 Transformer嵌入层实现","2.2 实现"]},"704":{"title":"输入输出","titles":["探秘Transformer系列之（7）--- embedding","0x02 Transformer嵌入层实现","2.2 实现"]},"705":{"title":"2.3 训练","titles":["探秘Transformer系列之（7）--- embedding","0x02 Transformer嵌入层实现"]},"706":{"title":"Embedding的来源","titles":["探秘Transformer系列之（7）--- embedding","0x02 Transformer嵌入层实现","2.3 训练"]},"707":{"title":"为何要再训练？","titles":["探秘Transformer系列之（7）--- embedding","0x02 Transformer嵌入层实现","2.3 训练"]},"708":{"title":"让模型自己设计","titles":["探秘Transformer系列之（7）--- embedding","0x02 Transformer嵌入层实现","2.3 训练"]},"709":{"title":"训练流程","titles":["探秘Transformer系列之（7）--- embedding","0x02 Transformer嵌入层实现","2.3 训练"]},"710":{"title":"0x03 文本嵌入","titles":["探秘Transformer系列之（7）--- embedding"]},"711":{"title":"3.1 历史","titles":["探秘Transformer系列之（7）--- embedding","0x03 文本嵌入"]},"712":{"title":"3.2 Word2Vec","titles":["探秘Transformer系列之（7）--- embedding","0x03 文本嵌入"]},"713":{"title":"思路","titles":["探秘Transformer系列之（7）--- embedding","0x03 文本嵌入","3.2 Word2Vec"]},"714":{"title":"架构","titles":["探秘Transformer系列之（7）--- embedding","0x03 文本嵌入","3.2 Word2Vec"]},"715":{"title":"问题","titles":["探秘Transformer系列之（7）--- embedding","0x03 文本嵌入","3.2 Word2Vec"]},"716":{"title":"3.3 ELMO","titles":["探秘Transformer系列之（7）--- embedding","0x03 文本嵌入"]},"717":{"title":"思路","titles":["探秘Transformer系列之（7）--- embedding","0x03 文本嵌入","3.3 ELMO"]},"718":{"title":"训练","titles":["探秘Transformer系列之（7）--- embedding","0x03 文本嵌入","3.3 ELMO"]},"719":{"title":"3.4 BERT","titles":["探秘Transformer系列之（7）--- embedding","0x03 文本嵌入"]},"720":{"title":"动机","titles":["探秘Transformer系列之（7）--- embedding","0x03 文本嵌入","3.4 BERT"]},"721":{"title":"思路","titles":["探秘Transformer系列之（7）--- embedding","0x03 文本嵌入","3.4 BERT"]},"722":{"title":"embedding","titles":["探秘Transformer系列之（7）--- embedding","0x03 文本嵌入","3.4 BERT"]},"723":{"title":"代码","titles":["探秘Transformer系列之（7）--- embedding","0x03 文本嵌入","3.4 BERT"]},"724":{"title":"3.5 BGE","titles":["探秘Transformer系列之（7）--- embedding","0x03 文本嵌入"]},"725":{"title":"数据集","titles":["探秘Transformer系列之（7）--- embedding","0x03 文本嵌入","3.5 BGE"]},"726":{"title":"训练","titles":["探秘Transformer系列之（7）--- embedding","0x03 文本嵌入","3.5 BGE"]},"727":{"title":"RetroMAE","titles":["探秘Transformer系列之（7）--- embedding","0x03 文本嵌入","3.5 BGE"]},"728":{"title":"3.6 LLM-As-Embedding","titles":["探秘Transformer系列之（7）--- embedding","0x03 文本嵌入"]},"729":{"title":"Backbone选择","titles":["探秘Transformer系列之（7）--- embedding","0x03 文本嵌入","3.6 LLM-As-Embedding"]},"730":{"title":"架构改进","titles":["探秘Transformer系列之（7）--- embedding","0x03 文本嵌入","3.6 LLM-As-Embedding"]},"731":{"title":"Pooling策略","titles":["探秘Transformer系列之（7）--- embedding","0x03 文本嵌入","3.6 LLM-As-Embedding","架构改进"]},"732":{"title":"注意力架构","titles":["探秘Transformer系列之（7）--- embedding","0x03 文本嵌入","3.6 LLM-As-Embedding","架构改进"]},"733":{"title":"额外投影层（Additional Projector）","titles":["探秘Transformer系列之（7）--- embedding","0x03 文本嵌入","3.6 LLM-As-Embedding","架构改进"]},"734":{"title":"3.7 LLM2Vec","titles":["探秘Transformer系列之（7）--- embedding","0x03 文本嵌入"]},"735":{"title":"3.8 NV Embedding","titles":["探秘Transformer系列之（7）--- embedding","0x03 文本嵌入"]},"736":{"title":"3.9 通过提示工程的方法","titles":["探秘Transformer系列之（7）--- embedding","0x03 文本嵌入"]},"737":{"title":"3.10 使用MoE进行Embedding","titles":["探秘Transformer系列之（7）--- embedding","0x03 文本嵌入"]},"738":{"title":"研究背景","titles":["探秘Transformer系列之（7）--- embedding","0x03 文本嵌入","3.10 使用MoE进行Embedding"]},"739":{"title":"动机","titles":["探秘Transformer系列之（7）--- embedding","0x03 文本嵌入","3.10 使用MoE进行Embedding"]},"740":{"title":"0xFF 参考","titles":["探秘Transformer系列之（7）--- embedding"]},"741":{"title":"探秘Transformer系列之（9）--- 位置编码分类","titles":[]},"742":{"title":"0x00 概述","titles":["探秘Transformer系列之（9）--- 位置编码分类"]},"743":{"title":"0x01 区别","titles":["探秘Transformer系列之（9）--- 位置编码分类"]},"744":{"title":"1.1 从直观角度来看","titles":["探秘Transformer系列之（9）--- 位置编码分类","0x01 区别"]},"745":{"title":"1.2 从模型处理角度来看","titles":["探秘Transformer系列之（9）--- 位置编码分类","0x01 区别"]},"746":{"title":"1.3 优劣","titles":["探秘Transformer系列之（9）--- 位置编码分类","0x01 区别"]},"747":{"title":"0x02 绝对位置编码","titles":["探秘Transformer系列之（9）--- 位置编码分类"]},"748":{"title":"2.1 基础方案","titles":["探秘Transformer系列之（9）--- 位置编码分类","0x02 绝对位置编码"]},"749":{"title":"2.2 训练式","titles":["探秘Transformer系列之（9）--- 位置编码分类","0x02 绝对位置编码"]},"750":{"title":"2.3 三角函数式","titles":["探秘Transformer系列之（9）--- 位置编码分类","0x02 绝对位置编码"]},"751":{"title":"2.4 其它","titles":["探秘Transformer系列之（9）--- 位置编码分类","0x02 绝对位置编码"]},"752":{"title":"0x03 相对位置编码","titles":["探秘Transformer系列之（9）--- 位置编码分类"]},"753":{"title":"3.1 意义","titles":["探秘Transformer系列之（9）--- 位置编码分类","0x03 相对位置编码"]},"754":{"title":"大脑中的参考系","titles":["探秘Transformer系列之（9）--- 位置编码分类","0x03 相对位置编码","3.1 意义"]},"755":{"title":"语义影响","titles":["探秘Transformer系列之（9）--- 位置编码分类","0x03 相对位置编码","3.1 意义"]},"756":{"title":"长度外推","titles":["探秘Transformer系列之（9）--- 位置编码分类","0x03 相对位置编码","3.1 意义"]},"757":{"title":"3.2 绝对位置编码的位置","titles":["探秘Transformer系列之（9）--- 位置编码分类","0x03 相对位置编码"]},"758":{"title":"3.3 绝对位置编码的公式","titles":["探秘Transformer系列之（9）--- 位置编码分类","0x03 相对位置编码"]},"759":{"title":"3.4 经典式","titles":["探秘Transformer系列之（9）--- 位置编码分类","0x03 相对位置编码"]},"760":{"title":"3.5 XLNET","titles":["探秘Transformer系列之（9）--- 位置编码分类","0x03 相对位置编码"]},"761":{"title":"3.6 TENER","titles":["探秘Transformer系列之（9）--- 位置编码分类","0x03 相对位置编码"]},"762":{"title":"3.7 T5","titles":["探秘Transformer系列之（9）--- 位置编码分类","0x03 相对位置编码"]},"763":{"title":"3.8 DeBERTa式","titles":["探秘Transformer系列之（9）--- 位置编码分类","0x03 相对位置编码"]},"764":{"title":"3.9 TUPE","titles":["探秘Transformer系列之（9）--- 位置编码分类","0x03 相对位置编码"]},"765":{"title":"3.10 ALiBi","titles":["探秘Transformer系列之（9）--- 位置编码分类","0x03 相对位置编码"]},"766":{"title":"3.11 偏置编码&amp;上下文模式","titles":["探秘Transformer系列之（9）--- 位置编码分类","0x03 相对位置编码"]},"767":{"title":"3.12 小结","titles":["探秘Transformer系列之（9）--- 位置编码分类","0x03 相对位置编码"]},"768":{"title":"0xFF 参考","titles":["探秘Transformer系列之（9）--- 位置编码分类"]},"769":{"title":"1 概念","titles":[]},"770":{"title":"2 卷积运算","titles":[]},"771":{"title":"3 体会卷积的作用","titles":[]},"772":{"title":"4 卷积 和 前馈神经网络的关系","titles":[]},"773":{"title":"5 工程上标准的卷积","titles":[]},"774":{"title":"6 1x1 卷积","titles":[]},"775":{"title":"7 分组卷积(group convolution)","titles":[]},"776":{"title":"8 深度可分离卷积（deepwise convolution）","titles":[]},"777":{"title":"9 空间可分离卷积（Spatially Separable Convolutions）","titles":[]},"778":{"title":"10 空洞卷积（膨胀卷积）（Dilated Convolution / Atrous Convolution）","titles":[]},"779":{"title":"11 反卷积（转置卷积)(Deconvolution / Transposed Convolution）","titles":[]},"780":{"title":"12 可变形卷积（deformable convolution）","titles":[]},"781":{"title":"12.1 原理","titles":["12 可变形卷积（deformable convolution）"]},"782":{"title":"12.2 过程","titles":["12 可变形卷积（deformable convolution）"]},"783":{"title":"13 3D 卷积","titles":[]},"784":{"title":"14 参考链接","titles":[]},"785":{"title":"1 什么是深度学习模型","titles":[]},"786":{"title":"2 下载一个预训练好的深度学习模型","titles":[]},"787":{"title":"3 可视化这个深度学习模型","titles":[]},"788":{"title":"1 pytorch 官网","titles":[]},"789":{"title":"2 pytorch 简介","titles":[]},"790":{"title":"2.1 认识pytorch","titles":["2 pytorch 简介"]},"791":{"title":"2.2 pytorch 软件栈","titles":["2 pytorch 简介"]},"792":{"title":"3 pytorch install","titles":[]},"793":{"title":"4 nvidia 相关软件库","titles":[]},"794":{"title":"4.1 显卡驱动","titles":["4 nvidia 相关软件库"]},"795":{"title":"4.2 cuda","titles":["4 nvidia 相关软件库"]},"796":{"title":"4.3 cudnn","titles":["4 nvidia 相关软件库"]},"797":{"title":"5 GPU","titles":[]},"798":{"title":"5.1 GPU 加速原理","titles":["5 GPU"]},"799":{"title":"5.2 最先进的GPU","titles":["5 GPU"]},"800":{"title":"1 Convolution","titles":[]},"801":{"title":"1.1 Conv2D","titles":["1 Convolution"]},"802":{"title":"1.2 ConvTranspose2d","titles":["1 Convolution"]},"803":{"title":"2 线性变换层","titles":[]},"804":{"title":"2.1 Linear/Gemm","titles":["2 线性变换层"]},"805":{"title":"2.2 matmul 相关","titles":["2 线性变换层"]},"806":{"title":"3 Normalization","titles":[]},"807":{"title":"3.1 BatchNorm2d","titles":["3 Normalization"]},"808":{"title":"3.2 LayerNorm","titles":["3 Normalization"]},"809":{"title":"3.3 Instance Normalization","titles":["3 Normalization"]},"810":{"title":"3.4  Group Normalization","titles":["3 Normalization"]},"811":{"title":"3.5 Switch norm","titles":["3 Normalization"]},"812":{"title":"3.6 RMS Norm","titles":["3 Normalization"]},"813":{"title":"4 Pooling","titles":[]},"814":{"title":"4.1 Max Pooling","titles":["4 Pooling"]},"815":{"title":"4.2 AveragePooling","titles":["4 Pooling"]},"816":{"title":"4.3 Global Average Pooling","titles":["4 Pooling"]},"817":{"title":"5 activation functions","titles":[]},"818":{"title":"6 reshape、 view、 permute、transpose","titles":[]},"819":{"title":"6.1 reshape","titles":["6 reshape、 view、 permute、transpose"]},"820":{"title":"6.2 view","titles":["6 reshape、 view、 permute、transpose"]},"821":{"title":"6.3 transpose","titles":["6 reshape、 view、 permute、transpose"]},"822":{"title":"6.4 permute","titles":["6 reshape、 view、 permute、transpose"]},"823":{"title":"7 sequenze 和 unequenze","titles":[]},"824":{"title":"8 concat、stack、expand 和 flatten","titles":[]},"825":{"title":"8.1 concat","titles":["8 concat、stack、expand 和 flatten"]},"826":{"title":"8.2 stack","titles":["8 concat、stack、expand 和 flatten"]},"827":{"title":"8.3 expand","titles":["8 concat、stack、expand 和 flatten"]},"828":{"title":"8.4 flatten","titles":["8 concat、stack、expand 和 flatten"]},"829":{"title":"9 pointwise","titles":[]},"830":{"title":"10 split 和 slice","titles":[]},"831":{"title":"10.1 split","titles":["10 split 和 slice"]},"832":{"title":"10.2 slice","titles":["10 split 和 slice"]},"833":{"title":"11 reduce 规约类算子","titles":[]},"834":{"title":"12 embedding","titles":[]},"835":{"title":"13 dropout","titles":[]},"836":{"title":"14 附录","titles":[]},"837":{"title":"15 参考链接","titles":[]},"838":{"title":"0 Activation 整体介绍","titles":[]},"839":{"title":"1 S 型激活函数","titles":["0 Activation 整体介绍"]},"840":{"title":"2 Relu 激活函数","titles":["0 Activation 整体介绍"]},"841":{"title":"3 ReLU6","titles":[]},"842":{"title":"4 其它ReLU 相关 激活函数","titles":[]},"843":{"title":"5 ELU(Exponential Linear Units) 和 SELU(Scaled ELU)","titles":[]},"844":{"title":"6 GeLU（Gaussian Error Linear Unit）","titles":[]},"845":{"title":"7 Swish 与 Hardswish","titles":[]},"846":{"title":"8 mish","titles":[]},"847":{"title":"9 Softmax","titles":[]},"848":{"title":"10 总结：好的激活函数应有的性质","titles":[]},"849":{"title":"11 参考链接","titles":[]},"850":{"title":"0 循环神经网络(Recurrent neural network：RNN)","titles":[]},"851":{"title":"2 典型的RNN网络","titles":[]},"852":{"title":"3 RNN 结构详解","titles":[]},"853":{"title":"3.1 RNN 循环过程如下图所示：","titles":["3 RNN 结构详解"]},"854":{"title":"3.2 按时间步展开如下：","titles":["3 RNN 结构详解"]},"855":{"title":"3.3 经典RNN的计算图如下：","titles":["3 RNN 结构详解"]},"856":{"title":"3.4 RNN具体计算公式为：","titles":["3 RNN 结构详解"]},"857":{"title":"3.5 RNN 工程图展示：","titles":["3 RNN 结构详解"]},"858":{"title":"3.6 RNN可扩展到双向的情况，其结构如下：","titles":["3 RNN 结构详解"]},"859":{"title":"3.7 RNN扩展到多层构成循环神经网络，结构如下：","titles":["3 RNN 结构详解"]},"860":{"title":"4 RNN 应用案例(意图识别)","titles":[]},"861":{"title":"5 经典RNN 存在的问题","titles":[]},"862":{"title":"6 LSTM(Long Short-Term Memory) 长短期记忆网络","titles":[]},"863":{"title":"6.1 LSTM 整体结构","titles":["6 LSTM(Long Short-Term Memory) 长短期记忆网络"]},"864":{"title":"6.2 LSTM cell 详解","titles":["6 LSTM(Long Short-Term Memory) 长短期记忆网络"]},"865":{"title":"6.2.1 遗忘门","titles":["6 LSTM(Long Short-Term Memory) 长短期记忆网络","6.2 LSTM cell 详解"]},"866":{"title":"6.2.2 输入门","titles":["6 LSTM(Long Short-Term Memory) 长短期记忆网络","6.2 LSTM cell 详解"]},"867":{"title":"6.2.3 cell state","titles":["6 LSTM(Long Short-Term Memory) 长短期记忆网络","6.2 LSTM cell 详解"]},"868":{"title":"6.2.4 输出门","titles":["6 LSTM(Long Short-Term Memory) 长短期记忆网络","6.2 LSTM cell 详解"]},"869":{"title":"6.2.5 总结","titles":["6 LSTM(Long Short-Term Memory) 长短期记忆网络","6.2 LSTM cell 详解"]},"870":{"title":"6.3 LSTM Cell 具体计算","titles":["6 LSTM(Long Short-Term Memory) 长短期记忆网络"]},"871":{"title":"6.4 与RNN 类似 LSTM 也有双向的","titles":["6 LSTM(Long Short-Term Memory) 长短期记忆网络"]},"872":{"title":"7 GRU ：门控循环单元（Gated Recurrent Unit）","titles":[]},"873":{"title":"7.1 LSTM 和 GRU 对比","titles":["7 GRU ：门控循环单元（Gated Recurrent Unit）"]},"874":{"title":"7.2 原理","titles":["7 GRU ：门控循环单元（Gated Recurrent Unit）"]},"875":{"title":"7.3 计算过程","titles":["7 GRU ：门控循环单元（Gated Recurrent Unit）"]},"876":{"title":"8 参考链接","titles":[]},"877":{"title":"1 从RNN 到 Seq2Seq","titles":[]},"878":{"title":"1.1 RNN 简述","titles":["1 从RNN 到 Seq2Seq"]},"879":{"title":"1.2 RNN 应用场景","titles":["1 从RNN 到 Seq2Seq"]},"880":{"title":"1.2.1 RNN 解决 N VS N 问题","titles":["1 从RNN 到 Seq2Seq","1.2 RNN 应用场景"]},"881":{"title":"1.2.2 RNN 解决 N Versus 1 问题","titles":["1 从RNN 到 Seq2Seq","1.2 RNN 应用场景"]},"882":{"title":"1.2.3 RNN 解决 1 VS N 问题","titles":["1 从RNN 到 Seq2Seq","1.2 RNN 应用场景"]},"883":{"title":"1.3 N VS M 型任务","titles":["1 从RNN 到 Seq2Seq"]},"884":{"title":"2 Seq2Seq 模型","titles":[]},"885":{"title":"2.1 Seq2Seq 定义","titles":[]},"886":{"title":"2.2 seq2seq 模型结构","titles":[]},"887":{"title":"2.2.1 encoder-decoder 架构","titles":["2.2 seq2seq 模型结构"]},"888":{"title":"2.2.2 encoder 部分","titles":["2.2 seq2seq 模型结构"]},"889":{"title":"2.2.3 decoder 部分","titles":["2.2 seq2seq 模型结构"]},"890":{"title":"2.3 Seq2Seq 实现举例","titles":["2.2 seq2seq 模型结构"]},"891":{"title":"3 Seq2Seq 中的 Attention 机制","titles":[]},"892":{"title":"3.1 Seq2Seq 中的 Attention 机制","titles":["3 Seq2Seq 中的 Attention 机制"]},"893":{"title":"4 Seq2Seq 的工作流程","titles":[]},"894":{"title":"4.1 预测时流程","titles":["4 Seq2Seq 的工作流程"]},"895":{"title":"4.2 训练时流程","titles":["4 Seq2Seq 的工作流程"]},"896":{"title":"4.2.1 Teacher Forcing","titles":["4 Seq2Seq 的工作流程","4.2 训练时流程"]},"897":{"title":"4.2.2 Scheduled sampling","titles":["4 Seq2Seq 的工作流程","4.2 训练时流程"]},"898":{"title":"4.3 Decoder的预训练","titles":["4 Seq2Seq 的工作流程"]},"899":{"title":"5 Seq2Seq 的损失函数","titles":[]},"900":{"title":"6 Decoding 中的 Beam search","titles":[]},"901":{"title":"6.1 贪心decoding","titles":["6 Decoding 中的 Beam search"]},"902":{"title":"6.2 Beam search 原理","titles":["6 Decoding 中的 Beam search"]},"903":{"title":"6.3 公式分析","titles":["6 Decoding 中的 Beam search"]},"904":{"title":"6.4 Beam search 分析","titles":["6 Decoding 中的 Beam search"]},"905":{"title":"7 NLP 从机器学习到深度学习","titles":[]},"906":{"title":"7.1 NLP 中常见任务","titles":["7 NLP 从机器学习到深度学习"]},"907":{"title":"7.2 机器翻译的发展历程","titles":["7 NLP 从机器学习到深度学习"]},"908":{"title":"7.3 SMT 方法简介","titles":["7 NLP 从机器学习到深度学习"]},"909":{"title":"7.4 NMT","titles":["7 NLP 从机器学习到深度学习"]},"910":{"title":"8 参考文献","titles":[]},"911":{"title":"1 Attention Is All You Need","titles":[]},"912":{"title":"2 Transformer Model Architecture","titles":[]},"913":{"title":"3 编码器和解码器堆栈","titles":[]},"914":{"title":"3.1 编码器：","titles":["3 编码器和解码器堆栈"]},"915":{"title":"3.2 解码器：","titles":["3 编码器和解码器堆栈"]},"916":{"title":"4 Scaled Dot-Product Attention（缩放版本的点积注意力）","titles":[]},"917":{"title":"4.1 模型结构图","titles":["4 Scaled Dot-Product Attention（缩放版本的点积注意力）"]},"918":{"title":"4.2 数学公式为","titles":["4 Scaled Dot-Product Attention（缩放版本的点积注意力）"]},"919":{"title":"4.3 推导过程详解","titles":["4 Scaled Dot-Product Attention（缩放版本的点积注意力）"]},"920":{"title":"4.2.1 self attention 的思想","titles":["4 Scaled Dot-Product Attention（缩放版本的点积注意力）","4.3 推导过程详解"]},"921":{"title":"4.2.2 自注意的思想","titles":["4 Scaled Dot-Product Attention（缩放版本的点积注意力）","4.3 推导过程详解"]},"922":{"title":"4.2.3 自注意机制运算过程","titles":["4 Scaled Dot-Product Attention（缩放版本的点积注意力）","4.3 推导过程详解"]},"923":{"title":"4.2.4 写成矩阵的形式","titles":["4 Scaled Dot-Product Attention（缩放版本的点积注意力）","4.3 推导过程详解"]},"924":{"title":"4.4 为什么要进行缩放","titles":["4 Scaled Dot-Product Attention（缩放版本的点积注意力）"]},"925":{"title":"5 Multi-Head self Attention","titles":[]},"926":{"title":"5.1 原理简介","titles":["5 Multi-Head self Attention"]},"927":{"title":"5.2 公式表达","titles":["5 Multi-Head self Attention"]},"928":{"title":"5.3 底层原理","titles":["5 Multi-Head self Attention"]},"929":{"title":"5.4 多头的实现细节展示","titles":["5 Multi-Head self Attention"]},"930":{"title":"6 实际工程上的 Multi-Head Attention 详解","titles":[]},"931":{"title":"7 Cross Multi-Head Attention","titles":[]},"932":{"title":"8 Mask Multi-Head Attention","titles":[]},"933":{"title":"8.1 padding mask","titles":["8 Mask Multi-Head Attention"]},"934":{"title":"8.2 sequence mask","titles":["8 Mask Multi-Head Attention"]},"935":{"title":"9 MQA（Multi Query Attention）","titles":[]},"936":{"title":"10 大模型神器：GQA（Grouped Query Attention）","titles":[]},"937":{"title":"10.1 GQA Structure","titles":["10 大模型神器：GQA（Grouped Query Attention）"]},"938":{"title":"10.2 精度改进：converting the checkpoint and uptraining","titles":[]},"939":{"title":"11 大模型加速利器：FlashAttention:","titles":[]},"940":{"title":"11.1 原理介绍","titles":["11 大模型加速利器：FlashAttention:"]},"941":{"title":"11.2 标准attention机制的算法实现","titles":["11 大模型加速利器：FlashAttention:"]},"942":{"title":"11.3 flash attention 算法思想","titles":["11 大模型加速利器：FlashAttention:"]},"943":{"title":"11.4 准备：切片的方式计算softmax","titles":["11 大模型加速利器：FlashAttention:"]},"944":{"title":"11.5 具体flashattention的算法","titles":["11 大模型加速利器：FlashAttention:"]},"945":{"title":"Flash-Attention 效果","titles":["11 大模型加速利器：FlashAttention:"]},"946":{"title":"11.6 重计算(recompute)","titles":["11 大模型加速利器：FlashAttention:"]},"947":{"title":"12 # flash-attention 2","titles":[]},"948":{"title":"13 大模型推理加速利器：KV Cache","titles":[]},"949":{"title":"14 大模型推理加速利器：Page-Attention","titles":[]},"950":{"title":"15 参考链接","titles":[]},"951":{"title":"1 MQA（Multi Query Attention）","titles":[]},"952":{"title":"2 大模型神器：GQA（Grouped Query Attention）","titles":[]},"953":{"title":"2.1 GQA Structure","titles":["2 大模型神器：GQA（Grouped Query Attention）"]},"954":{"title":"2.2 精度改进：converting the checkpoint and uptraining","titles":[]},"955":{"title":"3 MLA(Multi-Head Latent Attention): Boosting Inference Efficiency","titles":[]},"956":{"title":"3.1 MLA 原理","titles":["3 MLA(Multi-Head Latent Attention): Boosting Inference Efficiency"]},"957":{"title":"3.2 MLA 实现逻辑","titles":["3 MLA(Multi-Head Latent Attention): Boosting Inference Efficiency"]},"958":{"title":"4 大模型加速利器：FlashAttention:","titles":[]},"959":{"title":"4.1 原理及思想介绍","titles":["4 大模型加速利器：FlashAttention:"]},"960":{"title":"4.2 标准attention机制的算法实现","titles":["4 大模型加速利器：FlashAttention:"]},"961":{"title":"4.3 准备：切片的方式计算softmax","titles":["4 大模型加速利器：FlashAttention:"]},"962":{"title":"4.4 flash-attention-1 算法图解","titles":["4 大模型加速利器：FlashAttention:"]},"963":{"title":"4.5 FlashAttention1 Forward 伪代码","titles":["4 大模型加速利器：FlashAttention:"]},"964":{"title":"4.6 FlashAttention1 Backward 伪代码","titles":["4 大模型加速利器：FlashAttention:"]},"965":{"title":"4.7 Flash-Attention 效果","titles":["4 大模型加速利器：FlashAttention:"]},"966":{"title":"4.8 重计算(recompute)","titles":["4 大模型加速利器：FlashAttention:"]},"967":{"title":"4.9 FlashAttention1 的不足之处","titles":["4 大模型加速利器：FlashAttention:"]},"968":{"title":"5 FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning","titles":[]},"969":{"title":"5.1 softmax-trick v1 vs v2","titles":["5 FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"]},"970":{"title":"5.2 forward pass","titles":["5 FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"]},"971":{"title":"5.3 backward pass","titles":["5 FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"]},"972":{"title":"5.4 v2 相对于 v1 的改进","titles":["5 FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"]},"973":{"title":"6 FlashAttention3 : Fast and Accurate Attention with Asynchrony and Low-precision","titles":[]},"974":{"title":"7 RingAttention","titles":[]},"975":{"title":"7.1 具体实现原理","titles":["7 RingAttention"]},"976":{"title":"8 从ring-attention 到 context parallel","titles":[]},"977":{"title":"9 从context parallel 到 chunked pipeline parallelism","titles":[]},"978":{"title":"10 大模型推理加速利器：KV Cache","titles":[]},"979":{"title":"11 大模型推理加速利器：Page-Attention and vLLM","titles":[]},"980":{"title":"11.1 vLLM 简介","titles":["11 大模型推理加速利器：Page-Attention and vLLM"]},"981":{"title":"11.2 秘密武器：PagedAttention","titles":["11 大模型推理加速利器：Page-Attention and vLLM"]},"982":{"title":"11.3 优势1 : block 无需连续","titles":["11 大模型推理加速利器：Page-Attention and vLLM"]},"983":{"title":"11.4 优势2 ：内存共享","titles":["11 大模型推理加速利器：Page-Attention and vLLM"]},"984":{"title":"12 RadixAttention","titles":[]},"985":{"title":"12.1 当前KV Cache","titles":["12 RadixAttention"]},"986":{"title":"12.2 RedisAttention strategy","titles":["12 RadixAttention"]},"987":{"title":"13 参考链接","titles":[]},"988":{"title":"1 参数初始化概念(Parameters initialization)","titles":[]},"989":{"title":"2 参数初始化的重要性","titles":[]},"990":{"title":"2.1 为什么参数初始化很重要","titles":["2 参数初始化的重要性"]},"991":{"title":"2.1 不合理初始化的问题","titles":["2 参数初始化的重要性"]},"992":{"title":"3 全0或常量初始化","titles":[]},"993":{"title":"4 随机初始化","titles":[]},"994":{"title":"4.1 较小随机值时","titles":["4 随机初始化"]},"995":{"title":"4.2 较大随机初始值时","titles":["4 随机初始化"]},"996":{"title":"4.3 结论","titles":["4 随机初始化"]},"997":{"title":"5 理想的参数初始化","titles":[]},"998":{"title":"5.1 参数初始化的必要条件","titles":["5 理想的参数初始化"]},"999":{"title":"5.2 Glorot 条件","titles":["5 理想的参数初始化"]},"1000":{"title":"6 塞维尔初始化(Xavier initialization)","titles":[]},"1001":{"title":"7 kaiming initialization","titles":[]},"1002":{"title":"7.1 方差计算数学基础","titles":["7 kaiming initialization"]},"1003":{"title":"7.2 前向推导过程","titles":["7 kaiming initialization"]},"1004":{"title":"7.3 反向推导过程","titles":["7 kaiming initialization"]},"1005":{"title":"7.4 凯明初始化总结","titles":["7 kaiming initialization"]},"1006":{"title":"7.4.1 服从正态分布时","titles":["7 kaiming initialization","7.4 凯明初始化总结"]},"1007":{"title":"7.4.2 服从均匀分布时:","titles":["7 kaiming initialization","7.4 凯明初始化总结"]},"1008":{"title":"8 初始化策略选择","titles":[]},"1009":{"title":"9 使用预训练的weight","titles":[]},"1010":{"title":"10 参考文献","titles":[]},"1011":{"title":"1 正则化概念","titles":[]},"1012":{"title":"2 什么情况下容易出现过拟合","titles":[]},"1013":{"title":"3 常见的正则化方法","titles":[]},"1014":{"title":"3.1 参数范数惩罚","titles":["3 常见的正则化方法"]},"1015":{"title":"3.2 数据集增强","titles":["3 常见的正则化方法"]},"1016":{"title":"3.3 标签平滑（label smoothing）","titles":["3 常见的正则化方法"]},"1017":{"title":"3.4 droupout","titles":["3 常见的正则化方法"]},"1018":{"title":"3.5 dropconnet","titles":["3 常见的正则化方法"]},"1019":{"title":"3.6 dropblock","titles":["3 常见的正则化方法"]},"1020":{"title":"3.7 其它正则化方法","titles":["3 常见的正则化方法"]},"1021":{"title":"optimizer 概述","titles":[]},"1022":{"title":"1 Gradient Descend","titles":[]},"1023":{"title":"1.1 梯度下降法概念","titles":["1 Gradient Descend"]},"1024":{"title":"1.2 梯度下降法三个变种","titles":["1 Gradient Descend"]},"1025":{"title":"1.2.1 BGD(Batch Gradient Descend)","titles":["1 Gradient Descend","1.2 梯度下降法三个变种"]},"1026":{"title":"1.2.2 SGD(Stochastic Gradient Descend)","titles":["1 Gradient Descend","1.2 梯度下降法三个变种"]},"1027":{"title":"1.2.3 Mini-BGD","titles":["1 Gradient Descend","1.2 梯度下降法三个变种"]},"1028":{"title":"2 SGD with Momentum","titles":[]},"1029":{"title":"2.1 算法过程","titles":["2 SGD with Momentum"]},"1030":{"title":"2.2 算法图示","titles":["2 SGD with Momentum"]},"1031":{"title":"2.2 特点","titles":["2 SGD with Momentum"]},"1032":{"title":"2.3 作用","titles":["2 SGD with Momentum"]},"1033":{"title":"3  NAG（Nesterov Accelerated Gradient）","titles":[]},"1034":{"title":"3.1 算法原理","titles":["3  NAG（Nesterov Accelerated Gradient）"]},"1035":{"title":"3.2 算法原理图","titles":["3  NAG（Nesterov Accelerated Gradient）"]},"1036":{"title":"3.3 算法详述","titles":["3  NAG（Nesterov Accelerated Gradient）"]},"1037":{"title":"4 Pytorch 中实现 SGD","titles":[]},"1038":{"title":"4.1 算法过程","titles":["4 Pytorch 中实现 SGD"]},"1039":{"title":"4.2 代码实现","titles":["4 Pytorch 中实现 SGD"]},"1040":{"title":"5 AdaGrad 优化算法","titles":[]},"1041":{"title":"5.1 自适应学习率的概念","titles":["5 AdaGrad 优化算法"]},"1042":{"title":"5.2 AdaGrad 算法原理","titles":["5 AdaGrad 优化算法"]},"1043":{"title":"5.3 AdaGrad 算法","titles":["5 AdaGrad 优化算法"]},"1044":{"title":"5.4 特点","titles":["5 AdaGrad 优化算法"]},"1045":{"title":"5.5 缺点","titles":["5 AdaGrad 优化算法"]},"1046":{"title":"5.6 pytorch 实现","titles":["5 AdaGrad 优化算法"]},"1047":{"title":"6 RMSProp 优化算法","titles":[]},"1048":{"title":"6.1 理论基础","titles":["6 RMSProp 优化算法"]},"1049":{"title":"6.2 算法流程","titles":["6 RMSProp 优化算法"]},"1050":{"title":"6.3 pytorch 实现","titles":["6 RMSProp 优化算法"]},"1051":{"title":"7 Adadelta","titles":[]},"1052":{"title":"7.1 概述","titles":["7 Adadelta"]},"1053":{"title":"7.2 算法流程","titles":["7 Adadelta"]},"1054":{"title":"7.3 pytorch 实现","titles":["7 Adadelta"]},"1055":{"title":"8  不同优化算法效果对比","titles":[]},"1056":{"title":"8.1 loss 对比图","titles":["8  不同优化算法效果对比"]},"1057":{"title":"8.2 收敛过程对比","titles":["8  不同优化算法效果对比"]},"1058":{"title":"9 Adam 优化器","titles":[]},"1059":{"title":"9.1 原理概述","titles":[]},"1060":{"title":"9.2 算法实现流程","titles":["9.1 原理概述"]},"1061":{"title":"9.3 pytorch 实现","titles":["9.1 原理概述"]},"1062":{"title":"9.4 效果展示","titles":["9.1 原理概述"]},"1063":{"title":"10 AdamW","titles":[]},"1064":{"title":"10.1 算法原理","titles":["10 AdamW"]},"1065":{"title":"10.2 pytorch实现","titles":["10 AdamW"]},"1066":{"title":"11 Optimizer 收敛趋势对比图","titles":[]},"1067":{"title":"12 参考文献","titles":[]},"1068":{"title":"1 创建pytorch tensor","titles":[]},"1069":{"title":"1.1 用torch.Tensor 创建","titles":["1 创建pytorch tensor"]},"1070":{"title":"1.2 直接生成特殊的tensor","titles":["1 创建pytorch tensor"]},"1071":{"title":"1.3 仿照其它tensor生成","titles":["1 创建pytorch tensor"]},"1072":{"title":"1.4 从numpy生成","titles":["1 创建pytorch tensor"]},"1073":{"title":"2 工程实践","titles":[]},"1074":{"title":"3 Tensor 中的 to 方法","titles":[]},"1075":{"title":"3.1 数据类型转化","titles":["3 Tensor 中的 to 方法"]},"1076":{"title":"3.2 device 转化","titles":["3 Tensor 中的 to 方法"]},"1077":{"title":"4 Tensor 讲解","titles":[]},"1078":{"title":"4.1 两个角度认识 Tensor","titles":["4 Tensor 讲解"]},"1079":{"title":"4.2 代码实践之：视图到底是什么","titles":["4 Tensor 讲解"]},"1080":{"title":"4.3 代码实践之：Tensor 中数据的连续性","titles":["4 Tensor 讲解"]},"1081":{"title":"5 Tensor 运算的几种主要类型","titles":["4 Tensor 讲解"]},"1082":{"title":"6 Tensor 的属性全解","titles":[]},"1083":{"title":"7 外层 Tensor 方法汇总","titles":[]},"1084":{"title":"8 TensorBase 方法汇总","titles":[]},"1085":{"title":"8.1 魔术方法（基本运算符 + 构造函数 + 索引）","titles":["8 TensorBase 方法汇总"]},"1086":{"title":"8.2 私有方法","titles":["8 TensorBase 方法汇总"]},"1087":{"title":"8.3 Tensor 的 对外API接口","titles":["8 TensorBase 方法汇总"]},"1088":{"title":"1 pytorch autograd 原理概述","titles":[]},"1089":{"title":"1.1 原理概述","titles":["1 pytorch autograd 原理概述"]},"1090":{"title":"1.2 实现细节","titles":["1 pytorch autograd 原理概述"]},"1091":{"title":"2 pytorch 代码实现","titles":[]},"1092":{"title":"2.1 pytorch autograd 展示","titles":["2 pytorch 代码实现"]},"1093":{"title":"2.2 require_grad 的自动推理机制","titles":["2 pytorch 代码实现"]},"1094":{"title":"2.3 detach 隔离功能","titles":["2 pytorch 代码实现"]},"1095":{"title":"2.4 控制梯度计算","titles":["2 pytorch 代码实现"]},"1096":{"title":"2.5 梯度累加和清0","titles":["2 pytorch 代码实现"]},"1097":{"title":"2.6 小心 inplace-op","titles":["2 pytorch 代码实现"]},"1098":{"title":"2.7 pytorch autograd 解方程","titles":["2 pytorch 代码实现"]},"1099":{"title":"2.8 保存中间 activation tensor 的梯度","titles":["2 pytorch 代码实现"]},"1100":{"title":"2.9 customer 自定义自己的反向传播函数","titles":["2 pytorch 代码实现"]},"1101":{"title":"2.10 多维Tensor 如何backward","titles":["2 pytorch 代码实现"]},"1102":{"title":"2.11 example : train a model with two mlp layers","titles":["2 pytorch 代码实现"]},"1103":{"title":"3 要点总结","titles":[]},"1104":{"title":"3.1 自动微分机制(auto grad) 重点：","titles":["3 要点总结"]},"1105":{"title":"3.2 反向传播算法","titles":["3 要点总结"]},"1106":{"title":"3.3 tensor 的梯度","titles":["3 要点总结"]},"1107":{"title":"3.4 反向求导原理","titles":["3 要点总结"]},"1108":{"title":"3.5 动态图机制","titles":["3 要点总结"]},"1109":{"title":"3.6 auto grad 机制不足","titles":["3 要点总结"]},"1110":{"title":"3.7 autograd是什么","titles":["3 要点总结"]},"1111":{"title":"3.8 grad_fun","titles":["3 要点总结"]},"1112":{"title":"4 pytorch autograd(自动微分机制) extension ： 了解即可，不需要掌握","titles":[]},"1113":{"title":"4.1 自动微分如何编码历史记录","titles":["4 pytorch autograd(自动微分机制) extension ： 了解即可，不需要掌握"]},"1114":{"title":"4.2 Saved Tensors","titles":["4 pytorch autograd(自动微分机制) extension ： 了解即可，不需要掌握"]},"1115":{"title":"4.3 对于不可微分的函数的梯度计算","titles":["4 pytorch autograd(自动微分机制) extension ： 了解即可，不需要掌握"]},"1116":{"title":"4.4 局部禁用梯度计算","titles":["4 pytorch autograd(自动微分机制) extension ： 了解即可，不需要掌握"]},"1117":{"title":"4.5 设置 requires_grad","titles":["4 pytorch autograd(自动微分机制) extension ： 了解即可，不需要掌握"]},"1118":{"title":"4.6 梯度模式","titles":["4 pytorch autograd(自动微分机制) extension ： 了解即可，不需要掌握"]},"1119":{"title":"4.6.1 默认模式（Grad mode）","titles":["4 pytorch autograd(自动微分机制) extension ： 了解即可，不需要掌握","4.6 梯度模式"]},"1120":{"title":"4.6.2 无梯度模式","titles":["4 pytorch autograd(自动微分机制) extension ： 了解即可，不需要掌握","4.6 梯度模式"]},"1121":{"title":"4.6.3 推断模式(inference mode)","titles":["4 pytorch autograd(自动微分机制) extension ： 了解即可，不需要掌握","4.6 梯度模式"]},"1122":{"title":"4.6.4 评估模式（nn.Module.eval()）","titles":["4 pytorch autograd(自动微分机制) extension ： 了解即可，不需要掌握","4.6 梯度模式"]},"1123":{"title":"4.7 In-place operations with autograd","titles":["4 pytorch autograd(自动微分机制) extension ： 了解即可，不需要掌握"]},"1124":{"title":"深度学习调优指南中文版","titles":[]},"1125":{"title":"目录","titles":["深度学习调优指南中文版"]},"1126":{"title":"这份手册是为谁准备的？","titles":["深度学习调优指南中文版"]},"1127":{"title":"为什么需要这份调优手册？","titles":["深度学习调优指南中文版"]},"1128":{"title":"开始新项目的指南","titles":["深度学习调优指南中文版"]},"1129":{"title":"选择模型架构","titles":["深度学习调优指南中文版","开始新项目的指南"]},"1130":{"title":"选择优化器","titles":["深度学习调优指南中文版","开始新项目的指南"]},"1131":{"title":"选择BatchSize","titles":["深度学习调优指南中文版","开始新项目的指南"]},"1132":{"title":"确定可行的Batch Size并估计训练吞吐量","titles":["深度学习调优指南中文版","开始新项目的指南","选择BatchSize"]},"1133":{"title":"选择合适的Batch Size以最小化训练时间","titles":["深度学习调优指南中文版","开始新项目的指南","选择BatchSize"]},"1134":{"title":"选择合适的Batch Size以最小化资源消耗","titles":["深度学习调优指南中文版","开始新项目的指南","选择BatchSize"]},"1135":{"title":"更改Batch Size需要重新调整大多数超参数","titles":["深度学习调优指南中文版","开始新项目的指南","选择BatchSize"]},"1136":{"title":"Batch Norm会对Batch Size的选择造成什么影响？","titles":["深度学习调优指南中文版","开始新项目的指南","选择BatchSize"]},"1137":{"title":"选择初始配置","titles":["深度学习调优指南中文版","开始新项目的指南","选择BatchSize"]},"1138":{"title":"提高模型性能的科学方法","titles":["深度学习调优指南中文版"]},"1139":{"title":"增量调整策略","titles":["深度学习调优指南中文版","提高模型性能的科学方法"]},"1140":{"title":"探索与利用","titles":["深度学习调优指南中文版","提高模型性能的科学方法"]},"1141":{"title":"选择下一轮实验的目标","titles":["深度学习调优指南中文版","提高模型性能的科学方法"]},"1142":{"title":"设计下一轮实验","titles":["深度学习调优指南中文版","提高模型性能的科学方法"]},"1143":{"title":"识别目标超参数、冗余超参数和固定超参数","titles":["深度学习调优指南中文版","提高模型性能的科学方法","设计下一轮实验"]},"1144":{"title":"创建一组研究","titles":["深度学习调优指南中文版","提高模型性能的科学方法","设计下一轮实验"]},"1145":{"title":"平衡实验的信息量和成本","titles":["深度学习调优指南中文版","提高模型性能的科学方法","设计下一轮实验"]},"1146":{"title":"从实验结果中获取经验","titles":["深度学习调优指南中文版","提高模型性能的科学方法"]},"1147":{"title":"识别错误的搜索空间边界","titles":["深度学习调优指南中文版","提高模型性能的科学方法","从实验结果中获取经验"]},"1148":{"title":"没有在搜索空间中采样足够的点","titles":["深度学习调优指南中文版","提高模型性能的科学方法","从实验结果中获取经验"]},"1149":{"title":"检查训练曲线","titles":["深度学习调优指南中文版","提高模型性能的科学方法","从实验结果中获取经验"]},"1150":{"title":"使用isolation图检测更改是否有用","titles":["深度学习调优指南中文版","提高模型性能的科学方法","从实验结果中获取经验"]},"1151":{"title":"自动化常用的绘图","titles":["深度学习调优指南中文版","提高模型性能的科学方法","从实验结果中获取经验"]},"1152":{"title":"确定是否采用此训练工作流更改或超参数配置","titles":["深度学习调优指南中文版","提高模型性能的科学方法"]},"1153":{"title":"探索结束后","titles":["深度学习调优指南中文版","提高模型性能的科学方法"]},"1154":{"title":"确定每次训练运行的步数","titles":["深度学习调优指南中文版"]},"1155":{"title":"当训练不受计算限制时如何决定该训练多久","titles":["深度学习调优指南中文版","确定每次训练运行的步数"]},"1156":{"title":"使用学习率搜索算法来确定 max_train_steps 的初始值","titles":["深度学习调优指南中文版","确定每次训练运行的步数","当训练不受计算限制时如何决定该训练多久"]},"1157":{"title":"当训练受计算限制时如何决定该训练多久","titles":["深度学习调优指南中文版","确定每次训练运行的步数"]},"1158":{"title":"第一轮","titles":["深度学习调优指南中文版","确定每次训练运行的步数","当训练受计算限制时如何决定该训练多久"]},"1159":{"title":"第二轮","titles":["深度学习调优指南中文版","确定每次训练运行的步数","当训练受计算限制时如何决定该训练多久"]},"1160":{"title":"关于训练管道的额外补充","titles":["深度学习调优指南中文版"]},"1161":{"title":"优化输入管道","titles":["深度学习调优指南中文版","关于训练管道的额外补充"]},"1162":{"title":"评估模型性能","titles":["深度学习调优指南中文版","关于训练管道的额外补充"]},"1163":{"title":"评估设置","titles":["深度学习调优指南中文版","关于训练管道的额外补充","评估模型性能"]},"1164":{"title":"设置定期评估","titles":["深度学习调优指南中文版","关于训练管道的额外补充","评估模型性能"]},"1165":{"title":"选择样本进行定期评估","titles":["深度学习调优指南中文版","关于训练管道的额外补充","评估模型性能"]},"1166":{"title":"保存检查点并追溯选择最佳检查点","titles":["深度学习调优指南中文版","关于训练管道的额外补充"]},"1167":{"title":"设置实验跟踪","titles":["深度学习调优指南中文版","关于训练管道的额外补充"]},"1168":{"title":"BatchNorm的实现细节","titles":["深度学习调优指南中文版","关于训练管道的额外补充"]},"1169":{"title":"多主机管道的考虑因素","titles":["深度学习调优指南中文版","关于训练管道的额外补充"]},"1170":{"title":"常见问题的回答","titles":["深度学习调优指南中文版"]},"1171":{"title":"最好的学习率衰减方案是什么","titles":["深度学习调优指南中文版","常见问题的回答"]},"1172":{"title":"我应该使用哪种学习率衰减方案作为默认值？","titles":["深度学习调优指南中文版","常见问题的回答"]},"1173":{"title":"为什么有些论文有复杂的学习率衰减方案？","titles":["深度学习调优指南中文版","常见问题的回答"]},"1174":{"title":"Adam 的超参数应该如何调整？","titles":["深度学习调优指南中文版","常见问题的回答"]},"1175":{"title":"为什么在优化的探索阶段使用Quasi-Random-Search而不是更复杂的黑盒优化算法？","titles":["深度学习调优指南中文版","常见问题的回答"]},"1176":{"title":"在哪里可以找到Quasi-Random-Search的实现？","titles":["深度学习调优指南中文版","常见问题的回答"]},"1177":{"title":"需要多少次试验才能通过Quasi-Random-Search获得较好的结果？","titles":["深度学习调优指南中文版","常见问题的回答"]},"1178":{"title":"如何调试和缓解优化失败","titles":["深度学习调优指南中文版","常见问题的回答"]},"1179":{"title":"识别不稳定的训练任务","titles":["深度学习调优指南中文版","常见问题的回答","如何调试和缓解优化失败"]},"1180":{"title":"常见不稳定模式的潜在修复方式","titles":["深度学习调优指南中文版","常见问题的回答","如何调试和缓解优化失败"]},"1181":{"title":"学习率预热","titles":["深度学习调优指南中文版","常见问题的回答","如何调试和缓解优化失败"]},"1182":{"title":"何时对学习率进行预热","titles":["深度学习调优指南中文版","常见问题的回答","如何调试和缓解优化失败","学习率预热"]},"1183":{"title":"如何对学习率进行预热","titles":["深度学习调优指南中文版","常见问题的回答","如何调试和缓解优化失败","学习率预热"]},"1184":{"title":"梯度截断","titles":["深度学习调优指南中文版","常见问题的回答","如何调试和缓解优化失败"]},"1185":{"title":"为什么将学习率和其他优化参数称为超参数？ 它们不是任何先验分布的参数。","titles":["深度学习调优指南中文版","常见问题的回答"]},"1186":{"title":"为什么不应该调整Batch Size来直接提高验证集性能?","titles":["深度学习调优指南中文版","常见问题的回答"]},"1187":{"title":"所有流行的优化算法的更新规则是什么？","titles":["深度学习调优指南中文版","常见问题的回答"]},"1188":{"title":"Stochastic gradient descent (SGD)","titles":["深度学习调优指南中文版","常见问题的回答","所有流行的优化算法的更新规则是什么？"]},"1189":{"title":"Momentum","titles":["深度学习调优指南中文版","常见问题的回答","所有流行的优化算法的更新规则是什么？"]},"1190":{"title":"Nesterov","titles":["深度学习调优指南中文版","常见问题的回答","所有流行的优化算法的更新规则是什么？"]},"1191":{"title":"RMSProp","titles":["深度学习调优指南中文版","常见问题的回答","所有流行的优化算法的更新规则是什么？"]},"1192":{"title":"ADAM","titles":["深度学习调优指南中文版","常见问题的回答","所有流行的优化算法的更新规则是什么？"]},"1193":{"title":"NADAM","titles":["深度学习调优指南中文版","常见问题的回答","所有流行的优化算法的更新规则是什么？"]},"1194":{"title":"致谢","titles":["深度学习调优指南中文版"]},"1195":{"title":"引用","titles":["深度学习调优指南中文版"]},"1196":{"title":"关于贡献","titles":["深度学习调优指南中文版"]},"1197":{"title":"贡献者许可协议","titles":["深度学习调优指南中文版","关于贡献"]},"1198":{"title":"代码审核","titles":["深度学习调优指南中文版","关于贡献"]},"1199":{"title":"社区指南","titles":["深度学习调优指南中文版","关于贡献"]},"1200":{"title":"torch.nn.Module","titles":[]},"1201":{"title":"1 pytorch 自带的 torch.nn layer","titles":[]},"1202":{"title":"1.1 用 torch.nn 解决之前的问题","titles":["1 pytorch 自带的 torch.nn layer"]},"1203":{"title":"1.2 Tensor 和 Parameter 的区别","titles":["1 pytorch 自带的 torch.nn layer"]},"1204":{"title":"2 定义我们自己的module","titles":[]},"1205":{"title":"2.1 代码案例","titles":["2 定义我们自己的module"]},"1206":{"title":"2.2 customer layer 要点","titles":["2 定义我们自己的module"]},"1207":{"title":"3 nn.Module 中的容器","titles":[]},"1208":{"title":"4 nn.Module 属性详解","titles":[]},"1209":{"title":"5 torch.nn.Module 常用功能","titles":[]},"1210":{"title":"5.1  _parameters 设置机制","titles":["5 torch.nn.Module 常用功能"]},"1211":{"title":"5.2 _buffers 功能展示","titles":["5 torch.nn.Module 常用功能"]},"1212":{"title":"5.3 前向钩子函数展示","titles":["5 torch.nn.Module 常用功能"]},"1213":{"title":"5.4 反向钩子函数展示","titles":["5 torch.nn.Module 常用功能"]},"1214":{"title":"6 nn.Module 方法全解","titles":["5 torch.nn.Module 常用功能"]},"1215":{"title":"1 端到端训练一个深度学习模型","titles":[]},"1216":{"title":"Decoder","titles":[]},"1217":{"title":"Encoder layer","titles":[]},"1218":{"title":"transformer demo","titles":[]},"1219":{"title":"0 torch.optim","titles":[]},"1220":{"title":"1 如何使用torch.optim","titles":[]},"1221":{"title":"1.1 创建一个优化器对象","titles":["1 如何使用torch.optim"]},"1222":{"title":"1.2 逐参数选项(Per-parameter options)","titles":["1 如何使用torch.optim"]},"1223":{"title":"1.3 进行优化步骤","titles":["1 如何使用torch.optim"]},"1224":{"title":"2 torch.optim base class introduce","titles":[]},"1225":{"title":"2.1 torch.optim.Optimizer 的输入参数","titles":["2 torch.optim base class introduce"]},"1226":{"title":"2.2 torch.optim.Optimizer 属性","titles":["2 torch.optim base class introduce"]},"1227":{"title":"2.3 torch.optim.Optimizer 方法","titles":["2 torch.optim base class introduce"]},"1228":{"title":"3 不同实现与性能优化","titles":[]},"1229":{"title":"learning rate 调整方案","titles":[]},"1230":{"title":"2 pytorch中 torch.optim.lr_scheduler 使用方法","titles":[]},"1231":{"title":"2.1 使用方法","titles":["2 pytorch中 torch.optim.lr_scheduler 使用方法"]},"1232":{"title":"3 学习率调度器 策略全解 (learning-rate scheduler)","titles":["2 pytorch中 torch.optim.lr_scheduler 使用方法"]},"1233":{"title":"3.1 lr_scheduler.LambdaLR","titles":["2 pytorch中 torch.optim.lr_scheduler 使用方法"]},"1234":{"title":"3.2 lr_scheduler.MultiplicativeLR","titles":["2 pytorch中 torch.optim.lr_scheduler 使用方法"]},"1235":{"title":"3.3 lr_scheduler.StepLR","titles":["2 pytorch中 torch.optim.lr_scheduler 使用方法"]},"1236":{"title":"3.4 lr_scheduler.MultiStepLR","titles":["2 pytorch中 torch.optim.lr_scheduler 使用方法"]},"1237":{"title":"3.5 lr_scheduler.ConstantLR","titles":["2 pytorch中 torch.optim.lr_scheduler 使用方法"]},"1238":{"title":"3.6 lr_scheduler.LinearLR","titles":["2 pytorch中 torch.optim.lr_scheduler 使用方法"]},"1239":{"title":"3.7 lr_scheduler.ExponentialLR","titles":["2 pytorch中 torch.optim.lr_scheduler 使用方法"]},"1240":{"title":"3.8 lr_scheduler.PolynomialLR","titles":["2 pytorch中 torch.optim.lr_scheduler 使用方法"]},"1241":{"title":"3.9 lr_scheduler.CyclicLR","titles":["2 pytorch中 torch.optim.lr_scheduler 使用方法"]},"1242":{"title":"3.10 lr_scheduler.OneCycleLR","titles":["2 pytorch中 torch.optim.lr_scheduler 使用方法"]},"1243":{"title":"3.11 lr_scheduler.CosineAnnealingLR","titles":["2 pytorch中 torch.optim.lr_scheduler 使用方法"]},"1244":{"title":"3.12 lr_scheduler.CosineAnnealingWarmRestarts","titles":["2 pytorch中 torch.optim.lr_scheduler 使用方法"]},"1245":{"title":"3.13 ReduceLROnPlateau","titles":["2 pytorch中 torch.optim.lr_scheduler 使用方法"]},"1246":{"title":"3.14 lr_scheduler.ChainedScheduler","titles":["2 pytorch中 torch.optim.lr_scheduler 使用方法"]},"1247":{"title":"3.15 lr_scheduler.SequentialLR","titles":["2 pytorch中 torch.optim.lr_scheduler 使用方法"]},"1248":{"title":"4 探索源码","titles":["2 pytorch中 torch.optim.lr_scheduler 使用方法"]},"1249":{"title":"1 Dataset","titles":[]},"1250":{"title":"2 定义自己的数据集","titles":[]},"1251":{"title":"3 torch 中的 dataloader","titles":[]},"1252":{"title":"4 torchvision","titles":[]},"1253":{"title":"4.1 torchvision 中的dataset","titles":["4 torchvision"]},"1254":{"title":"torchvision 中的 transforms","titles":["4 torchvision"]},"1255":{"title":"1 tensor 的保存和加载","titles":[]},"1256":{"title":"2 模型状态的保存","titles":[]},"1257":{"title":"2.1 定义一个模型","titles":["2 模型状态的保存"]},"1258":{"title":"2.2 保存模型的状态","titles":["2 模型状态的保存"]},"1259":{"title":"2.3 加载模型的状态","titles":["2 模型状态的保存"]},"1260":{"title":"2.4 思考与尝试","titles":["2 模型状态的保存"]},"1261":{"title":"3 保存与加载模型","titles":[]},"1262":{"title":"3.1 保存模型","titles":["3 保存与加载模型"]},"1263":{"title":"3.2 加载模型","titles":["3 保存与加载模型"]},"1264":{"title":"3.3 思考与尝试","titles":["3 保存与加载模型"]},"1265":{"title":"4 训练中的保存和加载","titles":[]},"1266":{"title":"4.1 保存训练中的状态","titles":["4 训练中的保存和加载"]},"1267":{"title":"4.2 加载训练中的状态","titles":["4 训练中的保存和加载"]},"1268":{"title":"5 保存和加载模型的静态图","titles":[]},"1269":{"title":"5.1 保存模型静态图","titles":["5 保存和加载模型的静态图"]},"1270":{"title":"5.2 加载模型静态图","titles":["5 保存和加载模型的静态图"]},"1271":{"title":"6 通用格式onnx的保存","titles":[]},"1272":{"title":"6.1 保存onnx 静态图模型","titles":["6 通用格式onnx的保存"]},"1273":{"title":"6.2 运行onnx 模型","titles":["6 通用格式onnx的保存"]},"1274":{"title":"6.3 shape infer","titles":["6 通用格式onnx的保存"]},"1275":{"title":"1 tensorboard 介绍","titles":[]},"1276":{"title":"2 安装方式","titles":[]},"1277":{"title":"3 抓取log","titles":[]},"1278":{"title":"3.1 import SummaryWriter","titles":["3 抓取log"]},"1279":{"title":"3.2 plot scalar","titles":["3 抓取log"]},"1280":{"title":"3.3 plot loss and accuracy","titles":["3 抓取log"]},"1281":{"title":"4 执行方式：","titles":[]},"1282":{"title":"5 查看graph","titles":[]},"1283":{"title":"6 查看特征图","titles":[]},"1284":{"title":"7 性能分析profiler","titles":[]},"1285":{"title":"1 pytorch 几种模式概览","titles":[]},"1286":{"title":"1.1 pytorch 不仅仅是动态图","titles":["1 pytorch 几种模式概览"]},"1287":{"title":"1.2 理解动态图和静态图","titles":["1 pytorch 几种模式概览"]},"1288":{"title":"1.3 静态图的优势","titles":["1 pytorch 几种模式概览"]},"1289":{"title":"2 几种模式简介","titles":[]},"1290":{"title":"2.1 fx 图","titles":["2 几种模式简介"]},"1291":{"title":"2.2 torch.jit.script","titles":["2 几种模式简介"]},"1292":{"title":"2.3 torch.jit.trace","titles":["2 几种模式简介"]},"1293":{"title":"2.4 torch.compile","titles":["2 几种模式简介"]},"1294":{"title":"3 案例：","titles":[]},"1295":{"title":"3.1 模型准备","titles":["3 案例："]},"1296":{"title":"3.2 jit.script 代码展示","titles":["3 案例："]},"1297":{"title":"3.3 jit.traced 代码展示","titles":["3 案例："]},"1298":{"title":"4 export to onnx","titles":[]},"1299":{"title":"5 compile to graph (dynamo)","titles":[]},"1300":{"title":"1 ImageNet training in PyTorch","titles":[]},"1301":{"title":"2 Requirements","titles":[]},"1302":{"title":"3 数据集下载：","titles":[]},"1303":{"title":"4 Training","titles":[]},"1304":{"title":"Use Dummy Data","titles":["4 Training"]},"1305":{"title":"Multi-processing Distributed Data Parallel Training","titles":["4 Training"]},"1306":{"title":"Single node, multiple GPUs:","titles":["4 Training","Multi-processing Distributed Data Parallel Training"]},"1307":{"title":"Multiple nodes:","titles":["4 Training","Multi-processing Distributed Data Parallel Training"]},"1308":{"title":"Usage","titles":["4 Training"]},"1309":{"title":"1 模型跑通","titles":[]},"1310":{"title":"2 bert 介绍","titles":[]},"1311":{"title":"3 transformer 发展脉络","titles":[]},"1312":{"title":"3.1 transformer 概述","titles":["3 transformer 发展脉络"]},"1313":{"title":"3.2 迁移学习","titles":["3 transformer 发展脉络"]},"1314":{"title":"3.3 transformer 家族","titles":["3 transformer 发展脉络"]},"1315":{"title":"3.4 encoder 分支","titles":["3 transformer 发展脉络"]},"1316":{"title":"3.5 Decoder 分支","titles":["3 transformer 发展脉络"]},"1317":{"title":"3.6 Encoder-Decoder 分支","titles":["3 transformer 发展脉络"]},"1318":{"title":"3.7 大模型的爆发","titles":["3 transformer 发展脉络"]},"1319":{"title":"4 BERT + CRF(Conditional Random Field) 实现命名实体识别(NER) 任务","titles":[]},"1320":{"title":"4.1 任务概述","titles":["4 BERT + CRF(Conditional Random Field) 实现命名实体识别(NER) 任务"]},"1321":{"title":"4.2 CRF 原理详解","titles":["4 BERT + CRF(Conditional Random Field) 实现命名实体识别(NER) 任务"]},"1322":{"title":"4.2.1 线性CRF的定义","titles":["4 BERT + CRF(Conditional Random Field) 实现命名实体识别(NER) 任务","4.2 CRF 原理详解"]},"1323":{"title":"4.2.2 发射分数","titles":["4 BERT + CRF(Conditional Random Field) 实现命名实体识别(NER) 任务","4.2 CRF 原理详解"]},"1324":{"title":"4.2.3 转移分数","titles":["4 BERT + CRF(Conditional Random Field) 实现命名实体识别(NER) 任务","4.2 CRF 原理详解"]},"1325":{"title":"4.2.4 CRF 的损失函数计算","titles":["4 BERT + CRF(Conditional Random Field) 实现命名实体识别(NER) 任务"]},"1326":{"title":"4.2.5 CRF的Viterbi解码","titles":["4 BERT + CRF(Conditional Random Field) 实现命名实体识别(NER) 任务","4.2.4 CRF 的损失函数计算"]},"1327":{"title":"5 代码详解","titles":[]},"1328":{"title":"5.1 真实路径得分计算","titles":["5 代码详解"]},"1329":{"title":"5.2 总路径得分计算","titles":["5 代码详解"]},"1330":{"title":"5.3 Viterbi 解码过程","titles":["5 代码详解"]},"1331":{"title":"5.4 f1 score 的计算","titles":["5 代码详解"]},"1332":{"title":"1 模型跑通","titles":[]},"1333":{"title":"2 t5 介绍","titles":[]},"1334":{"title":"3 position embedding 总结","titles":[]},"1335":{"title":"3.1 绝对位置编码","titles":["3 position embedding 总结"]},"1336":{"title":"3.1.1 三角函数式(Sinusoidal)位置编码","titles":["3 position embedding 总结","3.1 绝对位置编码"]},"1337":{"title":"3.1.2 可学习(Learnable)的位置编码","titles":["3 position embedding 总结","3.1 绝对位置编码"]},"1338":{"title":"3.2 相对位置编码","titles":["3 position embedding 总结"]},"1339":{"title":"3.2.1 经典的相对位置编码","titles":["3 position embedding 总结","3.2 相对位置编码"]},"1340":{"title":"3.2.2 T5 中的相对位置编码","titles":["3 position embedding 总结","3.2 相对位置编码"]},"1341":{"title":"3.3 旋转位置编码","titles":["3 position embedding 总结"]},"1342":{"title":"3.3.1 RoPE 原理","titles":["3 position embedding 总结","3.3 旋转位置编码"]},"1343":{"title":"3.3.2 2 维扩展到多维","titles":["3 position embedding 总结","3.3 旋转位置编码"]},"1344":{"title":"3.3.3 RoPE 的高效计算","titles":["3 position embedding 总结","3.3 旋转位置编码"]},"1345":{"title":"3.3.4 llama 中的RoPE 代码实现","titles":["3 position embedding 总结","3.3 旋转位置编码"]},"1346":{"title":"1 代码介绍","titles":[]},"1347":{"title":"2 代码复现步骤","titles":[]},"1348":{"title":"3 stable diffusion 整体结构","titles":[]},"1349":{"title":"3.1 整体流程图","titles":["3 stable diffusion 整体结构"]},"1350":{"title":"3.2 sd 训练流程图","titles":["3 stable diffusion 整体结构"]},"1351":{"title":"3.3 sd 推理流程图","titles":["3 stable diffusion 整体结构"]},"1352":{"title":"3.4 clip 原理图","titles":["3 stable diffusion 整体结构"]},"1353":{"title":"3.5 latent space","titles":["3 stable diffusion 整体结构"]},"1354":{"title":"3.6 noiser and denoiser","titles":["3 stable diffusion 整体结构"]},"1355":{"title":"4 stable diffusion 具体模型结构","titles":[]},"1356":{"title":"4.1 clip 结构","titles":["4 stable diffusion 具体模型结构"]},"1357":{"title":"4.2 vae 模型结构","titles":["4 stable diffusion 具体模型结构"]},"1358":{"title":"4.2 unet-base 模型结构图","titles":["4 stable diffusion 具体模型结构"]},"1359":{"title":"5 评价指标","titles":[]},"1360":{"title":"5.1 clip score","titles":["5 评价指标"]},"1361":{"title":"5.2 FID","titles":["5 评价指标"]},"1362":{"title":"6 sd 进阶","titles":[]},"1363":{"title":"6.1 sd2 之前版本异同","titles":["6 sd 进阶"]},"1364":{"title":"6.2 从 SD 到 SDXL","titles":["6 sd 进阶"]},"1365":{"title":"6 参考链接","titles":[]},"1366":{"title":"SDXL","titles":[]},"1367":{"title":"参考链接","titles":[]},"1368":{"title":"VAE","titles":[]},"1369":{"title":"1 VAE 的作用 （数据压缩和数据生成）","titles":[]},"1370":{"title":"1.1 数据压缩","titles":["1 VAE 的作用 （数据压缩和数据生成）"]},"1371":{"title":"1.2 数据生成","titles":["1 VAE 的作用 （数据压缩和数据生成）"]},"1372":{"title":"1.3 数据压缩与数据生成的关系","titles":["1 VAE 的作用 （数据压缩和数据生成）"]},"1373":{"title":"1.4 example","titles":["1 VAE 的作用 （数据压缩和数据生成）"]},"1374":{"title":"1.5 可能出现的问题","titles":["1 VAE 的作用 （数据压缩和数据生成）"]},"1375":{"title":"1.6 VAE 要点总结","titles":["1 VAE 的作用 （数据压缩和数据生成）"]},"1376":{"title":"2 理论推导VAE","titles":[]},"1377":{"title":"2.1 引入变分","titles":["2 理论推导VAE"]},"1378":{"title":"4 参考文献","titles":[]},"1379":{"title":"Scaling Laws for Neural Language Models","titles":[]},"1380":{"title":"How to training realy large model","titles":[]},"1381":{"title":"1 llama-v1","titles":[]},"1382":{"title":"2 llama-v2","titles":[]},"1383":{"title":"3 llama-v3","titles":[]},"1384":{"title":"4 llama code implement","titles":[]},"1385":{"title":"神经网络案例展示","titles":[]},"1386":{"title":"1 题目：","titles":[]},"1387":{"title":"2 前向传播过程(feedforward)","titles":[]},"1388":{"title":"2.1 第一层求解","titles":["2 前向传播过程(feedforward)"]},"1389":{"title":"2.2 第二层计算","titles":["2 前向传播过程(feedforward)"]},"1390":{"title":"3 反向传播过程(back propagation)","titles":[]},"1391":{"title":"3.1 末层权重梯度计算","titles":["3 反向传播过程(back propagation)"]},"1392":{"title":"3.1.1 计算流程概述","titles":["3 反向传播过程(back propagation)"]},"1393":{"title":"3.1.2 具体计算过程","titles":["3 反向传播过程(back propagation)"]},"1394":{"title":"3.2 前一层权重梯度计算（以 w1","titles":["3 反向传播过程(back propagation)"]},"1395":{"title":"4 权重更新","titles":[]},"1396":{"title":"5 迭代训练","titles":[]},"1397":{"title":"6 将前馈网络写成矩阵形式","titles":[]},"1398":{"title":"7 代码展示","titles":[]},"1399":{"title":"DeepSeek-V2","titles":[]},"1400":{"title":"DeepSeek-MOE","titles":[]},"1401":{"title":"DeepSeek-V3","titles":[]},"1402":{"title":"DeepSeek-R1","titles":[]},"1403":{"title":"AI时代的算法学习","titles":[]},"1404":{"title":"Deep Learning Theroy","titles":[]},"1405":{"title":"4.1 进程同步","titles":[]},"1406":{"title":"1. 基本概念","titles":["4.1 进程同步"]},"1407":{"title":"1.1为什么要提出？","titles":["4.1 进程同步","1. 基本概念"]},"1408":{"title":"1.2 同步是什么？","titles":["4.1 进程同步","1. 基本概念"]},"1409":{"title":"1.3 什么又是互斥？","titles":["4.1 进程同步","1. 基本概念"]},"1410":{"title":"1.4 临界资源是啥？","titles":["4.1 进程同步","1. 基本概念"]},"1411":{"title":"1.4.1 系统资源","titles":["4.1 进程同步","1. 基本概念","1.4 临界资源是啥？"]},"1412":{"title":"1.4.2 临界资源（共享资源）","titles":["4.1 进程同步","1. 基本概念","1.4 临界资源是啥？"]},"1413":{"title":"1.4.3 临界区","titles":["4.1 进程同步","1. 基本概念","1.4 临界资源是啥？"]},"1414":{"title":"2. 同步如何实现","titles":["4.1 进程同步"]},"1415":{"title":"2.1 访问原则","titles":["4.1 进程同步","2. 同步如何实现"]},"1416":{"title":"2.2 软件实现（后续补充）","titles":["4.1 进程同步","2. 同步如何实现"]},"1417":{"title":"2.2.1 单标志法","titles":["4.1 进程同步","2. 同步如何实现","2.2 软件实现（后续补充）"]},"1418":{"title":"2.2.2 双标志先检查法","titles":["4.1 进程同步","2. 同步如何实现","2.2 软件实现（后续补充）"]},"1419":{"title":"2.2.3 双标志后检查法","titles":["4.1 进程同步","2. 同步如何实现","2.2 软件实现（后续补充）"]},"1420":{"title":"2.3 硬件实现（后续补充）","titles":["4.1 进程同步","2. 同步如何实现"]},"1421":{"title":"2.3.1 中断屏蔽方法","titles":["4.1 进程同步","2. 同步如何实现","2.3 硬件实现（后续补充）"]},"1422":{"title":"2.3.2 Test-And-Set（TS指令/TSL指令）","titles":["4.1 进程同步","2. 同步如何实现","2.3 硬件实现（后续补充）"]},"1423":{"title":"2.3.3 Swap指令（EXCHANGE，XCHG指令）","titles":["4.1 进程同步","2. 同步如何实现","2.3 硬件实现（后续补充）"]},"1424":{"title":"2.3.4 信号量机制（重点，下一节详细讲解）","titles":["4.1 进程同步","2. 同步如何实现","2.3 硬件实现（后续补充）"]},"1425":{"title":"3. 参考资料","titles":["4.1 进程同步"]},"1426":{"title":"4.4 信号量机制pv操作之“可见”","titles":[]},"1427":{"title":"408知识","titles":[]},"1428":{"title":"[DS]数据结构(Data Structures)","titles":["408知识"]},"1429":{"title":"[CN]计算机网络(Computer Networks)","titles":["408知识"]},"1430":{"title":"[CO]计算机组成原理(Computer Organization)","titles":["408知识"]},"1431":{"title":"[OS]操作系统(Operating Systems)","titles":["408知识"]},"1432":{"title":"[OR]碎片知识(Other)","titles":["408知识"]},"1433":{"title":"JavaSE 简介","titles":[]},"1434":{"title":"JavaSE 核心概念","titles":["JavaSE 简介"]},"1435":{"title":"1. JDK、JRE、JVM","titles":["JavaSE 简介","JavaSE 核心概念"]},"1436":{"title":"2. Java 基础语法","titles":["JavaSE 简介","JavaSE 核心概念"]},"1437":{"title":"1 概念理解","titles":[]},"1438":{"title":"1.1 神经网络训练流程概述","titles":["1 概念理解"]},"1439":{"title":"1.2 反向传播的定义","titles":["1 概念理解"]},"1440":{"title":"2 梯度下降算法简述","titles":[]},"1441":{"title":"3 BP 或 深度神经网络训练需要明确的几个概念","titles":[]},"1442":{"title":"4 链式求导法则","titles":[]},"1443":{"title":"5 BP 流程图示","titles":[]},"1444":{"title":"6 反向传播数学推导","titles":[]},"1445":{"title":"6.1 反向传播目的确认","titles":["6 反向传播数学推导"]},"1446":{"title":"6.2 线性连接层 weight 的梯度","titles":["6 反向传播数学推导"]},"1447":{"title":"6.3 激活函数 input 的梯度","titles":["6 反向传播数学推导"]},"1448":{"title":"6.4 激活函数 output 的梯度","titles":["6 反向传播数学推导"]},"1449":{"title":"6.5 下层激活 input(z\' and z\'\') 梯度求解","titles":["6 反向传播数学推导"]},"1450":{"title":"7 反向传播总结","titles":[]},"1451":{"title":"前馈神经网络(feedforward neural network)","titles":[]},"1452":{"title":"1 相关概念","titles":[]},"1453":{"title":"1.1 人工智能是什么","titles":["1 相关概念"]},"1454":{"title":"1.2 深度学习与人工智能的关系","titles":["1 相关概念"]},"1455":{"title":"1.3 深度学习的概念","titles":["1 相关概念"]},"1456":{"title":"1.4 什么是人工神经网络","titles":["1 相关概念"]},"1457":{"title":"1.5 前馈神经网络的概念","titles":["1 相关概念"]},"1458":{"title":"2 神经元模型","titles":[]},"1459":{"title":"2.1 M-P 神经元","titles":["2 神经元模型"]},"1460":{"title":"2.2 经典激活函数","titles":["2 神经元模型"]},"1461":{"title":"3 从神经元到感知机","titles":[]},"1462":{"title":"3.1 使用感知机解决线性可分问题","titles":["3 从神经元到感知机"]},"1463":{"title":"3.2 如何解决异或问题 ？？？","titles":["3 从神经元到感知机"]},"1464":{"title":"4 从感知机到深度神经网络","titles":[]},"1465":{"title":"4.1 为何要用深度神经网络？","titles":["4 从感知机到深度神经网络"]},"1466":{"title":"4.2 深度神经网络解决问题案例","titles":["4 从感知机到深度神经网络"]},"1467":{"title":"5 前馈神经网络计算流程","titles":[]},"1468":{"title":"6 深度学习与传统机器学习","titles":[]},"1469":{"title":"6.1 相同点","titles":["6 深度学习与传统机器学习"]},"1470":{"title":"6.2 不同点","titles":["6 深度学习与传统机器学习"]},"1471":{"title":"7 深度学习的特点","titles":[]},"1472":{"title":"8 深度学习的典型算法","titles":[]},"1473":{"title":"9 参考文献","titles":[]},"1474":{"title":"MyBatis框架","titles":[]},"1475":{"title":"01、Mybatis简介","titles":["MyBatis框架"]},"1476":{"title":"1.1、什么是MyBatis","titles":["MyBatis框架","01、Mybatis简介"]},"1477":{"title":"1.2、持久化","titles":["MyBatis框架","01、Mybatis简介"]},"1478":{"title":"1.3、持久层","titles":["MyBatis框架","01、Mybatis简介"]},"1479":{"title":"1.4、为什么需要Mybatis","titles":["MyBatis框架","01、Mybatis简介"]},"1480":{"title":"02、MyBatis第一个程序","titles":["MyBatis框架"]},"1481":{"title":"2.1、代码演示","titles":["MyBatis框架","02、MyBatis第一个程序"]},"1482":{"title":"2.2、问题说明","titles":["MyBatis框架","02、MyBatis第一个程序"]},"1483":{"title":"03、CRUD操作","titles":["MyBatis框架"]},"1484":{"title":"3.1、namespace","titles":["MyBatis框架","03、CRUD操作"]},"1485":{"title":"3.2、select","titles":["MyBatis框架","03、CRUD操作"]},"1486":{"title":"3.3、insert","titles":["MyBatis框架","03、CRUD操作"]},"1487":{"title":"3.4、update","titles":["MyBatis框架","03、CRUD操作"]},"1488":{"title":"3.5、delete","titles":["MyBatis框架","03、CRUD操作"]},"1489":{"title":"3.6、思考题","titles":["MyBatis框架","03、CRUD操作"]},"1490":{"title":"Java 后端学习框架","titles":[]},"1491":{"title":"1. Java 基础","titles":["Java 后端学习框架"]},"1492":{"title":"2. 数据库基础","titles":["Java 后端学习框架"]},"1493":{"title":"3. Java Web 开发","titles":["Java 后端学习框架"]},"1494":{"title":"4. Mybatis","titles":["Java 后端学习框架"]},"1495":{"title":"5. Spring","titles":["Java 后端学习框架"]},"1496":{"title":"6. Spring MVC","titles":["Java 后端学习框架"]},"1497":{"title":"7. SprintBoot","titles":["Java 后端学习框架"]},"1498":{"title":"8. 消息中间件","titles":["Java 后端学习框架"]},"1499":{"title":"9. 部署与监控","titles":["Java 后端学习框架"]},"1500":{"title":"10. 进阶主题","titles":["Java 后端学习框架"]},"1501":{"title":"参考资料","titles":["Java 后端学习框架"]},"1502":{"title":"Linux基础部分","titles":[]},"1503":{"title":"一、基本命令使用","titles":["Linux基础部分"]},"1504":{"title":"1. Linux 文件系统结构","titles":["Linux基础部分","一、基本命令使用"]},"1505":{"title":"简介","titles":["Linux基础部分","一、基本命令使用","1. Linux 文件系统结构"]},"1506":{"title":"常用目录","titles":["Linux基础部分","一、基本命令使用","1. Linux 文件系统结构"]},"1507":{"title":"示例","titles":["Linux基础部分","一、基本命令使用","1. Linux 文件系统结构"]},"1508":{"title":"2. 基本命令操作","titles":["Linux基础部分","一、基本命令使用"]},"1509":{"title":"文件和目录管理","titles":["Linux基础部分","一、基本命令使用","2. 基本命令操作"]},"1510":{"title":"文件操作","titles":["Linux基础部分","一、基本命令使用","2. 基本命令操作"]},"1511":{"title":"3. 文件权限管理","titles":["Linux基础部分","一、基本命令使用"]},"1512":{"title":"权限表示","titles":["Linux基础部分","一、基本命令使用","3. 文件权限管理"]},"1513":{"title":"查看和修改权限","titles":["Linux基础部分","一、基本命令使用","3. 文件权限管理"]},"1514":{"title":"4. 文本查看","titles":["Linux基础部分","一、基本命令使用"]},"1515":{"title":"查看文本文件","titles":["Linux基础部分","一、基本命令使用","4. 文本查看"]},"1516":{"title":"查找内容","titles":["Linux基础部分","一、基本命令使用","4. 文本查看"]},"1517":{"title":"5. Vim 编辑器基础","titles":["Linux基础部分","一、基本命令使用"]},"1518":{"title":"5.1 进入退出","titles":["Linux基础部分","一、基本命令使用","5. Vim 编辑器基础"]},"1519":{"title":"5.2 模式","titles":["Linux基础部分","一、基本命令使用","5. Vim 编辑器基础"]},"1520":{"title":"5.3 基本操作","titles":["Linux基础部分","一、基本命令使用","5. Vim 编辑器基础"]},"1521":{"title":"6. 进程管理","titles":["Linux基础部分","一、基本命令使用"]},"1522":{"title":"查看进程","titles":["Linux基础部分","一、基本命令使用","6. 进程管理"]},"1523":{"title":"管理进程","titles":["Linux基础部分","一、基本命令使用","6. 进程管理"]},"1524":{"title":"示例","titles":["Linux基础部分","一、基本命令使用","6. 进程管理"]},"1525":{"title":"7. 网络管理","titles":["Linux基础部分","一、基本命令使用"]},"1526":{"title":"查看网络配置","titles":["Linux基础部分","一、基本命令使用","7. 网络管理"]},"1527":{"title":"查看网络端口","titles":["Linux基础部分","一、基本命令使用","7. 网络管理"]},"1528":{"title":"抓取网页内容","titles":["Linux基础部分","一、基本命令使用","7. 网络管理"]},"1529":{"title":"8. 用户和组管理","titles":["Linux基础部分","一、基本命令使用"]},"1530":{"title":"用户管理","titles":["Linux基础部分","一、基本命令使用","8. 用户和组管理"]},"1531":{"title":"9. 文件查找","titles":["Linux基础部分","一、基本命令使用"]},"1532":{"title":"查找文件","titles":["Linux基础部分","一、基本命令使用","9. 文件查找"]},"1533":{"title":"查找可执行文件","titles":["Linux基础部分","一、基本命令使用","9. 文件查找"]},"1534":{"title":"10. 归档与压缩","titles":["Linux基础部分","一、基本命令使用"]},"1535":{"title":"打包和解压","titles":["Linux基础部分","一、基本命令使用","10. 归档与压缩"]},"1536":{"title":"11. 系统更新与软件管理","titles":["Linux基础部分","一、基本命令使用"]},"1537":{"title":"更新系统和安装软件包","titles":["Linux基础部分","一、基本命令使用","11. 系统更新与软件管理"]},"1538":{"title":"12. 日志管理","titles":["Linux基础部分","一、基本命令使用"]},"1539":{"title":"查看日志","titles":["Linux基础部分","一、基本命令使用","12. 日志管理"]},"1540":{"title":"二、VIM操作命令","titles":["Linux基础部分"]},"1541":{"title":"1. Vim 模式简介","titles":["Linux基础部分","二、VIM操作命令"]},"1542":{"title":"2. 启动和退出 Vim","titles":["Linux基础部分","二、VIM操作命令"]},"1543":{"title":"启动","titles":["Linux基础部分","二、VIM操作命令","2. 启动和退出 Vim"]},"1544":{"title":"退出","titles":["Linux基础部分","二、VIM操作命令","2. 启动和退出 Vim"]},"1545":{"title":"3. 插入模式操作","titles":["Linux基础部分","二、VIM操作命令"]},"1546":{"title":"4. 普通模式基础操作","titles":["Linux基础部分","二、VIM操作命令"]},"1547":{"title":"光标移动","titles":["Linux基础部分","二、VIM操作命令","4. 普通模式基础操作"]},"1548":{"title":"删除操作","titles":["Linux基础部分","二、VIM操作命令","4. 普通模式基础操作"]},"1549":{"title":"复制和粘贴","titles":["Linux基础部分","二、VIM操作命令","4. 普通模式基础操作"]},"1550":{"title":"撤销与重做","titles":["Linux基础部分","二、VIM操作命令","4. 普通模式基础操作"]},"1551":{"title":"5. 可视模式（选择操作）","titles":["Linux基础部分","二、VIM操作命令"]},"1552":{"title":"6. 查找与替换","titles":["Linux基础部分","二、VIM操作命令"]},"1553":{"title":"查找","titles":["Linux基础部分","二、VIM操作命令","6. 查找与替换"]},"1554":{"title":"替换","titles":["Linux基础部分","二、VIM操作命令","6. 查找与替换"]},"1555":{"title":"7. 多文件和多窗口操作","titles":["Linux基础部分","二、VIM操作命令"]},"1556":{"title":"打开多个文件","titles":["Linux基础部分","二、VIM操作命令","7. 多文件和多窗口操作"]},"1557":{"title":"分屏操作","titles":["Linux基础部分","二、VIM操作命令","7. 多文件和多窗口操作"]},"1558":{"title":"8. 文本缩进和格式调整","titles":["Linux基础部分","二、VIM操作命令"]},"1559":{"title":"自动缩进","titles":["Linux基础部分","二、VIM操作命令","8. 文本缩进和格式调整"]},"1560":{"title":"格式化代码","titles":["Linux基础部分","二、VIM操作命令","8. 文本缩进和格式调整"]},"1561":{"title":"MPI并行计算","titles":[]},"1562":{"title":"一、并行的引入","titles":["MPI并行计算"]},"1563":{"title":"1.1 基本概念","titles":["MPI并行计算","一、并行的引入"]},"1564":{"title":"1.2 什么是并行？","titles":["MPI并行计算","一、并行的引入"]},"1565":{"title":"1.3 并行有啥用？","titles":["MPI并行计算","一、并行的引入"]},"1566":{"title":"1.4 并行的实际案例","titles":["MPI并行计算","一、并行的引入"]},"1567":{"title":"二、并行的类型","titles":["MPI并行计算"]},"1568":{"title":"2.1 按照处理机划分","titles":["MPI并行计算","二、并行的类型"]},"1569":{"title":"2.2 按照实现方式划分","titles":["MPI并行计算","二、并行的类型"]},"1570":{"title":"三、MPI的基本原理","titles":["MPI并行计算"]},"1571":{"title":"3.1 基本原理","titles":["MPI并行计算","三、MPI的基本原理"]},"1572":{"title":"1. MPI的架构","titles":["MPI并行计算","三、MPI的基本原理","3.1 基本原理"]},"1573":{"title":"2. 进程和通信","titles":["MPI并行计算","三、MPI的基本原理","3.1 基本原理"]},"1574":{"title":"3. MPI的通信模式","titles":["MPI并行计算","三、MPI的基本原理","3.1 基本原理"]},"1575":{"title":"4. MPI的基本函数","titles":["MPI并行计算","三、MPI的基本原理","3.1 基本原理"]},"1576":{"title":"5. MPI的数据类型和消息标签","titles":["MPI并行计算","三、MPI的基本原理","3.1 基本原理"]},"1577":{"title":"6. 通信域（Communicator）","titles":["MPI并行计算","三、MPI的基本原理","3.1 基本原理"]},"1578":{"title":"7. MPI中的常见通信模式","titles":["MPI并行计算","三、MPI的基本原理","3.1 基本原理"]},"1579":{"title":"8. MPI的优势和劣势","titles":["MPI并行计算","三、MPI的基本原理","3.1 基本原理"]},"1580":{"title":"3.2 模型演示","titles":["MPI并行计算","三、MPI的基本原理"]},"1581":{"title":"四、基本环境配置（简略）","titles":["MPI并行计算"]},"1582":{"title":"4.1 Linux环境","titles":["MPI并行计算","四、基本环境配置（简略）"]},"1583":{"title":"4.2 ssh工具","titles":["MPI并行计算","四、基本环境配置（简略）"]},"1584":{"title":"4.3 VIM编辑器","titles":["MPI并行计算","四、基本环境配置（简略）"]},"1585":{"title":"4.4 MPI环境","titles":["MPI并行计算","四、基本环境配置（简略）"]},"1586":{"title":"4.4.1 安装","titles":["MPI并行计算","四、基本环境配置（简略）","4.4 MPI环境"]},"1587":{"title":"4.4.2 编译","titles":["MPI并行计算","四、基本环境配置（简略）","4.4 MPI环境"]},"1588":{"title":"4.4.3 配置","titles":["MPI并行计算","四、基本环境配置（简略）","4.4 MPI环境"]},"1589":{"title":"4.4.4 了解","titles":["MPI并行计算","四、基本环境配置（简略）","4.4 MPI环境"]},"1590":{"title":"五、MPI的基本使用","titles":["MPI并行计算"]},"1591":{"title":"5.1 快速使用","titles":["MPI并行计算","五、MPI的基本使用"]},"1592":{"title":"5.2 源码了解","titles":["MPI并行计算","五、MPI的基本使用"]},"1593":{"title":"5.3 进阶","titles":["MPI并行计算","五、MPI的基本使用"]},"1594":{"title":"5.3.1 分布式实现","titles":["MPI并行计算","五、MPI的基本使用","5.3 进阶"]},"1595":{"title":"六、学习策略与建议","titles":["MPI并行计算"]},"1596":{"title":"6.1 新东西学习","titles":["MPI并行计算","六、学习策略与建议"]},"1597":{"title":"6.2 遇事不决","titles":["MPI并行计算","六、学习策略与建议"]},"1598":{"title":"6.3 知识体系构建","titles":["MPI并行计算","六、学习策略与建议"]},"1599":{"title":"Linux模块","titles":[]},"1600":{"title":"C++编程语言 01：从 Python 到 C++ 的启航","titles":[]},"1601":{"title":"引言：欢迎来到 C++ 的世界！","titles":["C++编程语言 01：从 Python 到 C++ 的启航"]},"1602":{"title":"一、 C++ 与 Python 的对比：理解差异，扬长避短","titles":["C++编程语言 01：从 Python 到 C++ 的启航"]},"1603":{"title":"二、 C++ 的发展与平台：历史的脉络与选择","titles":["C++编程语言 01：从 Python 到 C++ 的启航"]},"1604":{"title":"三、 C++ 编译原理：从代码到执行","titles":["C++编程语言 01：从 Python 到 C++ 的启航"]},"1605":{"title":"四、 C++ 开发环境搭建：工欲善其事，必先利其器","titles":["C++编程语言 01：从 Python 到 C++ 的启航"]},"1606":{"title":"五、 第一个 C++ 程序：迈出 C++ 编程的第一步","titles":["C++编程语言 01：从 Python 到 C++ 的启航"]},"1607":{"title":"六、C++ 数据类型","titles":["C++编程语言 01：从 Python 到 C++ 的启航"]},"1608":{"title":"作业:","titles":["C++编程语言 01：从 Python 到 C++ 的启航"]},"1609":{"title":"C++ 教学课件 - 第三次课","titles":[]},"1610":{"title":"C++ 第三节课：深入理解内存与地址——指针与引用","titles":["C++ 教学课件 - 第三次课"]},"1611":{"title":"一、指针 (Pointer)","titles":["C++ 教学课件 - 第三次课"]},"1612":{"title":"二、引用 (Reference)","titles":["C++ 教学课件 - 第三次课"]},"1613":{"title":"三、变量与常量 (Variables and Constants)","titles":["C++ 教学课件 - 第三次课"]},"1614":{"title":"四、常量指针和指针常量 (Const Pointers and Pointer Constants)","titles":["C++ 教学课件 - 第三次课"]},"1615":{"title":"五、auto 自动类型推断 (Automatic Type Deduction)","titles":["C++ 教学课件 - 第三次课"]},"1616":{"title":"六、常用转义字符 (Escape Sequences)","titles":["C++ 教学课件 - 第三次课"]},"1617":{"title":"C++编程语言  02","titles":[]},"1618":{"title":"第一部分: 流程控制语句","titles":["C++编程语言  02"]},"1619":{"title":"1.  if 语句：让程序做出判断","titles":["C++编程语言  02","第一部分: 流程控制语句"]},"1620":{"title":"2. while 语句：重复执行直到条件不满足","titles":["C++编程语言  02","第一部分: 流程控制语句"]},"1621":{"title":"3. for 循环：结构化的循环","titles":["C++编程语言  02","第一部分: 流程控制语句"]},"1622":{"title":"第二部分: 复合数据类型","titles":["C++编程语言  02"]},"1623":{"title":"1. 数组：存储相同类型数据的集合","titles":["C++编程语言  02","第二部分: 复合数据类型"]},"1624":{"title":"2. 字符串：处理文本数据","titles":["C++编程语言  02","第二部分: 复合数据类型"]},"1625":{"title":"课后作业 (课后完成)","titles":["C++编程语言  02"]},"1626":{"title":"C++ 第五课","titles":[]},"1627":{"title":"课程目标","titles":["C++ 第五课"]},"1628":{"title":"0. 头文件的原理与使用","titles":["C++ 第五课"]},"1629":{"title":"1. 赋值总结","titles":["C++ 第五课"]},"1630":{"title":"2. 运算符总结","titles":["C++ 第五课"]},"1631":{"title":"3. 控制流语句总结","titles":["C++ 第五课"]},"1632":{"title":"4. 宏定义","titles":["C++ 第五课"]},"1633":{"title":"5. 指针的算术运算","titles":["C++ 第五课"]},"1634":{"title":"6. 数组 (回顾与补充)","titles":["C++ 第五课"]},"1635":{"title":"运算符优先级表","titles":[]},"1636":{"title":"C++ 第十课：深入类和对象","titles":[]},"1637":{"title":"一、回顾与引入 1","titles":["C++ 第十课：深入类和对象"]},"1638":{"title":"二、隐藏的 this 指针 4","titles":["C++ 第十课：深入类和对象"]},"1639":{"title":"三、静态成员 6","titles":["C++ 第十课：深入类和对象"]},"1640":{"title":"四、常量成员 6","titles":["C++ 第十课：深入类和对象"]},"1641":{"title":"五、构造函数参数初始化列表 4","titles":["C++ 第十课：深入类和对象"]},"1642":{"title":"作业","titles":[]},"1643":{"title":"C++ 第七节课：函数进阶与内存管理","titles":[]},"1644":{"title":"引言","titles":["C++ 第七节课：函数进阶与内存管理"]},"1645":{"title":"一、回调函数 (Callback Functions)","titles":["C++ 第七节课：函数进阶与内存管理"]},"1646":{"title":"二、函数的递归调用 (Recursive Function Calls)","titles":["C++ 第七节课：函数进阶与内存管理"]},"1647":{"title":"三、new 和 delete, new [], delete []","titles":["C++ 第七节课：函数进阶与内存管理"]},"1648":{"title":"四、内存中的栈区和堆区 (Stack and Heap)","titles":["C++ 第七节课：函数进阶与内存管理"]},"1649":{"title":"五、全局变量、局部变量、static 静态变量","titles":["C++ 第七节课：函数进阶与内存管理"]},"1650":{"title":"六、函数的指针传参和引用传参","titles":["C++ 第七节课：函数进阶与内存管理"]},"1651":{"title":"作业","titles":[]},"1652":{"title":"C++ 第10课：类的大小、继承与权限控制","titles":[]},"1653":{"title":"1. sizeof(自定义类)","titles":[]},"1654":{"title":"2. 类的继承","titles":[]},"1655":{"title":"3. protected 访问权限","titles":[]},"1656":{"title":"4. final 关键字","titles":[]},"1657":{"title":"课后作业：设计一个简单的员工管理系统","titles":[]},"1658":{"title":"C++ 第11次课：继承进阶","titles":[]},"1659":{"title":"1. 多重继承","titles":["C++ 第11次课：继承进阶"]},"1660":{"title":"2. 名词歧义","titles":["C++ 第11次课：继承进阶"]},"1661":{"title":"3. 菱形继承","titles":["C++ 第11次课：继承进阶"]},"1662":{"title":"4. 虚继承","titles":["C++ 第11次课：继承进阶"]},"1663":{"title":"5. 继承中函数的重载和覆盖","titles":["C++ 第11次课：继承进阶"]},"1664":{"title":"课后作业","titles":[]},"1665":{"title":"参考代码","titles":[]},"1666":{"title":"C++ 课程第九课：指针、内存管理和类的基础","titles":[]},"1667":{"title":"1. 数组传参的本质：指针","titles":["C++ 课程第九课：指针、内存管理和类的基础"]},"1668":{"title":"2. 动态内存分配：new 和 new[]","titles":["C++ 课程第九课：指针、内存管理和类的基础"]},"1669":{"title":"3. 动态内存释放：delete 和 delete[]","titles":["C++ 课程第九课：指针、内存管理和类的基础"]},"1670":{"title":"4. 内存越界及其危害","titles":["C++ 课程第九课：指针、内存管理和类的基础"]},"1671":{"title":"5. 内存泄漏及其危害","titles":["C++ 课程第九课：指针、内存管理和类的基础"]},"1672":{"title":"6. 指针的安全使用原则","titles":["C++ 课程第九课：指针、内存管理和类的基础"]},"1673":{"title":"7. C++ 输入输出流：std::cin 和 std::cout","titles":["C++ 课程第九课：指针、内存管理和类的基础"]},"1674":{"title":"8. 初探类：封装数据和行为","titles":["C++ 课程第九课：指针、内存管理和类的基础"]},"1675":{"title":"9. 构造函数：对象的初始化","titles":["C++ 课程第九课：指针、内存管理和类的基础"]},"1676":{"title":"10. 析构函数：对象的清理","titles":["C++ 课程第九课：指针、内存管理和类的基础"]},"1677":{"title":"11. 公有成员和私有成员：访问控制","titles":["C++ 课程第九课：指针、内存管理和类的基础"]},"1678":{"title":"sizeof(std::cout) 的深入理解","titles":["C++ 课程第九课：指针、内存管理和类的基础"]},"1679":{"title":"作业","titles":[]},"1680":{"title":"1. 封装一个 Student 类","titles":["作业"]},"1681":{"title":"C++课程 第12讲：类型转换、多态与虚函数","titles":[]},"1682":{"title":"一、类型转换 (Type Conversion)","titles":["C++课程 第12讲：类型转换、多态与虚函数"]},"1683":{"title":"1. 显示转换 (Explicit Conversion)","titles":["C++课程 第12讲：类型转换、多态与虚函数","一、类型转换 (Type Conversion)"]},"1684":{"title":"2. 隐式转换 (Implicit Conversion)","titles":["C++课程 第12讲：类型转换、多态与虚函数","一、类型转换 (Type Conversion)"]},"1685":{"title":"二、自定义类型转换","titles":["C++课程 第12讲：类型转换、多态与虚函数"]},"1686":{"title":"三、多态 (Polymorphism)","titles":["C++课程 第12讲：类型转换、多态与虚函数"]},"1687":{"title":"1. 静态多态 (Static Polymorphism)","titles":["C++课程 第12讲：类型转换、多态与虚函数","三、多态 (Polymorphism)"]},"1688":{"title":"2. 动态多态 (Dynamic Polymorphism)","titles":["C++课程 第12讲：类型转换、多态与虚函数","三、多态 (Polymorphism)"]},"1689":{"title":"作业描述：","titles":[]},"1690":{"title":"任务要求：","titles":["作业描述：","三、多态 (Polymorphism)"]},"1691":{"title":"代码示例：","titles":["作业描述：","三、多态 (Polymorphism)"]},"1692":{"title":"C++ 第 13 节课：纯虚函数、抽象类、深浅拷贝及智能指针","titles":[]},"1693":{"title":"一、纯虚函数与抽象类 9","titles":["C++ 第 13 节课：纯虚函数、抽象类、深浅拷贝及智能指针"]},"1694":{"title":"二、浅拷贝和深拷贝 9","titles":["C++ 第 13 节课：纯虚函数、抽象类、深浅拷贝及智能指针"]},"1695":{"title":"三、智能指针和裸指针 8","titles":["C++ 第 13 节课：纯虚函数、抽象类、深浅拷贝及智能指针"]},"1696":{"title":"作业:","titles":[]},"1697":{"title":"C++ 模板 (第 16 节课)","titles":[]},"1698":{"title":"1. 模板 - 实现泛型编程","titles":["C++ 模板 (第 16 节课)"]},"1699":{"title":"2. 函数模板","titles":["C++ 模板 (第 16 节课)"]},"1700":{"title":"3. 类模板","titles":["C++ 模板 (第 16 节课)"]},"1701":{"title":"4. 成员函数模板","titles":["C++ 模板 (第 16 节课)"]},"1702":{"title":"作业：","titles":[]},"1703":{"title":"C++课程 6：深入理解字符串、数组、指针与函数","titles":[]},"1704":{"title":"一、深入理解字符串常量","titles":["C++课程 6：深入理解字符串、数组、指针与函数"]},"1705":{"title":"二、深入理解二维数组与行指针","titles":["C++课程 6：深入理解字符串、数组、指针与函数"]},"1706":{"title":"三、函数与指针的深度应用","titles":["C++课程 6：深入理解字符串、数组、指针与函数"]},"1707":{"title":"四、函数的重载：提高代码的灵活性","titles":["C++课程 6：深入理解字符串、数组、指针与函数"]},"1708":{"title":"五、函数的默认参数值：简化函数调用","titles":["C++课程 6：深入理解字符串、数组、指针与函数"]},"1709":{"title":"六、内联函数：提升程序性能","titles":["C++课程 6：深入理解字符串、数组、指针与函数"]},"1710":{"title":"作业:","titles":["C++课程 6：深入理解字符串、数组、指针与函数"]},"1711":{"title":"C++ 第 14 课：运算符重载与 String 类详解","titles":[]},"1712":{"title":"1. 运算符重载","titles":["C++ 第 14 课：运算符重载与 String 类详解"]},"1713":{"title":"2. String 类详解","titles":["C++ 第 14 课：运算符重载与 String 类详解"]},"1714":{"title":"3. 静态数组和动态数组","titles":["C++ 第 14 课：运算符重载与 String 类详解"]},"1715":{"title":"4. 补充：C 风格字符串","titles":["C++ 第 14 课：运算符重载与 String 类详解"]},"1716":{"title":"作业：","titles":[]},"1717":{"title":"C++ 第 17 次课：迭代器与容器的应用","titles":[]},"1718":{"title":"1. 迭代器简介","titles":["C++ 第 17 次课：迭代器与容器的应用"]},"1719":{"title":"2. vector","titles":["C++ 第 17 次课：迭代器与容器的应用"]},"1720":{"title":"3. list","titles":["C++ 第 17 次课：迭代器与容器的应用"]},"1721":{"title":"4. forward_list","titles":["C++ 第 17 次课：迭代器与容器的应用"]},"1722":{"title":"5. deque","titles":["C++ 第 17 次课：迭代器与容器的应用"]},"1723":{"title":"6. queue","titles":["C++ 第 17 次课：迭代器与容器的应用"]},"1724":{"title":"7. set","titles":["C++ 第 17 次课：迭代器与容器的应用"]},"1725":{"title":"8. map","titles":["C++ 第 17 次课：迭代器与容器的应用"]},"1726":{"title":"作业","titles":[]},"1727":{"title":"C++ 第四课：自定义数据类型与函数","titles":[]},"1728":{"title":"第一部分： 自定义数据类型的魅力","titles":["C++ 第四课：自定义数据类型与函数"]},"1729":{"title":"第二部分：函数的魔力：代码的组织者和复用者","titles":["C++ 第四课：自定义数据类型与函数"]},"1730":{"title":"C++ 18：C++ 标准库常用算法","titles":[]},"1731":{"title":"课程目标","titles":["C++ 18：C++ 标准库常用算法"]},"1732":{"title":"课程内容","titles":["C++ 18：C++ 标准库常用算法"]},"1733":{"title":"1. 引言与简介 1","titles":["C++ 18：C++ 标准库常用算法"]},"1734":{"title":"什么是 algorithm 库？","titles":["C++ 18：C++ 标准库常用算法","1. 引言与简介 1"]},"1735":{"title":"2. 常用的非修改算法 4","titles":["C++ 18：C++ 标准库常用算法"]},"1736":{"title":"2.1 std::all_of","titles":["C++ 18：C++ 标准库常用算法","2. 常用的非修改算法 4"]},"1737":{"title":"2.2 std::any_of","titles":["C++ 18：C++ 标准库常用算法","2. 常用的非修改算法 4"]},"1738":{"title":"2.3 std::none_of","titles":["C++ 18：C++ 标准库常用算法","2. 常用的非修改算法 4"]},"1739":{"title":"2.4 std::for_each","titles":["C++ 18：C++ 标准库常用算法","2. 常用的非修改算法 4"]},"1740":{"title":"3. 常用的修改算法 4","titles":["C++ 18：C++ 标准库常用算法"]},"1741":{"title":"3.1 std::transform","titles":["C++ 18：C++ 标准库常用算法","3. 常用的修改算法 4"]},"1742":{"title":"3.2 std::copy","titles":["C++ 18：C++ 标准库常用算法","3. 常用的修改算法 4"]},"1743":{"title":"3.3 std::replace","titles":["C++ 18：C++ 标准库常用算法","3. 常用的修改算法 4"]},"1744":{"title":"3.4 std::fill","titles":["C++ 18：C++ 标准库常用算法","3. 常用的修改算法 4"]},"1745":{"title":"4. 数值算法 3","titles":["C++ 18：C++ 标准库常用算法"]},"1746":{"title":"4.1 std::accumulate","titles":["C++ 18：C++ 标准库常用算法","4. 数值算法 3"]},"1747":{"title":"4.2 std::iota","titles":["C++ 18：C++ 标准库常用算法","4. 数值算法 3"]},"1748":{"title":"5. 排序算法 6","titles":["C++ 18：C++ 标准库常用算法"]},"1749":{"title":"5.1 std::sort","titles":["C++ 18：C++ 标准库常用算法","5. 排序算法 6"]},"1750":{"title":"5.2 std::stable_sort","titles":["C++ 18：C++ 标准库常用算法","5. 排序算法 6"]},"1751":{"title":"5.3 std::partial_sort","titles":["C++ 18：C++ 标准库常用算法","5. 排序算法 6"]},"1752":{"title":"5.4 std::nth_element","titles":["C++ 18：C++ 标准库常用算法","5. 排序算法 6"]},"1753":{"title":"6. 查找算法 3","titles":["C++ 18：C++ 标准库常用算法"]},"1754":{"title":"6.1 std::find","titles":["C++ 18：C++ 标准库常用算法","6. 查找算法 3"]},"1755":{"title":"6.2 std::binary_search","titles":["C++ 18：C++ 标准库常用算法","6. 查找算法 3"]},"1756":{"title":"6.3 std::equal_range","titles":["C++ 18：C++ 标准库常用算法","6. 查找算法 3"]},"1757":{"title":"7. 总结与习题 3","titles":["C++ 18：C++ 标准库常用算法"]},"1758":{"title":"总结","titles":["C++ 18：C++ 标准库常用算法","7. 总结与习题 3"]},"1759":{"title":"作业：","titles":["C++ 18：C++ 标准库常用算法"]},"1760":{"title":"C++ 异常处理 - 第19次课","titles":[]},"1761":{"title":"第一部分: 引言 - 为什么需要异常处理？","titles":["C++ 异常处理 - 第19次课"]},"1762":{"title":"第二部分: C++ 异常处理机制详解","titles":["C++ 异常处理 - 第19次课"]},"1763":{"title":"第三部分: 自定义异常类型","titles":["C++ 异常处理 - 第19次课"]},"1764":{"title":"第四部分: 异常处理的最佳实践","titles":["C++ 异常处理 - 第19次课"]},"1765":{"title":"第五部分: 总结","titles":["C++ 异常处理 - 第19次课"]},"1766":{"title":"课后作业:","titles":["C++ 异常处理 - 第19次课"]},"1767":{"title":"第 20 节 课件: 友元及友元相关内容","titles":[]},"1768":{"title":"目录","titles":["第 20 节 课件: 友元及友元相关内容"]},"1769":{"title":"友元简介","titles":["第 20 节 课件: 友元及友元相关内容"]},"1770":{"title":"关键字 friend","titles":["第 20 节 课件: 友元及友元相关内容"]},"1771":{"title":"1. 友元函数","titles":["第 20 节 课件: 友元及友元相关内容"]},"1772":{"title":"1.1 定义友元函数","titles":["第 20 节 课件: 友元及友元相关内容","1. 友元函数"]},"1773":{"title":"1.2 友元函数的权限","titles":["第 20 节 课件: 友元及友元相关内容","1. 友元函数"]},"1774":{"title":"1.3 友元函数的特性与示例","titles":["第 20 节 课件: 友元及友元相关内容","1. 友元函数"]},"1775":{"title":"1.4 友元函数要点","titles":["第 20 节 课件: 友元及友元相关内容","1. 友元函数"]},"1776":{"title":"1.5 友元函数的优缺点","titles":["第 20 节 课件: 友元及友元相关内容","1. 友元函数"]},"1777":{"title":"2. 友元类","titles":["第 20 节 课件: 友元及友元相关内容"]},"1778":{"title":"2.1 定义友元类","titles":["第 20 节 课件: 友元及友元相关内容","2. 友元类"]},"1779":{"title":"2.2 友元类的特性与示例","titles":["第 20 节 课件: 友元及友元相关内容","2. 友元类"]},"1780":{"title":"2.3 友元类强调","titles":["第 20 节 课件: 友元及友元相关内容","2. 友元类"]},"1781":{"title":"友元关系的特性","titles":["第 20 节 课件: 友元及友元相关内容","2. 友元类","2.3 友元类强调"]},"1782":{"title":"3. 友元的继承","titles":["第 20 节 课件: 友元及友元相关内容"]},"1783":{"title":"3.1 友元的访问权限继承","titles":["第 20 节 课件: 友元及友元相关内容","3. 友元的继承"]},"1784":{"title":"3.2 友元继承示例","titles":["第 20 节 课件: 友元及友元相关内容","3. 友元的继承"]},"1785":{"title":"输出结果","titles":["第 20 节 课件: 友元及友元相关内容","3. 友元的继承"]},"1786":{"title":"3.3 友元与继承的关系","titles":["第 20 节 课件: 友元及友元相关内容","3. 友元的继承"]},"1787":{"title":"4. 运算符重载友元","titles":["第 20 节 课件: 友元及友元相关内容"]},"1788":{"title":"4.1 友元作为运算符重载函数","titles":["第 20 节 课件: 友元及友元相关内容","4. 运算符重载友元"]},"1789":{"title":"4.2 示例：友元运算符重载","titles":["第 20 节 课件: 友元及友元相关内容","4. 运算符重载友元"]},"1790":{"title":"作业","titles":["第 20 节 课件: 友元及友元相关内容"]},"1791":{"title":"作业 1：自定义立方体类，用友元函数访问它的私有成员","titles":["第 20 节 课件: 友元及友元相关内容","作业"]},"1792":{"title":"作业 2: 自定义立方体类，用友元函数实现立方体相加","titles":["第 20 节 课件: 友元及友元相关内容","作业"]},"1793":{"title":"总结与注意事项","titles":["第 20 节 课件: 友元及友元相关内容"]},"1794":{"title":"C++ 第 15次授课：有序容器与关联容器","titles":[]},"1795":{"title":"课程目录","titles":["C++ 第 15次授课：有序容器与关联容器"]},"1796":{"title":"1. 有序容器","titles":["C++ 第 15次授课：有序容器与关联容器","课程目录"]},"1797":{"title":"1.1 vector","titles":["C++ 第 15次授课：有序容器与关联容器","课程目录","1. 有序容器"]},"1798":{"title":"1.2 其他常用的有序容器","titles":["C++ 第 15次授课：有序容器与关联容器","课程目录","1. 有序容器"]},"1799":{"title":"1.2.1 list","titles":["C++ 第 15次授课：有序容器与关联容器","课程目录","1. 有序容器","1.2 其他常用的有序容器"]},"1800":{"title":"1.2.2 deque","titles":["C++ 第 15次授课：有序容器与关联容器","课程目录","1. 有序容器","1.2 其他常用的有序容器"]},"1801":{"title":"1.2.3 forward_list","titles":["C++ 第 15次授课：有序容器与关联容器","课程目录","1. 有序容器","1.2 其他常用的有序容器"]},"1802":{"title":"1.2.4 array","titles":["C++ 第 15次授课：有序容器与关联容器","课程目录","1. 有序容器","1.2 其他常用的有序容器"]},"1803":{"title":"1.2.5 string","titles":["C++ 第 15次授课：有序容器与关联容器","课程目录","1. 有序容器","1.2 其他常用的有序容器"]},"1804":{"title":"2. 关联容器","titles":["C++ 第 15次授课：有序容器与关联容器","课程目录"]},"1805":{"title":"2.1 pair","titles":["C++ 第 15次授课：有序容器与关联容器","课程目录","2. 关联容器"]},"1806":{"title":"2.2 set","titles":["C++ 第 15次授课：有序容器与关联容器","课程目录","2. 关联容器"]},"1807":{"title":"2.3 map","titles":["C++ 第 15次授课：有序容器与关联容器","课程目录","2. 关联容器"]},"1808":{"title":"作业 ：","titles":["C++ 第 15次授课：有序容器与关联容器"]},"1809":{"title":"第 21 课：C++ IO 流详解","titles":[]},"1810":{"title":"1. 什么是 IO 流？","titles":["第 21 课：C++ IO 流详解"]},"1811":{"title":"2. C++ 标准 IO 流对象","titles":["第 21 课：C++ IO 流详解"]},"1812":{"title":"3. 输入流（Input Stream）","titles":["第 21 课：C++ IO 流详解"]},"1813":{"title":"3.1. cin 常用方法","titles":["第 21 课：C++ IO 流详解","3. 输入流（Input Stream）"]},"1814":{"title":"3.2 输入流的状态","titles":["第 21 课：C++ IO 流详解","3. 输入流（Input Stream）"]},"1815":{"title":"4. 输出流（Output Stream）","titles":["第 21 课：C++ IO 流详解"]},"1816":{"title":"4.1. cout 常用方法","titles":["第 21 课：C++ IO 流详解","4. 输出流（Output Stream）"]},"1817":{"title":"4.2. 格式化输出","titles":["第 21 课：C++ IO 流详解","4. 输出流（Output Stream）"]},"1818":{"title":"5. 文件 IO","titles":["第 21 课：C++ IO 流详解"]},"1819":{"title":"5.1. 文件流类","titles":["第 21 课：C++ IO 流详解","5. 文件 IO"]},"1820":{"title":"5.2. 文件操作步骤","titles":["第 21 课：C++ IO 流详解","5. 文件 IO"]},"1821":{"title":"5.3 文件的随机访问","titles":["第 21 课：C++ IO 流详解","5. 文件 IO"]},"1822":{"title":"6. 字符串流（String Stream）","titles":["第 21 课：C++ IO 流详解"]},"1823":{"title":"6.1. 字符串流类","titles":["第 21 课：C++ IO 流详解","6. 字符串流（String Stream）"]},"1824":{"title":"6.2. 字符串流用法","titles":["第 21 课：C++ IO 流详解","6. 字符串流（String Stream）"]},"1825":{"title":"7、作业","titles":["第 21 课：C++ IO 流详解"]},"1826":{"title":"8. 总结","titles":["第 21 课：C++ IO 流详解"]},"1827":{"title":"第 21 课：C++ IO 流详解","titles":[]},"1828":{"title":"1. 什么是 IO 流？","titles":["第 21 课：C++ IO 流详解"]},"1829":{"title":"2. C++ 标准 IO 流对象","titles":["第 21 课：C++ IO 流详解"]},"1830":{"title":"3. 输入流（Input Stream）","titles":["第 21 课：C++ IO 流详解"]},"1831":{"title":"3.1. cin 常用方法","titles":["第 21 课：C++ IO 流详解","3. 输入流（Input Stream）"]},"1832":{"title":"3.2 输入流的状态","titles":["第 21 课：C++ IO 流详解","3. 输入流（Input Stream）"]},"1833":{"title":"4. 输出流（Output Stream）","titles":["第 21 课：C++ IO 流详解"]},"1834":{"title":"4.1. cout 常用方法","titles":["第 21 课：C++ IO 流详解","4. 输出流（Output Stream）"]},"1835":{"title":"4.2. 格式化输出","titles":["第 21 课：C++ IO 流详解","4. 输出流（Output Stream）"]},"1836":{"title":"5. 文件 IO","titles":["第 21 课：C++ IO 流详解"]},"1837":{"title":"5.1. 文件流类","titles":["第 21 课：C++ IO 流详解","5. 文件 IO"]},"1838":{"title":"5.2. 文件操作步骤","titles":["第 21 课：C++ IO 流详解","5. 文件 IO"]},"1839":{"title":"5.3 文件的随机访问","titles":["第 21 课：C++ IO 流详解","5. 文件 IO"]},"1840":{"title":"6. 字符串流（String Stream）","titles":["第 21 课：C++ IO 流详解"]},"1841":{"title":"6.1. 字符串流类","titles":["第 21 课：C++ IO 流详解","6. 字符串流（String Stream）"]},"1842":{"title":"6.2. 字符串流用法","titles":["第 21 课：C++ IO 流详解","6. 字符串流（String Stream）"]},"1843":{"title":"7、作业","titles":["第 21 课：C++ IO 流详解"]},"1844":{"title":"8. 总结","titles":["第 21 课：C++ IO 流详解"]},"1845":{"title":"C++ 继承方式：public, protected, private","titles":[]},"1846":{"title":"1. 课件介绍","titles":["C++ 继承方式：public, protected, private"]},"1847":{"title":"2. 继承的基本概念","titles":["C++ 继承方式：public, protected, private"]},"1848":{"title":"2.1. 继承的定义","titles":["C++ 继承方式：public, protected, private","2. 继承的基本概念"]},"1849":{"title":"2.2. 示例代码","titles":["C++ 继承方式：public, protected, private","2. 继承的基本概念"]},"1850":{"title":"3. Public 继承","titles":["C++ 继承方式：public, protected, private"]},"1851":{"title":"3.1. Public 继承的定义","titles":["C++ 继承方式：public, protected, private","3. Public 继承"]},"1852":{"title":"3.2. Public 继承的特点","titles":["C++ 继承方式：public, protected, private","3. Public 继承"]},"1853":{"title":"3.3. 示例代码","titles":["C++ 继承方式：public, protected, private","3. Public 继承"]},"1854":{"title":"4. Protected 继承","titles":["C++ 继承方式：public, protected, private"]},"1855":{"title":"4.1. Protected 继承的定义","titles":["C++ 继承方式：public, protected, private","4. Protected 继承"]},"1856":{"title":"4.2. Protected 继承的特点","titles":["C++ 继承方式：public, protected, private","4. Protected 继承"]},"1857":{"title":"4.3. 示例代码","titles":["C++ 继承方式：public, protected, private","4. Protected 继承"]},"1858":{"title":"5. Private 继承","titles":["C++ 继承方式：public, protected, private"]},"1859":{"title":"5.1. Private 继承的定义","titles":["C++ 继承方式：public, protected, private","5. Private 继承"]},"1860":{"title":"5.2. Private 继承的特点","titles":["C++ 继承方式：public, protected, private","5. Private 继承"]},"1861":{"title":"5.3. 示例代码","titles":["C++ 继承方式：public, protected, private","5. Private 继承"]},"1862":{"title":"6. 三种继承方式的对比","titles":["C++ 继承方式：public, protected, private"]},"1863":{"title":"6.1. 访问控制对比","titles":["C++ 继承方式：public, protected, private","6. 三种继承方式的对比"]},"1864":{"title":"6.2. 使用场景总结","titles":["C++ 继承方式：public, protected, private","6. 三种继承方式的对比"]},"1865":{"title":"6.4. 实际应用场景讨论","titles":["C++ 继承方式：public, protected, private"]},"1866":{"title":"1. 接口继承（Public 继承）","titles":["C++ 继承方式：public, protected, private","6.4. 实际应用场景讨论"]},"1867":{"title":"2. 部分封闭设计（Protected 继承）","titles":["C++ 继承方式：public, protected, private","6.4. 实际应用场景讨论"]},"1868":{"title":"3. 完全封闭继承（Private 继承）","titles":["C++ 继承方式：public, protected, private","6.4. 实际应用场景讨论"]},"1869":{"title":"4. 多继承与菱形问题","titles":["C++ 继承方式：public, protected, private","6.4. 实际应用场景讨论"]},"1870":{"title":"课后作业：C++ 继承方式应用","titles":[]},"1871":{"title":"作业目标","titles":["课后作业：C++ 继承方式应用"]},"1872":{"title":"作业内容","titles":["课后作业：C++ 继承方式应用"]},"1873":{"title":"任务描述","titles":["课后作业：C++ 继承方式应用","作业内容"]},"1874":{"title":"提示","titles":["课后作业：C++ 继承方式应用","作业内容"]},"1875":{"title":"C++11 新特性概述","titles":[]},"1876":{"title":"课程目标","titles":["C++11 新特性概述"]},"1877":{"title":"1. 自动类型推导 - auto","titles":["C++11 新特性概述"]},"1878":{"title":"1.1 简介","titles":["C++11 新特性概述","1. 自动类型推导 - auto"]},"1879":{"title":"1.2 示例","titles":["C++11 新特性概述","1. 自动类型推导 - auto"]},"1880":{"title":"1.3 意义","titles":["C++11 新特性概述","1. 自动类型推导 - auto"]},"1881":{"title":"2. Lambda 表达式","titles":["C++11 新特性概述"]},"1882":{"title":"2.1 简介","titles":["C++11 新特性概述","2. Lambda 表达式"]},"1883":{"title":"2.2 示例","titles":["C++11 新特性概述","2. Lambda 表达式"]},"1884":{"title":"2.3 意义","titles":["C++11 新特性概述","2. Lambda 表达式"]},"1885":{"title":"3. 右值引用与移动语义","titles":["C++11 新特性概述"]},"1886":{"title":"3.1 简介","titles":["C++11 新特性概述","3. 右值引用与移动语义"]},"1887":{"title":"3.2 示例","titles":["C++11 新特性概述","3. 右值引用与移动语义"]},"1888":{"title":"3.3 意义","titles":["C++11 新特性概述","3. 右值引用与移动语义"]},"1889":{"title":"4. 标准库的改进","titles":["C++11 新特性概述"]},"1890":{"title":"4.1 智能指针","titles":["C++11 新特性概述","4. 标准库的改进"]},"1891":{"title":"4.2 示例","titles":["C++11 新特性概述","4. 标准库的改进"]},"1892":{"title":"4.3 意义","titles":["C++11 新特性概述","4. 标准库的改进"]},"1893":{"title":"5. 多线程支持","titles":["C++11 新特性概述"]},"1894":{"title":"5.1 简介","titles":["C++11 新特性概述","5. 多线程支持"]},"1895":{"title":"5.2 示例","titles":["C++11 新特性概述","5. 多线程支持"]},"1896":{"title":"5.3 意义","titles":["C++11 新特性概述","5. 多线程支持"]},"1897":{"title":"6. 其他重要特性","titles":["C++11 新特性概述"]},"1898":{"title":"6.1 示例（Range-based for loop）","titles":["C++11 新特性概述","6. 其他重要特性"]},"1899":{"title":"6.2 意义","titles":["C++11 新特性概述","6. 其他重要特性"]},"1900":{"title":"课堂练习","titles":["C++11 新特性概述"]},"1901":{"title":"总结回顾","titles":["C++11 新特性概述"]},"1902":{"title":"作业：","titles":["C++11 新特性概述"]},"1903":{"title":"C++14 新特性","titles":[]},"1904":{"title":"目录","titles":["C++14 新特性"]},"1905":{"title":"1. 函数返回类型推导","titles":["C++14 新特性"]},"1906":{"title":"2. 泛型 Lambda 表达式 (Generic Lambdas)","titles":["C++14 新特性"]},"1907":{"title":"3. Lambda 捕获表达式 (Lambda Capture Expressions)","titles":["C++14 新特性"]},"1908":{"title":"4. 变量模板 (Variable Templates)","titles":["C++14 新特性"]},"1909":{"title":"5. [[deprecated]] 属性","titles":["C++14 新特性"]},"1910":{"title":"6. 二进制字面量和数字分隔符","titles":["C++14 新特性"]},"1911":{"title":"7. std::make_unique","titles":["C++14 新特性"]},"1912":{"title":"8. 编译期整数序列 (std::integer_sequence)","titles":["C++14 新特性"]},"1913":{"title":"9. 总结","titles":["C++14 新特性"]},"1914":{"title":"10. 作业","titles":["C++14 新特性"]},"1915":{"title":"27 课 多文件和 Makefile工程管理","titles":[]},"1916":{"title":"一、C++ 多文件编程 9","titles":["27 课 多文件和 Makefile工程管理"]},"1917":{"title":"二、Makefile 15","titles":["27 课 多文件和 Makefile工程管理"]},"1918":{"title":"作业","titles":[]},"1919":{"title":"C++17 新特性","titles":[]},"1920":{"title":"目录","titles":["C++17 新特性"]},"1921":{"title":"1. 结构化绑定 (Structured Bindings)","titles":["C++17 新特性"]},"1922":{"title":"2. if 和 switch 语句初始化","titles":["C++17 新特性"]},"1923":{"title":"3. 内联变量 (Inline Variables)","titles":["C++17 新特性"]},"1924":{"title":"4. constexpr Lambda 表达式","titles":["C++17 新特性"]},"1925":{"title":"5. 类模板参数推导 (Class Template Argument Deduction)","titles":["C++17 新特性"]},"1926":{"title":"6. std::variant","titles":["C++17 新特性"]},"1927":{"title":"7. std::optional","titles":["C++17 新特性"]},"1928":{"title":"8. std::any","titles":["C++17 新特性"]},"1929":{"title":"9. std::string_view","titles":["C++17 新特性"]},"1930":{"title":"10. 文件系统库 (File System Library)","titles":["C++17 新特性"]},"1931":{"title":"11. 其他特性","titles":["C++17 新特性"]},"1932":{"title":"12. 总结","titles":["C++17 新特性"]},"1933":{"title":"课后作业：统计单词频率","titles":["C++17 新特性","12. 总结"]},"1934":{"title":"C++ 主要就业方向与技术能力分析报告","titles":[]},"1935":{"title":"1. 游戏开发领域","titles":["C++ 主要就业方向与技术能力分析报告"]},"1936":{"title":"核心方向","titles":["C++ 主要就业方向与技术能力分析报告","1. 游戏开发领域"]},"1937":{"title":"核心技能要求","titles":["C++ 主要就业方向与技术能力分析报告","1. 游戏开发领域"]},"1938":{"title":"典型岗位","titles":["C++ 主要就业方向与技术能力分析报告","1. 游戏开发领域"]},"1939":{"title":"代表企业","titles":["C++ 主要就业方向与技术能力分析报告","1. 游戏开发领域"]},"1940":{"title":"2. 系统级软件开发","titles":["C++ 主要就业方向与技术能力分析报告"]},"1941":{"title":"核心方向","titles":["C++ 主要就业方向与技术能力分析报告","2. 系统级软件开发"]},"1942":{"title":"核心技能要求","titles":["C++ 主要就业方向与技术能力分析报告","2. 系统级软件开发"]},"1943":{"title":"典型岗位","titles":["C++ 主要就业方向与技术能力分析报告","2. 系统级软件开发"]},"1944":{"title":"代表企业","titles":["C++ 主要就业方向与技术能力分析报告","2. 系统级软件开发"]},"1945":{"title":"3. 金融科技领域","titles":["C++ 主要就业方向与技术能力分析报告"]},"1946":{"title":"核心方向","titles":["C++ 主要就业方向与技术能力分析报告","3. 金融科技领域"]},"1947":{"title":"核心技能要求","titles":["C++ 主要就业方向与技术能力分析报告","3. 金融科技领域"]},"1948":{"title":"典型岗位","titles":["C++ 主要就业方向与技术能力分析报告","3. 金融科技领域"]},"1949":{"title":"代表企业","titles":["C++ 主要就业方向与技术能力分析报告","3. 金融科技领域"]},"1950":{"title":"4. 基础设施开发","titles":["C++ 主要就业方向与技术能力分析报告"]},"1951":{"title":"核心方向","titles":["C++ 主要就业方向与技术能力分析报告","4. 基础设施开发"]},"1952":{"title":"核心技能要求","titles":["C++ 主要就业方向与技术能力分析报告","4. 基础设施开发"]},"1953":{"title":"典型岗位","titles":["C++ 主要就业方向与技术能力分析报告","4. 基础设施开发"]},"1954":{"title":"代表企业","titles":["C++ 主要就业方向与技术能力分析报告","4. 基础设施开发"]},"1955":{"title":"5. 工业软件领域","titles":["C++ 主要就业方向与技术能力分析报告"]},"1956":{"title":"核心方向","titles":["C++ 主要就业方向与技术能力分析报告","5. 工业软件领域"]},"1957":{"title":"核心技能要求","titles":["C++ 主要就业方向与技术能力分析报告","5. 工业软件领域"]},"1958":{"title":"典型岗位","titles":["C++ 主要就业方向与技术能力分析报告","5. 工业软件领域"]},"1959":{"title":"代表企业","titles":["C++ 主要就业方向与技术能力分析报告","5. 工业软件领域"]},"1960":{"title":"技术趋势分析","titles":["C++ 主要就业方向与技术能力分析报告"]},"1961":{"title":"职业发展建议","titles":["C++ 主要就业方向与技术能力分析报告"]},"1962":{"title":"C++28课：高性能部署之CMake工程管理","titles":[]},"1963":{"title":"1. 什么是 CMake？","titles":["C++28课：高性能部署之CMake工程管理"]},"1964":{"title":"1.1. 为什么选择 CMake？","titles":["C++28课：高性能部署之CMake工程管理","1. 什么是 CMake？"]},"1965":{"title":"2. 基本结构","titles":["C++28课：高性能部署之CMake工程管理"]},"1966":{"title":"2.1. 最简单的 CMakeLists.txt 示例","titles":["C++28课：高性能部署之CMake工程管理","2. 基本结构"]},"1967":{"title":"2.2. 生成与构建","titles":["C++28课：高性能部署之CMake工程管理","2. 基本结构"]},"1968":{"title":"2.2.1. Linux/ macOS","titles":["C++28课：高性能部署之CMake工程管理","2. 基本结构","2.2. 生成与构建"]},"1969":{"title":"2.2.2. Windows","titles":["C++28课：高性能部署之CMake工程管理","2. 基本结构","2.2. 生成与构建"]},"1970":{"title":"3. CMake 基本命令详解","titles":["C++28课：高性能部署之CMake工程管理"]},"1971":{"title":"3.1. cmake_minimum_required","titles":["C++28课：高性能部署之CMake工程管理","3. CMake 基本命令详解"]},"1972":{"title":"3.2. project","titles":["C++28课：高性能部署之CMake工程管理","3. CMake 基本命令详解"]},"1973":{"title":"3.3. add_executable","titles":["C++28课：高性能部署之CMake工程管理","3. CMake 基本命令详解"]},"1974":{"title":"3.4. add_library","titles":["C++28课：高性能部署之CMake工程管理","3. CMake 基本命令详解"]},"1975":{"title":"3.5. target_link_libraries","titles":["C++28课：高性能部署之CMake工程管理","3. CMake 基本命令详解"]},"1976":{"title":"3.6. include_directories","titles":["C++28课：高性能部署之CMake工程管理","3. CMake 基本命令详解"]},"1977":{"title":"3.7. find_package","titles":["C++28课：高性能部署之CMake工程管理","3. CMake 基本命令详解"]},"1978":{"title":"4. 组织大型项目","titles":["C++28课：高性能部署之CMake工程管理"]},"1979":{"title":"4.1. 子目录和子模块","titles":["C++28课：高性能部署之CMake工程管理","4. 组织大型项目"]},"1980":{"title":"4.1.1. 主目录的 CMakeLists.txt","titles":["C++28课：高性能部署之CMake工程管理","4. 组织大型项目","4.1. 子目录和子模块"]},"1981":{"title":"4.1.2. 子模块的 CMakeLists.txt (例如 ModuleA)","titles":["C++28课：高性能部署之CMake工程管理","4. 组织大型项目","4.1. 子目录和子模块"]},"1982":{"title":"4.2. 设置全局属性","titles":["C++28课：高性能部署之CMake工程管理","4. 组织大型项目"]},"1983":{"title":"5. CMake 高级用法","titles":["C++28课：高性能部署之CMake工程管理"]},"1984":{"title":"5.1. 定义编译选项","titles":["C++28课：高性能部署之CMake工程管理","5. CMake 高级用法"]},"1985":{"title":"5.2. 条件编译","titles":["C++28课：高性能部署之CMake工程管理","5. CMake 高级用法"]},"1986":{"title":"5.3. 测试支持","titles":["C++28课：高性能部署之CMake工程管理","5. CMake 高级用法"]},"1987":{"title":"5.4. 安装目标","titles":["C++28课：高性能部署之CMake工程管理","5. CMake 高级用法"]},"1988":{"title":"6. CMake 与第三方依赖管理","titles":["C++28课：高性能部署之CMake工程管理"]},"1989":{"title":"6.1. 使用 ExternalProject_Add","titles":["C++28课：高性能部署之CMake工程管理","6. CMake 与第三方依赖管理"]},"1990":{"title":"6.2. 使用 FetchContent","titles":["C++28课：高性能部署之CMake工程管理","6. CMake 与第三方依赖管理"]},"1991":{"title":"6.3. 使用 find_package","titles":["C++28课：高性能部署之CMake工程管理","6. CMake 与第三方依赖管理"]},"1992":{"title":"7. CMake 与自定义命令","titles":["C++28课：高性能部署之CMake工程管理"]},"1993":{"title":"7.1. 添加自定义命令","titles":["C++28课：高性能部署之CMake工程管理","7. CMake 与自定义命令"]},"1994":{"title":"7.2. 自定义目标","titles":["C++28课：高性能部署之CMake工程管理","7. CMake 与自定义命令"]},"1995":{"title":"8. 生成与构建","titles":["C++28课：高性能部署之CMake工程管理"]},"1996":{"title":"8.1. 常见命令总结","titles":["C++28课：高性能部署之CMake工程管理","8. 生成与构建"]},"1997":{"title":"CMake 课后作业","titles":["C++28课：高性能部署之CMake工程管理"]},"1998":{"title":"作业：","titles":["C++28课：高性能部署之CMake工程管理"]},"1999":{"title":"CMake 课后作业参考答案","titles":["C++28课：高性能部署之CMake工程管理"]},"2000":{"title":"c++ 基础笔记","titles":[]},"2001":{"title":"IT学习指南","titles":[]},"2002":{"title":"C++ 基础知识回顾","titles":[]},"2003":{"title":"作用域与变量生命周期","titles":["C++ 基础知识回顾"]},"2004":{"title":"函数重载与类型转换","titles":["C++ 基础知识回顾"]},"2005":{"title":"指针与引用的区别","titles":["C++ 基础知识回顾"]},"2006":{"title":"数组名的本质","titles":["C++ 基础知识回顾"]},"2007":{"title":"右值引用与移动语义","titles":["C++ 基础知识回顾"]},"2008":{"title":"静态局部变量","titles":["C++ 基础知识回顾"]},"2009":{"title":"计算机图形学的主要内容","titles":[]},"2010":{"title":"为什么要学习计算机图形学","titles":[]},"2011":{"title":"最新研究内容和成果","titles":[]},"2012":{"title":"yi直线光栅化","titles":[]},"2013":{"title":"1. 引入","titles":["yi直线光栅化"]},"2014":{"title":"2. 实现","titles":["yi直线光栅化"]},"2015":{"title":"2.1 基本实现思路","titles":["yi直线光栅化","2. 实现"]},"2016":{"title":"2.2 数值微分法（DDA算法）","titles":["yi直线光栅化","2. 实现"]},"2017":{"title":"2.2.1 DDA 算法概述","titles":["yi直线光栅化","2. 实现","2.2 数值微分法（DDA算法）"]},"2018":{"title":"2.2.2 算法步骤","titles":["yi直线光栅化","2. 实现","2.2 数值微分法（DDA算法）"]},"2019":{"title":"2.2.3 伪代码演示","titles":["yi直线光栅化","2. 实现","2.2 数值微分法（DDA算法）"]},"2020":{"title":"2.2.4 优化方向","titles":["yi直线光栅化","2. 实现","2.2 数值微分法（DDA算法）"]},"2021":{"title":"2.3 中心点画线法","titles":["yi直线光栅化","2. 实现"]},"2022":{"title":"2.3.1 算法思想","titles":["yi直线光栅化","2. 实现","2.3 中心点画线法"]},"2023":{"title":"2.3.2 算法步骤","titles":["yi直线光栅化","2. 实现","2.3 中心点画线法"]},"2024":{"title":"2.3.3 举例","titles":["yi直线光栅化","2. 实现","2.3 中心点画线法"]},"2025":{"title":"2.3.4 伪代码演示","titles":["yi直线光栅化","2. 实现","2.3 中心点画线法"]},"2026":{"title":"2.3 Bresenham算法","titles":["yi直线光栅化","2. 实现"]},"2027":{"title":"2.3.1 基本思想","titles":["yi直线光栅化","2. 实现","2.3 Bresenham算法"]},"2028":{"title":"2.3.2 改进策略","titles":["yi直线光栅化","2. 实现","2.3 Bresenham算法"]},"2029":{"title":"2.3.3 伪代码实现","titles":["yi直线光栅化","2. 实现","2.3 Bresenham算法"]},"2030":{"title":"2.3.4 举例","titles":["yi直线光栅化","2. 实现","2.3 Bresenham算法"]},"2031":{"title":"3. 思考","titles":["yi直线光栅化"]},"2032":{"title":"码医森","titles":[]},"2033":{"title":"主页:","titles":["码医森"]},"2034":{"title":"站点大纲:","titles":["码医森"]},"2035":{"title":"整体该博客(站点)指南","titles":[]},"2036":{"title":"该站点的整体结构","titles":[]},"2037":{"title":"主页🏠","titles":["该站点的整体结构"]},"2038":{"title":"站点大纲👀","titles":["该站点的整体结构"]},"2039":{"title":"个人提升","titles":[]},"2040":{"title":"更新日志","titles":[]},"2041":{"title":"2024-10","titles":["更新日志"]},"2042":{"title":"大更新：","titles":["更新日志","2024-10"]},"2043":{"title":"小更新：","titles":["更新日志","2024-10"]},"2044":{"title":"2024-11","titles":["更新日志"]},"2045":{"title":"大更新：","titles":["更新日志","2024-11"]},"2046":{"title":"小更新：","titles":["更新日志","2024-11"]},"2047":{"title":"2025-03","titles":["更新日志"]},"2048":{"title":"大更新:","titles":["更新日志","2025-03"]},"2049":{"title":"小更新:","titles":["更新日志","2025-03"]},"2050":{"title":"更新日志","titles":[]},"2051":{"title":"不同商家之间的视野——带室友修电脑","titles":[]},"2052":{"title":"论语之悟——学而篇","titles":[]},"2053":{"title":"1. 学而时习之","titles":["论语之悟——学而篇"]},"2054":{"title":"1.1 个人参悟","titles":["论语之悟——学而篇","1. 学而时习之"]},"2055":{"title":"1.2 不解之言","titles":["论语之悟——学而篇","1. 学而时习之"]},"2056":{"title":"士兵突击告诉让我明白的那些事","titles":[]},"2057":{"title":"第 22 节：位运算","titles":[]},"2058":{"title":"1. 课程导入 1","titles":["第 22 节：位运算"]},"2059":{"title":"2. 位运算符介绍 3","titles":["第 22 节：位运算"]},"2060":{"title":"3. 应用示例与实践 9","titles":["第 22 节：位运算"]},"2061":{"title":"4. 深入探讨：位运算的高级用法 4","titles":["第 22 节：位运算"]},"2062":{"title":"6. std::bitset 的使用 3","titles":["第 22 节：位运算"]},"2063":{"title":"课后作业 4","titles":[]},"2064":{"title":"Docker下的doccano添加普通用户","titles":[]},"2065":{"title":"1. 操作步骤","titles":["Docker下的doccano添加普通用户"]},"2066":{"title":"1.1 启动docker的doccano","titles":["Docker下的doccano添加普通用户"]},"2067":{"title":"1.2 进入Doccano容器","titles":["Docker下的doccano添加普通用户"]},"2068":{"title":"1.3 配置（添加）超级管理员或者普通管理员账号","titles":["Docker下的doccano添加普通用户"]},"2069":{"title":"1.3.1 管理员账户","titles":["Docker下的doccano添加普通用户","1.3 配置（添加）超级管理员或者普通管理员账号"]},"2070":{"title":"1.3.2 普通用户账户","titles":["Docker下的doccano添加普通用户","1.3 配置（添加）超级管理员或者普通管理员账号"]},"2071":{"title":"各类技术问题和踩坑","titles":[]},"2072":{"title":"Introduction to PyTorch","titles":[]},"2073":{"title":"1. What is PyTorch?","titles":["Introduction to PyTorch"]},"2074":{"title":"2. Key Features of PyTorch","titles":["Introduction to PyTorch"]},"2075":{"title":"2.1. Dynamic Computation Graphs","titles":["Introduction to PyTorch","2. Key Features of PyTorch"]},"2076":{"title":"2.2. Autograd (Automatic Gradient Calculation)","titles":["Introduction to PyTorch","2. Key Features of PyTorch"]},"2077":{"title":"2.3. Tensors","titles":["Introduction to PyTorch","2. Key Features of PyTorch"]},"2078":{"title":"2.4. Easy to Use with Python","titles":["Introduction to PyTorch","2. Key Features of PyTorch"]},"2079":{"title":"2.5. Rich Ecosystem of Libraries","titles":["Introduction to PyTorch","2. Key Features of PyTorch"]},"2080":{"title":"3. The Application Domains of PyTorch","titles":["Introduction to PyTorch"]},"2081":{"title":"3.1. Research","titles":["Introduction to PyTorch","3. The Application Domains of PyTorch"]},"2082":{"title":"3.2. Industry","titles":["Introduction to PyTorch","3. The Application Domains of PyTorch"]},"2083":{"title":"3.3. Education","titles":["Introduction to PyTorch","3. The Application Domains of PyTorch"]},"2084":{"title":"4. How To Use PyTorch？","titles":["Introduction to PyTorch"]},"2085":{"title":"4.1 Official Tutorials","titles":["Introduction to PyTorch","4. How To Use PyTorch？"]},"2086":{"title":"4.2 Basic Framework","titles":["Introduction to PyTorch","4. How To Use PyTorch？"]},"2087":{"title":"5. Conclusion","titles":["Introduction to PyTorch"]},"2088":{"title":"有关虚拟机创建的网络问题","titles":[]},"2089":{"title":"1. 基础环境","titles":["有关虚拟机创建的网络问题"]},"2090":{"title":"2. 相关知识","titles":["有关虚拟机创建的网络问题"]},"2091":{"title":"3. 常见网络问题","titles":["有关虚拟机创建的网络问题"]},"2092":{"title":"4. 解决方案","titles":["有关虚拟机创建的网络问题"]},"2093":{"title":"4.1 法一：在物理主机开启NAT和DHCP服务","titles":["有关虚拟机创建的网络问题","4. 解决方案"]},"2094":{"title":"4.2 法二：恢复默认网络配置","titles":["有关虚拟机创建的网络问题","4. 解决方案"]},"2095":{"title":"5. 固定虚拟机网络ip（自动ip分配的可以不用操作）","titles":["有关虚拟机创建的网络问题"]},"2096":{"title":"生活与算法","titles":[]},"2097":{"title":"1. 什么是算法？","titles":["生活与算法"]},"2098":{"title":"2. 生活中常常有算法","titles":["生活与算法"]},"2099":{"title":"2.1 选择顺序：排队和优先处理","titles":["生活与算法","2. 生活中常常有算法"]},"2100":{"title":"2.2 找东西：生活中的搜索","titles":["生活与算法","2. 生活中常常有算法"]},"2101":{"title":"2.3 每次都选最佳：贪心法则","titles":["生活与算法","2. 生活中常常有算法"]},"2102":{"title":"2.4 长远规划：逐步优化的过程","titles":["生活与算法","2. 生活中常常有算法"]},"2103":{"title":"3. 生活离不开算法思维","titles":["生活与算法"]},"2104":{"title":"3.1 化繁为简：一步步来","titles":["生活与算法","3. 生活离不开算法思维"]},"2105":{"title":"3.2 避免错误：逻辑推理","titles":["生活与算法","3. 生活离不开算法思维"]},"2106":{"title":"3.3 时间管理：高效完成任务","titles":["生活与算法","3. 生活离不开算法思维"]},"2107":{"title":"4. 算法交织于生活","titles":["生活与算法"]},"2108":{"title":"5. 简单总结一下","titles":["生活与算法"]},"2109":{"title":"站长随笔","titles":[]},"2110":{"title":"更新进度","titles":[]},"2111":{"title":"人的本性——贪心！！！","titles":[]},"2112":{"title":"1. 贪心与人生","titles":["人的本性——贪心！！！"]},"2113":{"title":"2. 贪心算法基本概念","titles":["人的本性——贪心！！！"]},"2114":{"title":"2.1 概念","titles":["人的本性——贪心！！！","2. 贪心算法基本概念"]},"2115":{"title":"2.2 通俗讲解","titles":["人的本性——贪心！！！","2. 贪心算法基本概念"]},"2116":{"title":"3. 贪心算法原理","titles":["人的本性——贪心！！！"]},"2117":{"title":"3.1 我想说","titles":["人的本性——贪心！！！","3. 贪心算法原理"]},"2118":{"title":"3.2 贪心策略","titles":["人的本性——贪心！！！","3. 贪心算法原理"]},"2119":{"title":"3.3 基本原理","titles":["人的本性——贪心！！！","3. 贪心算法原理"]},"2120":{"title":"4. 总结","titles":["人的本性——贪心！！！"]},"2121":{"title":"让我们简单步入“贪心”叭","titles":[]},"2122":{"title":"1. 最大四位数的例子：从数字中选最大","titles":["让我们简单步入“贪心”叭"]},"2123":{"title":"1.1 问题：","titles":["让我们简单步入“贪心”叭","1. 最大四位数的例子：从数字中选最大"]},"2124":{"title":"1.2 贪心思维：","titles":["让我们简单步入“贪心”叭","1. 最大四位数的例子：从数字中选最大"]},"2125":{"title":"1.3 数学过程：","titles":["让我们简单步入“贪心”叭","1. 最大四位数的例子：从数字中选最大"]},"2126":{"title":"1.4 结论：","titles":["让我们简单步入“贪心”叭","1. 最大四位数的例子：从数字中选最大"]},"2127":{"title":"大家可能会觉得这不是显而易见的吗？但这只能帮你解决简单的问题，复杂的问题就需要我们认真去分析了。","titles":["让我们简单步入“贪心”叭"]},"2128":{"title":"2. 选择最便宜的出行方案","titles":["让我们简单步入“贪心”叭"]},"2129":{"title":"2.1 问题：","titles":["让我们简单步入“贪心”叭","2. 选择最便宜的出行方案"]},"2130":{"title":"2.2 贪心思维：","titles":["让我们简单步入“贪心”叭","2. 选择最便宜的出行方案"]},"2131":{"title":"2.3 数学过程：","titles":["让我们简单步入“贪心”叭","2. 选择最便宜的出行方案"]},"2132":{"title":"3. 如何分配时间：优先完成最紧急的任务","titles":["让我们简单步入“贪心”叭"]},"2133":{"title":"3.1 问题：","titles":["让我们简单步入“贪心”叭","3. 如何分配时间：优先完成最紧急的任务"]},"2134":{"title":"3.2 贪心思维：","titles":["让我们简单步入“贪心”叭","3. 如何分配时间：优先完成最紧急的任务"]},"2135":{"title":"3.3 数学过程：","titles":["让我们简单步入“贪心”叭","3. 如何分配时间：优先完成最紧急的任务"]},"2136":{"title":"4. 找零问题：使用最少的硬币","titles":["让我们简单步入“贪心”叭"]},"2137":{"title":"4.1 问题：","titles":["让我们简单步入“贪心”叭","4. 找零问题：使用最少的硬币"]},"2138":{"title":"4.2 贪心思维：","titles":["让我们简单步入“贪心”叭","4. 找零问题：使用最少的硬币"]},"2139":{"title":"4.3 数学过程：","titles":["让我们简单步入“贪心”叭","4. 找零问题：使用最少的硬币"]},"2140":{"title":"5. 我们已经步入“贪心算法”啦","titles":["让我们简单步入“贪心”叭"]},"2141":{"title":"这年代，不吃面经咋活？","titles":[]},"2142":{"title":"场景问题内容","titles":[]},"2143":{"title":"前任铺路后人走","titles":[]},"2144":{"title":"NLP面经","titles":["前任铺路后人走"]},"2145":{"title":"前任铺路后人走","titles":[]},"2146":{"title":"分发饼干问题","titles":[]},"2147":{"title":"1. 问题描述","titles":["分发饼干问题"]},"2148":{"title":"2. 目标","titles":["分发饼干问题"]},"2149":{"title":"3. 输入：","titles":["分发饼干问题"]},"2150":{"title":"4. 输出：","titles":["分发饼干问题"]},"2151":{"title":"5. 贪心算法的解法思路","titles":["分发饼干问题"]},"2152":{"title":"6. 解题步骤","titles":["分发饼干问题"]},"2153":{"title":"7. 代码实现（Python）","titles":["分发饼干问题"]},"2154":{"title":"8. 示例","titles":["分发饼干问题"]},"2155":{"title":"9. 时间复杂度","titles":["分发饼干问题"]},"2156":{"title":"10. 贪心策略的解释","titles":["分发饼干问题"]},"2157":{"title":"11. 思考","titles":["分发饼干问题"]}},"dirtCount":0,"index":[["孩子的胃口",{"2":{"2154":1}}],["孩子得到满足",{"2":{"2153":1}}],["孩子指针",{"2":{"2153":1}}],["饼干的大小",{"2":{"2154":1}}],["饼干指针",{"2":{"2153":1}}],["饼干大小数组",{"2":{"2152":1}}],["胃口",{"2":{"2149":1}}],["场景问题内容",{"0":{"2142":1}}],["啦",{"0":{"2140":1}}],["枚硬币",{"2":{"2139":1}}],["枚举的应用场景",{"2":{"1728":1}}],["枚举常量本质上是整型",{"2":{"1728":1}}],["枚举是一种用户定义的数据类型",{"2":{"1728":1}}],["枚举和联合体",{"2":{"1728":1,"1729":1}}],["枚举和结构体声明等",{"2":{"1628":1}}],["枚举",{"2":{"1727":1,"1728":1}}],["突然想起之前新闻上报道的一个高中小伙拿着小钱钱坐公交从xxx坐到了xxx",{"2":{"2129":1}}],["突破语言建模的极限",{"2":{"513":1}}],["突破性的想法往往来自那些在实践中寻求解决问题的创新者",{"2":{"284":1}}],["叭",{"0":{"2121":1},"1":{"2122":1,"2123":1,"2124":1,"2125":1,"2126":1,"2127":1,"2128":1,"2129":1,"2130":1,"2131":1,"2132":1,"2133":1,"2134":1,"2135":1,"2136":1,"2137":1,"2138":1,"2139":1,"2140":1}}],["性价比",{"2":{"2112":1}}],["性能分析能力",{"2":{"1961":1}}],["性能分析profiler",{"0":{"1284":1}}],["性能稳定",{"2":{"1802":1}}],["性能良好",{"2":{"1715":1}}],["性能敏感型应用",{"2":{"1602":1}}],["性能优化等方面的理解",{"2":{"1602":1}}],["性能优化",{"2":{"1500":1}}],["性能优异",{"2":{"1315":1}}],["性能就越不会下降",{"2":{"1155":1}}],["性能取决于问题",{"2":{"1137":1}}],["性能不减",{"2":{"768":1}}],["性能",{"2":{"512":1}}],["性能变差",{"2":{"100":1}}],["决策树",{"2":{"2105":1}}],["决定之前cell",{"2":{"865":1}}],["决定哪些信息可以进入cell",{"2":{"863":1}}],["决定基于这种方法的模型性能好坏有两个关键因素",{"2":{"727":1}}],["决定真正地融合双向的上下文信息",{"2":{"720":1}}],["决定从关注到的token取回什么信息",{"2":{"45":1}}],["决定从当前token关注上下文中的哪个",{"2":{"45":1}}],["钱包的经历",{"2":{"2100":1}}],["买菜和看剧",{"2":{"2099":1}}],["买一张从北京到上海的车票",{"2":{"459":1}}],["买一张从上海到北京的车票",{"2":{"459":1}}],["洗脸",{"2":{"2097":1}}],["似乎是一种传统的",{"2":{"2097":1}}],["似乎是由单语义神经元编码的",{"2":{"477":1}}],["慢慢发现学的东西往往来源于生活",{"2":{"2097":1}}],["慢慢降低在训练阶段输入真值的频率",{"2":{"411":1}}],["觉得这是计算机和程序员的专属领域",{"2":{"2096":1}}],["法二",{"0":{"2094":1}}],["法一",{"0":{"2093":1}}],["法国",{"2":{"122":4}}],["法国的首都是",{"2":{"122":1}}],["桥接模式",{"2":{"2090":1}}],["域名解析",{"2":{"2090":1}}],["域名备案成功并添加底部footer",{"2":{"2043":1}}],["身边的一些同学遇到了一些网络问题",{"2":{"2088":1}}],["身高体重",{"2":{"316":1}}],["身高数据可能是175cm",{"2":{"313":1}}],["身高",{"2":{"313":1}}],["侧边栏",{"2":{"2070":1}}],["登录后发现创建成功了",{"2":{"2070":1}}],["登录成功",{"2":{"2069":1}}],["邮箱>",{"2":{"2069":1}}],["翻转所有位",{"2":{"2062":1}}],["翻译模型通过大量的平行语料",{"2":{"908":1}}],["翻译下一个单词",{"2":{"532":1}}],["翻译",{"2":{"516":1,"1317":1,"1604":1}}],["翻译到第i+1单词时候需要通过mask操作遮盖住",{"2":{"532":1}}],["翻译到",{"2":{"516":1}}],["翻译成英文",{"2":{"445":1,"453":1}}],["翻译成",{"2":{"245":2,"246":1,"407":1,"428":1}}],["翻译为中文",{"2":{"245":1}}],["警醒自己",{"2":{"2056":1}}],["警告",{"2":{"1684":1}}],["挫折时",{"2":{"2056":1}}],["骄傲的时候",{"2":{"2056":1}}],["焦虑的时候",{"2":{"2056":1}}],["怕寂寞",{"2":{"2056":1}}],["怕没得到",{"2":{"2056":1}}],["顶得住和顶不住是个选择题",{"2":{"2056":1}}],["顶层通常编码更接近预训练任务的信息",{"2":{"402":1}}],["艰巨在于漫长",{"2":{"2056":1}}],["讨厌别人就是在讨厌自己",{"2":{"2056":1}}],["讨论涉及的多继承特性和虚拟基类的使用方式",{"2":{"1869":1}}],["讨论要点",{"2":{"1869":1}}],["饭得一口一口吃",{"2":{"2056":1}}],["偷奸耍滑不是机会",{"2":{"2056":1}}],["偷看未来词",{"2":{"525":1}}],["偷看",{"2":{"78":1}}],["鬼和你怕的东西不都是自己想出来的",{"2":{"2056":1}}],["坚定不移",{"2":{"2056":1}}],["坚持",{"2":{"1671":1}}],["坚持的",{"2":{"683":1}}],["淳朴善良",{"2":{"2056":1}}],["寂寞难耐的自己",{"2":{"2056":1}}],["迷茫的时候",{"2":{"2056":1}}],["迷茫时的自己",{"2":{"2056":1}}],["迷途小书僮",{"2":{"233":1}}],["士兵突击",{"2":{"2056":1}}],["士兵突击告诉让我明白的那些事",{"0":{"2056":1}}],["沉得住气",{"2":{"2056":1}}],["沉沦",{"2":{"169":3}}],["耐得住寂寞",{"2":{"2056":1}}],["君子",{"2":{"2054":2}}],["心稳",{"2":{"2056":1}}],["心机狡猾的自己",{"2":{"2056":1}}],["心里自然而然的产生强烈的",{"2":{"2054":1}}],["心理学角度",{"0":{"163":1},"2":{"157":1}}],["途中你看到一个小山峰",{"2":{"2115":1}}],["途中",{"2":{"2054":1}}],["啊",{"2":{"2054":2}}],["难道不是志同道合的人就不能是孔子所言的",{"2":{"2054":1}}],["难以想象孔子之后的生活基本很少有",{"2":{"2054":1}}],["难以区分错误和正常结果",{"2":{"1761":1}}],["难以在一个逻辑单元中进行管理",{"2":{"1728":1}}],["难以泛化",{"2":{"746":1}}],["难以处理语义关系",{"2":{"565":1}}],["难以有效学习到这些token的表示",{"2":{"562":1}}],["难以分割",{"2":{"446":1}}],["难以展示",{"2":{"399":1}}],["难以训练",{"0":{"255":1,"332":1},"2":{"293":1}}],["难以并行优化",{"2":{"542":1}}],["难以并行",{"0":{"254":1},"2":{"413":1}}],["难以捕捉长距离依赖关系",{"2":{"253":1}}],["难以提取序列的长距离依赖关系",{"2":{"247":1}}],["难以阅读大量文献",{"2":{"235":1}}],["道德绑架",{"2":{"2054":1}}],["道",{"2":{"2054":1}}],["道理很简单",{"2":{"698":1}}],["志之",{"2":{"2054":1}}],["志",{"2":{"2054":1}}],["志同道合",{"2":{"2054":2}}],["志同道合的人",{"2":{"2054":1}}],["志于学",{"2":{"2054":7}}],["朋",{"2":{"2054":7}}],["吾二十有一而志于学",{"2":{"2054":1}}],["晚了至少两年半",{"2":{"2054":1}}],["愠",{"2":{"2054":1}}],["雪儿实习",{"2":{"2054":1}}],["孔子说自己",{"2":{"2054":1}}],["孔某人的低维认知",{"2":{"156":1}}],["孔某人",{"2":{"156":1}}],["嘿嘿",{"2":{"2054":1}}],["广义的学",{"2":{"2054":1}}],["广播",{"2":{"1575":1}}],["习",{"2":{"2054":1}}],["习惯上",{"2":{"1464":1}}],["习惯上卷积运算常用",{"2":{"770":1}}],["圣人",{"2":{"2054":3}}],["言不尽意",{"2":{"2052":1,"2054":1}}],["言有三",{"2":{"361":1}}],["价格合理",{"2":{"2051":1}}],["民大的",{"2":{"2051":1}}],["民主都是基于新皮质中不同的参照系进行定义的",{"2":{"754":1}}],["马上修",{"2":{"2051":1}}],["陪室友去维修他的笔记本",{"2":{"2051":1}}],["论语",{"2":{"2054":1}}],["论语之悟",{"0":{"2052":1},"1":{"2053":1,"2054":1,"2055":1},"2":{"2043":1,"2048":1,"2050":1}}],["论文译文",{"2":{"1333":1}}],["论文原文",{"2":{"1333":1}}],["论文地址",{"2":{"1241":1,"1379":2}}],["论文忽略了导致最终结果的过程",{"2":{"1127":1}}],["论文只讨论了sigmoid",{"2":{"1000":1}}],["论文采用了两种成熟的技术来解决这些挑战",{"2":{"940":1,"959":1}}],["论文采取了两个关键技术来解决技术难点",{"2":{"726":1}}],["论文链接",{"2":{"807":1,"808":1,"809":1,"811":1,"812":1,"816":1,"835":1,"840":1,"844":1,"846":1,"911":1,"938":1,"946":1,"947":1,"949":1,"954":1,"955":1,"958":1,"968":1,"973":1,"974":1,"979":1,"1000":1,"1001":1,"1242":1,"1318":1,"1340":1,"1360":1,"1368":1,"1381":1,"1382":1,"1383":1,"1399":2,"1400":2,"1401":2,"1402":2}}],["论文经过实验发现对超参数",{"2":{"765":1}}],["论文质疑将符号",{"2":{"764":1}}],["论文认为主要目标是避免将注意力矩阵读取和写入到hbm",{"2":{"940":1,"959":1}}],["论文认为存在两个问题",{"2":{"764":1}}],["论文认为这种网络效果肯定不会比",{"2":{"301":1}}],["论文分享|arxiv2024",{"2":{"740":1}}],["论文分析确定了造成这种不足的两个主要原因",{"2":{"212":1}}],["论文又通过聚类分析和相关性分析",{"2":{"739":1}}],["论文参考cot和icl方式",{"2":{"736":1}}],["论文没有使用llm2vec和grit的技巧",{"2":{"735":1}}],["论文题目提到了表示论",{"2":{"713":1}}],["论文给出的生成embedding的llm如下图所示",{"2":{"711":1}}],["论文结论是",{"2":{"692":1}}],["论文对比了标准注意力机制和新提出的pattention机制",{"2":{"621":1}}],["论文用另一个token",{"2":{"620":1}}],["论文用这种glu变体替换ffn中的第一层全连接和激活函数",{"2":{"103":1}}],["论文使用3阶段训练策略",{"2":{"726":1}}],["论文使用下标",{"2":{"614":1}}],["论文使用的三种patch函数如下",{"2":{"613":1}}],["论文使用bpe将频繁出现的子词合并为单个单元",{"2":{"595":1}}],["论文使用了仅解码器",{"2":{"501":1}}],["论文阅读",{"2":{"513":1}}],["论文阅读笔记",{"2":{"156":1}}],["论文揭示了prompt的真正潜力",{"2":{"504":1}}],["论文在mlp层之后",{"2":{"735":1}}],["论文在理论和技术层面上的主要贡献是",{"2":{"504":1}}],["论文在全连接层中得出一个封闭形式的解",{"2":{"145":1}}],["论文还探讨了通过初始化预训练的基于token的模型",{"2":{"611":1}}],["论文还为自注意机制引入了一个更简单好用的替代模型",{"2":{"499":1}}],["论文还提出了一种新的思路",{"2":{"148":1}}],["论文通过找常微分方程更好的数值解的方法",{"2":{"498":1}}],["论文成功地用邻接法在在残差层中没有内存消耗的情况下对非常深的resnets进行微调",{"2":{"497":1}}],["论文首次发表于1997年",{"2":{"862":1}}],["论文首次系统性地分析了注意力掩码对transformer中秩崩溃现象的影响",{"2":{"92":1}}],["论文首先量化resnets的隐藏层状态轨迹与其对应的neural",{"2":{"497":1}}],["论文从计算角度探讨了星形胶质细胞在大脑中发挥的作用",{"2":{"487":1}}],["论文进一步论证",{"2":{"480":1}}],["论文得出结论",{"2":{"477":1}}],["论文希望克服rnn不能并行的缺点",{"2":{"434":1}}],["论文希望解决rnn在序列长距离上的限制",{"2":{"434":1}}],["论文也提出3",{"2":{"561":1}}],["论文也提出了环自注意力",{"2":{"420":1}}],["论文也参考",{"2":{"501":1}}],["论文也给了一个计算的模型",{"2":{"185":1}}],["论文以最为简单的线性不可分的异或数据为例",{"2":{"320":1}}],["论文选取一个准确率饱和的浅层",{"2":{"301":1}}],["论文判断是因为神经网络并不容易学习恒等映射",{"2":{"299":1}}],["论文实现",{"0":{"330":1},"2":{"293":1}}],["论文v2",{"0":{"302":1},"2":{"293":1}}],["论文v1",{"0":{"301":1},"2":{"293":1}}],["论文self",{"2":{"289":1}}],["论文qkv",{"2":{"288":1}}],["论文引入intra",{"2":{"287":1}}],["论文引入了一种新的架构",{"2":{"226":1}}],["论文设计了memory",{"2":{"287":1}}],["论文recurrent",{"2":{"281":1}}],["论文速览",{"2":{"233":1}}],["论文提出将query",{"2":{"926":1}}],["论文提出可以使用无记忆",{"2":{"497":1}}],["论文提出了一种名为flashattention的新型注意力算法",{"2":{"940":1,"959":1}}],["论文提出了一种称为",{"2":{"485":1}}],["论文提出了一种称为动态tanh",{"2":{"359":1}}],["论文提出了三种不同的泰坦变体来有效地将记忆融合到该系统架构中",{"2":{"229":1}}],["论文提出了泰坦架构",{"2":{"229":1}}],["论文提到",{"2":{"88":1}}],["论文探索了不同的记忆结构",{"2":{"228":1}}],["论文指出",{"2":{"226":1,"305":1,"480":1,"496":1}}],["论文共设计了4种细化机制来计算注意力权重之间的依赖关系",{"2":{"209":1}}],["论文是这样解释其设计思路",{"2":{"160":1}}],["论文解读",{"2":{"156":1,"233":1}}],["论文笔记",{"2":{"156":1}}],["论文主要观点如下",{"2":{"155":1}}],["论文主要关注如何将这些梯度信息有效地应用于模型的知识更新与编辑中",{"2":{"148":1}}],["论文主要关注了one",{"2":{"122":1}}],["论文将所有",{"2":{"739":1}}],["论文将抽象层次限制为2种",{"2":{"628":1}}],["论文将",{"2":{"320":1}}],["论文将w",{"2":{"145":1}}],["论文将每个事实表示为一个知识三元组",{"2":{"145":1}}],["论文的研究团队观察到",{"2":{"618":1}}],["论文的研究表明",{"2":{"507":1}}],["论文的主要贡献是",{"2":{"611":1}}],["论文的具体结论如下",{"2":{"561":1}}],["论文的模型只关注",{"2":{"499":1}}],["论文的3",{"2":{"419":1}}],["论文的探索始于一个观察",{"2":{"359":1}}],["论文的注意力计算过程如下",{"2":{"289":1}}],["论文的动机如下",{"2":{"287":1}}],["论文的目标是将它的",{"2":{"137":1}}],["论文的架构图中",{"2":{"25":1}}],["论文针对两个问题的回答如下",{"2":{"136":1}}],["论文做了一些实验",{"2":{"130":1}}],["论文glu",{"2":{"105":1}}],["论文中还试过以",{"2":{"1344":1}}],["论文中指出",{"2":{"1342":1}}],["论文中指出可能会把多个非线性层权重设置为接近于0",{"2":{"301":1}}],["论文中最先提出来的概念",{"2":{"775":1}}],["论文中基于",{"2":{"735":1}}],["论文中的详细架构图展示了tokenformer的完整设计",{"2":{"620":1}}],["论文中的一些有趣的结论如下",{"2":{"137":1}}],["论文中提出的关键创新包括",{"2":{"617":1}}],["论文中设置n",{"2":{"517":1}}],["论文中",{"2":{"445":1}}],["论文中对这三种注意力作用的解释如下图所示",{"2":{"441":1}}],["论文中也称之为球面投影",{"2":{"320":1}}],["论文中也阐述了这一点",{"2":{"301":1}}],["论文中给出的原因是基于效率和建模能力的考虑",{"2":{"175":1}}],["论文中给这个",{"2":{"101":1}}],["论文中h=8",{"2":{"7":1}}],["论文作者对这四项做了可视化",{"2":{"764":1}}],["论文作者为通用中文嵌入训练策划了最大的数据集c",{"2":{"725":1}}],["论文作者为了解决注意力噪音问题",{"2":{"500":1}}],["论文作者认为",{"2":{"628":1}}],["论文作者提出了retraomae",{"2":{"727":1}}],["论文作者提出了tokenformer",{"2":{"618":1}}],["论文作者提出了一种动态",{"2":{"612":1}}],["论文作者提出了字节级子词bbpe",{"2":{"606":1}}],["论文作者提出了把",{"2":{"498":1}}],["论文作者提出了归一化",{"2":{"351":1}}],["论文作者也指出",{"2":{"498":1}}],["论文作者表明",{"2":{"489":1}}],["论文作者从计算角度探讨了星形胶质细胞在大脑中发挥的作用",{"2":{"489":1}}],["论文作者假设星形胶质细胞可以在",{"2":{"488":1}}],["论文作者发现这些信息代表了token",{"2":{"485":1}}],["论文作者发现bert无法对所有的依存关系有比较好的处理",{"2":{"20":1}}],["论文作者将原始模型的输出划分成两部分",{"2":{"288":1}}],["论文作者把这种通用架构叫做任务模型",{"2":{"263":1}}],["论文作者通过将logit",{"2":{"148":1}}],["论文作者针对移去尾部词和移去头部词的效果做了实验",{"2":{"127":1}}],["论文作者标注了一批键值对应的句子",{"2":{"127":1}}],["论文作者首先展示一个负面结果",{"2":{"94":1}}],["论文证明了在准强连通图的情况下",{"2":{"92":1}}],["论文",{"2":{"18":1,"19":1,"20":2,"41":1,"42":1,"43":1,"89":1,"91":1,"106":1,"115":1,"122":3,"125":1,"130":1,"131":1,"134":1,"136":1,"137":1,"143":2,"144":1,"147":1,"148":1,"151":1,"154":1,"155":1,"175":1,"185":1,"209":1,"210":1,"211":1,"218":1,"226":1,"232":1,"260":1,"263":1,"282":1,"283":1,"284":1,"285":1,"290":1,"298":1,"305":1,"306":1,"314":1,"316":1,"320":2,"330":1,"348":1,"349":1,"350":1,"358":1,"393":1,"437":3,"446":2,"477":1,"480":1,"483":1,"487":1,"490":1,"493":1,"497":1,"498":1,"499":1,"500":1,"504":1,"507":1,"542":1,"560":1,"561":1,"562":1,"605":1,"610":1,"616":1,"625":1,"637":1,"692":1,"698":1,"711":1,"736":1,"737":1,"842":1,"843":2,"845":2,"892":1,"935":1,"937":1,"938":1,"951":1,"953":1,"954":1,"1036":1,"1046":1,"1054":1,"1059":1,"1064":1,"1067":1,"1159":1,"1243":1,"1244":1,"1341":1}}],["审核中",{"2":{"2043":1}}],["美化主页和导航",{"2":{"2043":1}}],["美国国家工程院院士杰夫",{"2":{"754":1}}],["冥想锻炼",{"2":{"2039":1}}],["©",{"2":{"2034":1}}],["版权所有",{"2":{"2034":1}}],["版本号",{"2":{"1208":1}}],["版本的对应关系",{"2":{"792":1}}],["站长随笔",{"0":{"2109":1}}],["站点",{"0":{"2035":1}}],["站点大纲👀",{"0":{"2038":1}}],["站点大纲",{"0":{"2034":1}}],["站在巨人的肩膀上",{"2":{"1313":1}}],["站在单字视角很难获得一个有价值的语义信息",{"2":{"566":1}}],["横纵坐标定位一个点",{"2":{"2016":1}}],["横向或纵向",{"2":{"1596":1}}],["影视",{"2":{"2011":1}}],["影响可读性",{"2":{"1631":1}}],["影响力",{"2":{"844":1}}],["影响其泛化能力",{"2":{"562":1}}],["影响模型效果",{"2":{"288":1}}],["影响全局概率值",{"2":{"54":1}}],["培养创造力",{"2":{"2010":1}}],["医疗成像",{"2":{"2010":1}}],["碰撞检测",{"2":{"2009":1}}],["火焰",{"2":{"2009":1}}],["粒子系统和烟雾",{"2":{"2009":1}}],["刚体和流体仿真",{"2":{"2009":1}}],["刚开始需要摸索入门",{"2":{"402":1}}],["着色器语言",{"2":{"2009":1}}],["着就会导致loss的收敛很缓慢",{"2":{"991":1}}],["阴影等复杂效果",{"2":{"2009":1}}],["纹理映射",{"2":{"2009":1}}],["镜面反射等",{"2":{"2009":1}}],["漫反射",{"2":{"2009":1}}],["照明与着色",{"2":{"2009":1}}],["曲面造型等非常重要",{"2":{"2009":1}}],["曲面等",{"2":{"2009":1}}],["曲线的变化",{"2":{"2009":1}}],["曲线",{"2":{"2009":1}}],["坐标点绘制",{"2":{"2018":1}}],["坐标点等",{"2":{"1728":1}}],["坐标系统",{"2":{"2009":1}}],["└──",{"2":{"1997":2,"1999":2}}],["│",{"2":{"1997":6,"1999":4}}],["├──",{"2":{"1997":7,"1999":5}}],["贡献开源项目",{"2":{"1961":1}}],["贡献者许可协议",{"0":{"1197":1}}],["职业发展建议",{"0":{"1961":1}}],["协程等新特性逐步落地",{"2":{"1960":1}}],["协作开发",{"2":{"1916":1}}],["达索系统",{"2":{"1959":1}}],["达到某一性能目标所需的步数通常会减少",{"2":{"1133":1}}],["达到压缩维度的效果",{"2":{"19":1}}],["西门子工业软件",{"2":{"1959":1}}],["腾讯teg",{"2":{"1954":1}}],["腾讯互娱",{"2":{"1939":1}}],["蚂蚁oceanbase",{"2":{"1954":1}}],["云原生技术栈集成",{"2":{"1952":1}}],["云服务",{"2":{"1499":1}}],["九坤投资",{"2":{"1949":1}}],["幻方量化",{"2":{"1949":1}}],["幻觉等",{"2":{"474":1}}],["驱动开发工程师",{"2":{"1943":1}}],["芯片设计工具链",{"2":{"1956":1}}],["芯片级底层开发",{"2":{"1941":1}}],["芯片上",{"2":{"944":1,"963":1}}],["育碧",{"2":{"1939":1}}],["逆水寒团队",{"2":{"1939":1}}],["逆双曲余弦",{"2":{"1087":1}}],["米哈游",{"2":{"1939":1}}],["米田嵌入采用对象的所有关系来表征该对象",{"2":{"472":1}}],["客户端核心开发工程师",{"2":{"1938":1}}],["客户端开发",{"2":{"1936":1}}],["客体",{"2":{"145":1}}],["折射",{"2":{"2009":1}}],["折叠表达式",{"2":{"1912":1}}],["折中的更新速度",{"2":{"1027":1}}],["泛型lambda",{"2":{"1913":2}}],["泛型",{"0":{"1906":1},"2":{"1904":1,"1906":1}}],["泛化性能更好",{"2":{"542":1}}],["泛化能力强",{"2":{"1":1}}],["储蓄账户",{"2":{"1873":1}}],["储存的模式来源于训练数据",{"2":{"126":1}}],["账户",{"2":{"1873":1}}],["账户余额",{"2":{"1677":1}}],["授课目标",{"2":{"1846":1}}],["恢复默认网络配置",{"0":{"2094":1}}],["恢复到十进制",{"2":{"1817":1,"1835":1}}],["恢复它",{"2":{"1173":1}}],["八进制",{"2":{"1817":1,"1835":1}}],["八仙过海",{"2":{"746":1}}],["屏幕等",{"2":{"1810":1,"1828":1}}],["乃至嵌套的",{"2":{"1797":1}}],["乃至整篇文章的核心概念是什么",{"2":{"627":1}}],["破坏了类的封装性",{"2":{"1776":1}}],["友元可以访问类的私有和受保护成员",{"2":{"1793":1}}],["友元访问权限只限于子类中父类部分私有和受保护成员",{"2":{"1783":1}}],["友元关系是单向的",{"2":{"1793":1}}],["友元关系不能被继承",{"2":{"1786":1}}],["友元关系不具有对称性",{"2":{"1781":1}}],["友元关系的特性",{"0":{"1781":1},"2":{"1768":1}}],["友元分为友元函数和友元类",{"2":{"1769":1}}],["友元",{"2":{"1769":1}}],["友元运算符重载",{"0":{"1789":1},"2":{"1768":1}}],["友元作为运算符重载函数",{"0":{"1788":1},"2":{"1768":1}}],["友元与继承的关系",{"0":{"1786":1},"2":{"1768":1}}],["友元继承示例",{"0":{"1784":1},"2":{"1768":1}}],["友元的访问权限继承",{"0":{"1783":1},"2":{"1768":1}}],["友元的继承",{"0":{"1782":1},"1":{"1783":1,"1784":1,"1785":1,"1786":1},"2":{"1768":1}}],["友元类强调",{"0":{"1780":1},"1":{"1781":1}}],["友元类拥有被友元声明类的所有访问权限",{"2":{"1779":1}}],["友元类的声明方法如下",{"2":{"1778":1}}],["友元类的特性与示例",{"0":{"1779":1},"2":{"1768":1}}],["友元类是一个类",{"2":{"1778":1}}],["友元类声明",{"2":{"1770":1}}],["友元类",{"0":{"1777":1},"1":{"1778":1,"1779":1,"1780":1,"1781":1},"2":{"1768":1,"1769":1}}],["友元函数在重载运算符",{"2":{"1793":1}}],["友元函数和友元类都是通过",{"2":{"1793":1}}],["友元函数和友元类的具体应用在接下来的部分详细讲解",{"2":{"1770":1}}],["友元函数通常被集中声明",{"2":{"1775":1}}],["友元函数不是该类的成员函数",{"2":{"1775":1}}],["友元函数要点",{"0":{"1775":1}}],["友元函数定义",{"2":{"1774":1}}],["友元函数并不是类的成员函数",{"2":{"1774":1}}],["友元函数拥有类内所有权限",{"2":{"1773":1}}],["友元函数可以是类的成员函数或者非成员函数",{"2":{"1772":1}}],["友元函数声明",{"2":{"1770":1,"1774":1}}],["友元函数的声明可以在类的任意访问权限区域内声明",{"2":{"1773":1}}],["友元函数的优缺点",{"0":{"1776":1},"2":{"1768":1}}],["友元函数的特性与示例",{"0":{"1774":1},"2":{"1768":1}}],["友元函数的权限",{"0":{"1773":1},"2":{"1768":1}}],["友元函数",{"0":{"1771":1},"1":{"1772":1,"1773":1,"1774":1,"1775":1,"1776":1},"2":{"1768":1,"1769":1}}],["友元函数重载",{"2":{"1712":1,"1789":2}}],["友元简介",{"0":{"1769":1},"2":{"1768":1}}],["友元及友元相关内容",{"0":{"1767":1},"1":{"1768":1,"1769":1,"1770":1,"1771":1,"1772":1,"1773":1,"1774":1,"1775":1,"1776":1,"1777":1,"1778":1,"1779":1,"1780":1,"1781":1,"1782":1,"1783":1,"1784":1,"1785":1,"1786":1,"1787":1,"1788":1,"1789":1,"1790":1,"1791":1,"1792":1,"1793":1}}],["抛出和捕获特定类型的异常",{"2":{"1764":1}}],["抛出异常对象",{"2":{"1762":1}}],["抛弃冯诺依曼架构",{"2":{"206":1}}],["偶数减5",{"2":{"1759":1}}],["闰年判断逻辑",{"2":{"1729":1}}],["闰年的判断规则",{"2":{"1729":1}}],["房间",{"2":{"1728":1}}],["联合体不能包含带有构造函数或析构函数的类类型的成员",{"2":{"1728":1}}],["联合体常常与结构体结合使用",{"2":{"1728":1}}],["联合体的应用场景",{"2":{"1728":1}}],["联合体的大小由其最大的成员的大小决定",{"2":{"1728":1}}],["联合体的所有成员共享同一块内存",{"2":{"1728":1}}],["联合体是一种特殊的数据类型",{"2":{"1728":1}}],["联合体",{"2":{"1728":1}}],["联系自已已有的其他知识体系",{"2":{"1597":1}}],["游戏引擎架构师",{"2":{"1938":1}}],["游戏引擎开发",{"2":{"1936":1}}],["游戏角色的不同职业等",{"2":{"1728":1}}],["游戏开发领域",{"0":{"1935":1},"1":{"1936":1,"1937":1,"1938":1,"1939":1}}],["游戏开发",{"2":{"1602":1,"2010":1}}],["商品",{"2":{"1728":1}}],["约",{"2":{"1728":3,"1729":4}}],["透彻理解函数作为代码组织和复用的基本单元",{"2":{"1727":1}}],["透过网络相互连接传递消息",{"2":{"1563":1}}],["良好的编程习惯",{"2":{"1714":1}}],["良好的编程习惯是使用",{"2":{"1647":1}}],["追求每一步的",{"2":{"2101":1}}],["追加字符串",{"2":{"1713":1}}],["追加",{"2":{"1713":1}}],["追溯了近年来语言模型的发展",{"2":{"540":1}}],["拷贝构造",{"2":{"1887":1}}],["拷贝构造函数",{"2":{"1675":1,"1713":1,"1887":1}}],["拷贝初始化",{"2":{"1713":1}}],["压栈",{"2":{"1709":1}}],["压缩",{"2":{"957":2,"1535":1}}],["压缩维度与解压维度",{"2":{"823":1}}],["压缩率提高了10倍",{"2":{"637":1}}],["压缩率很高",{"2":{"580":1}}],["压缩h",{"2":{"341":1}}],["面对一项复杂的工作任务时",{"2":{"2104":1}}],["面积2",{"2":{"1708":2}}],["面积1",{"2":{"1708":1}}],["面向小白",{"2":{"2121":1}}],["面向对象编程",{"2":{"1491":1}}],["面向大语言模型的知识编辑",{"2":{"156":2}}],["面向应用层读者",{"2":{"156":1}}],["面向",{"2":{"141":1}}],["留一个位置给",{"2":{"1704":1}}],["留一颗小星星",{"2":{"1196":1}}],["码医森",{"0":{"2032":1},"1":{"2033":1,"2034":1}}],["码为",{"2":{"1704":2}}],["码表中的字符",{"2":{"1607":1}}],["括起来的一系列字符",{"2":{"1704":1}}],["弱",{"2":{"1695":1}}],["裸指针",{"2":{"1695":2}}],["节省时间",{"2":{"2106":1}}],["节省内存空间",{"2":{"1728":1}}],["节",{"0":{"1767":1,"2057":1},"1":{"1768":1,"1769":1,"1770":1,"1771":1,"1772":1,"1773":1,"1774":1,"1775":1,"1776":1,"1777":1,"1778":1,"1779":1,"1780":1,"1781":1,"1782":1,"1783":1,"1784":1,"1785":1,"1786":1,"1787":1,"1788":1,"1789":1,"1790":1,"1791":1,"1792":1,"1793":1,"2058":1,"2059":1,"2060":1,"2061":1,"2062":1}}],["节课",{"0":{"1692":1,"1697":1},"1":{"1693":1,"1694":1,"1695":1,"1698":1,"1699":1,"1700":1,"1701":1}}],["节点",{"2":{"986":1,"1563":1}}],["节点根据其状态进行颜色编码",{"2":{"986":1}}],["节点它的",{"2":{"661":1,"1104":1}}],["节点之间的边是学习到的",{"2":{"201":1}}],["鸟叫声为",{"2":{"1690":1}}],["鸟",{"2":{"1690":1}}],["鸟等",{"2":{"1689":1}}],["猫",{"2":{"1690":1}}],["狗叫声为",{"2":{"1690":1}}],["狗",{"2":{"1689":1,"1690":1}}],["触发析构函数",{"2":{"1676":1}}],["触发反向求导操作",{"2":{"1107":1}}],["喵",{"2":{"1675":1}}],["冒号后面的部分是初始化列表",{"2":{"1674":1}}],["汪汪",{"2":{"1674":1}}],["岁了",{"2":{"1674":1,"1675":1}}],["岁",{"2":{"1673":1}}],["谨慎使用原始指针",{"2":{"1672":1}}],["立即调用lambda计算平方",{"2":{"1883":1}}],["立即将指针设置为",{"2":{"1672":1}}],["立刻",{"2":{"683":1}}],["立刻和一块v进行矩阵矩阵乘法运算",{"2":{"180":1}}],["务必检查它是否为",{"2":{"1672":1}}],["务必确保递归函数有一个明确的终止条件",{"2":{"1646":1}}],["务必确保循环体内有能够最终使条件表达式为假的操作",{"2":{"1620":1}}],["养成良好的编程习惯",{"2":{"1671":1}}],["谁释放",{"2":{"1671":1}}],["谁分配",{"2":{"1671":1}}],["谁创作了",{"2":{"613":1}}],["泄漏的内存由操作系统回收",{"2":{"1671":1}}],["泄漏的内存会越来越多",{"2":{"1647":1,"1671":1}}],["仔细检查数组的索引范围",{"2":{"1670":1}}],["熟悉常见的算法函数及其用途",{"2":{"1731":1}}],["熟悉了",{"2":{"1678":1}}],["熟悉",{"2":{"1666":1,"1937":1,"1997":1}}],["熟练掌握",{"2":{"1666":1}}],["轮子数量和排水量并进行初始化",{"2":{"1664":1}}],["轮子数量",{"2":{"1664":1}}],["轮是最实用的",{"2":{"1157":1}}],["陆地交通工具",{"2":{"1664":1}}],["覆盖其他变量或数据结构的内容",{"2":{"1670":1}}],["覆盖发生在不同的作用域",{"2":{"1663":1}}],["覆盖",{"2":{"1663":3}}],["覆盖了原来的值",{"2":{"661":1,"1104":1}}],["菱形继承存在的问题",{"2":{"1661":1}}],["菱形继承指的是一个基类被两个或多个子类继承",{"2":{"1661":1}}],["菱形继承",{"0":{"1661":1},"2":{"1658":1,"1661":1}}],["销售额",{"2":{"1657":2}}],["销售员类",{"2":{"1657":1}}],["员工基础薪水",{"2":{"1657":2}}],["员工类",{"2":{"1657":1}}],["阻止该函数被重写",{"2":{"1656":1}}],["私有属性",{"2":{"1873":1}}],["私有成员变量",{"2":{"1677":1}}],["私有成员只能在类的内部访问",{"2":{"1677":1}}],["私有成员",{"2":{"1674":1}}],["私有继承和保护继承会缩小继承成员的访问权限",{"2":{"1655":1}}],["私有继承",{"2":{"1654":1}}],["私有方法",{"0":{"1086":1}}],["父类所有成员在子类中都变成私有权限",{"2":{"1654":1}}],["父类成员的访问权限在子类中保持不变",{"2":{"1654":1}}],["父类的私有成员在子类中无法访问",{"2":{"1654":1}}],["父类指针或引用转换为子类指针或引用",{"2":{"1629":1}}],["父类指针到子类指针的转换",{"2":{"1629":1}}],["斐波那契数列定义",{"2":{"1651":1}}],["斐波那契数列第",{"2":{"1646":1}}],["入栈",{"2":{"1648":1}}],["栈展开",{"2":{"1762":1}}],["栈溢出",{"2":{"1648":1}}],["栈溢出和堆溢出",{"2":{"1648":1}}],["栈溢出的风险",{"2":{"1646":1}}],["栈区和堆区的内存分配方式",{"2":{"1648":1}}],["栈区和堆区的区别",{"2":{"1648":1}}],["栈区和堆区是",{"2":{"1648":1}}],["栈区变量",{"2":{"1648":3}}],["栈区的内存由编译器自动管理",{"2":{"1648":1}}],["栈区",{"2":{"1648":3,"1714":1}}],["悬挂指针指向的内存已经被释放",{"2":{"1672":1}}],["悬",{"2":{"1647":1}}],["释放现有资源",{"2":{"1887":1}}],["释放动态分配的内存",{"2":{"1716":1}}],["释放动态分配的数组内存",{"2":{"1714":1}}],["释放当前管理的对象",{"2":{"1695":1}}],["释放后置空",{"2":{"1672":2}}],["释放后将指针置空",{"2":{"1647":2}}],["释放了指针所指向的内存后",{"2":{"1647":1}}],["释放",{"2":{"1647":1,"1672":1,"1694":1}}],["释放数组的内存",{"2":{"1647":1}}],["释放数组内存",{"2":{"1647":1,"1668":1}}],["释放单个对象的内存",{"2":{"1647":1,"1668":1}}],["释放内存后",{"2":{"1672":1}}],["释放内存",{"2":{"1647":1,"1694":2,"1714":1}}],["堆溢出",{"2":{"1648":1}}],["堆区变量",{"2":{"1648":4}}],["堆区的大小相对较大",{"2":{"1648":1}}],["堆区",{"2":{"1648":3,"1714":1}}],["堆积",{"2":{"1647":1}}],["堆叠多少次",{"2":{"522":1}}],["堆叠再多层自注意力",{"2":{"115":1}}],["堆叠而成的",{"2":{"517":1}}],["堆叠而成",{"2":{"99":1,"419":1}}],["插件机制",{"2":{"1645":1}}],["插入运算符",{"2":{"1824":1,"1842":1}}],["插入元素",{"2":{"1807":1}}],["插入相同值的元素时",{"2":{"1806":1}}],["插入一个键值对",{"2":{"1725":1}}],["插入等",{"2":{"1713":1}}],["插入换行符",{"2":{"1673":1}}],["插入模式操作",{"0":{"1545":1}}],["插入模式",{"2":{"1519":1,"1541":1}}],["插入",{"2":{"1492":1}}],["插入最佳标签",{"2":{"1330":1}}],["插入和淘汰变得高效",{"2":{"985":1}}],["插入事实",{"2":{"145":1}}],["升序排序后",{"2":{"1645":1}}],["升维把输入的词向量映射到一个更大维度的特征空间",{"2":{"116":1}}],["升维可以提取更多的特征",{"2":{"116":1}}],["升维操作有效扩展了网络的自由度",{"2":{"116":1}}],["升维",{"2":{"101":1,"116":1,"118":1}}],["月份",{"2":{"1642":1}}],["析构函数等",{"2":{"1902":1}}],["析构函数以及公有和私有成员的概念",{"2":{"1678":1}}],["析构函数在对象即将被销毁时自动调用",{"2":{"1676":1}}],["析构函数也是一种特殊的成员函数",{"2":{"1676":1}}],["析构函数",{"0":{"1676":1},"2":{"1637":1,"1674":1,"1694":1,"1887":1}}],["逗号",{"2":{"1635":1}}],["逗号运算符",{"2":{"1630":1}}],["替代方案",{"2":{"1632":1}}],["替换子串",{"2":{"1713":1}}],["替换指定范围内的子串",{"2":{"1713":1}}],["替换为",{"2":{"1604":1,"1632":1}}],["替换为全一掩码矩阵",{"2":{"734":1}}],["替换整个文件中的所有",{"2":{"1554":1}}],["替换当前行所有",{"2":{"1554":1}}],["替换当前行第一个匹配的",{"2":{"1554":1}}],["替换",{"0":{"1554":1},"2":{"1594":1,"1803":1}}],["替换之后",{"2":{"714":1}}],["替换成",{"2":{"713":1,"1016":1}}],["替换掉标准",{"2":{"621":1}}],["替换掉主语末尾token",{"2":{"145":1}}],["替换更新方程",{"2":{"357":1}}],["替换输出",{"2":{"141":1}}],["穿透",{"2":{"1631":1}}],["宏展开后的代码不易调试",{"2":{"1632":1}}],["宏展开可以避免函数调用的开销",{"2":{"1632":1}}],["宏不是变量",{"2":{"1632":1}}],["宏定义的优缺点",{"2":{"1632":1}}],["宏定义的作用域从定义处开始",{"2":{"1632":1}}],["宏定义的特点",{"2":{"1632":1}}],["宏定义仅仅是文本替换",{"2":{"1632":1}}],["宏定义也可以放在头文件中",{"2":{"1628":1}}],["宏定义和常量",{"2":{"1628":1}}],["宏定义",{"0":{"1632":1},"2":{"1628":1}}],["宏观理解所属地位和作用",{"2":{"1596":1}}],["余额不足",{"2":{"1677":1}}],["余数为偶数的数",{"2":{"1625":2}}],["余弦距离",{"2":{"692":1}}],["余弦相似度给出的答案经常与实际情况不符",{"2":{"692":1}}],["余弦相似度已广泛应用于从推荐系统到自然语言处理的各种应用中",{"2":{"692":1}}],["余弦相似度长期以来一直是衡量高维对象之间语义相似度的首选指标",{"2":{"692":1}}],["余弦相似度就没有考虑到不同用户之间评分尺度的差异",{"2":{"692":1}}],["余弦相似度经常被用作抵消高维欧式距离问题",{"2":{"692":1}}],["余弦相似度的值范围是从",{"2":{"692":1}}],["余弦相似度衡量的是两个向量之间的角度差异",{"2":{"692":1}}],["余弦相似度通过测量两个向量的夹角的余弦值来度量它们之间的相似性",{"2":{"692":1}}],["余弦相似度可能没用",{"2":{"233":1,"740":1}}],["余弦相似度",{"2":{"175":3,"689":1,"692":1}}],["剩下时间做截止时间是",{"2":{"2135":1}}],["剩下的关键问题便是如何从记忆中检索信息",{"2":{"230":1}}],["剩余元素将被初始化为",{"2":{"1634":1}}],["剩余的位置会被初始化为",{"2":{"1624":1}}],["剩余的元素将自动初始化为",{"2":{"1623":1}}],["递减语句",{"2":{"1621":3}}],["递增",{"2":{"1621":2}}],["递归",{"2":{"1709":1}}],["递归与迭代的比较",{"2":{"1646":1}}],["递归可能会存在重复计算的问题",{"2":{"1646":1}}],["递归调用",{"2":{"1646":1}}],["递归调用就像这样",{"2":{"1646":1}}],["递归终止条件",{"2":{"1646":1}}],["递归的优缺点",{"2":{"1646":1}}],["递归的递推关系",{"2":{"1646":1}}],["递归的终止条件",{"2":{"1646":1}}],["递归的将所有子模块应用",{"2":{"1214":1}}],["递归的将所有子模块的",{"2":{"1214":1}}],["递归必须有一个明确的结束条件",{"2":{"1646":1}}],["递归删除目录及其内容",{"2":{"1509":1}}],["递归神经网络",{"2":{"850":1,"885":1}}],["递归状态维护和可靠的历史访问",{"2":{"118":1}}],["易读性较差",{"2":{"1619":1}}],["易于理解和维护",{"2":{"1729":1}}],["易于理解和实现",{"2":{"632":1}}],["易于理解",{"2":{"1646":1}}],["易于使用",{"2":{"1479":1}}],["易于学习",{"2":{"1479":1}}],["易于训练",{"2":{"332":1}}],["💡",{"2":{"1612":1}}],["😉",{"2":{"47":1}}],["间接基类会被间接子类重复继承",{"2":{"1661":1}}],["间接访问",{"2":{"1612":1}}],["间接就代表着流失",{"2":{"247":1}}],["拥有父类所有非私有成员",{"2":{"1654":1}}],["拥有不同的作用域和生命周期",{"2":{"1649":1}}],["拥有自己的内存空间",{"2":{"1612":1}}],["拥有多个定义的词",{"2":{"715":1}}],["赋值给指针变量",{"2":{"1704":1}}],["赋值给指针",{"2":{"1668":1}}],["赋值",{"2":{"1635":1,"1680":1,"1729":1}}],["赋值运算符重载",{"2":{"1712":1}}],["赋值运算符",{"2":{"1630":1}}],["赋值运算符会返回左操作数的引用",{"2":{"1629":1}}],["赋值运算符的返回值",{"2":{"1629":1}}],["赋值总结",{"0":{"1629":1}}],["赋值操作会改变引用所指向变量的值",{"2":{"1612":1}}],["赋予",{"2":{"140":1}}],["绰号",{"2":{"1612":1}}],["练习与思考",{"2":{"1645":1,"1646":1,"1647":1,"1648":1,"1649":1,"1650":1}}],["练习",{"2":{"1611":1,"1620":1,"1621":1,"1623":1}}],["✍️",{"2":{"1611":1}}],["寻址和取值",{"2":{"1611":1}}],["寻找与抛出的异常类型匹配的",{"2":{"1762":1}}],["寻找最常出现的字节对",{"2":{"581":1}}],["寻找一个向量",{"2":{"145":1}}],["🤔",{"2":{"1611":1,"1614":1}}],["🤖使用额外的步骤来延长高学习率的训练时间",{"2":{"1159":1}}],["🤖",{"2":{"1130":1,"1133":1,"1148":1,"1184":2}}],["牺牲了部分底层控制权",{"2":{"1611":1}}],["牺牲远程依赖换来的",{"2":{"204":1}}],["错误处理",{"2":{"1930":1}}],["错误码",{"2":{"1764":1}}],["错误信息可能丢失或处理不当",{"2":{"1761":1}}],["错误信息难以向上层传递",{"2":{"1761":1}}],["错误示例",{"2":{"1669":1}}],["错误",{"2":{"1614":4,"1640":1,"1655":1,"1656":1,"1669":2,"1670":1,"1683":2,"1685":1,"1687":1,"1693":1,"1695":1,"1729":1,"1811":1,"1829":1,"1853":1,"1857":2,"1861":2,"1911":1}}],["错误使用指针可能导致程序崩溃或产生不可预测的行为",{"2":{"1611":1}}],["错误的搜索空间可能会导致自欺欺人",{"2":{"1156":1}}],["摄氏转华氏",{"2":{"1608":3}}],["摄氏度",{"2":{"1608":4}}],["摄氏温度",{"2":{"1608":1}}],["华大九天",{"2":{"1959":1}}],["华泰证券",{"2":{"1949":1}}],["华为2012实验室",{"2":{"1944":1}}],["华为提出存储代替计算的transformer新架构",{"2":{"156":1}}],["华氏度",{"2":{"1608":1}}],["华氏转摄氏",{"2":{"1608":3}}],["华氏温度",{"2":{"1608":1}}],["声明与定义",{"2":{"1916":1}}],["声明函数",{"2":{"1916":1}}],["声明格式",{"2":{"1797":1,"1805":1,"1806":1,"1807":1}}],["声明友元函数",{"2":{"1772":1}}],["声明不抛出异常的函数",{"2":{"1764":1}}],["声明枚举类型的变量与声明其他类型的变量类似",{"2":{"1728":1}}],["声明并定义",{"2":{"1923":1}}],["声明并初始化",{"2":{"1714":1}}],["声明并赋值一个计数器",{"2":{"1621":1}}],["声明和定义",{"2":{"1709":1}}],["声明模板参数列表",{"2":{"1699":1,"1700":1,"1701":1}}],["声明它的源文件",{"2":{"1649":1}}],["声明了一个指向返回值为",{"2":{"1645":1}}],["声明了一个名为",{"2":{"1611":1}}],["声明静态成员函数",{"2":{"1639":1}}],["声明静态成员变量",{"2":{"1639":1}}],["声明方式",{"2":{"1639":2,"1640":2}}],["声明而非定义",{"2":{"1628":1}}],["声明字符数组来存储字符串时",{"2":{"1624":1}}],["声明数组需要指定数组的类型",{"2":{"1623":1}}],["声明常量成员函数",{"2":{"1640":1}}],["声明常量成员变量",{"2":{"1640":1}}],["声明常量",{"2":{"1613":1}}],["声明",{"2":{"1613":2,"1614":3,"1628":1,"1693":1,"1699":1,"1700":1,"1701":1,"1916":2,"1923":1}}],["声明引用",{"2":{"1612":1}}],["声明指针变量",{"2":{"1611":1}}],["声明一个包含",{"2":{"1714":1}}],["声明一个指向返回",{"2":{"1706":1}}],["声明一个可以存储",{"2":{"1623":2}}],["声明一个",{"2":{"1611":1,"1728":1}}],["声明一个布尔类型变量",{"2":{"1607":1}}],["声明一个单精度浮点型变量",{"2":{"1607":1}}],["声明一个字符型变量",{"2":{"1607":1}}],["声明一个整型变量",{"2":{"1607":1,"1729":1}}],["声明变量并赋值",{"2":{"1607":1}}],["声名远播的无监督预训练模型",{"2":{"844":1}}],["零和正数",{"2":{"1607":1}}],["零中心激活函数并非在所有情况下都具有更好的性能",{"2":{"838":1}}],["零中心激活函数具有对称性",{"2":{"838":1}}],["零中心激活函数可以更好地捕捉数据中的不同特征",{"2":{"838":1}}],["零中心激活函数可以更好地支持梯度传播",{"2":{"838":1}}],["零中心激活函数可以提供更好的网络表示能力",{"2":{"838":1}}],["零中心激活函数在某些情况下可能具有一些优势",{"2":{"838":1}}],["浮点型",{"2":{"1607":1}}],["浮点数",{"2":{"762":1,"1728":1}}],["浮点数或者向量",{"2":{"545":1}}],["布尔类型",{"2":{"1607":1}}],["布局一致",{"2":{"501":1}}],["迈出",{"0":{"1606":1}}],["迈向单义性",{"2":{"156":1}}],["迈向nlu",{"2":{"156":1}}],["汇编器的输出结果是一个或多个目标文件",{"2":{"1604":1}}],["汇编",{"2":{"1604":1}}],["汇编语言是一种更接近机器语言的低级语言",{"2":{"1604":1}}],["汇编语言",{"2":{"1603":1}}],["垃圾回收机制",{"2":{"1602":1}}],["扬长避短",{"0":{"1602":1}}],["市面上重要的信息很多",{"2":{"1598":1}}],["遇到过两次",{"2":{"2054":1}}],["遇到",{"2":{"1813":2,"1831":2}}],["遇到换行符或文件结束符停止",{"2":{"1813":2,"1831":2}}],["遇到一个新词",{"2":{"594":1}}],["遇事不决",{"0":{"1597":1}}],["扫一遍",{"2":{"1584":1}}],["散播",{"2":{"1573":1}}],["散度",{"2":{"145":1}}],["兼具",{"2":{"1800":1}}],["兼顾开发效率和执行效率",{"2":{"1603":1}}],["兼顾scale",{"2":{"725":1}}],["兼容mpi标准",{"2":{"1569":1}}],["爬取url的函数",{"2":{"1566":1}}],["垂直分屏",{"2":{"1557":1}}],["抓取内容",{"2":{"1528":1}}],["抓取网页内容",{"0":{"1528":1}}],["抓取log",{"0":{"1277":1},"1":{"1278":1,"1279":1,"1280":1}}],["撤销上一步操作",{"2":{"1550":1}}],["撤销",{"2":{"1520":1}}],["撤销与重做",{"0":{"1550":1},"2":{"1520":1}}],["粘贴到光标后",{"2":{"1520":1}}],["启动docker的doccano",{"0":{"2066":1}}],["启动代码模块的执行",{"2":{"1729":1}}],["启动和退出",{"0":{"1542":1},"1":{"1543":1,"1544":1}}],["启动",{"0":{"1543":1},"2":{"1518":1,"1605":1}}],["启发式搜索的方法来处理",{"2":{"908":1}}],["权限管理",{"2":{"1930":1}}],["权限用于保护成员",{"2":{"1655":1}}],["权限表示",{"0":{"1512":1}}],["权重的梯度什么时候计算的",{"2":{"1443":1}}],["权重的梯度吗",{"2":{"1441":1}}],["权重的梯度",{"2":{"1441":1}}],["权重的产生随着输入变化",{"2":{"276":1}}],["权重更新",{"0":{"1395":1},"2":{"1215":1,"1441":1}}],["权重初始化",{"2":{"1202":1}}],["权重初始化是神经网络训练的重要步骤之一",{"2":{"403":1}}],["权重衰减",{"2":{"1149":1,"1221":1}}],["权重衰减强度的最佳值通常取决于模型大小",{"2":{"1143":1}}],["权重进行缩放",{"2":{"1086":2}}],["权重系数的计算",{"2":{"922":1}}],["权重分配",{"2":{"536":1}}],["权重就越接近于",{"2":{"332":1}}],["权重共享",{"2":{"250":1,"772":1}}],["权重来自p",{"2":{"198":1}}],["权重越大表示投射更多的注意力在对应的值上",{"2":{"169":1}}],["权重向量",{"2":{"163":1}}],["权重量化以及重新设计注意力机制",{"2":{"152":1}}],["权重",{"2":{"145":1,"158":1,"172":1,"1441":1}}],["权重形状",{"2":{"113":1}}],["权重矩阵wt",{"2":{"172":1}}],["权重矩阵",{"0":{"9":1,"172":1},"2":{"0":1,"157":1}}],["临时文件目录",{"2":{"1506":1}}],["临界区指的是一个访问共用资源",{"2":{"1413":1}}],["临界区",{"0":{"1413":1},"2":{"1425":1}}],["临界区外的进程阻塞其他的进程",{"2":{"1412":1}}],["临界资源",{"0":{"1412":1},"2":{"1412":1,"1425":1}}],["临界资源是啥",{"0":{"1410":1},"1":{"1411":1,"1412":1,"1413":1}}],["观察两者的用时表现",{"2":{"1702":1}}],["观察对象在复制",{"2":{"1680":1}}],["观察输出结果",{"2":{"1664":1}}],["观察者等",{"2":{"1500":1}}],["观看过程中",{"2":{"5":1}}],["监控工具",{"2":{"1499":1}}],["部门等",{"2":{"1657":1}}],["部署",{"2":{"1499":1}}],["部署与监控",{"0":{"1499":1}}],["部分封闭设计",{"0":{"1867":1}}],["部分初始化",{"2":{"1623":1}}],["部分高性能库和框架使用",{"2":{"1602":1}}],["部分的作用就是接受encoder",{"2":{"889":1}}],["部分原因在于其训练数据中包含了各种范畴的信息",{"2":{"505":1}}],["部分",{"0":{"888":1,"889":1},"2":{"84":2,"473":1,"522":1,"541":1,"888":1}}],["订阅模式",{"2":{"1498":1}}],["封装一个",{"0":{"1680":1}}],["封装性",{"2":{"1677":1}}],["封装数据和行为",{"0":{"1674":1}}],["封装",{"2":{"1491":1}}],["欢迎来到ethan的的随笔天地",{"2":{"2109":1}}],["欢迎来到我们的第三节",{"2":{"1610":1}}],["欢迎来到",{"0":{"1601":1},"2":{"1490":1}}],["欢迎在github的讨论区提出相关问题",{"2":{"1196":1}}],["删",{"2":{"1486":1}}],["删除文件和目录",{"2":{"1930":1}}],["删除文件",{"2":{"1930":1}}],["删除键为",{"2":{"1725":2}}],["删除值为",{"2":{"1724":2}}],["删除队列开头的元素",{"2":{"1723":1}}],["删除所有值为",{"2":{"1720":1,"1721":1}}],["删除指定迭代器位置之后的元素",{"2":{"1721":1}}],["删除指定迭代器位置的元素",{"2":{"1719":1,"1720":1,"1722":1,"1724":1,"1725":1}}],["删除指定位置或范围内的字符",{"2":{"1713":1}}],["删除最后一个元素",{"2":{"1714":1}}],["删除元素",{"2":{"1714":1,"1719":1,"1720":1,"1721":1,"1722":1,"1723":1,"1724":1,"1725":1,"1800":1,"1801":1}}],["删除时",{"2":{"1676":1}}],["删除选择内容",{"2":{"1551":1}}],["删除一个单词",{"2":{"1548":1}}],["删除至行尾",{"2":{"1548":2}}],["删除至行首",{"2":{"1548":1}}],["删除整行",{"2":{"1548":1}}],["删除光标所在字符",{"2":{"1548":1}}],["删除操作的场景",{"2":{"1799":1}}],["删除操作",{"0":{"1548":1}}],["删除当前行",{"2":{"1520":1}}],["删除和复制",{"2":{"1520":1}}],["删除空目录",{"2":{"1509":1}}],["删除词语",{"2":{"1317":1}}],["删除",{"2":{"762":1,"1214":1,"1492":1,"1719":1,"1720":3,"1721":2,"1722":3,"1801":1,"1930":2}}],["删除使得最大似然概率减小最小的子词",{"2":{"604":1}}],["删除的是一批subword",{"2":{"601":1}}],["十五岁才上学",{"2":{"2054":1}}],["十有五",{"2":{"2054":1}}],["十六进制",{"2":{"1817":1,"1835":1}}],["十进制",{"2":{"1817":2,"1835":2}}],["十分重要",{"2":{"1481":1}}],["十倍加速",{"2":{"233":1}}],["密钥将生成在",{"2":{"1594":1}}],["密钥",{"2":{"1594":1}}],["密码>",{"2":{"2069":1}}],["密码",{"2":{"1481":1,"1485":1}}],["密集向量",{"2":{"711":1}}],["姓名",{"2":{"1481":1,"1728":1,"1825":1,"1843":1}}],["张三",{"2":{"1481":1,"1680":1}}],["张量的钩子",{"2":{"1114":1}}],["张量的数组视图描述",{"2":{"1083":1}}],["张量是否会被打包为不同的张量对象取决于它是否是其自身",{"2":{"1114":1}}],["张量会根据需要自动保存",{"2":{"1114":1}}],["张量相乘",{"2":{"1086":1}}],["张量模型并行",{"2":{"976":1}}],["张量存储器加速器",{"2":{"973":1}}],["张量核心",{"2":{"973":1}}],["张量不变",{"2":{"530":1}}],["张量形状开始变化",{"2":{"530":1}}],["张量形状保持不变",{"2":{"520":3,"530":3}}],["张量形状发生变化",{"2":{"520":1,"530":1}}],["张量形状变化",{"0":{"520":1,"530":1}}],["张量维度",{"0":{"420":1}}],["张量",{"2":{"50":1,"773":2,"1082":1,"1083":1}}],["搭建实验数据库",{"2":{"1481":1}}],["搭建环境",{"2":{"1480":1}}],["搭建模型也不方便",{"2":{"1109":1}}],["吃早饭",{"2":{"2097":1}}],["吃的时候再解冻的方法也是",{"2":{"1477":1}}],["吃了",{"2":{"170":2,"267":4,"418":1,"547":1}}],["股票价格走势",{"2":{"1472":1}}],["水等效果",{"2":{"2009":1}}],["水陆两栖车",{"2":{"1665":1}}],["水上交通工具正在航行",{"2":{"1664":1}}],["水上交通工具",{"2":{"1664":1}}],["水平制表符",{"2":{"1616":1}}],["水平分屏",{"2":{"1557":1}}],["水管网络中",{"2":{"1466":1}}],["水管网络的层数",{"2":{"1466":1}}],["水流",{"2":{"1466":1}}],["水分",{"2":{"334":1}}],["凸域将可以形成任意的形状",{"2":{"1465":1}}],["阈值逻辑单元",{"2":{"1461":1}}],["阈值更小",{"2":{"1184":1}}],["亦称",{"2":{"1461":1}}],["挤压函数",{"2":{"1460":1}}],["阶跃函数具有不连续",{"2":{"1460":1}}],["阶段发生",{"2":{"1227":1}}],["阶段来说",{"2":{"333":1}}],["阶段不需要",{"2":{"83":1}}],["阶段需要",{"2":{"83":1}}],["阶段无需应用",{"2":{"69":1}}],["阶段",{"2":{"69":1,"148":2,"335":1,"392":1,"541":1,"612":1}}],["碎片知识",{"0":{"1432":1}}],["碎碎念",{"2":{"361":1}}],["百度百科",{"2":{"1425":2}}],["萤幕等",{"2":{"1411":1}}],["死循环",{"2":{"1620":1}}],["死锁",{"2":{"1407":1}}],["死代码消除",{"2":{"1288":1}}],["掌握位运算在实际编程中的应用",{"2":{"2057":1}}],["掌握图形学有助于在这些新兴领域占据优势",{"2":{"2010":1}}],["掌握二维和三维空间中的坐标系",{"2":{"2009":1}}],["掌握使用",{"2":{"1997":1}}],["掌握这些知识对于编写结构良好",{"2":{"1729":1}}],["掌握定义和调用函数的完整流程",{"2":{"1727":1}}],["掌握自定义数据类型的力量",{"2":{"1727":1}}],["掌握其基本语法和使用方法",{"2":{"1711":1}}],["掌握了",{"2":{"1678":1}}],["掌握访问控制的概念",{"2":{"1666":1}}],["掌握析构函数的定义和作用",{"2":{"1666":1}}],["掌握构造函数的定义和作用",{"2":{"1666":1}}],["掌握指针安全使用的各项原则",{"2":{"1666":1}}],["掌握指针的算术运算",{"2":{"1627":1}}],["掌握数组名作为指针的本质",{"2":{"1666":1}}],["掌握类的继承机制",{"2":{"1652":1}}],["掌握",{"2":{"1602":1,"1627":2,"1666":1,"1765":1,"1826":1,"1844":1,"1918":1,"1937":1}}],["掌握c++高性能推理部署",{"2":{"1403":1}}],["掌握pytorch原理和应用",{"2":{"1403":1}}],["掌握知识一直是人工智能系统发展的核心追求",{"2":{"121":1}}],["紧跟时代",{"2":{"1403":1}}],["紧急的",{"2":{"683":1}}],["题目描述",{"2":{"1933":1}}],["题目",{"0":{"1386":1},"2":{"1664":1,"1825":1,"1843":1}}],["怎么样去补齐这个差距",{"2":{"1598":1}}],["怎么去求那么复杂的高斯分布也就是隐空间呢",{"2":{"1376":1}}],["怎样把不同注意力头的输出合起来呢",{"2":{"10":1}}],["椭圆之间完全没有交集",{"2":{"1374":1}}],["椭圆",{"2":{"1373":1}}],["μ",{"2":{"1373":2}}],["μ2",{"2":{"1361":1}}],["μ1",{"2":{"1361":1}}],["μ​2​​",{"2":{"1361":1}}],["μ​1​​",{"2":{"1361":1}}],["μ​q​​",{"2":{"1361":1}}],["μ​p​​",{"2":{"1361":1}}],["μq",{"2":{"1361":1}}],["μp",{"2":{"1361":1}}],["μiμi",{"2":{"313":1}}],["别混日子了",{"2":{"2056":1}}],["别人叫外号和叫正式名字都是指的同一栋房子",{"2":{"1650":1}}],["别人可以通过钥匙进入你的房子并进行修改",{"2":{"1650":1}}],["别人在复印件上修改不会影响你的原件",{"2":{"1650":1}}],["别名",{"2":{"1612":1}}],["别担心",{"2":{"1611":1}}],["别致",{"2":{"1340":1}}],["别忘了公式里还有一个argmax",{"2":{"908":1}}],["⊤",{"2":{"1339":1,"1343":1}}],["⊤y≈",{"2":{"192":2}}],["风控核心系统开发",{"2":{"1948":1}}],["风控系统开发",{"2":{"1946":1}}],["风格的数组类似",{"2":{"1802":1}}],["风格的字符数组",{"2":{"1713":1}}],["风格的字符串",{"2":{"1616":1}}],["风格类型转换",{"2":{"1629":2}}],["风格字符串转换为",{"2":{"1715":1}}],["风格字符串是以",{"2":{"1715":1}}],["风格字符串类似",{"2":{"1713":1}}],["风格字符串追加到现有字符串末尾",{"2":{"1713":1}}],["风格字符串",{"0":{"1715":1},"2":{"1713":1,"1715":3,"1763":1}}],["风格字符串初始化",{"2":{"1713":1}}],["风格字符串操作中常见的错误",{"2":{"1713":1}}],["风格字符串及其与",{"2":{"1711":1}}],["风格字符串更方便",{"2":{"1624":1}}],["风格字符串中的单个字符",{"2":{"1624":1}}],["风格字符串中的字符",{"2":{"1624":1}}],["风格字符串的转换",{"2":{"1715":1}}],["风格字符串的常用函数",{"2":{"1715":1}}],["风格字符串的常量指针",{"2":{"1713":1}}],["风格字符串的定义和特点",{"2":{"1715":1}}],["风格字符串的约定",{"2":{"1704":1}}],["风格字符串的声明和初始化",{"2":{"1624":1}}],["风格字符串的概念",{"2":{"1624":1}}],["风格字符串本质上是字符数组",{"2":{"1624":1}}],["风极一时的",{"2":{"1337":1}}],["风光无限",{"2":{"908":1}}],["周老师推导",{"2":{"1426":1}}],["周期会明显变长",{"2":{"1336":1}}],["周围的词语",{"2":{"1312":1}}],["防置到最后一个维度",{"2":{"1330":1}}],["防止隐式共享",{"2":{"1911":1}}],["防止上一次的输入影响当前读取行",{"2":{"1813":1,"1831":1}}],["防止程序因未处理的错误而崩溃",{"2":{"1761":1}}],["防止产生悬挂指针",{"2":{"1672":1}}],["防止内存泄漏",{"2":{"1647":1,"1714":1}}],["防止悬空指针",{"2":{"1714":1}}],["防止悬",{"2":{"1647":1}}],["防止优先级问题",{"2":{"1632":1}}],["防止",{"2":{"1631":1,"1647":1}}],["防止生成不属于任何分类的图片",{"2":{"1374":1}}],["防止因固定超参数的设置漏掉可能的优解",{"2":{"1149":1}}],["防止因输入数据分布变化对结果产生影响",{"2":{"807":1}}],["防止在相对较慢的gpu高带宽存储器",{"2":{"940":1,"962":1}}],["防止网络过拟合",{"2":{"813":1}}],["防止梯度消失",{"2":{"807":1}}],["防止模型看到未来的标记token",{"2":{"542":1}}],["防止模型看到未来时刻的输入",{"2":{"71":1}}],["防止过度自信",{"2":{"437":1}}],["防止过拟合",{"2":{"116":1,"448":1}}],["防止错误的累积",{"2":{"407":1}}],["防止其过度相信预测结果",{"2":{"399":1}}],["防止根号运算出现根号0计算非法的问题",{"2":{"346":1}}],["防止分母为零",{"2":{"343":1}}],["防止除以零的微小值",{"2":{"343":1}}],["防止计算这些位置的注意力",{"2":{"198":1}}],["防止解码器在当前时间步预测时",{"2":{"78":1}}],["防止偷看",{"0":{"56":1},"1":{"57":1,"58":1,"59":1},"2":{"49":1,"59":1}}],["住的tag",{"2":{"1330":1}}],["轴",{"2":{"2018":2}}],["轴上",{"2":{"1329":1}}],["轴变换",{"2":{"1081":1}}],["书本上是远远无法理解的",{"2":{"2052":1}}],["书不尽言",{"2":{"2052":1,"2054":1}}],["书中的定义",{"2":{"1322":1}}],["书籍推荐",{"2":{"1501":1,"2043":1}}],["书籍",{"2":{"367":1,"950":1,"987":1,"1010":1,"1728":1}}],["铺平了道路",{"2":{"1318":1}}],["亿参数的大版本",{"2":{"1317":1}}],["亿个参数",{"2":{"1316":2}}],["及将所有",{"2":{"1317":1}}],["及其变种",{"2":{"333":1}}],["及其变体gelu",{"2":{"103":1}}],["鉴别器也参与微调",{"2":{"1315":1}}],["鉴别器",{"2":{"1315":1}}],["鉴于此",{"2":{"209":1}}],["鉴于所有注意力头的输出在添加到残差流之前被等权地相加在一起",{"2":{"122":1}}],["拓展到多语言输入",{"2":{"1315":1}}],["界",{"2":{"1315":1}}],["抽取式问答",{"2":{"1315":1}}],["抽象类主要用于定义接口",{"2":{"1693":1}}],["抽象类不能实例化",{"2":{"1693":1}}],["抽象类",{"0":{"1692":1},"1":{"1693":1,"1694":1,"1695":1},"2":{"1693":2}}],["抽象成计算机上可执行的算子如conv2d",{"2":{"785":1}}],["抽象原子见解",{"2":{"628":1}}],["抽象",{"2":{"437":1}}],["家族",{"0":{"1314":1}}],["展开",{"2":{"1912":1}}],["展望",{"2":{"1367":1}}],["展板",{"2":{"1275":1}}],["展示多态功能",{"2":{"1690":1}}],["展示的是一个超参数轴图",{"2":{"1182":1}}],["展示了成员函数模板的灵活性",{"2":{"1701":1}}],["展示了整数除法的截断行为",{"2":{"1607":1}}],["展示了其在减少训练flops方面的潜力",{"2":{"611":1}}],["展示了blt在8b参数和4t训练字节的规模上",{"2":{"611":1}}],["展示了提示的图灵完备性",{"2":{"504":1}}],["展示了如何将它们与神经元一起",{"2":{"489":1}}],["展示了如何将它们与神经元一起构建一个生物学上合理的",{"2":{"487":1}}],["展示了kans在资源受限环境中作为高效非线性逼近器的潜力",{"2":{"155":1}}],["展示出了另外一种变体状态",{"2":{"170":1}}],["展示",{"0":{"1092":1},"2":{"122":3}}],["冷却",{"2":{"1242":1}}],["冷却阶段",{"2":{"402":1}}],["退出程序",{"2":{"1814":1,"1832":1}}],["退出循环",{"2":{"1814":1,"1832":1}}],["退出",{"0":{"1544":1},"2":{"1518":1,"1519":1,"1544":1}}],["退火",{"2":{"1242":1}}],["退火是一种通过加热和冷却金属来改变其晶体结构",{"2":{"1242":1}}],["退化",{"2":{"446":1,"1705":1}}],["退化发生时",{"2":{"305":1}}],["退化的softmax",{"2":{"191":2}}],["退化为",{"2":{"188":1}}],["状态是按参数保存的",{"2":{"1227":1}}],["状态字典的重载",{"2":{"1214":1}}],["闭包应该清除梯度",{"2":{"1223":1}}],["命令设置全局属性",{"2":{"1982":1}}],["命令在当前行替换",{"2":{"1554":1}}],["命令",{"2":{"1544":1,"1556":1,"1917":1}}],["命令行模式",{"2":{"1519":1}}],["命令行参数解析",{"2":{"1215":1}}],["命名规则与变量名相同",{"2":{"1729":1}}],["命名空间中唯一的标识符",{"2":{"1485":1}}],["命名实体识别等",{"2":{"906":1}}],["命名实体识别",{"2":{"906":1,"1315":1}}],["命名上将激活函数的缩写加在glu前面作为前缀",{"2":{"103":1}}],["副本引用原始module",{"2":{"1214":1}}],["副本本身不具有参数",{"2":{"1214":1}}],["副线也垂下来",{"2":{"259":1}}],["派生自",{"2":{"1869":1}}],["派生类只希望继承基类的功能",{"2":{"1867":1}}],["派生类内部依然可以访问基类的public成员函数",{"2":{"1857":1}}],["派生类",{"2":{"1654":2,"1867":1,"1873":2}}],["派生于哪些类型",{"2":{"1210":1}}],["派生出三个向量q",{"2":{"172":1}}],["派生出q",{"2":{"172":1}}],["库源代码",{"2":{"1997":1}}],["库和头文件",{"2":{"1589":1}}],["库和头文件的路径和链接选项",{"2":{"1589":1}}],["库",{"0":{"1734":1},"2":{"1200":1,"1930":1}}],["库的基本概念",{"2":{"1731":1}}],["库的",{"2":{"346":1}}],["阅读起来会非常困难",{"2":{"1729":1}}],["阅读本手稿并提出宝贵的意见来改进我们的内容",{"2":{"1194":1}}],["阅读笔记",{"2":{"292":1}}],["∇l",{"2":{"1191":2,"1192":4,"1193":6}}],["∇j",{"2":{"1023":1}}],["何为不愠",{"2":{"2055":1}}],["何为君子",{"2":{"2054":1,"2055":1}}],["何况",{"2":{"2054":1}}],["何时调用析构函数",{"2":{"1676":1}}],["何时使用",{"2":{"1638":1}}],["何时对学习率进行预热",{"0":{"1182":1}}],["何凯明大神提出的残差连接",{"2":{"296":1}}],["众所周知",{"2":{"1180":1}}],["误差对",{"2":{"1393":1}}],["误差在训练期间上升而不下降",{"2":{"1179":1}}],["误差爆炸",{"2":{"895":1}}],["诊断和纠正训练失败是一个活跃的研究领域",{"2":{"1178":1}}],["信念不是说出来的",{"2":{"2056":1}}],["信号依次从输出传播到输入",{"2":{"1443":1}}],["信号量机制pv操作之",{"0":{"1426":1}}],["信号量机制",{"0":{"1424":1},"2":{"1568":1}}],["信号",{"2":{"1175":1}}],["信息通过网络向前流动",{"2":{"1438":1}}],["信息传不过去",{"2":{"996":1}}],["信息抽取",{"2":{"906":1}}],["信息检索和聚类",{"2":{"711":1}}],["信息熵用来衡量系统不确定性或随机性",{"2":{"612":1}}],["信息熵衡量的是不确定性",{"2":{"194":1}}],["信息的流动由沿着时序穿过子层",{"2":{"334":1}}],["信息的过程比喻成模型正在形成隐式函数的参数",{"2":{"122":1}}],["信息融合效率高",{"2":{"274":1}}],["信息衰减得越多",{"2":{"253":1}}],["信息遗失问题和长依赖问题",{"2":{"272":1}}],["信息遗失或者混淆",{"2":{"253":1}}],["信息遗失",{"0":{"253":1}}],["信息瓶颈等",{"2":{"251":1}}],["信息会在过深的网络传播过程之后只有部分保留",{"2":{"247":1}}],["信息交换在一定程度上起到了rnn中记忆力模块的作用",{"2":{"261":1}}],["信息交换",{"0":{"261":1},"2":{"172":1}}],["信息在这些组件中流动",{"2":{"130":1}}],["信息根本不是存储在特定的文件柜中",{"2":{"129":1}}],["信息",{"2":{"78":2}}],["抖动的",{"2":{"1175":1}}],["贝叶斯优化就会显得更具吸引力",{"2":{"1175":1}}],["贝叶斯优化和与其类似的工具更适合开发阶段",{"2":{"1175":1}}],["贝叶斯优化工具就是一个值得考虑的选择",{"2":{"1153":1}}],["离自己的生活太遥远",{"2":{"2096":1}}],["离开这个作用域时",{"2":{"1891":1}}],["离开了",{"2":{"1674":1}}],["离线评估可能会相当复杂",{"2":{"1163":1}}],["离线评估中使用的数据的子集",{"2":{"1163":1}}],["离线评估",{"2":{"1163":1}}],["离散的卷积公式如下",{"2":{"770":1}}],["昂贵的在线数据预处理",{"2":{"1161":1}}],["松散",{"2":{"1158":1}}],["译注",{"2":{"1155":1,"1175":1,"1180":1}}],["绘制不同n下的position",{"2":{"1336":1}}],["绘制学习率略高于",{"2":{"1179":1}}],["绘制回顾检查点选择发现的训练steps",{"2":{"1155":1}}],["绘制的训练误差和验证误差与训练期间训练步数的关系图",{"2":{"1149":1}}],["吝啬的训练时间预算可能需要将学习率衰减计划调整到完美",{"2":{"1154":1}}],["慷慨的训练时间预算可以使调整更容易",{"2":{"1154":1}}],["竞赛",{"2":{"1153":1}}],["估计试验方差的成本太高",{"2":{"1152":1}}],["辛顿",{"2":{"1151":1}}],["制度中",{"2":{"1149":2}}],["制表符",{"2":{"552":1}}],["幸运",{"2":{"1149":2}}],["坏",{"2":{"1149":1}}],["坏处",{"2":{"333":1}}],["冗余参数",{"2":{"1145":1}}],["冗余超参数后模型的性能",{"2":{"1150":1}}],["冗余超参数上最佳试验的性能",{"2":{"1150":1}}],["冗余超参数",{"2":{"1144":1}}],["冗余超参数越多",{"2":{"1143":1}}],["冗余超参数还是固定超参数是根据实验目标来决定的",{"2":{"1143":1}}],["冗余超参数是指",{"2":{"1143":1}}],["冗余超参数或固定超参数",{"2":{"1143":1}}],["冗余超参数和固定超参数",{"0":{"1143":1},"2":{"1142":1}}],["试验被定义为产生",{"2":{"1182":1}}],["试验是否会以有利于某些目标或冗余超参数像",{"2":{"1149":1}}],["试验都没有表现出有过拟合",{"2":{"1149":1}}],["试验",{"2":{"1144":1}}],["试图读取一个整数",{"2":{"1814":1,"1832":1}}],["试图将一个基类指针转换为派生类指针",{"2":{"1683":1}}],["试图将一个字符串转换为整数",{"2":{"1683":1}}],["试图达到",{"2":{"512":1}}],["试图通过可视化权重和隐状态来解读语言模型的内部运作",{"2":{"482":1}}],["试图通过可视化权重和隐藏状态来解读语言模型的内部运作",{"2":{"148":1}}],["试图更好地控制网络层输出值的方差",{"2":{"329":1}}],["延长局部变量的生命周期",{"2":{"1649":1}}],["延迟和内存需求",{"2":{"1143":1}}],["延续了2017年pascal的fp16和2020年ampere的bf16的趋势",{"2":{"973":1}}],["围绕最佳值缩小我们的搜索空间",{"2":{"1140":1}}],["朝着这个目标取得进展",{"2":{"1139":1}}],["朝东细数着一丝一丝漏下来的日光",{"2":{"744":1}}],["循序渐进",{"2":{"1139":1}}],["循环访问元素",{"2":{"1797":1}}],["循环更加简洁",{"2":{"1713":1}}],["循环引用的问题",{"2":{"1695":1}}],["循环条件应为",{"2":{"1670":1}}],["循环语句",{"2":{"1631":1,"1649":1}}],["循环语句嵌套",{"2":{"1631":1}}],["循环找出",{"2":{"1625":1}}],["循环打印",{"2":{"1625":1}}],["循环打印出",{"2":{"1621":1}}],["循环遍历数组",{"2":{"1623":1}}],["循环的三个部分都可以省略",{"2":{"1621":1}}],["循环提供了一种更简洁的方式来组织需要重复执行的代码",{"2":{"1621":1}}],["循环实现一个程序",{"2":{"1620":1}}],["循环计算",{"2":{"1620":1,"1621":2}}],["循环后面的代码",{"2":{"1620":1}}],["循环执行的过程",{"2":{"1620":1}}],["循环体内的代码至少会被执行一次",{"2":{"1620":1}}],["循环体",{"2":{"1620":2,"1621":2}}],["循环体将不会执行",{"2":{"1620":1}}],["循环处理",{"2":{"1607":1}}],["循环中的总步数可以通过以下两种方式之一进行确定",{"2":{"1242":1}}],["循环学习率策略在每个批次之后改变学习率",{"2":{"1241":1}}],["循环",{"0":{"1621":1},"2":{"1228":5,"1436":1,"1621":1,"1631":1}}],["循环顺序更改",{"2":{"970":1}}],["循环一次q才完成output的一次累计",{"2":{"967":1}}],["循环解码",{"2":{"886":1}}],["循环过程如下图所示",{"0":{"853":1}}],["循环神经网络",{"0":{"850":1},"2":{"850":1}}],["循环迭代学习结合规则",{"0":{"581":1},"1":{"582":1,"583":1,"584":1}}],["循环层最大的问题是不能并行训练",{"2":{"511":1}}],["循环或递归能力",{"2":{"504":1}}],["循环调用解码器来预测下一个token",{"2":{"428":1}}],["循环模型旨在将数据压缩成固定大小的记忆",{"2":{"227":1}}],["循环模型和注意力机制在深度学习领域得到了广泛应用",{"2":{"227":1}}],["合适的学习率调整策略可以帮助模型更快地收敛或避免陷入局部最优点",{"2":{"1229":1}}],["合理",{"2":{"1137":2}}],["合并成一个最终的可执行文件",{"2":{"1604":1}}],["合并+通信",{"2":{"1570":1}}],["合并",{"2":{"592":1}}],["合并子词",{"2":{"590":1,"604":2}}],["合并最高频字符对",{"2":{"582":1}}],["合并字符可以让你用最少的token来表示语料库",{"2":{"581":1}}],["合并它们",{"2":{"581":1}}],["合并操作",{"2":{"576":1}}],["合并为该数据中没有出现过的一个",{"2":{"576":1}}],["耗时且成本高昂",{"2":{"1135":1}}],["电脑系统中的软件虚拟元件",{"2":{"1411":1}}],["电脑系统内部的任何元件都是资源",{"2":{"1411":1}}],["电力",{"2":{"1134":1}}],["电路复杂度分析用",{"2":{"480":1}}],["永远不会增加",{"2":{"1133":1}}],["资源被销毁",{"2":{"1891":1}}],["资源已释放",{"2":{"1676":1}}],["资源已分配",{"2":{"1676":1}}],["资源",{"2":{"1425":1}}],["资源浪费等",{"2":{"1407":1}}],["资源消耗相对较低的配置",{"2":{"1137":1}}],["资源消耗是增加还是减少",{"2":{"1134":1}}],["资源消耗",{"2":{"1134":2}}],["资源消耗和batch",{"2":{"1131":1}}],["资源分配",{"0":{"260":1}}],["致谢",{"0":{"1194":1},"2":{"1125":1}}],["禁用",{"2":{"1116":1}}],["禁止用",{"2":{"1098":1}}],["环境查询",{"2":{"1575":1}}],["环境说明",{"2":{"1474":1}}],["环境之外使用它",{"2":{"1115":1}}],["环境词向量和环境词的数量对eieie",{"2":{"326":1}}],["↦",{"2":{"1114":1}}],["您输入的整数是",{"2":{"1814":1,"1832":1}}],["您输入的文本是",{"2":{"1813":2,"1831":2}}],["您输入的字符是",{"2":{"1813":1,"1831":1}}],["您的年龄是",{"2":{"1811":1,"1829":1}}],["您仍然可以将选项作为关键字参数传递",{"2":{"1222":1}}],["您需要提供一个可迭代的对象",{"2":{"1221":1}}],["您需要构建一个优化器对象",{"2":{"1221":1}}],["您只需要签署一次cla协议即可",{"2":{"1197":1}}],["您都会学到新事物",{"2":{"1151":1}}],["您查看这类图表的频率就越低",{"2":{"1151":1}}],["您还打算在下一次前向传播中使用更新后的参数进行梯度模式的计算",{"2":{"1120":1}}],["您希望原地更新参数",{"2":{"1120":1}}],["您可以用",{"2":{"1982":1}}],["您可以随时尝试它们",{"2":{"1228":1}}],["您可以随时切换回无梯度模式",{"2":{"1121":1}}],["您可以指定优化器特定的选项",{"2":{"1221":1}}],["您可以确保模型的子模块被正确地注册和命名",{"2":{"1214":1}}],["您可以探索",{"2":{"1114":1}}],["您可以使用",{"2":{"1114":2}}],["您可以使用链式法则自动计算梯度",{"2":{"1113":1}}],["您运行的就是您要求微分的内容",{"2":{"1113":1}}],["您不必编码所有可能的路径",{"2":{"1113":1}}],["冻结权重",{"2":{"1107":1}}],["告诉编译器某个函数",{"2":{"1916":1}}],["告诉torch",{"2":{"1106":1}}],["告知transformer架构两两元素之间的距离",{"2":{"744":1}}],["忽略标点符号",{"2":{"1933":1}}],["忽略无效输入",{"2":{"1814":1,"1832":1}}],["忽略",{"2":{"1813":1,"1831":1}}],["忽略输入流中的",{"2":{"1813":1,"1831":1}}],["忽略输入流中的一个字符",{"2":{"1813":1,"1831":1}}],["忽略大小写查找",{"2":{"1516":1}}],["忽略填充索引",{"2":{"1218":1}}],["忽略这些警告",{"2":{"1098":1}}],["忽略靠前的输入",{"2":{"274":1}}],["隔离功能",{"0":{"1094":1}}],["∣δy∣",{"2":{"2018":2}}],["∣δy∣|",{"2":{"2018":1}}],["∣δx∣|",{"2":{"2018":1}}],["∣δx∣",{"2":{"2018":2}}],["∣∣​2​​",{"2":{"1361":1}}],["∣∣2||",{"2":{"1361":1}}],["∣∣⋅",{"2":{"1361":2}}],["∣g∣",{"2":{"1184":1}}],["∣other∣",{"2":{"1087":1}}],["∣input−other∣",{"2":{"1087":1}}],["绕过padding",{"2":{"1086":1}}],["介绍数组的概念",{"2":{"1623":1}}],["介绍",{"0":{"1275":1,"1310":1,"1333":1},"2":{"1086":1}}],["介于word和character之间",{"2":{"564":1}}],["嵌套lambda",{"2":{"1883":1}}],["嵌套",{"2":{"1631":1,"1797":1}}],["嵌套tensor",{"2":{"1086":2}}],["嵌套张量",{"2":{"1086":2}}],["嵌入式开发专家",{"2":{"1943":1}}],["嵌入式系统开发",{"2":{"1941":1}}],["嵌入式系统",{"2":{"1602":1}}],["嵌入技术",{"2":{"740":1}}],["嵌入矩阵实际就是一个查找表",{"2":{"706":1}}],["嵌入矩阵的向量在模型的训练过程中通过训练数据进行学习",{"2":{"709":1}}],["嵌入矩阵的初始化有两种方式",{"2":{"709":1}}],["嵌入矩阵的大小由词汇表的大小和词向量的维度决定",{"2":{"700":1}}],["嵌入矩阵的每一行对应一个词表中的词",{"2":{"700":1}}],["嵌入矩阵的维度可以看作一个大小为",{"2":{"700":1}}],["嵌入矩阵就有多少行",{"2":{"700":1}}],["嵌入矩阵本质就是一个由词表衍生出来的查找表",{"2":{"700":1}}],["嵌入矩阵",{"0":{"700":1}}],["嵌入层会根据",{"2":{"702":1}}],["嵌入层会利用这些",{"2":{"700":1}}],["嵌入层输出的就是这些对应的向量",{"2":{"700":1}}],["嵌入层本质上是一个嵌入矩阵",{"2":{"700":1}}],["嵌入层的目标是把token序列转换为词嵌入",{"2":{"699":1}}],["嵌入层的作用就是",{"2":{"697":1}}],["嵌入层或者其他模型",{"2":{"263":1}}],["嵌入向量之间的非归一化点积被发现优于余弦相似度",{"2":{"692":1}}],["嵌入大小的固定深度transformer可以解决任何可由大小为t的布尔电路解决的问题",{"2":{"480":1}}],["嵌入",{"2":{"431":1,"457":1,"671":1,"676":1,"739":1,"740":1}}],["嵌入都是归一化的",{"2":{"352":1}}],["~bset1",{"2":{"2062":2}}],["~n",{"2":{"2059":2}}],["~filemanager",{"2":{"1902":1}}],["~resource",{"2":{"1891":1}}],["~resourceholder",{"2":{"1676":1}}],["~mystring",{"2":{"1887":1}}],["~myarray",{"2":{"1700":1}}],["~mydata",{"2":{"1696":1}}],["~myclass",{"2":{"1694":1}}],["~animal",{"2":{"1691":1}}],["~amphibiousvehicle",{"2":{"1665":1}}],["~dog",{"2":{"1674":2}}],["~watervehicle",{"2":{"1665":1}}],["~landvehicle",{"2":{"1665":1}}],["~vehicle",{"2":{"1665":1}}],["~~~~~~~~~~~~~~~~~~",{"2":{"1254":1}}],["~10",{"2":{"1157":1}}],["~",{"2":{"1083":1,"1085":1,"1509":1,"1594":2,"1630":1,"1635":1,"1676":2,"2059":1}}],["~hsh~s",{"2":{"285":1}}],["检测到是list类型",{"2":{"1210":1}}],["检查写权限",{"2":{"2060":1}}],["检查读权限",{"2":{"2060":1}}],["检查文件是否存在",{"2":{"1933":1}}],["检查文件是否成功打开",{"2":{"1820":1,"1838":1}}],["检查是否是字母",{"2":{"1933":1}}],["检查是否存在",{"2":{"1927":1}}],["检查原因",{"2":{"1814":1,"1832":1}}],["检查流状态非常重要",{"2":{"1814":1,"1832":1}}],["检查范围内的所有元素是否都不满足一个给定的条件",{"2":{"1738":1}}],["检查范围内的所有元素是否都满足一个给定的条件",{"2":{"1736":1}}],["检查范围内是否至少有一个元素满足一个给定的条件",{"2":{"1737":1}}],["检查字符串是否为空",{"2":{"1713":1}}],["检查",{"2":{"1695":1}}],["检查空指针",{"2":{"1611":1}}],["检查点和数据分片方面",{"2":{"1169":1}}],["检查点选择使用得当",{"2":{"1155":1}}],["检查训练曲线是识别常见故障的一种简单方法",{"2":{"1149":1}}],["检查训练曲线",{"0":{"1149":1}}],["检查一个tensor",{"2":{"1086":1}}],["检查张量是否是视图",{"2":{"1086":1}}],["检查张量是否位于共享内存中",{"2":{"1083":1}}],["检索记忆",{"2":{"230":1}}],["检索机制",{"2":{"154":1}}],["钩子将以",{"2":{"1227":2}}],["钩子注册函数",{"2":{"1099":1}}],["钩子",{"2":{"1083":1,"1099":1,"1214":2}}],["钩子函数的自定义操作",{"2":{"1212":1}}],["钩子函数",{"2":{"662":1,"1208":2,"1214":2,"1227":1}}],["讲解要点",{"2":{"1645":1,"1646":1,"1647":1,"1648":1,"1649":1,"1650":1}}],["讲解",{"0":{"1077":1},"1":{"1078":1,"1079":1,"1080":1,"1081":1},"2":{"1698":1,"1699":1,"1700":1,"1701":1}}],["尾零",{"2":{"1704":1}}],["尾递归",{"2":{"1646":1}}],["尾数",{"2":{"1075":2}}],["尾部实体",{"2":{"122":1}}],["仿佛是个难以理解的复杂系统",{"2":{"2096":1}}],["仿真系统",{"2":{"1956":1}}],["仿照其它tensor生成",{"0":{"1071":1}}],["仿射变换",{"2":{"838":1}}],["修复主页底部页脚的版权信息未居中问题",{"2":{"2043":1}}],["修饰符",{"2":{"1712":1}}],["修饰的虚函数无法被重写",{"2":{"1656":1}}],["修饰的类无法被继承",{"2":{"1656":1}}],["修饰的是",{"2":{"1614":2}}],["修饰类的成员函数",{"2":{"1649":1}}],["修饰类的成员变量",{"2":{"1649":1}}],["修饰全局变量",{"2":{"1649":1}}],["修饰局部变量",{"2":{"1649":1}}],["修饰",{"2":{"1614":2,"1656":1,"1712":1}}],["修正从原点初始化的一阶矩",{"2":{"1059":1}}],["修改字符串内容",{"2":{"1929":1}}],["修改私有成员",{"2":{"1774":1}}],["修改算法",{"2":{"1758":1}}],["修改等",{"2":{"1734":1}}],["修改元素",{"2":{"1719":1,"1720":1,"1721":1,"1722":1,"1725":1}}],["修改函数名或参数列表",{"2":{"1687":1}}],["修改会反映到",{"2":{"1667":1}}],["修改后的数组",{"2":{"1667":1}}],["修改指针本身",{"2":{"1650":1}}],["修改指向的值",{"2":{"1614":1}}],["修改接口时只需修改头文件",{"2":{"1628":1}}],["修改存储的地址",{"2":{"1612":1}}],["修改用户组",{"2":{"1530":1}}],["修改用户的信息",{"2":{"1487":1}}],["修改权限",{"2":{"1513":1}}],["修改一个用户",{"2":{"1487":1}}],["修改batch",{"2":{"1186":1}}],["修改或细化",{"2":{"1083":1}}],["修改adagrad",{"2":{"1048":1}}],["修改了因果解码器的掩蔽机制",{"2":{"541":1}}],["修改为",{"2":{"194":1,"246":1}}],["修改和擦除之间的相互作用构成了针对",{"2":{"140":1}}],["修改",{"2":{"121":1,"140":1,"1083":1,"1614":3,"1664":1,"1725":1,"1779":1,"1914":1,"2049":1}}],["修改知识",{"0":{"138":1},"1":{"139":1,"140":1,"141":1,"142":1,"143":1,"144":1,"145":1},"2":{"96":1}}],["仍依赖于人工设置一个全局学习率",{"2":{"1045":1}}],["仍然引用",{"2":{"1612":1}}],["仍然有子字符串没有匹配但所有token都已迭代完毕",{"2":{"587":1}}],["仍然等价于vanilla",{"2":{"169":1}}],["仍然会发生token以指数级的速度崩溃到一个共同向量",{"2":{"94":1}}],["净效果是在参数空间中更为平缓的倾斜方向会取得更大的进步",{"2":{"1042":1}}],["世纪80",{"2":{"1016":1}}],["色彩抖动",{"2":{"1015":1}}],["锐化",{"2":{"1015":1}}],["裁剪",{"2":{"1015":1}}],["迁移重构",{"2":{"2038":1}}],["迁移到了google",{"2":{"1476":1}}],["迁移到其他的神经网络中",{"2":{"1009":1}}],["迁移",{"2":{"1009":1,"1313":1}}],["迁移学习",{"0":{"1313":1},"2":{"670":1,"1009":1}}],["服务周到",{"2":{"2051":1}}],["服务",{"2":{"2051":1,"2093":1}}],["服务拆分与",{"2":{"1500":1}}],["服务端应用和基础库的标准",{"2":{"1433":1}}],["服务器更新",{"2":{"2042":1}}],["服务器",{"2":{"1563":1}}],["服务器接收来自第一个聊天会话的新消息",{"2":{"986":1}}],["服务器接收一个请求",{"2":{"986":1}}],["服务器接收一个少样本学习查询",{"2":{"986":1}}],["服务器接收一批额外的少样本学习查询",{"2":{"986":1}}],["服务器在基数树中找到提示的前缀",{"2":{"986":1}}],["服务器处理一个传入的用户消息",{"2":{"986":1}}],["服从均匀分布时",{"0":{"1007":1}}],["服从正态分布时",{"0":{"1006":1}}],["凯明初始化总结",{"0":{"1005":1},"1":{"1006":1,"1007":1}}],["涉及到反向传播",{"2":{"1004":1}}],["≠0",{"2":{"1003":1}}],["≠0e",{"2":{"1003":1}}],["塞维尔初始化",{"0":{"1000":1}}],["∀i",{"2":{"1000":4}}],["∀",{"2":{"999":4}}],["说",{"2":{"2054":1}}],["说到这里",{"2":{"2054":1}}],["说实话",{"2":{"2054":2}}],["说白了就是用来操作数据库存在的",{"2":{"1478":1}}],["说的更通俗点就算",{"2":{"999":1}}],["说明是",{"2":{"1226":1}}],["说明当前状态下的模型参数需要更大的更新量才能更好的逼近最优解",{"2":{"1179":1}}],["说明我们最佳的学习率比较临界",{"2":{"1179":1}}],["说明position和token之间没有太大的关联",{"2":{"764":1}}],["说明与与大模型本身预训练阶段更加吻合",{"2":{"736":1}}],["说明没有需要合并的pair",{"2":{"592":1}}],["说明",{"0":{"432":1},"2":{"405":1,"407":1,"935":1,"951":1,"2018":2}}],["说明前馈网络是持续不断地更新残差的分布",{"2":{"306":1}}],["说明两个向量的相关性越高",{"2":{"176":1}}],["±1",{"2":{"995":1}}],["糟糕的初始化方案不仅会影响网络收敛",{"2":{"990":1}}],["恰当的权重初始化是非常重要的",{"2":{"990":1}}],["链式求导有两种情况需要考虑",{"2":{"1442":1}}],["链式求导法则",{"0":{"1442":1}}],["链式求导",{"2":{"1393":1}}],["链式思维",{"2":{"504":1}}],["链接等过程",{"2":{"1963":1}}],["链接规则",{"2":{"1917":1}}],["链接选项",{"2":{"1917":1}}],["链接器负责解决符号引用问题",{"2":{"1604":1}}],["链接1",{"2":{"1365":1}}],["链接",{"2":{"1084":1,"1275":1,"1302":1,"1604":1,"1916":1}}],["链接到一个新节点",{"2":{"986":1}}],["跨平台支持",{"2":{"1964":1}}],["跨平台框架集成",{"2":{"1961":1}}],["跨平台能力",{"2":{"1961":1}}],["跨平台开发工具",{"2":{"1605":1}}],["跨语言语言模型",{"2":{"1315":1}}],["跨batch",{"2":{"1154":1}}],["跨过山谷后梯度方向会发生变化",{"2":{"1034":1}}],["跨九个时间点进行演示",{"2":{"986":1}}],["跨多个节点扩展张量并行性",{"2":{"977":1}}],["跨多个节点进行处理",{"2":{"977":1}}],["淘汰",{"2":{"986":1}}],["淘汰策略可以有效地管理缓存空间",{"2":{"986":1}}],["淘汰策略",{"2":{"985":1,"986":1}}],["占用内存更少",{"2":{"1646":1}}],["占用栈空间",{"2":{"1646":1}}],["占用高达1",{"2":{"981":1}}],["占位符就无法吸收到query的注意力",{"2":{"66":1}}],["秘密武器",{"0":{"981":1}}],["配置项目",{"2":{"1996":1}}],["配置",{"0":{"1588":1,"2068":1},"1":{"2069":1,"2070":1},"2":{"1594":1,"1605":1}}],["配置部分不在该讲解范围内",{"2":{"1502":1}}],["配置文件中namespace中的名称为对应mapper接口或者dao接口的完整包名",{"2":{"1484":1}}],["配置空间可能非常大",{"2":{"1139":1}}],["配备pagedattention的vllm重新定义了llm",{"2":{"980":1}}],["配合",{"2":{"333":1}}],["今天完成一部分",{"2":{"2104":1}}],["今天是周末",{"2":{"1728":1}}],["今天星期三",{"2":{"1728":1}}],["今天",{"2":{"980":1,"1610":1,"1644":1}}],["管理员账户",{"0":{"2069":1}}],["管理津贴",{"2":{"1657":2}}],["管理方式",{"2":{"1648":1}}],["管理进程",{"0":{"1523":1}}],["管理哪些过去信息应该被遗忘非常重要",{"2":{"230":1}}],["管道模型并行",{"2":{"976":1}}],["旁边的通信是为了",{"2":{"976":1}}],["运用所学知识解决实际问题",{"2":{"1727":1}}],["运用所学的类",{"2":{"1657":1}}],["运气特别好和运气特别差的研究之间的差异也可能大于使用固定超参数在不同随机种子上重新训练该模型之间的典型差异",{"2":{"1177":1}}],["运行可执行文件",{"2":{"1999":1}}],["运行程序后",{"2":{"1825":1,"1843":1}}],["运行安装程序",{"2":{"1605":1}}],["运行测试",{"2":{"1481":1,"1996":1}}],["运行环境",{"2":{"1435":1}}],["运行onnx",{"0":{"1273":1}}],["运行案例",{"2":{"1216":1}}],["运行速度会降低",{"2":{"1214":1}}],["运行次数",{"2":{"1167":1}}],["运行固定步长的训练",{"2":{"1166":1}}],["运行第一轮中最佳的超参数配置",{"2":{"1159":1}}],["运行更多的试验当然更好",{"2":{"1148":1}}],["运行设定步数的试验",{"2":{"1133":1}}],["运行所有实验",{"2":{"1133":1}}],["运行少量的训练实验",{"2":{"1132":1}}],["运行",{"2":{"1098":1,"1651":1,"1999":1}}],["运行请求的操作来计算结果张量",{"2":{"1089":1}}],["运行时错误",{"2":{"1762":1}}],["运行时检查类型安全",{"2":{"1629":1}}],["运行时协同设计的好处",{"2":{"986":1}}],["运行时可以自动识别和利用这些重用模式",{"2":{"985":1}}],["运行前缀匹配和重用",{"2":{"986":1}}],["运行cp需要megatron",{"2":{"976":1}}],["运行的",{"2":{"976":1}}],["运算单位取决于指针类型",{"2":{"1633":1}}],["运算的结果就是指针",{"2":{"1611":1}}],["运算的结果就是变量",{"2":{"1611":1}}],["运算的几种主要类型",{"0":{"1081":1}}],["运算符使其能相加两个类对象",{"2":{"1788":1}}],["运算符分配内存",{"2":{"1714":1}}],["运算符函数定义在类外部",{"2":{"1712":1}}],["运算符函数作为类的成员",{"2":{"1712":1}}],["运算符都可以重载",{"2":{"1712":1}}],["运算符的具体操作",{"2":{"1712":1}}],["运算符来实现两个点的向量相加",{"2":{"1712":1}}],["运算符重载可以通过友元函数来完成",{"2":{"1788":1}}],["运算符重载友元函数",{"2":{"1788":1}}],["运算符重载友元",{"0":{"1787":1},"1":{"1788":1,"1789":1},"2":{"1768":1}}],["运算符重载使得我们可以自定义运算符对类对象的操作行为",{"2":{"1715":1}}],["运算符重载使得代码更加直观和易于理解",{"2":{"1712":1}}],["运算符重载通过定义特殊的函数来实现",{"2":{"1712":1}}],["运算符重载的注意事项和最佳实践",{"2":{"1712":1}}],["运算符重载的语法",{"2":{"1712":1}}],["运算符重载的概念和意义",{"2":{"1712":1}}],["运算符重载",{"0":{"1712":1},"2":{"1788":1}}],["运算符重载与",{"0":{"1711":1},"1":{"1712":1,"1713":1,"1714":1,"1715":1}}],["运算符在运行时动态地分配内存",{"2":{"1668":1}}],["运算符将其归还给系统",{"2":{"1647":1}}],["运算符优先级表",{"0":{"1635":1}}],["运算符优先级和结合性",{"2":{"1630":1}}],["运算符总结",{"0":{"1630":1}}],["运算符等基础知识",{"2":{"1610":1}}],["运算符获取变量",{"2":{"1607":1}}],["运算符",{"2":{"1436":1,"1607":2,"1630":1,"1635":1,"1647":1,"1712":2,"1789":5}}],["运算",{"2":{"1344":1}}],["运算融合",{"2":{"1288":1}}],["运算转换为概率",{"2":{"464":1}}],["运算逻辑",{"0":{"24":1},"1":{"25":1,"26":1,"27":1,"28":1,"29":1,"30":1,"31":1,"32":1,"33":1,"34":1,"35":1,"36":1},"2":{"0":1}}],["青色框",{"2":{"975":1}}],["青稞ai",{"2":{"156":1}}],["卡越多",{"2":{"974":1}}],["卡尔",{"2":{"129":1}}],["术语一点地说",{"2":{"974":1}}],["异构计算",{"2":{"1960":1}}],["异步操作完成后的通知",{"2":{"1645":1}}],["异步编程等场景中",{"2":{"1645":1}}],["异步发送和接收消息",{"2":{"1575":1}}],["异步通信允许发送方在消息尚未接收完毕时继续执行",{"2":{"1574":1}}],["异步通信",{"2":{"1574":1}}],["异步消息处理",{"2":{"1498":1}}],["异步性要求在matmul和softmax之间重叠计算",{"2":{"973":1}}],["异步性是硬件专门化的结果",{"2":{"973":1}}],["异常安全",{"2":{"1911":1}}],["异常对象应该携带足够的错误信息",{"2":{"1764":1}}],["异常对象可以携带关于错误的详细信息",{"2":{"1761":1}}],["异常应该用于处理真正异常的情况",{"2":{"1764":1}}],["异常被处理",{"2":{"1762":1}}],["异常类型",{"2":{"1762":1}}],["异常发生在程序运行时",{"2":{"1761":1}}],["异常",{"2":{"1647":1,"1713":2,"1926":1}}],["异常处理的最佳实践",{"0":{"1764":1}}],["异常处理的优势",{"2":{"1761":1}}],["异常处理流程",{"2":{"1762":1}}],["异常处理机制详解",{"0":{"1762":1}}],["异常处理",{"0":{"1760":1},"1":{"1761":1,"1762":1,"1763":1,"1764":1,"1765":1,"1766":1},"2":{"1491":1}}],["异常检测",{"2":{"696":1}}],["异或问题如下图所示",{"2":{"1463":1}}],["略有改动",{"2":{"2111":1}}],["略",{"2":{"972":2}}],["略显复杂",{"2":{"603":1}}],["情商",{"2":{"1598":1}}],["情况二",{"2":{"1442":1}}],["情况一",{"2":{"1442":1}}],["情况相同",{"2":{"971":1}}],["情感分析",{"2":{"720":1,"906":1}}],["情感强度",{"2":{"712":1}}],["情感色彩",{"2":{"712":1}}],["情感",{"2":{"676":1}}],["超级管理员或者普通管理员账号",{"0":{"2068":1},"1":{"2069":1,"2070":1}}],["超级权重",{"2":{"156":1}}],["超出作用域",{"2":{"1891":2}}],["超出作用域时",{"2":{"1891":1}}],["超出有效范围",{"2":{"1762":1}}],["超出范围等",{"2":{"1762":1}}],["超详细的vmware虚拟机安装linux图文教程保姆级",{"2":{"1582":1}}],["超参",{"2":{"1441":1}}],["超参数",{"2":{"1185":3,"1218":1}}],["超参数搜索方差或学习方差",{"2":{"1152":1}}],["超参数的准备",{"2":{"1130":1}}],["超过句子长度则",{"2":{"1330":2}}],["超过该临界总步数的减少效果将会下降",{"2":{"1133":1}}],["超过最大理论tflops",{"2":{"968":1}}],["举个例子",{"2":{"968":1,"1141":1,"1143":1,"1149":1,"1324":1,"2101":1}}],["举例来说",{"2":{"1788":1}}],["举例如下",{"2":{"588":1,"1779":1,"1977":1}}],["举例为",{"2":{"451":1}}],["举例是",{"2":{"450":1}}],["举例",{"0":{"2024":1,"2030":1},"2":{"8":1,"700":1}}],["伪代码实现",{"0":{"2029":1}}],["伪代码演示",{"0":{"2019":1,"2025":1}}],["伪代码展示",{"2":{"1233":1,"1235":1,"1236":1,"1237":1,"1238":1,"1239":1,"1240":1,"1241":1,"1242":1,"1243":1,"1244":2,"1245":1,"1246":1,"1247":1}}],["伪代码",{"0":{"963":1,"964":1}}],["×0",{"2":{"1394":4}}],["×w​8​​",{"2":{"1394":1}}],["×w​7​​",{"2":{"1394":1}}],["×w8",{"2":{"1394":1}}],["×w7",{"2":{"1394":1}}],["×​∂net​h1​​​​∂out​h1​​​​×​∂w​1​​​​∂net​h1​​​​",{"2":{"1394":1}}],["×∂outh1∂neth1×∂neth1∂w1",{"2":{"1394":1}}],["×outo2×",{"2":{"1394":1}}],["×outo1×",{"2":{"1392":1,"1394":1}}],["×out​o2​​×",{"2":{"1394":1}}],["×out​o1​​×",{"2":{"1392":1,"1394":1}}],["×out​h1​​",{"2":{"1392":1}}],["×outh1",{"2":{"1392":1}}],["×",{"2":{"1087":1}}],["×d",{"2":{"957":1}}],["×vattention",{"2":{"510":2}}],["×var",{"2":{"189":2}}],["课件介绍",{"0":{"1846":1}}],["课件",{"0":{"1767":1},"1":{"1768":1,"1769":1,"1770":1,"1771":1,"1772":1,"1773":1,"1774":1,"1775":1,"1776":1,"1777":1,"1778":1,"1779":1,"1780":1,"1781":1,"1782":1,"1783":1,"1784":1,"1785":1,"1786":1,"1787":1,"1788":1,"1789":1,"1790":1,"1791":1,"1792":1,"1793":1}}],["课件链接",{"2":{"948":1,"978":1,"1380":1}}],["课",{"0":{"1711":1,"1809":1,"1827":1,"1915":1},"1":{"1712":1,"1713":1,"1714":1,"1715":1,"1810":1,"1811":1,"1812":1,"1813":1,"1814":1,"1815":1,"1816":1,"1817":1,"1818":1,"1819":1,"1820":1,"1821":1,"1822":1,"1823":1,"1824":1,"1825":1,"1826":1,"1828":1,"1829":1,"1830":1,"1831":1,"1832":1,"1833":1,"1834":1,"1835":1,"1836":1,"1837":1,"1838":1,"1839":1,"1840":1,"1841":1,"1842":1,"1843":1,"1844":1,"1916":1,"1917":1}}],["课后完成",{"0":{"1625":1}}],["课后作业的参考答案",{"2":{"1999":1}}],["课后作业参考答案",{"0":{"1999":1}}],["课后作业",{"0":{"1625":1,"1657":1,"1664":1,"1766":1,"1870":1,"1933":1,"1997":1,"2063":1},"1":{"1871":1,"1872":1,"1873":1,"1874":1},"2":{"1678":1}}],["课程导入",{"0":{"2058":1}}],["课程目录",{"0":{"1795":1},"1":{"1796":1,"1797":1,"1798":1,"1799":1,"1800":1,"1801":1,"1802":1,"1803":1,"1804":1,"1805":1,"1806":1,"1807":1}}],["课程目标",{"0":{"1627":1,"1731":1,"1876":1},"2":{"1652":1,"1711":1,"1727":1,"2057":1}}],["课程内容",{"0":{"1732":1},"2":{"1727":1}}],["课程第九课",{"0":{"1666":1},"1":{"1667":1,"1668":1,"1669":1,"1670":1,"1671":1,"1672":1,"1673":1,"1674":1,"1675":1,"1676":1,"1677":1,"1678":1}}],["课程",{"2":{"1610":1}}],["课堂练习",{"0":{"1900":1},"2":{"1485":1}}],["课堂记录",{"0":{"655":1},"1":{"656":1,"657":1,"658":1,"659":1,"660":1,"661":1,"662":1,"663":1,"664":1,"665":1,"666":1,"667":1,"668":1,"669":1,"670":1,"671":1}}],["≤",{"2":{"944":4,"1087":1}}],["ℓ",{"2":{"943":8,"944":9,"946":1,"961":8,"963":1,"966":1}}],["访问时需要小心处理异常",{"2":{"1926":1}}],["访问者模式",{"2":{"1926":1}}],["访问私有成员",{"2":{"1772":1,"1774":1}}],["访问并输出",{"2":{"1728":1}}],["访问队列末尾的元素",{"2":{"1723":1}}],["访问队列开头的元素",{"2":{"1723":1}}],["访问元素速度快",{"2":{"1802":1}}],["访问元素",{"2":{"1714":1,"1723":1}}],["访问速度快",{"2":{"1714":1}}],["访问字符串元素",{"2":{"1713":2}}],["访问安全",{"2":{"1694":1}}],["访问控制对比",{"0":{"1863":1}}],["访问控制",{"0":{"1677":1},"2":{"1852":1,"1856":1,"1860":1}}],["访问它会导致未定义行为",{"2":{"1672":1}}],["访问teacher继承的age",{"2":{"1661":1}}],["访问student继承的age",{"2":{"1661":1}}],["访问权限",{"0":{"1655":1}}],["访问效率",{"2":{"1653":1}}],["访问指针所指向的内存",{"2":{"1650":1}}],["访问悬",{"2":{"1647":1}}],["访问",{"2":{"1639":2,"1779":1,"1926":1,"1928":1}}],["访问方式",{"2":{"1639":2}}],["访问二维数组元素",{"2":{"1634":1}}],["访问超出数组边界的元素会导致未定义的行为",{"2":{"1634":1}}],["访问超出数组边界的下标会导致未定义的行为",{"2":{"1623":1}}],["访问第一个元素",{"2":{"1634":1}}],["访问数组元素",{"2":{"1623":1,"1634":3}}],["访问原则",{"0":{"1415":1}}],["访问量",{"2":{"942":1,"959":1}}],["访问的二次方增长",{"2":{"941":1,"960":1}}],["呈现一发不可收的迹象",{"2":{"911":1}}],["呈现在高维度空间中的另一位置",{"2":{"498":1}}],["企图就是用一个简洁的神经网络结构",{"2":{"909":1}}],["繁多的功能模块",{"2":{"908":1}}],["描述",{"2":{"1506":1,"1635":1,"1713":2,"1811":1,"1820":1,"1829":1,"1838":1}}],["描述了项目中文件之间的依赖关系和编译规则",{"2":{"1917":1}}],["描述了",{"2":{"1083":1}}],["描述句子中的事件和参与者",{"2":{"906":1}}],["描绘的贝叶斯大脑",{"2":{"363":1}}],["判断中点与直线位置关系",{"2":{"2023":1}}],["判断与0的关系即可知道下一个点的位置具体怎么确定",{"2":{"2022":1}}],["判断余额和",{"2":{"1873":1}}],["判断年份是否不能被",{"2":{"1729":1}}],["判断年份是否能被",{"2":{"1729":2}}],["判断指针是否为",{"2":{"1694":1}}],["判断指针是否指向有效的内存地址",{"2":{"1633":1}}],["判断当前日期是否为闰年",{"2":{"1642":1}}],["判断条件表达式的值",{"2":{"1621":1}}],["判断一个字符是否为大写字母",{"2":{"1619":1}}],["判断整除性",{"2":{"1607":1}}],["判断",{"2":{"1331":1}}],["判断句子顺序是否被交换",{"2":{"1315":1}}],["判断前向钩子函数是否带参数",{"2":{"1208":1}}],["判断反向传播钩子函数是否完全覆盖了模块的所有梯度输出",{"2":{"1208":1}}],["判断超参数的优化空间是否已经饱和",{"2":{"1140":1}}],["判断文本是正面的",{"2":{"906":1}}],["判断是否使用dropout进行随机置0",{"2":{"199":1}}],["判断是否使用掩码张量",{"2":{"199":1}}],["地平线机器人",{"2":{"1944":1}}],["地设置参数的requires",{"2":{"1214":1}}],["地拟合训练集",{"2":{"1156":1}}],["地名",{"2":{"906":1}}],["地址相同",{"2":{"1705":1}}],["地址增加了",{"2":{"1633":2}}],["地址",{"2":{"503":1,"1302":1,"1594":1,"1649":3}}],["挑选出最好的那个",{"2":{"904":1}}],["挑战了这种常规做法",{"2":{"610":1}}],["挑战传统",{"2":{"361":1}}],["候选结果的数量并不会一直增多",{"2":{"904":1}}],["选中代码块并按",{"2":{"1560":1}}],["选取nbest个最佳路径的index",{"2":{"1330":1}}],["选出最优的组合",{"2":{"901":1}}],["选择顺序",{"0":{"2099":1}}],["选择成员列表并设置权限",{"2":{"2070":1}}],["选择1",{"2":{"1961":1}}],["选择合适的贪心策略",{"2":{"2119":1}}],["选择合适的工具",{"2":{"1612":1}}],["选择合适的数据类型可以帮助我们节省内存",{"2":{"1607":1}}],["选择合适的开发环境能大大提高我们的学习和开发效率",{"2":{"1605":1}}],["选择合适的batch",{"0":{"1133":1,"1134":1}}],["选择操作",{"0":{"1551":1}}],["选择一种跟踪系统",{"2":{"1167":1}}],["选择样本进行定期评估",{"0":{"1165":1}}],["选择较小的学习率可以通过阻碍优化过程来规范训练",{"2":{"1149":1}}],["选择最便宜的出行方案",{"0":{"2128":1},"1":{"2129":1,"2130":1,"2131":1}}],["选择最佳试验会抑制出现过拟合问题的配置",{"2":{"1149":1}}],["选择最频繁出现的字符对",{"2":{"576":1}}],["选择试验次数",{"2":{"1144":1}}],["选择试验所需的超参数变量",{"2":{"1144":1}}],["选择这些超参数的取值范围",{"2":{"1144":1}}],["选择训练步数的数量涉及平衡以下方面",{"2":{"1137":1}}],["选择快速且消耗最少资源的初始配置将使超参数调整更加高效",{"2":{"1137":1}}],["选择用于同类问题的最常用优化器",{"2":{"1130":1}}],["选择架构实际上意味着选择一整个系列的各种模型",{"2":{"1129":1}}],["选择下一轮实验的目标",{"0":{"1141":1},"2":{"1125":1}}],["选择初始配置",{"0":{"1137":1},"2":{"1125":1}}],["选择batchsize",{"0":{"1131":1},"1":{"1132":1,"1133":1,"1134":1,"1135":1,"1136":1,"1137":1},"2":{"1125":1}}],["选择优化器",{"0":{"1130":1},"2":{"1125":1}}],["选择模型架构",{"0":{"1129":1},"2":{"1125":1}}],["选择概率最大的那个",{"2":{"908":1}}],["选择激活函数时",{"2":{"838":1}}],["选择子词方法",{"2":{"604":1}}],["选择性处理信息",{"0":{"275":1},"1":{"276":1,"277":1}}],["选择",{"2":{"145":2,"568":1,"902":1,"1344":1}}],["捡了芝麻",{"2":{"901":1}}],["肯定会有问题",{"2":{"901":1}}],["贪心思维",{"0":{"2124":1,"2130":1,"2134":1,"2138":1}}],["贪心选择性质",{"2":{"2119":1}}],["贪心策略的解释",{"0":{"2156":1}}],["贪心策略是",{"2":{"2131":1}}],["贪心策略",{"0":{"2118":1}}],["贪心与人生",{"0":{"2112":1}}],["贪心更是人的一种本性",{"2":{"2111":1}}],["贪心算法的解法思路",{"0":{"2151":1}}],["贪心算法并不复杂",{"2":{"2121":1}}],["贪心算法原理",{"0":{"2116":1},"1":{"2117":1,"2118":1,"2119":1}}],["贪心算法是一种常用的算法设计策略",{"2":{"2114":1}}],["贪心算法基本概念",{"0":{"2113":1},"1":{"2114":1,"2115":1}}],["贪心算法大纲内容",{"2":{"2111":1}}],["贪心算法",{"0":{"2140":1},"2":{"2101":1,"2112":1}}],["贪心算法可能不一定是全局最优",{"2":{"576":1}}],["贪心法则",{"0":{"2101":1},"2":{"2101":1}}],["贪心",{"0":{"2111":1,"2121":1},"1":{"2112":1,"2113":1,"2114":1,"2115":1,"2116":1,"2117":1,"2118":1,"2119":1,"2120":1,"2122":1,"2123":1,"2124":1,"2125":1,"2126":1,"2127":1,"2128":1,"2129":1,"2130":1,"2131":1,"2132":1,"2133":1,"2134":1,"2135":1,"2136":1,"2137":1,"2138":1,"2139":1,"2140":1},"2":{"901":1,"2043":1,"2112":1,"2120":1,"2121":2,"2140":1}}],["贪心decoding",{"0":{"901":1}}],["明天再解决一部分",{"2":{"2104":1}}],["明确地表示指针",{"2":{"1611":1}}],["明确地说",{"2":{"976":1}}],["明确提供",{"2":{"1242":1}}],["明确了编码器和解码器的概念",{"2":{"282":1}}],["明显",{"2":{"908":1}}],["明白了内部结构",{"2":{"899":1}}],["橘黄点是线性变换后的值",{"2":{"890":1}}],["蓝点是某一时刻的输出向量",{"2":{"890":1}}],["蓝色表示在当前时间点访问的缓存节点",{"2":{"986":1}}],["蓝色表示高频词",{"2":{"185":1}}],["蓝色框表示可共享的提示部分",{"2":{"985":1}}],["蓝色箭头",{"2":{"940":1,"962":1}}],["蓝色虚线表示qkv注意力机制中涉及的两个矩阵乘法",{"2":{"735":1}}],["蓝色虚线框处的矩阵乘法全部为矩阵乘向量",{"2":{"17":1}}],["蓝色分支",{"2":{"540":1}}],["蓝色圈是编码器层",{"2":{"436":1}}],["蓝色",{"2":{"305":1,"485":1,"541":1}}],["绿点是rnn单元",{"2":{"890":1}}],["绿色表示新添加的节点",{"2":{"986":1}}],["绿色框表示不可共享的部分",{"2":{"985":1}}],["绿色分支",{"2":{"540":1}}],["绿色通道",{"2":{"332":2}}],["绿色",{"2":{"295":1,"305":1,"541":1}}],["绿色图说明该头更关注全局信息",{"2":{"18":1}}],["圆圈或方块表示的是向量",{"2":{"878":1}}],["忘记",{"2":{"1671":1}}],["忘记或未能释放不再使用的内存",{"2":{"1671":1}}],["忘记该遗忘的",{"2":{"863":1}}],["忘了要问啥了",{"2":{"665":1}}],["短小",{"2":{"1709":1}}],["短信",{"2":{"1645":1}}],["短路求值",{"2":{"1630":1}}],["短整型",{"2":{"1607":1}}],["短时傅里叶逆变换",{"2":{"1083":1}}],["短时傅里叶变换",{"2":{"1083":1}}],["短期的记忆影响较大",{"2":{"861":1}}],["短语",{"2":{"707":1,"715":1}}],["∞",{"2":{"845":1}}],["√​​​n​i​​​^​​​​6​​​​​",{"2":{"1007":1}}],["√​​n​i​​​​6​​​​​",{"2":{"1007":1}}],["√​​n​in​​+n​out​​​​2​​​​​",{"2":{"1000":1}}],["√​​π​​2​​​​​",{"2":{"844":1}}],["√dd",{"2":{"186":1,"187":1,"761":1}}],["√d",{"2":{"175":1,"199":2}}],["√dk1",{"2":{"187":1}}],["√dkdk",{"2":{"173":1}}],["√dk",{"2":{"71":1}}],["据说",{"2":{"1168":1}}],["据不完全统计",{"2":{"844":1}}],["据此计算出一组权重",{"2":{"261":1}}],["斜率",{"2":{"2018":1,"2019":1}}],["斜率具有衰减性",{"2":{"843":1}}],["斜截式",{"2":{"2016":1}}],["斜线这几个不同的强调维度",{"2":{"3":1}}],["虽同样都是激活了负半轴",{"2":{"843":1}}],["虽然贪心策略不一定总能找到最优解",{"2":{"2121":1}}],["虽然是二倍速快看",{"2":{"2056":1}}],["虽然它们被编译器认为是类的朋友",{"2":{"1774":1}}],["虽然枚举常量本质上是整型",{"2":{"1728":1}}],["虽然看起来简单",{"2":{"1704":1}}],["虽然看起来好像使用26个英文字母就可以表示所有单词",{"2":{"580":1}}],["虽然全局变量方便访问",{"2":{"1649":1}}],["虽然不是转义字符",{"2":{"1616":1}}],["虽然不一定需要完全理解其中的所有内容",{"2":{"1112":1}}],["虽然硬件不支持真正的并行计算",{"2":{"1566":1}}],["虽然新的",{"2":{"1312":1,"1314":1}}],["虽然这可能效率稍低",{"2":{"1176":1}}],["虽然这些方法可以自动学习文本嵌入",{"2":{"711":1}}],["虽然我们不知道最好的方案是什么",{"2":{"1171":1}}],["虽然我们不能一一列举",{"2":{"1149":1}}],["虽然我们只想采用能够产生真正改进的更改",{"2":{"1152":1}}],["虽然深度学习已经从少数学术研究实验室中实践的机器学习方法发展成为为数十亿人使用的产品提供动力的技术",{"2":{"1127":1}}],["虽然随机梯度下降仍然是非常受欢迎的优化方法",{"2":{"1028":1}}],["虽然已经提出了梯度检查点技术来减少所需的最大内存量",{"2":{"946":1,"966":1}}],["虽然",{"2":{"838":1,"1228":1,"1728":1}}],["虽然其是线性偏置项",{"2":{"765":1}}],["虽然说位置编码主要是绝对位置编码和相对位置编码两大类",{"2":{"746":1}}],["虽然decoder",{"2":{"730":1}}],["虽然都是",{"2":{"721":1}}],["虽然都是把新词加到之前的输入上",{"2":{"426":1}}],["虽然由于对向量大小敏感而在文本数据中不太流行",{"2":{"692":1}}],["虽然得到的向量坐标不能单独解释",{"2":{"691":1}}],["虽然可以定义一个空的数组",{"2":{"1704":1}}],["虽然可以根据模板",{"2":{"1316":1}}],["虽然可以表达丰富的语义",{"2":{"684":1}}],["虽然可以使用近似向量相似性技术",{"2":{"154":1}}],["虽然语义搜索对普通人来说似乎很容易",{"2":{"678":1}}],["虽然语言本身的词序和语法也构成了一个偏序结构",{"2":{"252":1}}],["虽然两种方式都是向量化",{"2":{"676":1}}],["虽然能流利地说话",{"2":{"627":1}}],["虽然能写诗",{"2":{"627":1}}],["虽然transformer架构在多个领域取得了巨大成功",{"2":{"618":1}}],["虽然transformer模型是在完全没有生物学知识辅助的情况下开发出来的",{"2":{"490":1}}],["虽然按字符分词对于中文比较合理",{"2":{"566":1}}],["虽然扩展词表能够显著提升目标语言的编解码效率",{"2":{"560":1}}],["虽然预测时候有优势",{"2":{"542":1}}],["虽然未来词还没有生成",{"2":{"525":1}}],["虽然有些系统能够在某些场景中处理kv",{"2":{"985":1}}],["虽然有些东西我们直观上操作没问题",{"2":{"474":1}}],["虽然有若干优势",{"2":{"160":1}}],["虽然上述的并行可以一次性计算所有时间步对应的输出",{"2":{"409":1}}],["虽然所有类别的概率和仍然是归一的",{"2":{"399":1}}],["虽然大多数实时因果数据只知道过去的状态并期望影响未来的决策",{"2":{"256":1}}],["虽然大但是p",{"2":{"185":1}}],["虽然此方案可以保证输入信息的完整性",{"2":{"245":1}}],["虽然输入与输出的顺序相同",{"2":{"245":1}}],["虽然组合性和可扩展性对于有效适应至关重要",{"2":{"222":1}}],["虽然论文题目叫transformer2transformer2transformer^2",{"2":{"218":1}}],["虽然在定义中也可以指定",{"2":{"1708":1}}],["虽然在许多情况下",{"2":{"1149":1}}],["虽然在每一步都会考虑k个候选项",{"2":{"904":1}}],["虽然在很多任务上取得了成功",{"2":{"626":1}}],["虽然在上面源码中的构造函数中没有提及",{"2":{"467":1}}],["虽然在大多数任务中",{"2":{"175":1}}],["虽然在某些情况下使用相同的值进行自身的点乘",{"2":{"172":1}}],["虽然在经过初筛的神经元集合中",{"2":{"135":1}}],["虽然前馈网络原则上",{"2":{"154":1}}],["虽然多头注意力机制在捕捉序列数据的内在关系方面表现出色",{"2":{"152":1}}],["虽然变换的权重是通过非线性的softmax计算得到",{"2":{"117":1}}],["虽然推理时候所有输入都是已知输入",{"2":{"81":1,"525":1}}],["虽然从",{"2":{"28":1}}],["脆弱",{"2":{"840":1}}],["型号名称",{"2":{"1664":1}}],["型数组",{"2":{"1634":1}}],["型变量",{"2":{"1611":1}}],["型运算",{"2":{"1081":5}}],["型任务",{"0":{"883":1}}],["型非线性饱和激活函数",{"2":{"839":1}}],["型激活函数",{"0":{"839":1}}],["饱和指的是在某些区间梯度接近于零",{"2":{"838":1}}],["抑制噪声",{"2":{"813":1}}],["抑制低分隐藏状态",{"2":{"267":1}}],["池化",{"2":{"813":1}}],["示意图",{"2":{"801":1}}],["示例3",{"2":{"1914":1}}],["示例2",{"2":{"1914":1}}],["示例1",{"2":{"1914":1}}],["示例运行",{"2":{"1729":1}}],["示例输出",{"2":{"1608":1}}],["示例代码结构",{"2":{"1997":1}}],["示例代码及详细解释",{"2":{"1704":1,"1705":1,"1706":1,"1707":1,"1708":1,"1709":1}}],["示例代码",{"0":{"1849":1,"1853":1,"1857":1,"1861":1},"2":{"1566":1,"1638":1,"1639":1,"1640":1,"1641":1,"1645":1,"1646":1,"1647":1,"1648":1,"1649":1,"1650":1,"1706":1,"1736":1,"1737":1,"1738":1,"1739":1,"1741":1,"1742":1,"1743":1,"1744":1,"1746":1,"1747":1,"1749":1,"1750":1,"1751":1,"1752":1,"1754":1,"1755":1,"1756":1}}],["示例剖析",{"0":{"577":1},"1":{"578":1,"579":1,"580":1,"581":1,"582":1,"583":1,"584":1,"585":1}}],["示例如下",{"2":{"552":1}}],["示例",{"0":{"407":1,"496":1,"1507":1,"1524":1,"1789":1,"1879":1,"1883":1,"1887":1,"1891":1,"1895":1,"1898":1,"1966":1,"2154":1},"2":{"557":1,"1336":1,"1607":2,"1616":1,"1619":1,"1620":1,"1621":2,"1623":1,"1633":1,"1645":1,"1713":5,"1714":2,"1768":1,"1811":1,"1813":1,"1816":1,"1817":1,"1820":2,"1821":1,"1829":1,"1831":1,"1834":1,"1835":1,"1838":2,"1839":1,"1866":1,"1867":1,"1868":1,"1869":1,"1905":1,"1906":1,"1907":1,"1908":1,"1909":1,"1910":1,"1911":1,"1912":1,"1916":5,"1917":4,"1921":1,"1922":1,"1923":1,"1924":1,"1925":1,"1926":1,"1927":1,"1928":1,"1929":1,"1930":1}}],["擅长逻辑控制",{"2":{"796":1}}],["统一初始化",{"2":{"1897":1}}],["统一接口",{"2":{"1810":1,"1828":1}}],["统一计算设备架构",{"2":{"795":1}}],["统计文件中每个单词出现的频率",{"2":{"1933":1}}],["统计单词频率",{"0":{"1933":1}}],["统计",{"2":{"1651":1}}],["统计学习方法",{"2":{"1322":1}}],["统计损失和正确率",{"2":{"1295":1}}],["统计非负整数数组中每个值的频率",{"2":{"1087":1}}],["统计机器翻译",{"2":{"907":1,"908":1}}],["统计机器学习时代",{"2":{"711":1}}],["统计字频",{"2":{"590":1}}],["统计字符对频率",{"2":{"582":1}}],["统计频率",{"0":{"578":1}}],["统计共现频率",{"2":{"576":1}}],["统计数据集中词语的频率",{"2":{"557":1}}],["统计和范畴",{"2":{"513":1}}],["官网链接",{"2":{"1037":1}}],["官网",{"0":{"788":1}}],["官方文档",{"2":{"1082":1,"1501":1,"1596":1}}],["官方文档中是这样解释的",{"2":{"688":1}}],["官方逻辑图",{"2":{"963":1}}],["官方双语",{"2":{"513":1}}],["沿着输入图像的",{"2":{"783":1}}],["沿着这两个维度是一定需要进行归一化操作的",{"2":{"325":1}}],["旋转",{"2":{"2009":1}}],["旋转和平移等等",{"2":{"1015":1}}],["旋转180度",{"2":{"779":1}}],["旋转位置编码计算",{"2":{"1345":1}}],["旋转位置编码",{"0":{"1341":1},"1":{"1342":1,"1343":1,"1344":1,"1345":1},"2":{"1341":1}}],["旋转位置编码的完备性分析",{"2":{"768":1}}],["旋转位置编码中的旋转矩阵",{"2":{"201":1}}],["膨胀",{"2":{"778":1}}],["膨胀卷积",{"0":{"778":1}}],["次课",{"0":{"1717":1},"1":{"1718":1,"1719":1,"1720":1,"1721":1,"1722":1,"1723":1,"1724":1,"1725":1}}],["次调用",{"2":{"1702":1}}],["次试验调整",{"2":{"1177":1}}],["次",{"2":{"776":1,"844":1,"1651":1}}],["滑动时的步长",{"2":{"773":1}}],["互相关函数形式",{"2":{"770":1}}],["互信息越大",{"2":{"598":1}}],["互信息",{"2":{"598":1,"604":2}}],["工业软件架构师",{"2":{"1958":1}}],["工业软件领域",{"0":{"1955":1},"1":{"1956":1,"1957":1,"1958":1,"1959":1}}],["工业通信协议",{"2":{"1957":1}}],["工业控制系统",{"2":{"1956":1}}],["工号",{"2":{"1657":1}}],["工欲善其事",{"0":{"1605":1}}],["工具可以根据文件后缀自动推导编译规则",{"2":{"1917":1}}],["工具可以根据",{"2":{"1917":1}}],["工具链升级",{"2":{"1960":1}}],["工具链",{"2":{"1605":1}}],["工具",{"2":{"1597":1}}],["工厂",{"2":{"1500":1}}],["工作环境",{"2":{"2105":1}}],["工作",{"2":{"1315":1}}],["工作负载中很常见",{"2":{"985":1}}],["工作里的符号",{"2":{"210":1}}],["工程模拟",{"2":{"1569":1}}],["工程实践",{"0":{"1073":1}}],["工程图展示",{"0":{"857":1}}],["工程上的计算图",{"2":{"875":1}}],["工程上计算图",{"2":{"870":1}}],["工程上3d卷积",{"2":{"783":1}}],["工程上",{"2":{"773":1,"785":1}}],["工程上标准的卷积",{"0":{"773":1}}],["工程上大多数据都是多维度的",{"2":{"770":1}}],["工程中处理的数据大都是离散的",{"2":{"770":1}}],["滤波器",{"2":{"770":1}}],["叫做核函数",{"2":{"770":1}}],["号表示",{"2":{"770":1}}],["号代表同型点乘",{"2":{"343":1}}],["花括号",{"2":{"1728":3}}],["花时间在模型架构和训练配置上是有意义的",{"2":{"1128":1}}],["花书定义",{"2":{"769":1}}],["花费1小时在",{"2":{"169":1}}],["羊驼再度进化",{"2":{"768":1}}],["干货",{"2":{"768":1}}],["干预可以显著降低记忆所需内存",{"2":{"437":1}}],["科学计数法",{"2":{"1817":1,"1835":1}}],["科学计算",{"2":{"1569":1}}],["科学空间|scientific",{"2":{"768":2}}],["科技猛兽",{"2":{"361":1}}],["河畔草lxr",{"2":{"768":1}}],["惩罚项越大",{"2":{"765":1}}],["惩罚项",{"2":{"765":1}}],["刻画语义关系",{"2":{"764":1}}],["刻画将相对位置映射为特征向量",{"2":{"763":1}}],["刻画相对距离与上下文语义之间的关系",{"2":{"760":1}}],["项任务",{"2":{"2135":1}}],["项是",{"2":{"1646":1}}],["项目名称",{"2":{"1966":1}}],["项目开发的必要手段",{"2":{"1918":1}}],["项目",{"2":{"1918":2}}],["项目变得更加容易",{"2":{"1605":1}}],["项目经理负责项目把控",{"2":{"5":1}}],["项为bijbijb",{"2":{"762":1}}],["项",{"2":{"762":1,"1651":1}}],["笔者将带领读者分析一些较为经典的相对位置编码工作",{"2":{"758":1}}],["早期较大的梯度范数",{"2":{"1184":1}}],["早期算法背景下",{"2":{"1059":1}}],["早期的",{"2":{"756":1,"907":1}}],["早期方案也有涉及特征序列",{"2":{"745":1}}],["湖南人怕不辣",{"2":{"755":1}}],["贵州人辣不怕",{"2":{"755":1}}],["千脑理论",{"2":{"754":1}}],["霍金斯",{"2":{"754":1}}],["巧妙地将复值函数的振幅和相位与词义和位置相联系",{"2":{"751":1}}],["般的编码方案",{"2":{"746":1}}],["额外的状态应该是可",{"2":{"1214":1}}],["额外的校正才能有所缓解",{"2":{"746":1,"766":1}}],["额外投影层",{"0":{"733":1}}],["漏",{"2":{"744":1}}],["日期",{"2":{"1642":1}}],["日志管理",{"0":{"1538":1},"1":{"1539":1}}],["日志",{"2":{"1506":1}}],["日",{"2":{"744":1}}],["光荣在于平淡",{"2":{"2056":1}}],["光线追踪的延伸",{"2":{"2009":1}}],["光线追踪",{"2":{"2009":1}}],["光栅图形算法",{"2":{"2009":1}}],["光栅图形与光栅化",{"2":{"2009":1}}],["光栅化",{"2":{"2009":1}}],["光子",{"2":{"1939":1}}],["光标移动",{"0":{"1547":1}}],["光盘等外存相比",{"2":{"1477":1}}],["光",{"2":{"744":1}}],["底",{"2":{"744":1}}],["底层会有一个新tensor的创建过程的",{"2":{"1097":1}}],["底层的实现机制",{"2":{"1097":1}}],["底层的浅表模式的记忆系数对",{"2":{"127":1}}],["底层原理",{"0":{"928":1}}],["底层是基于set",{"2":{"660":1}}],["底层通常编码通用和基础的信息",{"2":{"402":1}}],["底层",{"2":{"335":1}}],["底层趋向于捕捉显示的图像特征",{"2":{"127":1}}],["绝对位置信息是在输入层做文章",{"2":{"745":1}}],["绝对位置信息",{"2":{"744":1,"757":1}}],["绝对位置编码公式表达如下",{"2":{"1335":1}}],["绝对位置编码会加到输入中",{"2":{"1335":1}}],["绝对位置编码具有实现简单",{"2":{"767":1}}],["绝对位置编码对句子语义的影响不大",{"2":{"755":1}}],["绝对位置编码是相对简单的一种方案",{"2":{"747":1}}],["绝对位置编码考虑的是各个独立token的位置信息",{"2":{"745":1}}],["绝对位置编码和相对位置编码的区别",{"2":{"744":1}}],["绝对位置编码的一个最朴素方案是不特意去设计什么",{"2":{"1337":1}}],["绝对位置编码的优点是",{"2":{"746":1}}],["绝对位置编码的作用方式是告知transformer架构每个元素在输入序列的位置",{"2":{"744":1}}],["绝对位置编码的核心思想是在每个输入序列的元素上添加一个位置向量",{"2":{"742":1}}],["绝对位置编码的公式",{"0":{"758":1},"2":{"741":1}}],["绝对位置编码的位置",{"0":{"757":1},"2":{"741":1}}],["绝对位置编码",{"0":{"747":1,"1335":1},"1":{"748":1,"749":1,"750":1,"751":1,"1336":1,"1337":1},"2":{"741":1,"744":1,"746":1}}],["绝对值很大的点积在训练中会收到几乎为0的梯度",{"2":{"192":1}}],["安排时间",{"2":{"2107":1}}],["安装项目",{"2":{"1996":1}}],["安装目标",{"0":{"1987":1}}],["安装包",{"2":{"1605":1}}],["安装演示",{"2":{"1605":1}}],["安装程序会自动配置这些工具",{"2":{"1589":1}}],["安装时的配置",{"2":{"1589":1}}],["安装完",{"2":{"1589":1}}],["安装验证",{"2":{"1584":1}}],["安装",{"0":{"1586":1},"2":{"1584":1}}],["安装与基本使用",{"2":{"1492":1}}],["安装方式",{"0":{"1276":1}}],["安装指定版本",{"2":{"792":1}}],["安装最新版本",{"2":{"792":1}}],["安德安德鲁",{"2":{"740":1}}],["安全关键系统",{"2":{"1960":1}}],["安全加密协议开发经验",{"2":{"1942":1}}],["安全访问",{"2":{"1926":1,"1927":2,"1928":1}}],["安全性",{"2":{"1715":1}}],["安全的形式",{"2":{"1928":1}}],["安全的代码至关重要",{"2":{"1703":1}}],["安全的向下转型",{"2":{"1683":1}}],["安全使用指针至关重要",{"2":{"1672":1}}],["安全漏洞",{"2":{"1670":1}}],["安全",{"2":{"122":1}}],["麦吉尔大学|llm2vec",{"2":{"740":1}}],["阿里",{"2":{"740":1}}],["老板直接说换了贵一点",{"2":{"2051":1}}],["老板直接问啥问题",{"2":{"2051":1}}],["老板一心想着我们不怎么懂这些",{"2":{"2051":1}}],["老板当时大致是这样说的",{"2":{"2051":1}}],["老板的话语让我选择带他去别的店看看",{"2":{"2051":1}}],["老牛同学",{"2":{"740":1}}],["老师这里说的学问基本就是狭义的知识",{"2":{"2054":1}}],["老师讲了encoder时候input不足seq",{"2":{"656":1}}],["老师慢慢放手让学生自主学",{"2":{"411":1}}],["老师也会为我们提供问题的正确答案",{"2":{"406":1}}],["五",{"0":{"1590":1,"1606":1,"1615":1,"1641":1,"1649":1,"1708":1},"1":{"1591":1,"1592":1,"1593":1,"1594":1},"2":{"740":1,"768":1}}],["五个句子在该位置的字分别是",{"2":{"316":1}}],["框架",{"2":{"740":1,"1492":1}}],["六个接口",{"2":{"1590":1}}],["六",{"0":{"1595":1,"1607":1,"1616":1,"1650":1,"1709":1},"1":{"1596":1,"1597":1,"1598":1},"2":{"740":1}}],["刘聪nlp",{"2":{"740":1}}],["刘翔出生在哪个城市",{"2":{"121":1}}],["番子xiwa",{"2":{"740":1}}],["番外01",{"2":{"513":1}}],["啥是embedding",{"2":{"740":1}}],["手机",{"2":{"2100":1}}],["手也就稳了",{"2":{"2056":1}}],["手动删除动态分配的对象",{"2":{"1676":1}}],["手动管理",{"2":{"1648":1}}],["手动地在这片区域申请和释放内存",{"2":{"1648":1}}],["手动实现",{"2":{"807":1,"808":1,"809":1,"810":1}}],["手写数字0",{"2":{"1373":1}}],["手撕llm",{"2":{"740":2}}],["手工",{"2":{"708":1}}],["陌路",{"2":{"740":1}}],["泽龙",{"2":{"740":2}}],["串联组合",{"2":{"739":1}}],["串行操作的复杂度",{"2":{"511":1}}],["串行处理则通过引入中间信息",{"2":{"480":1}}],["严重依赖于最后一个标记的输出嵌入",{"2":{"735":1}}],["严重影响计算效率和消耗内存",{"2":{"565":1}}],["截距",{"2":{"2018":1}}],["截取子串",{"2":{"1713":1}}],["截断行为",{"2":{"1607":1}}],["截止2025",{"2":{"735":1}}],["截至目前",{"2":{"129":1}}],["疏远不相似文本之间的距离",{"2":{"734":1}}],["跟算法中的",{"2":{"2102":1,"2105":1}}],["跟随",{"2":{"1315":1}}],["跟bert预训练的mlm任务相似",{"2":{"734":1}}],["跟h2",{"2":{"727":1}}],["～4",{"2":{"841":1}}],["～70",{"2":{"727":1}}],["～30",{"2":{"727":1}}],["粗体0后1表示对应的单词",{"2":{"722":1}}],["段向量",{"2":{"718":1}}],["旧的知识",{"2":{"715":1}}],["温故",{"2":{"715":1}}],["温度转换器",{"2":{"1608":1}}],["温度",{"2":{"352":1}}],["缘故",{"2":{"715":1}}],["公共方法",{"2":{"1873":3}}],["公共属性",{"2":{"1873":1}}],["公有成员函数",{"2":{"1677":1}}],["公有成员可以在类的内部和外部访问",{"2":{"1677":1}}],["公有成员和私有成员",{"0":{"1677":1}}],["公有成员",{"2":{"1674":1}}],["公有继承自",{"2":{"1664":2}}],["公有继承保持继承成员的访问权限不变",{"2":{"1655":1}}],["公有继承",{"2":{"1654":1}}],["公司需要",{"2":{"1479":1}}],["公布",{"2":{"768":1}}],["公问其故",{"2":{"715":1}}],["公式化简",{"2":{"1448":1}}],["公式表达",{"0":{"927":1}}],["公式分析",{"0":{"903":1}}],["公式",{"0":{"313":1,"319":1},"2":{"293":2,"841":1,"844":1,"845":1,"846":1,"847":1,"1243":1,"1244":1}}],["公式如下",{"2":{"134":1,"843":1,"1000":1}}],["公式如下图标号3",{"2":{"42":1}}],["公式如下图标号2",{"2":{"42":1}}],["公式中的max",{"2":{"113":1}}],["孤立的单词大多没有意义",{"2":{"715":1}}],["麟",{"2":{"714":2}}],["麒",{"2":{"714":2}}],["麒麟",{"2":{"714":1}}],["四级甚至更多级的指针",{"2":{"1611":1}}],["四",{"0":{"1581":1,"1605":1,"1614":1,"1640":1,"1648":1,"1707":1},"1":{"1582":1,"1583":1,"1584":1,"1585":1,"1586":1,"1587":1,"1588":1,"1589":1}}],["四项注意力的组合",{"2":{"1340":1}}],["四川人不怕辣",{"2":{"755":1}}],["四个字",{"2":{"714":1}}],["四肢中长",{"2":{"713":1}}],["藉以研究结构的性质",{"2":{"713":1}}],["藉此对transformer的总体有进一步了解",{"2":{"363":1}}],["金融衍生品定价模型",{"2":{"1947":1}}],["金融科技领域",{"0":{"1945":1},"1":{"1946":1,"1947":1,"1948":1,"1949":1}}],["金钱豹",{"2":{"713":3}}],["金朝老师来上课",{"2":{"233":1}}],["猎食小马和牛等",{"2":{"713":1}}],["绒鼠和其他四足小动物",{"2":{"713":1}}],["鸵鸟",{"2":{"713":1}}],["沼泽以及任何有足够遮盖物和猎物的地区",{"2":{"713":1}}],["灌丛",{"2":{"713":1}}],["草原",{"2":{"713":1}}],["丘陵",{"2":{"713":1}}],["丘奇编码是数学逻辑中的一个思想",{"2":{"691":1}}],["丛林",{"2":{"713":1}}],["栖息于森林",{"2":{"713":1}}],["轻量级但功能强大的代码编辑器",{"2":{"1605":1}}],["轻量级方法",{"2":{"138":1}}],["轻轻一跃能达8",{"2":{"713":1}}],["跳转",{"2":{"1709":1}}],["跳出",{"2":{"1631":1}}],["跳出循环",{"2":{"1621":1}}],["跳出狭隘",{"2":{"1597":1}}],["跳出局部最小值",{"2":{"1032":1}}],["跳跃能力极强",{"2":{"713":1}}],["跳过连接打破了线性依赖",{"2":{"305":1}}],["跳过连接再次打破退化",{"2":{"305":1}}],["跳过连接可以恢复一定的可识别性",{"2":{"305":1}}],["跳过连接",{"2":{"305":1}}],["跳过前两个维度",{"2":{"34":1}}],["善于游泳和爬树",{"2":{"713":1}}],["犬齿及裂齿极发达",{"2":{"713":1}}],["嗅觉均很发达",{"2":{"713":1}}],["趾行性",{"2":{"713":1}}],["躯体均匀",{"2":{"713":1}}],["躯干",{"2":{"515":1}}],["雌性头体长860",{"2":{"713":1}}],["雄性头体长1",{"2":{"713":1}}],["雄性大于雌性",{"2":{"713":1}}],["颜色特征",{"2":{"712":1}}],["聚合所有步数的梯度范数直方图",{"2":{"1184":1}}],["聚合",{"2":{"735":1,"1573":1,"1575":1}}],["聚类",{"2":{"711":1}}],["聚焦程度",{"2":{"194":1}}],["聚焦",{"2":{"93":2}}],["聚焦于剩下的维度",{"2":{"24":1}}],["潜在注意力层作为一种交叉注意力的形式",{"2":{"735":1}}],["潜在注意力层",{"2":{"735":1}}],["潜在狄利克雷分配",{"2":{"711":1}}],["潜在全局",{"2":{"614":1}}],["潜在全局transformer模型",{"2":{"614":1}}],["历史的脉络与选择",{"0":{"1603":1}}],["历史",{"0":{"711":1}}],["固化的或者说静态词嵌入有一个巨大的软肋",{"2":{"715":1}}],["固化",{"2":{"709":1}}],["固定虚拟机网络ip",{"0":{"2095":1}}],["固定大小",{"2":{"1802":1}}],["固定维度d为500",{"2":{"1336":1}}],["固定为",{"2":{"1330":1}}],["固定这个参数所带来的限制就越多",{"2":{"1143":1}}],["固定的超参数对我们的实验结论做了限定",{"2":{"1143":1}}],["固定超参数",{"2":{"1143":2}}],["固定超参数的值不需要",{"2":{"1143":1}}],["固定超参数是指",{"2":{"1143":1}}],["固定训练样本的数量达到设定的效果所花的时间",{"2":{"1133":1}}],["固定跨步易于实现训练和推理",{"2":{"613":1}}],["固定长度的隐状态或者说有限的记忆能力在处理长序列时会导致接下来的几个问题",{"2":{"251":1}}],["固定点表示法",{"2":{"1817":1,"1835":1}}],["固定点",{"2":{"204":1}}],["序号来查找就相当于用独热编码乘以embedding矩阵",{"2":{"700":1}}],["序列索引",{"2":{"1345":1}}],["序列化时会触发",{"2":{"1227":1}}],["序列的连续逻辑块通过块表映射到非连续的物理块",{"2":{"982":1}}],["序列的第一个词和最后一个词要通过多层卷积后才可以建立联系",{"2":{"256":1}}],["序列视为进程",{"2":{"982":1}}],["序列标注",{"2":{"906":1}}],["序列越长",{"2":{"904":1}}],["序列得分",{"2":{"902":3,"903":1}}],["序列输入过程",{"2":{"853":1}}],["序列识别问题",{"2":{"754":1}}],["序列后",{"2":{"700":1}}],["序列",{"2":{"628":1,"1176":1}}],["序列长度增大",{"2":{"566":1}}],["序列填充与截断",{"2":{"555":1}}],["序列计算复杂度是o",{"2":{"511":1}}],["序列被转换成嵌入后送入解码器",{"2":{"427":1}}],["序列并行性将请求的输入序列在不同节点之间分区",{"2":{"977":1}}],["序列并行",{"2":{"768":1,"976":1}}],["序列并行则是将这个过程并行完成",{"2":{"420":1}}],["序列并行首先由论文",{"2":{"420":1}}],["序列中的元素之间的相对位置关系对于理解序列的语义和结构非常重要",{"2":{"755":1}}],["序列中的每个词嵌入向量",{"2":{"1344":1}}],["序列中的每个元素都可以直接访问序列中的其它元素",{"2":{"442":1}}],["序列中的每个单词都需要了解序列中其它单词的信息",{"2":{"265":1}}],["序列中的上下文已经被汇聚完成",{"2":{"101":1}}],["序列中两个词之间的关系距离不同",{"2":{"256":1}}],["序列早期部分的记忆会随着随着距离的增加产生传播衰减",{"2":{"253":1}}],["序列建模的核心就是研究如何把长序列的上下文压缩到一个较小的状态中",{"2":{"241":1}}],["序列结束",{"2":{"241":1}}],["序列数据的生成",{"2":{"239":1}}],["序列到序列",{"2":{"237":1}}],["序列信息",{"2":{"97":1}}],["序列掩码",{"2":{"50":1}}],["投资理财",{"2":{"1598":1}}],["投射到我们自定义的维度大小",{"2":{"699":1}}],["投影和着色的基础",{"2":{"2009":1}}],["投影和切分通过3×h3×h3",{"2":{"25":1}}],["投影与观察",{"2":{"2009":1}}],["投影等操作",{"2":{"2009":1}}],["投影层是文本嵌入中常用的策略",{"2":{"733":1}}],["投影层以及输出层的参数",{"2":{"347":1}}],["投影到键和值中",{"2":{"230":1}}],["投影到不同空间",{"2":{"172":1}}],["投影值越大",{"2":{"176":1}}],["投影输出的shape由",{"2":{"36":1}}],["投影是逻辑投影",{"2":{"29":1}}],["投影",{"0":{"26":1},"2":{"0":1}}],["欺诈检测",{"2":{"696":1}}],["⎟⎠=",{"2":{"694":1}}],["⎟",{"2":{"694":1}}],["⎜⎝w11w12w13w14w21w22w23w24w31w32w33w34w41w42w43w44⎞⎟",{"2":{"694":1}}],["⎜",{"2":{"694":1}}],["⎛⎜",{"2":{"694":1}}],["软件实现",{"0":{"1416":1},"1":{"1417":1,"1418":1,"1419":1}}],["软件栈",{"0":{"791":1}}],["软件层面上的话",{"2":{"206":1}}],["软余弦相似度",{"2":{"692":1}}],["夹角越小",{"2":{"692":1}}],["欧式距离会在两个向量之间形成一条直线",{"2":{"692":1}}],["欧氏距离的作用也会变小",{"2":{"692":1}}],["欧几里得距离缺点是",{"2":{"692":1}}],["欧几里得距离是定义两点",{"2":{"692":1}}],["欧几里得距离",{"2":{"692":3}}],["曼哈顿距离似乎工作得很好",{"2":{"692":1}}],["曼哈顿距离在计算距离时不涉及对角线移动",{"2":{"692":1}}],["曼哈顿距离",{"2":{"692":3}}],["距离越远",{"2":{"1340":1,"1398":2}}],["距离越小",{"2":{"692":1}}],["距离",{"2":{"744":1,"757":1,"1377":1}}],["距离等",{"2":{"691":1}}],["距离太远",{"2":{"247":1}}],["蕴含深层语义信息",{"2":{"689":1}}],["远程机访问测试管理员账户",{"2":{"2069":1}}],["远",{"2":{"2054":3}}],["远离",{"2":{"689":1}}],["远大于其实际的",{"2":{"561":1}}],["唯一键",{"2":{"1807":1}}],["唯一性",{"2":{"689":1}}],["唯一的区别在于",{"2":{"125":1}}],["听",{"2":{"713":1}}],["听觉",{"2":{"689":1}}],["听得见它们的啼唱",{"2":{"246":1}}],["社区参与",{"2":{"1961":1}}],["社区指南",{"0":{"1199":1}}],["社区需要更多涵盖有用方法的资源",{"2":{"1127":1}}],["社区才开始采用仅解码器的llm来生成嵌入文本",{"2":{"729":1}}],["社区给出的定义是",{"2":{"688":1}}],["社区开始转向更大的词汇量",{"2":{"561":1}}],["郭靖",{"2":{"685":1}}],["足以表示",{"2":{"1611":2}}],["足够好",{"2":{"1158":1}}],["足球运动员和橄榄球运动员的语义差异比数学家大",{"2":{"685":1}}],["足秤",{"2":{"335":1}}],["篮球运动员",{"2":{"685":1}}],["引言与简介",{"0":{"1733":1},"1":{"1734":1},"2":{"1732":1}}],["引言",{"0":{"1601":1,"1644":1,"1761":1}}],["引发了深度学习在各个领域的广泛应用和发展",{"2":{"840":1}}],["引用计数变为0",{"2":{"1891":1}}],["引用计数减为",{"2":{"1695":1}}],["引用计数加",{"2":{"1695":1}}],["引用传递使用",{"2":{"1729":1}}],["引用传递后",{"2":{"1650":1}}],["引用传递函数内部",{"2":{"1650":2}}],["引用传递",{"2":{"1650":4,"1729":1}}],["引用在声明时必须初始化",{"2":{"1650":1}}],["引用在声明时必须绑定到一个有效的对象",{"2":{"1612":1}}],["引用在声明时必须立即被初始化",{"2":{"1612":1}}],["引用一旦初始化",{"2":{"1612":1}}],["引用不能扩大被引用变量的访问权限",{"2":{"1612":1}}],["引用就像是给变量取了一个",{"2":{"1612":1}}],["引用的特点",{"2":{"1612":1}}],["引用的权限",{"2":{"1612":1}}],["引用的概念",{"2":{"1612":1}}],["引用的是与",{"2":{"1114":1}}],["引用是",{"2":{"1612":2}}],["引用",{"0":{"1195":1,"1612":1},"2":{"1125":1,"1612":2,"1650":1}}],["引用t5模型中的偏置项",{"2":{"764":1}}],["引用苏神的话",{"2":{"694":1}}],["引理",{"2":{"684":1}}],["引入的基于范围的",{"2":{"1713":1}}],["引入的右值引用",{"2":{"1629":1}}],["引入的非常方便的特性",{"2":{"1615":1}}],["引入的空指针常量",{"2":{"1611":1}}],["引入",{"0":{"2013":1},"2":{"1606":1,"1620":1,"1624":1,"1894":1,"1916":1,"1917":1}}],["引入变分",{"0":{"1377":1}}],["引入正则化可以限制模型的复杂度",{"2":{"1012":1}}],["引入一个隐变量a",{"2":{"908":1}}],["引入特殊token",{"2":{"731":1}}],["引入了标准的",{"2":{"1909":1}}],["引入了变量模板",{"2":{"1908":1}}],["引入了",{"2":{"1905":1,"1911":1,"1930":1}}],["引入了右值引用",{"2":{"1886":1}}],["引入了大量现代特性",{"2":{"1603":1}}],["引入了冗余",{"2":{"1143":2}}],["引入了一种新的模型架构",{"2":{"612":1}}],["引入了缩放因子susus",{"2":{"356":1}}],["引入encoder会显得多余",{"2":{"542":1}}],["事儿得一件一件办",{"2":{"2056":1}}],["事件处理",{"2":{"1645":1}}],["事件抽取等",{"2":{"906":1}}],["事先未准备",{"2":{"683":1}}],["事实并非如此",{"2":{"299":1}}],["事实上是一种早已被淘汰掉的数值解法",{"2":{"498":1}}],["事实上越深的",{"2":{"333":1}}],["事实上就是利用每个词的隐藏状态向量和记忆向量用来做注意力计算",{"2":{"287":1}}],["事实上",{"2":{"249":1,"296":1,"341":1,"559":1,"689":1,"692":1}}],["事实知识的定位可以分为两步",{"2":{"133":1}}],["事实信息和偏见也会通过注意力头传递",{"2":{"131":1}}],["事实的定位",{"0":{"133":1},"1":{"134":1,"135":1},"2":{"96":1}}],["瞬间产生",{"2":{"683":1}}],["瞬时意外",{"2":{"230":1}}],["急迫的",{"2":{"683":1}}],["速食的",{"2":{"683":1}}],["速溶的",{"2":{"683":1}}],["速度较快",{"2":{"1714":1}}],["速度和效率",{"2":{"1602":1}}],["速度也可能令人惊讶地缓慢",{"2":{"980":1}}],["速度慢",{"2":{"840":1}}],["速度又会变慢下来",{"2":{"402":1}}],["速度快",{"2":{"250":1,"840":1}}],["刹那",{"2":{"683":1}}],["顷刻",{"2":{"683":1}}],["款式等",{"2":{"683":1}}],["材质",{"2":{"683":1}}],["乐",{"2":{"681":1,"2054":2}}],["快捷键",{"2":{"1545":1,"1547":1,"1548":1,"1549":1,"1550":1,"1551":1}}],["快很多",{"2":{"935":1,"951":1}}],["快",{"2":{"681":1,"1648":1}}],["快速原型开发等",{"2":{"1602":1}}],["快速上手",{"2":{"1596":1}}],["快速使用",{"0":{"1591":1}}],["快速查找",{"2":{"1532":2}}],["快速查看文件内容",{"2":{"1515":1}}],["快速训练blt模型的方法",{"2":{"611":1}}],["快速准确的修改",{"2":{"138":1}}],["箭头所指代表向量的方向",{"2":{"680":1}}],["遵循异常处理的最佳实践",{"2":{"1765":1}}],["遵循",{"2":{"1723":1}}],["遵循mpi标准",{"2":{"1569":1}}],["遵循因果原则",{"2":{"239":1}}],["遵守",{"2":{"679":1}}],["搜索信息",{"2":{"2107":1}}],["搜索空间和预算往往更为重要",{"2":{"1175":1}}],["搜索空间够大吗",{"2":{"1146":1}}],["搜索空间",{"2":{"1144":1}}],["搜索",{"2":{"696":1,"2100":1}}],["搜索是基于关键词的搜索",{"2":{"678":1}}],["搜到了匹配的商品描述",{"2":{"463":1}}],["独占式拥有对象",{"2":{"1695":1}}],["独热向量编码会产生一个高维稀疏向量",{"2":{"694":1}}],["独热向量",{"2":{"694":1,"711":1}}],["独热编码是单维向量",{"2":{"683":1}}],["独热编码是硬编码",{"2":{"682":1}}],["独热编码这种稀疏性不是我们想要的",{"2":{"682":1}}],["独热编码的每个向量只有一个有信息量的维度",{"2":{"681":1}}],["独热编码的编码过程具体如下",{"2":{"681":1}}],["独热编码有以下显著的缺点",{"2":{"681":1}}],["独热编码就是给这四个字分别用0",{"2":{"681":1}}],["独热编码",{"0":{"681":1},"2":{"676":1,"681":1}}],["独立函数或类中的函数可以作为另一个类的友元函数",{"2":{"1769":1}}],["独立的函数",{"2":{"1729":1}}],["独立的部分",{"2":{"221":1}}],["独立地适应所有模型参数的学习率",{"2":{"1042":1}}],["独立同分布",{"2":{"1000":2}}],["独立训练处两个representation然后拼接",{"2":{"721":1}}],["独立应用于每个",{"2":{"503":1}}],["独立运行的前馈网络",{"2":{"461":1}}],["独立开展",{"2":{"326":1}}],["独立计算具有非常简单的并行化过程",{"2":{"34":1}}],["典型岗位",{"0":{"1938":1,"1943":1,"1948":1,"1953":1,"1958":1}}],["典型案例包括",{"2":{"1566":1}}],["典型的sigmoid",{"2":{"1460":1}}],["典型的",{"2":{"1184":1}}],["典型的rnn网络",{"0":{"851":1}}],["典型方法",{"2":{"676":1}}],["典型差异是少了编码器解码器注意层",{"2":{"541":1}}],["音视频",{"2":{"682":1}}],["音频等等",{"2":{"676":1}}],["音素转录等",{"2":{"611":1}}],["演示如何使用不同的容器和不同的lambda表达式来调用",{"2":{"1914":1}}],["演示文件操作",{"2":{"1902":1}}],["演示静态局部变量在函数多次调用之间保持状态的特性",{"2":{"1649":1}}],["演示了加",{"2":{"1607":1}}],["演进思路",{"0":{"675":1},"1":{"676":1,"677":1,"678":1,"679":1,"680":1,"681":1,"682":1,"683":1,"684":1,"685":1,"686":1,"687":1,"688":1,"689":1,"690":1,"691":1,"692":1,"693":1,"694":1,"695":1,"696":1}}],["演绎等推理步骤提供支撑",{"2":{"386":1}}],["震荡类型的学习率调整是减少进入局部最优解的情况",{"2":{"667":1}}],["续pytorch",{"0":{"663":1},"1":{"664":1,"665":1,"666":1,"667":1,"668":1,"669":1,"670":1,"671":1}}],["续写文本",{"2":{"1316":1}}],["续写",{"2":{"542":1}}],["送入神经网络的后续层进行处理",{"2":{"700":1}}],["送入模型进行训练",{"2":{"53":1}}],["送参数给优化器的时候将所有的parameters送到optim",{"2":{"662":1}}],["静态数组的内存在栈上分配",{"2":{"1714":1}}],["静态数组",{"2":{"1714":1,"1795":1}}],["静态数组和动态数组",{"0":{"1714":1}}],["静态多态",{"0":{"1687":1}}],["静态全局变量",{"2":{"1649":1}}],["静态局部变量",{"0":{"2008":1},"2":{"1649":2}}],["静态变量",{"0":{"1649":1},"2":{"1649":2}}],["静态成员函数",{"2":{"1639":1}}],["静态成员变量在程序开始时分配内存",{"2":{"1639":1}}],["静态成员变量",{"2":{"1639":1}}],["静态成员变量和静态成员函数都属于静态成员",{"2":{"1639":1}}],["静态成员是属于类本身的成员",{"2":{"1639":1}}],["静态成员",{"0":{"1639":1}}],["静态分析",{"2":{"1291":1}}],["静态方法",{"2":{"1227":1}}],["静态原理图",{"2":{"1089":1}}],["静态计算图",{"2":{"875":1}}],["静态",{"2":{"715":1}}],["静态embedding本质上是从查找表中读取出来",{"2":{"715":1}}],["静态embedding",{"2":{"707":1}}],["静态图需要在转换过程中进行编译",{"2":{"1288":1}}],["静态图允许pytorch在转换过程中应用一系列的图优化技术",{"2":{"1288":1}}],["静态图的优势",{"0":{"1288":1}}],["静态图的一个典型示例是tensorflow的计算图",{"2":{"1287":1}}],["静态图在模型执行前可以进行静态优化和编译",{"2":{"1287":1}}],["静态图和动态图各有其优势和应用场景",{"2":{"1287":1}}],["静态图是在模型定义阶段构建的计算图",{"2":{"1287":1}}],["静态图模型",{"0":{"1272":1}}],["静态图如下",{"2":{"854":1}}],["静态图",{"2":{"661":1,"1104":1,"1287":1}}],["静域ai",{"2":{"47":1}}],["叶子节点的梯度会自动保存下来的",{"2":{"661":1,"1104":1}}],["叶子节点",{"2":{"661":1,"1098":2,"1104":1}}],["叶子结点的tensor变量不能进行in",{"2":{"660":1}}],["叶子结点+requests",{"2":{"659":1}}],["累加grad",{"2":{"659":1}}],["累计从上次打印日志开始处理过得tokens",{"2":{"385":1}}],["累计处理过的tokens",{"2":{"385":1}}],["累计loss",{"2":{"385":1}}],["读和写权限",{"2":{"2060":1}}],["读",{"2":{"1820":1,"1838":1}}],["读取学生成绩数据",{"2":{"1825":1,"1843":1}}],["读取和写入文件",{"2":{"1825":1,"1843":1}}],["读取文件中的数据",{"2":{"1825":1,"1843":1}}],["读取文件",{"2":{"1820":1,"1838":1}}],["读取文件内容",{"2":{"1761":1,"1902":1}}],["读取一行",{"2":{"1813":1,"1831":1}}],["读取一行文本",{"2":{"1813":3,"1831":3}}],["读取一个字符到",{"2":{"1813":1,"1831":1}}],["读取一个字符",{"2":{"1813":2,"1831":2}}],["读取最多",{"2":{"1813":2,"1831":2}}],["读取数据",{"2":{"1812":1,"1830":1}}],["读取图片对应的label",{"2":{"1250":1}}],["读权限",{"2":{"1512":1}}],["读者自行看源码就好",{"2":{"1340":1}}],["读者常常想知道作者是如何得出如此复杂的研究结果的",{"2":{"1173":1}}],["读好书",{"2":{"755":1}}],["读书好",{"2":{"755":1}}],["读还是取",{"2":{"656":1}}],["读论文",{"2":{"156":2}}],["​\\t\\tstd",{"2":{"1726":1}}],["​2−1​​∗−1+0=−",{"2":{"1393":1}}],["​2​​=0",{"2":{"1389":1}}],["​2​​​​1​​=out​o1​​",{"2":{"1393":1}}],["​2​​​​1+e​−net​​−1​​=​1+e​−net​​​​1​​−​",{"2":{"1393":1}}],["​2​​​​e​−net​​​​=​",{"2":{"1393":1}}],["​2​​​​",{"2":{"1007":1}}],["​2​​1​​​n​l​​​^​​var",{"2":{"1004":1}}],["​2​​1​​n​l​​var",{"2":{"1003":1}}],["​2​​var",{"2":{"1003":1}}],["​2​​+​2​​1​​",{"2":{"1389":1}}],["​2​​+",{"2":{"1003":1}}],["​2​​+var",{"2":{"1002":1}}],["​2​​",{"2":{"1002":2,"1003":2,"1191":1,"1192":1,"1193":1,"1393":1}}],["​2​​x​​",{"2":{"845":1}}],["​∂out​h1​​​​∂e​o2​​​​",{"2":{"1394":1}}],["​∂out​h1​​​​∂e​o2​​​​=​∂out​o2​​​​∂e​o2​​​​×​∂net​o2​​​​∂out​o2​​​​×​∂out​h1​​​​∂net​o2​​​​=−",{"2":{"1394":1}}],["​∂out​h1​​​​∂e​o1​​​​=−",{"2":{"1394":1}}],["​∂out​h1​​​​∂e​o1​​​​=​∂out​o1​​​​∂e​o1​​​​×​∂net​o1​​​​∂out​o1​​​​×​∂out​h1​​​​∂net​o1​​​​=−",{"2":{"1394":1}}],["​∂out​h1​​​​∂e​o1​​​​+​∂out​h1​​​​∂e​o2​​​​",{"2":{"1394":1}}],["​∂out​o1​​​​∂e​total​​​​=2×​2​​1​​",{"2":{"1393":1}}],["​∂w​​∂loss​​",{"2":{"1442":1}}],["​∂w​1​​​​∂e​total​​​​=0",{"2":{"1394":1}}],["​∂w​1​​​​∂e​total​​​​=​∂out​h1​​​​∂e​total​​​​×​∂net​h1​​​​∂out​h1​​​​×​∂w​1​​​​∂net​h1​​​​=",{"2":{"1394":1}}],["​∂w​7​​​​∂net​o1​​​​=1×out​h1​​+0+0+0=0",{"2":{"1393":1}}],["​∂w​7​​​​∂e​total​​​​=0",{"2":{"1393":1}}],["​∂w​7​​​​∂e​total​​​​=−",{"2":{"1392":1}}],["​∂w​7​​​​∂e​total​​​​=​∂out​o1​​​​∂e​total​​​​×​∂net​o1​​​​∂out​o1​​​​×​∂w​7​​​​∂net​o1​​​​",{"2":{"1392":1}}],["​∂net​o1​​​​∂out​o1​​​​=out​o1​​",{"2":{"1393":1}}],["​∂s​i​",{"2":{"999":1}}],["​∂s​i​​​​∂cost​​",{"2":{"999":1}}],["​⊤​​",{"2":{"1339":1,"1343":1}}],["​10000​2i",{"2":{"1336":2}}],["​12​​",{"2":{"1007":1}}],["​t​i​​​​t​cur​​​​π",{"2":{"1244":1}}],["​t​max​​​​t​cur​​​​π",{"2":{"1243":1}}],["​power​​",{"2":{"1240":1}}],["​p​^​​",{"2":{"899":2}}],["​j​~​​",{"2":{"1014":1}}],["​fan​out​​​​2​​",{"2":{"1006":1}}],["​fan​in​​​​2​​",{"2":{"1006":1}}],["​64×3×3​​2​​",{"2":{"1004":1}}],["​w​l​​​^​​",{"2":{"1004":2}}],["​w​^​​",{"2":{"1004":1}}],["​32×3×3​​2​​",{"2":{"1003":1}}],["​√​n​j​​+n​j+1​​​​​​​√​6​​​​​",{"2":{"1000":1}}],["​√​d​k​​​​​​​1​​",{"2":{"924":1}}],["​√​d​k​​​​​​​qk​t​​​​",{"2":{"918":1}}],["​4d​​m​​",{"2":{"944":1}}],["​i​​",{"2":{"943":1,"961":1}}],["​e​x​1​​−m",{"2":{"943":1,"961":1}}],["​y​i​​​^​​",{"2":{"899":1}}],["​y​n​​​^​​",{"2":{"899":1}}],["​y​2​​​^​​",{"2":{"899":1}}],["​y​1​​​^​​",{"2":{"899":1}}],["​​b​t+1​​",{"2":{"1193":1}}],["​​是新的梯度",{"2":{"1184":1}}],["​​=a​x​​+b​y​​+c=0",{"2":{"2021":1}}],["​​=argmax​y​​p",{"2":{"908":1}}],["​​=​",{"2":{"1393":1}}],["​​=λ×​∣g∣​​g​​",{"2":{"1184":1}}],["​​ℓ",{"2":{"943":2,"961":2}}],["​​f",{"2":{"943":4,"961":4}}],["​​​​",{"2":{"943":1,"961":1,"999":1}}],["​​​​​​∂cost​​",{"2":{"999":1}}],["​​​​​",{"2":{"943":1}}],["​​​e​x​b​​−m",{"2":{"943":1}}],["​​p",{"2":{"908":2,"1377":1}}],["​​exp",{"2":{"847":1}}],["​​",{"2":{"845":1,"847":1,"908":1,"943":16,"961":17,"999":2,"1004":1,"1377":1}}],["​​∗γ+β",{"2":{"640":1}}],["​dx​​dtanh​​=1−tanh​2​​",{"2":{"839":1}}],["​dx​​dsigmoid​​=sigmoid",{"2":{"839":1}}],["​",{"2":{"650":1,"1726":6}}],["称作为cross",{"2":{"649":1,"931":1}}],["称为成员函数或方法",{"2":{"1728":1}}],["称为下标",{"2":{"1623":1}}],["称为多维潜隐注意力",{"2":{"956":1}}],["称为",{"2":{"402":1,"473":1,"1134":1}}],["称为pre",{"2":{"334":1}}],["称为multi",{"2":{"290":1}}],["称为特征",{"2":{"137":2}}],["倍预算的随机搜索变得更加困难",{"2":{"1175":1}}],["倍",{"2":{"647":1,"924":1,"926":1,"1239":1,"1315":1,"1316":1}}],["🌟0301",{"0":{"656":1}}],["🌟loss",{"0":{"644":1}}],["🌟🌟🌟常见的正则化方法",{"0":{"654":1}}],["🌟🌟🌟k",{"0":{"653":1}}],["🌟🌟🌟🌟🌟回顾attention",{"0":{"646":1},"1":{"647":1,"648":1,"649":1,"650":1,"651":1,"652":1}}],["🌟🌟🌟🌟🌟normalization",{"0":{"640":1}}],["🌟🌟🌟🌟optimizer",{"0":{"645":1}}],["🌟🌟🌟activation",{"0":{"641":1},"1":{"642":1,"643":1}}],["伍鹏",{"2":{"638":1}}],["碳原子与token",{"2":{"638":1}}],["详细理解",{"2":{"1846":1}}],["详细讲解",{"2":{"1621":1}}],["详细说明见第v",{"2":{"637":1}}],["详见bergstra",{"2":{"1176":1}}],["详见",{"2":{"1131":1}}],["详解",{"0":{"864":1,"930":1},"1":{"865":1,"866":1,"867":1,"868":1,"869":1}}],["详解基于调整rope旋转角度的大模型长度外推方法",{"2":{"768":1}}],["详解albert",{"2":{"740":1}}],["详解深度学习中的normalization",{"2":{"361":1}}],["海量的专家知识",{"2":{"908":1}}],["海",{"2":{"714":2}}],["海平面上升",{"2":{"629":1}}],["海马体",{"0":{"490":1},"2":{"513":1}}],["猜测可能是为了能让周期是一个很大的数",{"2":{"1336":1}}],["猜",{"2":{"627":1}}],["聊天",{"2":{"627":1}}],["聊一聊transformer中的ffn",{"2":{"156":1}}],["局限性的深刻反思",{"2":{"625":1}}],["局部最优解在全局最优解内",{"2":{"2119":1}}],["局部最优",{"2":{"2115":1}}],["局部对象的析构函数会被调用",{"2":{"1762":1}}],["局部对象在离开其作用域时",{"2":{"1676":1}}],["局部静态变量",{"2":{"1649":1}}],["局部变量",{"0":{"1649":1},"2":{"1648":1,"1649":4}}],["局部变量等",{"2":{"1646":1}}],["局部禁用梯度计算",{"0":{"1116":1}}],["局部解码器使用类似的模块",{"2":{"615":1}}],["局部解码器",{"2":{"614":2}}],["局部编码器使用一个交叉注意力模块将字节表示编码为patch表示",{"2":{"615":1}}],["局部编码器模型",{"2":{"614":1}}],["局部编码器",{"2":{"614":1}}],["局部信息的获取不如rnn和cnn强",{"2":{"512":1}}],["局部性意味着llm倾向于主要依赖相邻token来预测下一个token",{"2":{"204":1}}],["局部化注意力就是通过限制注意力的感知范围",{"2":{"204":1}}],["局部注意力模式则只计算部分子集的隐状态",{"2":{"285":1}}],["局部注意力",{"2":{"204":1}}],["局部敏感哈希是一种高效的近似最近邻搜索算法",{"2":{"153":1}}],["局部敏感哈希",{"2":{"153":1}}],["局部和泛化模型更新",{"2":{"142":1}}],["局部",{"2":{"93":1}}],["计划",{"2":{"2103":1}}],["计划采样",{"2":{"897":1}}],["计为",{"2":{"623":2}}],["计算初值",{"2":{"2023":1}}],["计算几何算法实现",{"2":{"1957":1}}],["计算数字的总和",{"2":{"1914":1}}],["计算数组中所有元素的平均值",{"2":{"1716":1}}],["计算数组元素的积",{"2":{"1710":2}}],["计算数组元素的和",{"2":{"1710":2}}],["计算数组元素的平均值",{"2":{"1623":1}}],["计算容器中所有元素的总和",{"2":{"1914":1}}],["计算等",{"2":{"1914":1}}],["计算均分",{"2":{"1825":1,"1843":1}}],["计算总和以及平均值分别输出",{"2":{"1759":1}}],["计算总价",{"2":{"1729":1}}],["计算它们的乘积",{"2":{"1729":1}}],["计算两个整数的乘积",{"2":{"1729":1}}],["计算两个整数的最大值",{"2":{"1709":1}}],["计算平方",{"2":{"1709":1}}],["计算长度为",{"2":{"1708":1}}],["计算矩形面积",{"2":{"1708":1}}],["计算在内",{"2":{"1704":1}}],["计算方式为",{"2":{"1657":2}}],["计算阶乘",{"2":{"1646":1,"1729":1}}],["计算从",{"2":{"1594":2}}],["计算结果是个复数向量",{"2":{"1345":1}}],["计算m",{"2":{"1345":1}}],["计算词向量元素两两分组之后",{"2":{"1345":1}}],["计算速度提高",{"2":{"1315":1}}],["计算速度快等优点",{"2":{"767":1}}],["计算限制",{"2":{"1149":1}}],["计算也不会被记录在反向图中",{"2":{"1120":1}}],["计算行为就好像没有任何输入需要梯度一样",{"2":{"1120":1}}],["计算某个函数的输出",{"2":{"1094":1}}],["计算完该张量的梯度时",{"2":{"1083":1}}],["计算当前批次的均值和方差",{"2":{"1211":1}}],["计算当前张量相对于计算图叶节子节点的梯度",{"2":{"1083":1}}],["计算当前上下文向量context",{"2":{"267":1}}],["计算简图",{"2":{"960":1}}],["计算简单",{"2":{"838":1}}],["计算简单理解容易",{"2":{"766":1}}],["计算如下",{"2":{"943":1,"961":1}}],["计算一个实对称或复数方阵的特征值和特征向量",{"2":{"1083":1}}],["计算一个句子的均值和标准差",{"2":{"318":1}}],["计算一次score",{"2":{"902":1}}],["计算每一条路的",{"2":{"902":1}}],["计算每个元素的平方",{"2":{"1883":1}}],["计算每个进程负责的范围",{"2":{"1594":1}}],["计算每个相邻对出现的次数",{"2":{"592":1}}],["计算每个维度的均值和标准差",{"2":{"313":1}}],["计算复杂度低",{"2":{"848":1}}],["计算原理图",{"2":{"782":1}}],["计算规则又是怎样的",{"2":{"773":1}}],["计算为",{"2":{"713":1}}],["计算图不再进行",{"2":{"665":1}}],["计算部分才执行",{"2":{"662":1}}],["计算attention的",{"2":{"649":1,"931":1}}],["计算主要依赖于固定的",{"2":{"624":1}}],["计算资源没有动态分配到最需要的地方",{"2":{"613":1}}],["计算成本更高",{"2":{"608":1}}],["计算删除每个subword后对总loss的影响",{"2":{"602":1}}],["计算相邻字节的频数",{"2":{"592":1}}],["计算效率低下",{"2":{"746":1}}],["计算效率",{"2":{"568":1}}],["计算位置编码",{"2":{"450":2}}],["计算概率",{"2":{"431":1}}],["计算概率分布",{"2":{"267":1}}],["计算ziziz",{"2":{"418":1}}],["计算模型损失",{"2":{"399":1}}],["计算损失并返回它",{"2":{"1223":1}}],["计算损失函数",{"2":{"1215":1}}],["计算损失函数的代码如下",{"2":{"398":1}}],["计算损失",{"2":{"399":1,"427":2,"1213":1}}],["计算梯度",{"2":{"385":1,"1094":1}}],["计算目标语言句子中非填充词的数量",{"2":{"380":1}}],["计算输入x在最后一个维度上的标准差",{"2":{"343":1}}],["计算输入x在最后一个维度",{"2":{"343":1}}],["计算输出预测词",{"2":{"267":1}}],["计算隐状态时",{"2":{"285":1}}],["计算对齐系数a",{"2":{"267":1}}],["计算出下一个词的概率分布",{"2":{"242":1}}],["计算出这些k与q的相似度",{"2":{"164":1}}],["计算的时间",{"2":{"848":1}}],["计算的基础是数",{"2":{"679":1}}],["计算的误差会逐渐累加",{"2":{"495":1}}],["计算的",{"2":{"204":1,"420":1}}],["计算的算术强度却始终小于",{"2":{"17":1}}],["计算注意力和采样",{"2":{"431":1}}],["计算注意力架构参数量与",{"2":{"217":1}}],["计算注意力权重之间的注意力权重",{"2":{"209":1}}],["计算注意力得分",{"2":{"201":1}}],["计算注意力步骤",{"0":{"63":1},"2":{"49":1}}],["计算公式如下所示",{"2":{"1361":1}}],["计算公式",{"2":{"1025":1,"1026":1,"1027":1}}],["计算公式是",{"2":{"197":1}}],["计算公式为",{"2":{"840":1}}],["计算公式为attention",{"2":{"173":1,"271":1}}],["计算公式为a=softmax",{"2":{"173":1,"271":1}}],["计算公式为si=a",{"2":{"173":1,"271":1}}],["计算就可以了",{"2":{"184":1}}],["计算更简单",{"2":{"175":1}}],["计算分数",{"2":{"173":1,"271":1}}],["计算过程位于multiheadedattention类的forward",{"2":{"198":1}}],["计算过程",{"0":{"173":1,"875":1},"2":{"157":1,"1206":1}}],["计算篇",{"2":{"156":1}}],["计算神经元的知识归因得分",{"2":{"135":1}}],["计算颗粒度是",{"2":{"101":1}}],["计算量小",{"2":{"840":1}}],["计算量大",{"2":{"840":1}}],["计算量更小",{"2":{"346":1}}],["计算量呈平方级增长",{"2":{"279":1}}],["计算量将是一个严峻的问题",{"2":{"184":1}}],["计算量也平方增加",{"2":{"162":1}}],["计算量",{"2":{"90":1,"768":1}}],["计算第i个时间步的context信息时实际上只是使用前i",{"2":{"71":1}}],["计算值",{"2":{"63":1}}],["计算加权和",{"2":{"63":1}}],["计算机图形学",{"2":{"2045":1,"2046":1}}],["计算机图形学的应用潜力将持续扩大",{"2":{"2011":1}}],["计算机图形学的主要内容",{"0":{"2009":1}}],["计算机图形学领域的最新研究聚焦在高效物理模拟和逼真动画生成等方面",{"2":{"2011":1}}],["计算机的内存可以想象成一个巨大的仓库",{"2":{"1648":1}}],["计算机可以直接执行",{"2":{"1604":1}}],["计算机最早只能理解的语言",{"2":{"1603":1}}],["计算机组成原理",{"0":{"1430":1}}],["计算机网络",{"0":{"1429":1}}],["计算机科学",{"2":{"1425":2}}],["计算机会根据这种变换自动计算这里的值",{"2":{"36":1}}],["计算机视觉等多领域都能出色应用",{"2":{"1":1}}],["计算得到解码器本次的输出",{"2":{"267":1}}],["计算得到",{"2":{"16":1,"125":1}}],["计算多次注意力矩阵",{"2":{"14":1}}],["计算强度",{"0":{"17":1},"2":{"0":1}}],["计算流程概述",{"0":{"1392":1}}],["计算流程",{"0":{"16":1,"266":1},"1":{"267":1,"268":1,"269":1,"270":1,"271":1},"2":{"0":1}}],["计算",{"0":{"15":1},"1":{"16":1,"17":1},"2":{"0":1,"9":1,"16":2,"17":1,"158":1,"180":1,"304":1,"944":1,"1343":1,"1646":1}}],["灵感就来自于我们日常寻找最快捷路线的行为",{"2":{"2107":1}}],["灵活性高",{"2":{"1714":1}}],["灵活地管理内存",{"2":{"1647":1}}],["灵活的",{"2":{"1621":1}}],["灵活的性质",{"2":{"623":1}}],["灵活",{"2":{"1479":1}}],["灵魂",{"2":{"515":1}}],["灵魂变了",{"2":{"170":1}}],["τ",{"2":{"621":1}}],["θi",{"2":{"1344":3}}],["θi=10000−2i",{"2":{"1344":4}}],["θi=10000−2",{"2":{"1343":1}}],["θ​i​​",{"2":{"1344":3}}],["θ​i​​=10000​−2i",{"2":{"1344":4}}],["θ​i​​=10000​−2",{"2":{"1343":1}}],["θ​t+1​​=θ​t​​−α​t​​​√​v​t+1​​​​​+ϵ​​β​1​​m​t+1​​+",{"2":{"1193":1}}],["θ​t+1​​=θ​t​​−α​t​​​√​v​t+1​​​​​+ϵ​​m​t+1​​​​b​t+1​​",{"2":{"1192":1}}],["θ​t+1​​=θ​t​​−m​t+1​​",{"2":{"1191":1}}],["θ​t+1​​=θ​t​​−η​t​​",{"2":{"1190":1}}],["θ​t+1​​=θ​t​​−η​t​​v​t+1​​",{"2":{"1189":1}}],["θ​t+1​​=θ​t​​−η​t​​∇l",{"2":{"1188":1}}],["θ​t​​",{"2":{"1188":1,"1189":1,"1190":2,"1191":2,"1192":2,"1193":3}}],["θ=",{"2":{"1343":2}}],["θt+1=θt−αtβ1mt+1+",{"2":{"1193":1}}],["θt+1=θt−αtmt+1vt+1+ϵbt+1",{"2":{"1192":1}}],["θt+1=θt−mt+1",{"2":{"1191":1}}],["θt+1=θt−ηt",{"2":{"1190":1}}],["θt+1=θt−ηtvt+1",{"2":{"1189":1}}],["θt+1=θt−ηt∇l",{"2":{"1188":1}}],["θt",{"2":{"1188":1,"1189":1,"1190":2,"1191":2,"1192":2,"1193":3}}],["θ表示要更新的参数向量或矩阵",{"2":{"1023":1}}],["θ",{"2":{"621":1,"633":1,"945":4,"965":4,"1023":4,"1377":4,"1438":2}}],["边界",{"2":{"614":1}}],["边界偏差",{"2":{"553":1}}],["映射实体类与数据库表",{"2":{"1492":1}}],["映射成数据库中的记录",{"2":{"1476":1}}],["映射关系如下",{"2":{"1340":1}}],["映射的过程就是把最大概率的那个词找出来作为预测出的词",{"2":{"899":1}}],["映射",{"2":{"676":1}}],["映射为表达性",{"2":{"614":1}}],["映射到",{"2":{"926":1}}],["映射到输出",{"2":{"916":1}}],["映射到另一种语言的对应语句上",{"2":{"885":1}}],["映射到序列的神经网络",{"2":{"885":1}}],["映射到一个固定大小的向量",{"2":{"702":1}}],["映射到一系列输出",{"2":{"614":1}}],["映射到连续的稠密向量空间中",{"2":{"702":1}}],["映射到高维稠密向量空间的工作是由嵌入层",{"2":{"674":1}}],["映射到所需的输出",{"2":{"122":1}}],["映射到不同子空间其实就是在模仿卷积神经网络以支持多通道模式的输出",{"2":{"12":1}}],["ε",{"2":{"614":3}}],["莫扎特",{"2":{"613":1}}],["魔法数字",{"2":{"1728":1}}],["魔术方法",{"0":{"1085":1},"2":{"1214":1}}],["魔笛",{"2":{"613":1}}],["魔改注意力",{"2":{"47":1}}],["鲁棒性提升",{"2":{"611":1}}],["片段",{"2":{"610":1}}],["拉平后",{"2":{"1087":1}}],["拉美体系的语言来说使用bpe分词足以在可接受的词表大小下解决oov的问题",{"2":{"606":1}}],["拉什利",{"2":{"129":1}}],["走的是合并的思路",{"2":{"598":1}}],["�",{"2":{"591":1}}],["稍后我们将看到",{"2":{"579":1}}],["终生不变",{"2":{"1612":1}}],["终生学习编辑的目的是确保大模型经历数百上千次编辑之后",{"2":{"143":1}}],["终端中输入",{"2":{"1594":1}}],["终止",{"2":{"1764":1}}],["终止并跳出当前循环",{"2":{"1631":1}}],["终止mpi环境",{"2":{"1575":1}}],["终止某个进程",{"2":{"1524":1}}],["终止指定进程",{"2":{"1523":1}}],["终止符也有助于算法理解",{"2":{"579":1}}],["终止符",{"2":{"579":1}}],["频繁调用的函数是内联的理想选择",{"2":{"1709":1}}],["频数也不一定是最好的合并指标",{"2":{"576":1}}],["频率",{"2":{"201":1}}],["丢掉整块的有效特征",{"2":{"1019":1}}],["丢了西瓜",{"2":{"901":1}}],["丢失了边界信息导致无法承载单词级别的丰富语义",{"2":{"566":1}}],["丢弃哪些神经元是随机决定",{"2":{"1017":1}}],["丢弃",{"2":{"392":1,"394":1}}],["颗粒度",{"2":{"564":1}}],["劣势",{"0":{"562":1,"595":1},"2":{"565":1,"566":1,"600":1,"603":1,"1579":1}}],["种语言之间进行翻译的模型",{"2":{"1317":1}}],["种预测最优词表大小的方法",{"2":{"561":1}}],["种解决方案",{"2":{"89":1}}],["扩大张量模型并行性",{"2":{"976":1}}],["扩充词典",{"2":{"592":1}}],["扩增词表有助于减少模型在理解这些词汇时的歧义",{"2":{"560":1}}],["扩展练习",{"2":{"1608":2}}],["扩展精度浮点型",{"2":{"1607":1}}],["扩展可以获得代码补全",{"2":{"1605":1}}],["扩展性强",{"2":{"1568":1}}],["扩展到",{"2":{"1317":1}}],["扩展到更多gpu并增加tp都可能会遇到重叠问题",{"2":{"976":1}}],["扩展",{"2":{"1114":1}}],["扩展研究",{"2":{"611":1}}],["扩展词表",{"2":{"563":1}}],["扩展数据仍然是最关键的部分",{"2":{"561":1}}],["扩展比对",{"0":{"336":1},"1":{"337":1,"338":1,"339":1,"340":1,"341":1},"2":{"293":1}}],["扩展了编译时计算的能力",{"2":{"1932":1}}],["扩展了右值的概念",{"2":{"1629":1}}],["扩展了特征空间",{"2":{"172":1}}],["扩展了现有的可解释性方法",{"2":{"148":1}}],["扩展至1280亿个参数",{"2":{"156":1}}],["扩展记忆层时面临的一个瓶颈是",{"2":{"154":1}}],["扩展记忆层",{"2":{"154":1}}],["扩展中的实用性",{"2":{"154":1}}],["满足特定任务和数据集的需求",{"2":{"560":1}}],["满足max",{"2":{"71":1}}],["ü",{"2":{"552":1}}],["顾名思义",{"2":{"547":1}}],["索引",{"0":{"1085":1},"2":{"890":1}}],["索引化",{"2":{"545":1}}],["索引都从进程组中收集",{"2":{"154":1}}],["添加不同商家之间的视野",{"2":{"2050":1}}],["添加虚拟机网络问题",{"2":{"2046":1}}],["添加重温士兵突击",{"2":{"2046":1,"2050":1}}],["添加专英翻转课堂内容",{"2":{"2043":1}}],["添加文章",{"2":{"2043":3}}],["添加安装规则",{"2":{"1999":1}}],["添加原始命令",{"2":{"1993":1}}],["添加自定义命令",{"0":{"1993":1}}],["添加测试",{"2":{"1986":1}}],["添加新元素",{"2":{"1797":1}}],["添加元素",{"2":{"1714":1,"1719":1,"1720":1,"1721":1,"1722":1,"1723":1,"1724":1,"1725":1}}],["添加用户到组",{"2":{"1530":1}}],["添加用户",{"2":{"1530":1}}],["添加",{"0":{"2068":1},"1":{"2069":1,"2070":1},"2":{"1214":1,"2043":2,"2045":1,"2046":2,"2053":1}}],["添加一个库文件",{"2":{"1974":1}}],["添加一个可执行文件",{"2":{"1973":1}}],["添加一个用户",{"2":{"1486":1}}],["添加一个参数组",{"2":{"1227":1}}],["添加一个参数范数惩罚ω",{"2":{"1014":1}}],["添加一个buffer",{"2":{"1214":1}}],["添加了一些类型的正则化",{"2":{"1154":1}}],["添加了总是彼此进行注意力操作的标记",{"2":{"204":1}}],["添加到",{"2":{"1226":1}}],["添加到我们的训练流程中",{"2":{"1143":1}}],["添加到训练集中",{"2":{"1015":1}}],["添加分词器的特殊token",{"2":{"545":1}}],["牛山ai公园",{"2":{"543":3}}],["浪子",{"2":{"543":3}}],["浪费训练资源",{"2":{"239":1}}],["浪费非常多的空间",{"2":{"87":1}}],["填空比逐字逐句地写出整篇文章更容易",{"2":{"542":1}}],["填充词对应的掩码",{"2":{"382":1}}],["填充词的对应位置放置一个非常大的负数",{"2":{"62":1}}],["填充",{"0":{"376":1},"1":{"377":1,"378":1},"2":{"201":1,"557":1}}],["填充掩码",{"2":{"50":1}}],["纯虚类",{"2":{"1693":1}}],["纯虚函数与抽象类",{"0":{"1693":1}}],["纯虚函数",{"0":{"1692":1},"1":{"1693":1,"1694":1,"1695":1},"2":{"1691":1,"1693":4}}],["纯",{"2":{"1312":2,"1315":3,"1316":4}}],["纯生成",{"2":{"542":1}}],["纯自注意力机制在深度增加时会经历秩崩溃",{"2":{"91":1}}],["黄色框表示不可共享的模型输出",{"2":{"985":1}}],["黄色框",{"2":{"975":1}}],["黄色和灰色圆角矩形分别表示前缀token之间的注意力",{"2":{"541":1}}],["黄蓉",{"2":{"685":1}}],["黄海平",{"2":{"513":1}}],["开头的元素",{"2":{"1720":1,"1721":2,"1722":2}}],["开头的预处理指令",{"2":{"1604":1}}],["开头插入元素",{"2":{"1720":1,"1721":1,"1722":1}}],["开头添加元素",{"2":{"1720":1,"1721":1,"1722":1}}],["开源编译器",{"2":{"1605":1}}],["开源模型用实心方块表示",{"2":{"540":1}}],["开发环境搭建",{"0":{"1605":1}}],["开发效率不高",{"2":{"1603":1}}],["开发",{"0":{"1493":1},"2":{"1435":1,"1602":2}}],["开发工具包",{"2":{"1435":1}}],["开发更好的位置编码方法已经成为增强transformer长度外推的主要途径",{"2":{"756":1}}],["开始值",{"2":{"1594":1}}],["开始遍历",{"2":{"1330":1}}],["开始run",{"2":{"1298":1}}],["开始执行单次训练",{"2":{"1173":1}}],["开始新项目的指南",{"0":{"1128":1},"1":{"1129":1,"1130":1,"1131":1,"1132":1,"1133":1,"1134":1,"1135":1,"1136":1,"1137":1},"2":{"1125":1}}],["开始到终点",{"2":{"765":1}}],["开始位置和结束位置的距离为",{"2":{"511":1}}],["开始",{"2":{"99":1,"1623":1,"1634":1,"1713":1}}],["粉色分支",{"2":{"540":1}}],["待解码向量",{"2":{"537":1}}],["询问",{"2":{"536":1}}],["子问题的最优解可以组合成原问题的最优解",{"2":{"2119":1}}],["子网掩码",{"2":{"2090":1}}],["子网",{"2":{"2090":1}}],["子曰",{"2":{"2053":1}}],["子目录和子模块",{"0":{"1979":1},"1":{"1980":1,"1981":1}}],["子对象共享",{"2":{"1665":1}}],["子类想要复用基类的实现",{"2":{"1868":1}}],["子类",{"2":{"1866":1,"1868":1}}],["子类意图保留基类的接口和行为",{"2":{"1866":1}}],["子类新添加的成员无法被父类友元访问",{"2":{"1783":1}}],["子类将拥有所有父类的成员变量和成员函数",{"2":{"1659":1}}],["子类成员可以覆盖父类的同名成员",{"2":{"1654":1}}],["子类继承父类",{"2":{"1654":1}}],["子类可以定义与父类同名函数",{"2":{"1654":1}}],["子类可以使用local",{"2":{"1214":1}}],["子类可以通过覆盖此方法并添加自定义逻辑来实现特定于类的行为",{"2":{"1214":1}}],["子类实现此函数",{"2":{"1214":1}}],["子module",{"2":{"1214":1}}],["子模块的",{"0":{"1981":1}}],["子模块还可以有子模块",{"2":{"1208":1}}],["子模块为当前模块的普通属性",{"2":{"1206":1}}],["子模块啥时候定义的呢",{"2":{"664":1}}],["子词token",{"2":{"628":1}}],["子词排序",{"2":{"587":1}}],["子词可组成单词",{"2":{"567":1}}],["子词分词法有很多不同方法",{"2":{"564":1}}],["子词",{"2":{"564":1}}],["子层堆叠的目的是逐层细化和优化生成词汇的表示",{"2":{"525":1}}],["子空间",{"0":{"12":1},"2":{"0":1}}],["购买者最终输出一个组装好的玩具",{"2":{"524":1}}],["购买者就需要在组装说明书中查询每个零件的说明",{"2":{"524":1}}],["售货员对该玩具的各个组件进行研究",{"2":{"524":1}}],["顺序运行两个函数",{"2":{"523":1}}],["顺便也和这两个方案做下对比分析",{"2":{"272":1}}],["至关重要",{"2":{"1604":1}}],["至于具体原理和证明",{"2":{"2120":1}}],["至于",{"2":{"2054":1}}],["至于稍远的位置",{"2":{"1340":1}}],["至于注意力",{"2":{"976":1}}],["至少目前我很多时候还是会做不到",{"2":{"2054":1}}],["至少演示三种不同的使用情况",{"2":{"1914":1}}],["至少执行一次的循环",{"2":{"1620":1}}],["至少需要一直供电吧",{"2":{"1477":1}}],["至少得有一定的规律性才能让神经网络进行学习",{"2":{"1372":1}}],["至少",{"2":{"1151":1,"1156":1}}],["至少有一些是冗余超参数",{"2":{"1143":1}}],["至少能够并行发起数个训练流程",{"2":{"1138":1}}],["至少在局部上",{"2":{"1115":2}}],["至少可以让模型可承载的信息量变大",{"2":{"119":1}}],["至80",{"2":{"981":1}}],["至此",{"2":{"519":1}}],["摘要任务",{"2":{"629":1}}],["摘要",{"0":{"515":1}}],["摘录如下",{"2":{"370":1}}],["喔家archiself",{"2":{"513":1}}],["半沙漠和高山等多种生境",{"2":{"713":1}}],["半吊子全栈工匠",{"2":{"513":1}}],["半严格的物理分析给出了权重空间的分布和数据驱动的权重的对称性破缺",{"2":{"506":1}}],["智源研究院在论文",{"2":{"724":1}}],["智源论坛",{"2":{"513":1}}],["智能指针的常用操作",{"2":{"1695":1}}],["智能指针",{"0":{"1890":1},"2":{"1695":1}}],["智能指针和裸指针",{"0":{"1695":1}}],["智能推荐等多种应用场景",{"2":{"678":1}}],["智能连接",{"2":{"638":1}}],["智驻未来",{"2":{"513":1}}],["药物分子设计",{"2":{"513":1}}],["清理规则",{"2":{"1917":1}}],["清理生成的文件",{"2":{"1917":1}}],["清理内存",{"2":{"1691":1}}],["清除字符串流的错误状态",{"2":{"1824":1,"1842":1}}],["清除错误状态",{"2":{"1814":1,"1832":1}}],["清空字符串内容",{"2":{"1713":1}}],["清熙",{"2":{"513":4}}],["清华大学发布的",{"2":{"347":1}}],["黑箱",{"2":{"512":1}}],["黑色的评估点会根据预设的误差容忍度自动调整其位置",{"2":{"496":1}}],["系统级软件开发",{"0":{"1940":1},"1":{"1941":1,"1942":1,"1943":1,"1944":1}}],["系统级编程",{"2":{"1602":1}}],["系统架构师",{"2":{"1943":1}}],["系统架构",{"2":{"1678":1}}],["系统崩溃",{"2":{"1671":1}}],["系统需要更频繁地进行内存管理",{"2":{"1671":1}}],["系统消息日志",{"2":{"1539":1}}],["系统事件日志",{"2":{"1539":1}}],["系统日志文件",{"2":{"1539":1}}],["系统",{"2":{"1537":2,"1539":2}}],["系统更新与软件管理",{"0":{"1536":1},"1":{"1537":1}}],["系统管理员",{"2":{"1506":1}}],["系统配置文件",{"2":{"1506":1}}],["系统资源",{"0":{"1411":1},"2":{"1411":1}}],["系统提示",{"2":{"986":1}}],["系统将淘汰所有缓存的token",{"2":{"986":1}}],["系统动态地分配内存用于缓存和运行请求",{"2":{"986":1}}],["系统动力学表现为黎曼梯度向量场",{"2":{"507":1}}],["系统都会收敛到由第一个token决定的共识状态",{"2":{"507":1}}],["系统会沿着函数调用栈向上查找",{"2":{"1762":1}}],["系统会找到合适的空闲块分配出去",{"2":{"1648":1}}],["系统会自动在合适的地方",{"2":{"1647":1}}],["系统会收敛到共识平衡点",{"2":{"507":1}}],["系统会在d这个维度",{"2":{"10":1}}],["系统或者函数的性质如何变化",{"2":{"507":1}}],["渐近特性",{"2":{"507":1}}],["态射",{"2":{"505":1}}],["范围内",{"2":{"1154":1}}],["范围内为线性",{"2":{"759":1}}],["范围内的点积",{"2":{"352":1}}],["范围内的余弦相似度的点积",{"2":{"352":1}}],["范式在各种下游任务中表现良好",{"2":{"711":1}}],["范数异常值非常大",{"2":{"1179":1}}],["范数是指在深度学习中",{"2":{"1179":1}}],["范数的变化",{"2":{"1179":1}}],["范数",{"2":{"702":1}}],["范畴的相变与知识的形成",{"2":{"513":1}}],["范畴视角下的transformer",{"2":{"505":1}}],["范畴学也在其他领域",{"2":{"505":1}}],["范畴学是数学中进行类比的一个非常有效的工具",{"2":{"505":1}}],["范畴学是一种研究数学结构和它们之间关系的数学分支",{"2":{"505":1}}],["范畴学的一些基本概念",{"2":{"505":1}}],["范畴学能够帮助我们发现不同数学领域之间的类似性",{"2":{"505":1}}],["范畴学提供了一种统一的语言",{"2":{"505":1}}],["范畴论提供了一种清晰的方式来描述模型的组件和它们之间的相互作用",{"2":{"505":1}}],["范畴论",{"0":{"505":1}}],["∈rbr×bc",{"2":{"944":1}}],["∈rbrm",{"2":{"944":1}}],["∈r​b​r​​×b​c​​​​",{"2":{"944":1}}],["∈r​b​r​​​​",{"2":{"944":1}}],["∈r​n×n​​",{"2":{"941":1,"960":1}}],["∈rn×n",{"2":{"941":1,"960":1}}],["∈r^d",{"2":{"125":1}}],["∈",{"2":{"503":1,"941":2,"942":2,"943":2,"946":1,"957":3,"959":2,"960":2,"961":2,"966":1}}],["促使模型关注上下文中的关键信息",{"2":{"500":1}}],["促进其进入更有序的空间",{"2":{"363":1}}],["流处理各种输入输出任务",{"2":{"1826":1,"1844":1}}],["流是",{"2":{"1826":1,"1844":1}}],["流的基本概念和标准",{"2":{"1826":1,"1844":1}}],["流的各个方面",{"2":{"1826":1,"1844":1}}],["流方式",{"2":{"1822":1,"1840":1}}],["流对象",{"0":{"1811":1,"1829":1},"2":{"1811":1,"1826":1,"1829":1,"1844":1}}],["流具有类型检查",{"2":{"1810":1,"1828":1}}],["流操作",{"2":{"1810":1,"1828":1}}],["流出",{"2":{"1810":1,"1828":1}}],["流入",{"2":{"1810":1,"1828":1}}],["流动的管道",{"2":{"1810":1,"1828":1}}],["流想象成一个",{"2":{"1810":1,"1828":1}}],["流",{"0":{"1810":1,"1828":1},"2":{"1810":2,"1828":2}}],["流详解",{"0":{"1809":1,"1827":1},"1":{"1810":1,"1811":1,"1812":1,"1813":1,"1814":1,"1815":1,"1816":1,"1817":1,"1818":1,"1819":1,"1820":1,"1821":1,"1822":1,"1823":1,"1824":1,"1825":1,"1826":1,"1828":1,"1829":1,"1830":1,"1831":1,"1832":1,"1833":1,"1834":1,"1835":1,"1836":1,"1837":1,"1838":1,"1839":1,"1840":1,"1841":1,"1842":1,"1843":1,"1844":1}}],["流水线",{"2":{"1578":1}}],["流体动力学等",{"2":{"1569":1}}],["流行的优化器",{"2":{"1130":1}}],["流畅的语句",{"2":{"898":1}}],["流转过程",{"0":{"695":1}}],["流映射视角",{"0":{"499":1}}],["流程控制语句是编程中至关重要的组成部分",{"2":{"1618":1}}],["流程控制语句",{"0":{"1618":1},"1":{"1619":1,"1620":1,"1621":1}}],["流程图示",{"0":{"1443":1}}],["流程",{"0":{"427":1,"431":1,"519":1,"527":1,"698":1},"1":{"528":1,"529":1}}],["流程如下",{"2":{"164":1}}],["思考共同特征和区别",{"2":{"1657":1}}],["思考指针和引用在实现上有什么不同",{"2":{"1650":1}}],["思考为什么全局静态变量可以避免命名冲突",{"2":{"1649":1}}],["思考在函数中返回指向局部变量的指针是否安全",{"2":{"1648":1}}],["思考在哪些情况下使用动态内存分配是必要的",{"2":{"1647":1}}],["思考在日常生活中",{"2":{"1645":1}}],["思考它们的优缺点",{"2":{"1646":1}}],["思考题",{"0":{"1489":1},"2":{"1619":1}}],["思考与尝试",{"0":{"1260":1,"1264":1}}],["思考就产生了",{"2":{"754":1}}],["思考是一种特殊形式的移动",{"2":{"754":1}}],["思考篇",{"2":{"740":1}}],["思考",{"0":{"2031":1,"2157":1},"2":{"640":1,"698":1,"773":1,"775":1,"783":2,"807":1,"827":1,"829":1,"831":1,"835":1,"847":1,"856":1,"857":1,"858":3,"889":1,"891":1,"895":2,"896":1,"898":1,"904":3,"921":1,"927":1,"932":1,"933":1,"934":1,"1003":1,"1011":1,"1014":1,"1016":1,"1017":1,"1018":1,"1057":1,"1077":1,"1079":2,"1201":1,"1203":1,"1221":1,"1226":1,"1233":1,"1265":1,"1268":1,"1341":1,"1396":1,"1441":3,"1443":1,"1462":1,"1611":1,"1761":1,"1762":1,"1763":1}}],["思考的基础上",{"2":{"498":1}}],["思维树中的搜索历史",{"2":{"985":1}}],["思维",{"2":{"627":1}}],["思想",{"0":{"598":1,"607":1}}],["思路二",{"2":{"1485":1}}],["思路一",{"2":{"1485":1}}],["思路流程",{"2":{"1480":1}}],["思路图如下",{"2":{"892":1}}],["思路",{"0":{"46":1,"213":1,"223":1,"249":1,"267":1,"575":1,"628":1,"713":1,"717":1,"721":1},"1":{"224":1,"225":1},"2":{"0":1,"157":2}}],["条件编译",{"0":{"1985":1},"2":{"1632":1}}],["条件运算符",{"2":{"1630":1,"1712":1}}],["条件表达式2",{"2":{"1619":1}}],["条件表达式1",{"2":{"1619":1}}],["条件表达式",{"2":{"1619":3,"1620":3,"1621":2}}],["条件语句",{"2":{"1436":1}}],["条件语言模型意味着decoder",{"2":{"898":1}}],["条件语言模型可以使用一些预训练好的语言模型来对decoder的参数进行初始化",{"2":{"894":1}}],["条件分支和循环等情况",{"2":{"1287":1}}],["条件超参数可能会是一个问题",{"2":{"1144":1}}],["条件推导如下",{"2":{"1000":1}}],["条件论文链接",{"2":{"999":1}}],["条件",{"0":{"999":1},"2":{"497":1,"1316":1,"1630":1}}],["条样本",{"2":{"57":1}}],["星形胶质细胞网络理论上与",{"2":{"489":1}}],["星形胶质细胞网络的概述",{"2":{"489":1}}],["星形胶质细胞网络的数学模型",{"2":{"489":1}}],["星形胶质细胞在数据流经网络时会调节突触权重h",{"2":{"489":1}}],["星形胶质细胞可以形成一种记忆缓冲区",{"2":{"488":1}}],["星形胶质细胞可以向神经元发出信号",{"2":{"488":1}}],["星形胶质细胞",{"0":{"487":1},"1":{"488":1,"489":1}}],["附近是对称的",{"2":{"1004":1}}],["附录",{"0":{"836":1}}],["附加到",{"2":{"485":1}}],["附源码",{"2":{"361":1}}],["团队",{"2":{"485":1}}],["团队的成果",{"2":{"282":1}}],["梅西效力",{"2":{"485":1}}],["梅杰波尔坦的冬天",{"2":{"47":1}}],["莱昂内尔",{"2":{"485":1}}],["矢量被转置展示以强调跨度效果",{"2":{"485":1}}],["探秘transformer之",{"0":{"672":1}}],["探秘transformer系列之",{"0":{"0":1,"49":1,"96":1,"157":1,"234":1,"293":1,"362":1,"388":1,"514":1,"544":1,"673":1,"741":1},"1":{"1":1,"2":1,"3":1,"4":1,"5":1,"6":1,"7":1,"8":1,"9":1,"10":1,"11":1,"12":1,"13":1,"14":1,"15":1,"16":1,"17":1,"18":1,"19":1,"20":1,"21":1,"22":1,"23":1,"24":1,"25":1,"26":1,"27":1,"28":1,"29":1,"30":1,"31":1,"32":1,"33":1,"34":1,"35":1,"36":1,"37":1,"38":1,"39":1,"40":1,"41":1,"42":1,"43":1,"44":1,"45":1,"46":1,"47":1,"50":1,"51":1,"52":1,"53":1,"54":1,"55":1,"56":1,"57":1,"58":1,"59":1,"60":1,"61":1,"62":1,"63":1,"64":1,"65":1,"66":1,"67":1,"68":1,"69":1,"70":1,"71":1,"72":1,"73":1,"74":1,"75":1,"76":1,"77":1,"78":1,"79":1,"80":1,"81":1,"82":1,"83":1,"84":1,"85":1,"86":1,"87":1,"88":1,"89":1,"90":1,"91":1,"92":1,"93":1,"94":1,"95":1,"97":1,"98":1,"99":1,"100":1,"101":1,"102":1,"103":1,"104":1,"105":1,"106":1,"107":1,"108":1,"109":1,"110":1,"111":1,"112":1,"113":1,"114":1,"115":1,"116":1,"117":1,"118":1,"119":1,"120":1,"121":1,"122":1,"123":1,"124":1,"125":1,"126":1,"127":1,"128":1,"129":1,"130":1,"131":1,"132":1,"133":1,"134":1,"135":1,"136":1,"137":1,"138":1,"139":1,"140":1,"141":1,"142":1,"143":1,"144":1,"145":1,"146":1,"147":1,"148":1,"149":1,"150":1,"151":1,"152":1,"153":1,"154":1,"155":1,"156":1,"158":1,"159":1,"160":1,"161":1,"162":1,"163":1,"164":1,"165":1,"166":1,"167":1,"168":1,"169":1,"170":1,"171":1,"172":1,"173":1,"174":1,"175":1,"176":1,"177":1,"178":1,"179":1,"180":1,"181":1,"182":1,"183":1,"184":1,"185":1,"186":1,"187":1,"188":1,"189":1,"190":1,"191":1,"192":1,"193":1,"194":1,"195":1,"196":1,"197":1,"198":1,"199":1,"200":1,"201":1,"202":1,"203":1,"204":1,"205":1,"206":1,"207":1,"208":1,"209":1,"210":1,"211":1,"212":1,"213":1,"214":1,"215":1,"216":1,"217":1,"218":1,"219":1,"220":1,"221":1,"222":1,"223":1,"224":1,"225":1,"226":1,"227":1,"228":1,"229":1,"230":1,"231":1,"232":1,"233":1,"235":1,"236":1,"237":1,"238":1,"239":1,"240":1,"241":1,"242":1,"243":1,"244":1,"245":1,"246":1,"247":1,"248":1,"249":1,"250":1,"251":1,"252":1,"253":1,"254":1,"255":1,"256":1,"257":1,"258":1,"259":1,"260":1,"261":1,"262":1,"263":1,"264":1,"265":1,"266":1,"267":1,"268":1,"269":1,"270":1,"271":1,"272":1,"273":1,"274":1,"275":1,"276":1,"277":1,"278":1,"279":1,"280":1,"281":1,"282":1,"283":1,"284":1,"285":1,"286":1,"287":1,"288":1,"289":1,"290":1,"291":1,"292":1,"294":1,"295":1,"296":1,"297":1,"298":1,"299":1,"300":1,"301":1,"302":1,"303":1,"304":1,"305":1,"306":1,"307":1,"308":1,"309":1,"310":1,"311":1,"312":1,"313":1,"314":1,"315":1,"316":1,"317":1,"318":1,"319":1,"320":1,"321":1,"322":1,"323":1,"324":1,"325":1,"326":1,"327":1,"328":1,"329":1,"330":1,"331":1,"332":1,"333":1,"334":1,"335":1,"336":1,"337":1,"338":1,"339":1,"340":1,"341":1,"342":1,"343":1,"344":1,"345":1,"346":1,"347":1,"348":1,"349":1,"350":1,"351":1,"352":1,"353":1,"354":1,"355":1,"356":1,"357":1,"358":1,"359":1,"360":1,"361":1,"363":1,"364":1,"365":1,"366":1,"367":1,"368":1,"369":1,"370":1,"371":1,"372":1,"373":1,"374":1,"375":1,"376":1,"377":1,"378":1,"379":1,"380":1,"381":1,"382":1,"383":1,"384":1,"385":1,"386":1,"387":1,"389":1,"390":1,"391":1,"392":1,"393":1,"394":1,"395":1,"396":1,"397":1,"398":1,"399":1,"400":1,"401":1,"402":1,"403":1,"404":1,"405":1,"406":1,"407":1,"408":1,"409":1,"410":1,"411":1,"412":1,"413":1,"414":1,"415":1,"416":1,"417":1,"418":1,"419":1,"420":1,"421":1,"422":1,"423":1,"424":1,"425":1,"426":1,"427":1,"428":1,"429":1,"515":1,"516":1,"517":1,"518":1,"519":1,"520":1,"521":1,"522":1,"523":1,"524":1,"525":1,"526":1,"527":1,"528":1,"529":1,"530":1,"531":1,"532":1,"533":1,"534":1,"535":1,"536":1,"537":1,"538":1,"539":1,"540":1,"541":1,"542":1,"543":1,"545":1,"546":1,"547":1,"548":1,"549":1,"550":1,"551":1,"552":1,"553":1,"554":1,"555":1,"556":1,"557":1,"558":1,"559":1,"560":1,"561":1,"562":1,"563":1,"564":1,"565":1,"566":1,"567":1,"568":1,"569":1,"570":1,"571":1,"572":1,"573":1,"574":1,"575":1,"576":1,"577":1,"578":1,"579":1,"580":1,"581":1,"582":1,"583":1,"584":1,"585":1,"586":1,"587":1,"588":1,"589":1,"590":1,"591":1,"592":1,"593":1,"594":1,"595":1,"596":1,"597":1,"598":1,"599":1,"600":1,"601":1,"602":1,"603":1,"604":1,"605":1,"606":1,"607":1,"608":1,"609":1,"610":1,"611":1,"612":1,"613":1,"614":1,"615":1,"616":1,"617":1,"618":1,"619":1,"620":1,"621":1,"622":1,"623":1,"624":1,"625":1,"626":1,"627":1,"628":1,"629":1,"630":1,"631":1,"632":1,"633":1,"634":1,"635":1,"636":1,"637":1,"638":1,"674":1,"675":1,"676":1,"677":1,"678":1,"679":1,"680":1,"681":1,"682":1,"683":1,"684":1,"685":1,"686":1,"687":1,"688":1,"689":1,"690":1,"691":1,"692":1,"693":1,"694":1,"695":1,"696":1,"697":1,"698":1,"699":1,"700":1,"701":1,"702":1,"703":1,"704":1,"705":1,"706":1,"707":1,"708":1,"709":1,"710":1,"711":1,"712":1,"713":1,"714":1,"715":1,"716":1,"717":1,"718":1,"719":1,"720":1,"721":1,"722":1,"723":1,"724":1,"725":1,"726":1,"727":1,"728":1,"729":1,"730":1,"731":1,"732":1,"733":1,"734":1,"735":1,"736":1,"737":1,"738":1,"739":1,"740":1,"742":1,"743":1,"744":1,"745":1,"746":1,"747":1,"748":1,"749":1,"750":1,"751":1,"752":1,"753":1,"754":1,"755":1,"756":1,"757":1,"758":1,"759":1,"760":1,"761":1,"762":1,"763":1,"764":1,"765":1,"766":1,"767":1,"768":1},"2":{"0":1,"49":1,"96":1,"157":1,"293":1,"741":1}}],["探索了构建多语言模型的多个预训练目标",{"2":{"1315":1}}],["探索源码",{"0":{"1248":1}}],["探索阶段",{"2":{"1175":1}}],["探索",{"2":{"1140":1}}],["探索结束后",{"0":{"1153":1},"2":{"1125":1}}],["探索与利用",{"0":{"1140":1},"2":{"1125":1}}],["探索agi系列",{"2":{"513":1}}],["探索设计出更好的网络结构和有效的训练方法",{"2":{"474":1}}],["于",{"2":{"470":1,"1603":1}}],["于是排查错误",{"2":{"2070":1}}],["于是我尝试使用create",{"2":{"2070":1}}],["于是我们寻找新的方法",{"2":{"1377":1}}],["于是我们得到随机变量",{"2":{"1003":1}}],["于是我们得到最终方差传递公式",{"2":{"1003":1}}],["于是我们可以得出这样的结论",{"2":{"1465":1}}],["于是我们可以得到两组结论",{"2":{"1000":1}}],["于是我们可以通过key搜索出来很多书",{"2":{"169":1}}],["于是我们以词汇在词典中的索引来表示词汇",{"2":{"679":1}}],["于是我们来看看如何找合适的",{"2":{"145":1}}],["于是可得到",{"2":{"1394":1}}],["于是上式可化简为",{"2":{"1003":1}}],["于是它将整个模型分为两部分来理解",{"2":{"763":1}}],["于是研究人员就设计了mntp这个任务",{"2":{"734":1}}],["于是研究人员提出了一种新的解码方法",{"2":{"727":1}}],["于是针对one",{"2":{"682":1}}],["于是就出现了layer",{"2":{"316":1}}],["于是就萌发了自己写一个系列的想法",{"2":{"235":1}}],["于是人们想出了一个大力出奇迹的方法",{"2":{"712":1}}],["于是人们想到使用神经网络来拟合",{"2":{"242":1}}],["于是人们使用了一种新的架构",{"2":{"241":1}}],["于是修改后的词如下",{"2":{"167":1}}],["于是解码器的每一步都会得到一个不一样的对齐向量",{"2":{"165":1}}],["于是论文作者直接使用目标知识的嵌入来修改相应的值槽",{"2":{"143":1}}],["辅助函数",{"2":{"1887":1}}],["辅助字符串解析和数据类型转换",{"2":{"1825":1,"1843":1}}],["辅助架构",{"0":{"467":1},"1":{"468":1,"469":1,"470":1}}],["辅助记忆会被使用",{"2":{"143":1}}],["辅助记忆设计",{"2":{"143":1}}],["施加softmax和dropout操作",{"2":{"464":1}}],["施加掩码",{"0":{"75":1},"2":{"49":1}}],["窥视",{"2":{"464":1}}],["窥视未来信息",{"2":{"77":1}}],["得以优化",{"2":{"1456":1}}],["得分",{"2":{"904":1}}],["得出的",{"2":{"463":1}}],["得到实际绘制的像素点",{"2":{"2018":1}}],["得到第二行的首地址",{"2":{"1705":1}}],["得到结果",{"2":{"1371":1}}],["得到比baseline更好的结果",{"2":{"1152":1}}],["得到非常糟糕的损失值",{"2":{"1146":1}}],["得到只有",{"2":{"776":1}}],["得到对应的的句向量",{"2":{"727":1}}],["得到这个token在该层的表示",{"2":{"717":1}}],["得到单词的上下文信息",{"2":{"717":1}}],["得到隐层权重表",{"2":{"709":1}}],["得到文本语义含义的信息密集表示",{"2":{"676":1}}],["得到最小值",{"2":{"592":1}}],["得到最终掩码",{"2":{"74":1,"380":1}}],["得到最终掩码如下",{"2":{"74":1,"382":1}}],["得到最终的上下文相关词向量",{"2":{"718":1}}],["得到最终的嵌入表示",{"2":{"460":1}}],["得到最终的word",{"2":{"455":1,"698":1}}],["得到最终的zz",{"2":{"71":1}}],["得到最终的输出softmax",{"2":{"199":1}}],["得到最终的输出",{"2":{"5":1}}],["得到原始字节",{"2":{"592":1}}],["得到一张图片的完整路径",{"2":{"1250":1}}],["得到一种划分粒度介于词与字符之间的中间粒度表示",{"2":{"567":1}}],["得到一个类似于样本的结果",{"2":{"1371":1}}],["得到一个输入句子的向量表示m",{"2":{"289":1}}],["得到一个单独句子中邻接词的一些依赖",{"2":{"289":1}}],["得到一个d×d的矩阵",{"2":{"180":1}}],["得到更小的单元",{"2":{"553":1}}],["得到该token在词表中的序号",{"2":{"545":1}}],["得到了一个新的model",{"2":{"1269":1}}],["得到了新的ids",{"2":{"592":1}}],["得到了r1和r2",{"2":{"519":1}}],["得到了目标序列内部元素之间的彼此相关性",{"2":{"515":1}}],["得到了源序列内部元素之间的彼此相关性",{"2":{"515":1}}],["得到了每个字对应的唯一序号",{"2":{"453":1}}],["得到transformer的一层为标号7",{"2":{"498":1}}],["得到token序列",{"2":{"545":1}}],["得到token",{"2":{"455":1}}],["得到标号3",{"2":{"498":1}}],["得到网络的新输出为f",{"2":{"470":1}}],["得到网络的输出为f",{"2":{"294":1,"300":1}}],["得到得到input",{"2":{"450":1}}],["得到word",{"2":{"450":2}}],["得到input",{"2":{"450":1}}],["得到编码结果memory",{"2":{"450":1}}],["得到目标语言词典中",{"2":{"423":1}}],["得到logits",{"2":{"397":1}}],["得到权重之后",{"2":{"394":1}}],["得到通道",{"2":{"315":1}}],["得到通用",{"2":{"43":1}}],["得到的点有意义",{"2":{"1375":1}}],["得到的结果也是一些手写数字",{"2":{"1371":1}}],["得到的结果是源端",{"2":{"649":1,"931":1}}],["得到的张量与",{"2":{"1114":1}}],["得到的矩阵我们称之为",{"2":{"923":1}}],["得到的英文应该是",{"2":{"516":1}}],["得到的均值和标准差分别记作",{"2":{"313":1}}],["得到的注意力权重为",{"2":{"267":1}}],["得到的注意力分数",{"2":{"62":1}}],["得到向量权重a",{"2":{"289":1}}],["得到context的方式有很多种",{"2":{"888":1}}],["得到c4=",{"2":{"267":1}}],["得到c3=",{"2":{"267":1}}],["得到c2=",{"2":{"267":1}}],["得到c1=",{"2":{"267":1}}],["得到解码器当前时刻的上下文语义向量context",{"2":{"267":1}}],["得到上下文向量c",{"2":{"264":1}}],["得到h2",{"2":{"249":1}}],["得到输出memory",{"2":{"528":1}}],["得到输出概率分布后就可以计算loss了",{"2":{"426":1}}],["得到输出h1=f",{"2":{"249":1}}],["得到输出矩阵中的值",{"2":{"189":1}}],["得到当前的xtxtx",{"2":{"240":1}}],["得到现在的隐变量htℎtℎ",{"2":{"240":1}}],["得到新的命题",{"2":{"510":1}}],["得到新权重b",{"2":{"209":1}}],["得到新矩阵的每个元素",{"2":{"199":1}}],["得到如下图所示",{"2":{"213":1}}],["得到如下",{"2":{"204":1,"582":1}}],["得到xq",{"2":{"201":1}}],["得到相关权重矩阵",{"2":{"200":1}}],["得到相关权重矩阵score",{"2":{"200":2}}],["得到相似度之后",{"2":{"164":1}}],["得到注意力的最终输出",{"2":{"201":1}}],["得到注意力权重a",{"2":{"537":1}}],["得到注意力权重之后",{"2":{"270":1}}],["得到注意力权重w",{"2":{"267":1}}],["得到注意力权重",{"2":{"199":2}}],["得到注意力分数之后",{"2":{"269":1}}],["得到注意力分数",{"2":{"173":1,"199":1,"271":1}}],["得到key的转置矩阵k^t",{"2":{"199":1}}],["得到两个向量的乘积为",{"2":{"176":1}}],["得到加权之后的向量",{"2":{"170":1}}],["得到需要获取的商品id",{"2":{"164":1}}],["得到词表分布",{"2":{"128":1}}],["得到词嵌入xx",{"2":{"71":1}}],["得到a=softmax",{"2":{"71":1}}],["得到遮蔽的注意力分数分布",{"2":{"71":1}}],["得到qktqkt",{"2":{"71":1}}],["得到qq",{"2":{"71":1}}],["得到",{"2":{"27":1,"128":2,"161":1,"169":1,"446":1,"587":1,"908":1,"1705":1}}],["得到多个独立的输出特征张量",{"2":{"7":1}}],["涵盖内容",{"2":{"2001":1}}],["涵盖了对自然语言进行理解和生成的各个方面",{"2":{"906":1}}],["涵盖了对词汇关系的理解",{"2":{"462":1}}],["涵盖自然语言和代码中各种知识与结构",{"2":{"386":1}}],["视图层展示",{"2":{"1493":1}}],["视图",{"2":{"1079":1}}],["视图到底是什么",{"0":{"1079":1}}],["视",{"2":{"713":1}}],["视觉听觉语言",{"2":{"676":1}}],["视觉听觉语言等信息都映射",{"2":{"460":1}}],["视频",{"2":{"676":1,"696":1}}],["视角",{"2":{"520":1,"530":1}}],["视为可训练参数",{"2":{"1344":1}}],["视为",{"2":{"1340":1}}],["视为不具有拟合能力的线性变换的惯性认知",{"2":{"320":1}}],["视为更值得记忆的",{"2":{"228":1}}],["视为线性联想记忆",{"2":{"145":1}}],["程序=数据结构+算法",{"2":{"2097":1}}],["程序是怎样跑起来的",{"2":{"2043":1}}],["程序应输出一个单词频率列表",{"2":{"1933":1}}],["程序需要从文件读取文本",{"2":{"1933":1}}],["程序查找匹配的",{"2":{"1762":1}}],["程序逻辑错误",{"2":{"1762":1}}],["程序会发生什么",{"2":{"1762":1}}],["程序会调用",{"2":{"1762":1,"1764":2}}],["程序会立即停止执行",{"2":{"1762":1}}],["程序会尝试执行",{"2":{"1762":1}}],["程序会提示你输入一个年份",{"2":{"1729":1}}],["程序中的错误和异常",{"2":{"1761":1}}],["程序结束时",{"2":{"1671":1}}],["程序性能下降",{"2":{"1671":1}}],["程序崩溃",{"2":{"1670":1}}],["程序启动时",{"2":{"1649":2}}],["程序开始到程序结束",{"2":{"1649":3}}],["程序开始时",{"2":{"1608":1}}],["程序员可能忘记检查返回值",{"2":{"1761":1}}],["程序员可以使用",{"2":{"1648":1}}],["程序员手动分配和释放",{"2":{"1648":1}}],["程序员需要重点理解的两个区域",{"2":{"1648":1}}],["程序可能会崩溃或产生不可预测的结果",{"2":{"1647":1}}],["程序至关重要",{"2":{"1644":1,"1729":1}}],["程序执行的入口",{"2":{"1606":1}}],["程序",{"0":{"1606":1},"2":{"1589":2,"1594":1,"1606":1,"1608":1,"1727":1,"1810":1,"1828":1}}],["程序的入口点",{"2":{"1729":1}}],["程序的基石",{"2":{"1709":1}}],["程序的工具",{"2":{"1589":1}}],["程序的运行工具",{"2":{"1589":1}}],["程序的性能",{"2":{"985":2}}],["程序而专门设计的",{"2":{"1589":1}}],["程序运行的实例",{"2":{"1563":1}}],["程序运行的核心",{"2":{"1435":1}}],["程序所需的类库",{"2":{"1435":1}}],["程序语言",{"2":{"460":1,"676":1}}],["程度",{"2":{"93":1}}],["继续用下一块大饼干尝试满足同一个小孩",{"2":{"2152":1}}],["继续讲解剩下的内容",{"2":{"1749":1}}],["继续判断条件",{"2":{"1621":1}}],["继续finetune",{"2":{"1363":1}}],["继续训练",{"2":{"1173":1}}],["继续通过多个独立同分布变量求一个变量",{"2":{"1004":1}}],["继续被替换成y",{"2":{"575":1}}],["继承来创建一组类型",{"2":{"1866":1}}],["继承基类时",{"2":{"1866":1}}],["继承使得基类的接口完全隐藏",{"2":{"1861":1}}],["继承增强了封装性",{"2":{"1857":1}}],["继承时",{"2":{"1851":1,"1855":1,"1859":1}}],["继承是面向对象编程的基石之一",{"2":{"1848":1}}],["继承是面向对象编程中一个重要的概念",{"2":{"1654":1}}],["继承的特点",{"0":{"1852":1,"1856":1,"1860":1}}],["继承的意义",{"2":{"1848":1}}],["继承的定义",{"0":{"1848":1,"1851":1,"1855":1,"1859":1}}],["继承的基本概念",{"0":{"1847":1},"1":{"1848":1,"1849":1}}],["继承中函数的重载和覆盖",{"0":{"1663":1},"2":{"1658":1}}],["继承进阶",{"0":{"1658":1},"1":{"1659":1,"1660":1,"1661":1,"1662":1,"1663":1}}],["继承自",{"2":{"1657":2,"1869":1}}],["继承自pytorch的nn",{"2":{"533":1,"701":1}}],["继承方式应用",{"0":{"1870":1},"1":{"1871":1,"1872":1,"1873":1,"1874":1}}],["继承方式",{"0":{"1845":1},"1":{"1846":1,"1847":1,"1848":1,"1849":1,"1850":1,"1851":1,"1852":1,"1853":1,"1854":1,"1855":1,"1856":1,"1857":1,"1858":1,"1859":1,"1860":1,"1861":1,"1862":1,"1863":1,"1864":1,"1865":1,"1866":1,"1867":1,"1868":1,"1869":1},"2":{"1654":1,"1863":1,"1871":1}}],["继承与权限控制",{"0":{"1652":1}}],["继承",{"0":{"1850":1,"1854":1,"1858":1,"1866":1,"1867":1,"1868":1},"1":{"1851":1,"1852":1,"1853":1,"1855":1,"1856":1,"1857":1,"1859":1,"1860":1,"1861":1},"2":{"1491":1,"1603":1,"1728":2,"1763":1,"1864":3,"1867":1,"1873":2,"1874":1}}],["继承nn",{"2":{"450":1}}],["继resnet论文之后",{"2":{"302":1}}],["容器化应用",{"2":{"1499":1}}],["容器内模块的顺序就是模型处理数据的顺序",{"2":{"449":1}}],["容错性",{"2":{"696":1}}],["容量有限",{"2":{"565":1}}],["容易忽略错误",{"2":{"1761":1}}],["容易产生副作用",{"2":{"1632":1}}],["容易引入类型错误",{"2":{"1632":1}}],["容易导致内存泄漏或数据不一致",{"2":{"1694":1}}],["容易导致程序流程混乱",{"2":{"1631":1}}],["容易导致梯度爆炸",{"2":{"403":1}}],["容易出现内存泄漏或悬空指针等问题",{"2":{"1695":1}}],["容易出现",{"2":{"626":1}}],["容易切分句子",{"2":{"565":1}}],["容易保持语义",{"2":{"565":1}}],["容易过拟合",{"2":{"396":1}}],["容易累积错误",{"2":{"239":1,"405":1}}],["容易因为参数量达到容量上限而造成模型性能不足",{"2":{"9":1}}],["容易会过度的将注意力集中于当前的位置",{"2":{"3":1}}],["构成条件随机场",{"2":{"1322":1}}],["构成的网络",{"2":{"1143":1}}],["构成",{"2":{"485":1}}],["构造函数与初始化列表",{"2":{"2006":1}}],["构造函数是一种特殊的成员函数",{"2":{"1675":1}}],["构造函数参数初始化表是在构造函数的参数列表和函数体之间",{"2":{"1641":1}}],["构造函数参数初始化列表",{"0":{"1641":1}}],["构造函数初始化列表中初始化常量成员变量",{"2":{"1640":1}}],["构造函数",{"0":{"1085":1,"1675":1},"2":{"1214":1,"1227":1,"1637":1,"1642":1,"1674":1,"1694":1,"1902":1}}],["构造的",{"2":{"504":2}}],["构造",{"2":{"449":1,"1481":1}}],["构建云服务并备案",{"2":{"2043":1}}],["构建和运行",{"2":{"1999":1}}],["构建项目",{"2":{"1997":1,"1999":1}}],["构建包含多个源文件和库的项目",{"2":{"1997":1}}],["构建代码模块",{"2":{"1729":1}}],["构建更复杂的数据结构",{"2":{"1728":1}}],["构建复杂的数据结构",{"2":{"1611":1}}],["构建系统无缝集成",{"2":{"1605":1}}],["构建概念体系很重要",{"2":{"1598":1}}],["构建文本嵌入模型的主导范式依赖于预训练的双向编码器模型或仅编码器模型",{"2":{"729":1}}],["构建模型时候会用到embeddings类生成encoderdecoder类的实例",{"2":{"703":1}}],["构建初始词表",{"0":{"580":1}}],["构建词汇表需要对数据集进行分词",{"2":{"557":1}}],["构建词表",{"0":{"557":1}}],["构建词和词之间的关系",{"2":{"431":1}}],["构建注意力掩码以区分实际token和填充token",{"2":{"555":1}}],["构建注意力掩码",{"2":{"555":1}}],["构建输入到输出的circuit",{"2":{"475":1}}],["构建位置编码模块的实例",{"2":{"449":1}}],["构建前馈神经网络层的实例",{"2":{"449":1}}],["构建多头注意力层的实例",{"2":{"449":1}}],["构建逻辑",{"0":{"449":1}}],["构建学习率策略",{"2":{"423":1}}],["构建优化器",{"2":{"423":1}}],["构建数据加载器",{"2":{"423":1}}],["构建损失函数",{"2":{"423":1}}],["构建一个大型的双语对照表",{"2":{"907":1}}],["构建一个生物学上合理的",{"2":{"489":1}}],["构建一个6层模型",{"2":{"423":1}}],["构建一个层归一化",{"2":{"343":1}}],["构建batch",{"0":{"383":1}}],["构建验证数据加载器",{"2":{"375":1}}],["构建训练数据加载器",{"2":{"375":1}}],["构建字典",{"2":{"371":1}}],["构建了一个全新的序列转换架构",{"2":{"291":1}}],["构建",{"0":{"447":1},"1":{"448":1,"449":1,"450":1,"451":1},"2":{"135":1,"428":1,"449":1}}],["却形成了一个概念空间",{"2":{"689":1}}],["却无法直接理解文字",{"2":{"679":1}}],["却无法传递词汇的深层含义",{"2":{"678":1}}],["却产生了所有个体都不具备的高阶能力",{"2":{"446":1}}],["却又不必显式考虑那么多单词组合可能性",{"2":{"242":1}}],["双端操作",{"2":{"1800":1}}],["双端队列",{"2":{"1795":1,"1800":1}}],["双引号",{"2":{"1616":2}}],["双精度浮点型",{"2":{"1607":1}}],["双隐层感知器就足以解决任何复杂的分类问题",{"2":{"1465":1}}],["双标志后检查法",{"0":{"1419":1}}],["双标志先检查法",{"0":{"1418":1}}],["双塔扩散lcm",{"0":{"635":1}}],["双向链表",{"2":{"1795":1,"1799":1}}],["双向注意力",{"2":{"735":1}}],["双向注意力机制",{"2":{"734":1}}],["双向注意力是一把双刃剑",{"2":{"542":1}}],["双向",{"2":{"721":1,"1315":1}}],["双向双层lstm",{"2":{"717":1}}],["双向自注意力是自编码模式",{"2":{"444":1}}],["双块注意力",{"2":{"204":1}}],["末尾的元素",{"2":{"1719":1,"1720":1,"1722":1}}],["末尾的部分",{"2":{"89":1}}],["末尾添加元素",{"2":{"1719":1,"1720":1,"1722":1}}],["末层权重梯度计算",{"0":{"1391":1}}],["末层的mlp似乎在将残差流引导向一个",{"2":{"437":1}}],["较小的batch",{"2":{"1186":1}}],["较小随机值时",{"0":{"994":1}}],["较长的预热时间可以纠正梯度截断无法纠正的不稳定性",{"2":{"1184":1}}],["较新的slm",{"2":{"559":1}}],["较浅层的注意力模块对模型的泛化和推理能力至关重要",{"2":{"437":1}}],["较深层的注意力模块",{"2":{"437":1}}],["较大随机初始值时",{"0":{"995":1}}],["较大的batch",{"2":{"1186":1}}],["较大的模型相对较少受到注意力的内存带宽开销的影响",{"2":{"937":1,"953":1}}],["较大的词汇表意味着模型需要更多的计算资源来处理存储分词嵌入",{"2":{"562":1}}],["较大的词汇表可以提高模型覆盖不同词汇和表达的能力",{"2":{"561":1}}],["较大的dqkdqkd",{"2":{"93":1}}],["较大学习率会增加收敛难度",{"2":{"402":1}}],["剖析了",{"2":{"437":1}}],["符合或近似符合0",{"2":{"848":1}}],["符合现有的gpu框架",{"2":{"434":1}}],["符号在形式参数类型前声明",{"2":{"1729":1}}],["符号声明引用类型",{"2":{"1612":1}}],["符号位",{"2":{"1075":2}}],["符号与其他位置分开",{"2":{"764":1}}],["符号属性空间和位置空间通过线性映射到一个统一的",{"2":{"722":1}}],["符号✔",{"2":{"139":1}}],["采取对应的策略和方法",{"2":{"2115":1}}],["采样",{"2":{"431":1}}],["采用的是普通的",{"2":{"1364":1}}],["采用这种方法时",{"2":{"1144":1}}],["采用这种方法可以自然地使用温度",{"2":{"636":1}}],["采用",{"2":{"986":1}}],["采用可分离卷积的计算量比标准卷积要少",{"2":{"777":1}}],["采用文本的结尾的eos",{"2":{"735":1}}],["采用in",{"2":{"726":1}}],["采用迁移学习",{"2":{"670":1}}],["采用了更大的text",{"2":{"1363":1}}],["采用了不同的方法来处理数字",{"2":{"595":1}}],["采用了",{"2":{"501":1}}],["采用了负幂形式",{"2":{"402":1}}],["采用递归位置编码的transformer可以精确复制海马结构",{"2":{"490":1}}],["采用左填充的原因",{"2":{"387":1}}],["采用适合自己的训练配置",{"2":{"334":1}}],["采用混合架构",{"2":{"215":1}}],["化繁为简",{"0":{"2104":1}}],["化并转化为词表",{"2":{"700":1}}],["化",{"2":{"431":1,"624":1}}],["拆解为若干独立的词汇单元",{"2":{"431":1}}],["拆分成",{"2":{"576":1}}],["拆分成了h个小的",{"2":{"28":1}}],["拆分开来",{"2":{"553":1}}],["拆分为两个步骤",{"2":{"320":1}}],["拆分为两部分",{"2":{"301":1}}],["咕噜咕噜day的博客",{"2":{"429":1}}],["迭代通常使用循环结构实现",{"2":{"1646":1}}],["迭代训练",{"0":{"1396":1}}],["迭代一次又一次",{"2":{"709":1}}],["迭代次数多",{"2":{"582":1}}],["迭代次数太小",{"2":{"582":1}}],["迭代器提供了一种访问容器元素的通用方法",{"2":{"1718":1}}],["迭代器简介",{"0":{"1718":1}}],["迭代器与容器的应用",{"0":{"1717":1},"1":{"1718":1,"1719":1,"1720":1,"1721":1,"1722":1,"1723":1,"1724":1,"1725":1}}],["迭代器的类型通常很长",{"2":{"1615":1}}],["迭代器",{"2":{"557":1}}],["迭代运行2~4",{"2":{"427":1}}],["迭代",{"2":{"427":1}}],["曝光误差",{"2":{"411":1}}],["复习考试",{"2":{"2099":1}}],["复合赋值运算符重载",{"2":{"1712":1}}],["复合赋值运算符",{"2":{"1630":1}}],["复合数据类型",{"0":{"1622":1},"1":{"1623":1,"1624":1}}],["复数可以设置",{"2":{"1106":1}}],["复用",{"0":{"623":1}}],["复制构造函数与临时对象",{"2":{"2006":1}}],["复制",{"2":{"1734":1,"1891":1,"1930":1}}],["复制字符串",{"2":{"1715":1}}],["复制选择内容",{"2":{"1551":1}}],["复制当前单词",{"2":{"1549":1}}],["复制当前行",{"2":{"1520":1,"1549":1}}],["复制和粘贴",{"0":{"1549":1}}],["复制和移动文件",{"2":{"1510":1}}],["复制文件",{"2":{"1510":1}}],["复制给",{"2":{"944":1}}],["复制过来的对象和源对象没有任何关联",{"2":{"449":1}}],["复制5次构成一个矩阵",{"2":{"408":1}}],["复杂类型推导",{"2":{"1879":1}}],["复杂控制流的函数",{"2":{"1709":1}}],["复杂的问题就需要我们认真去分析了",{"0":{"2127":1}}],["复杂的宏定义可能降低代码可读性",{"2":{"1632":1}}],["复杂的知识分解成更小的",{"2":{"221":1}}],["复杂模型具有较强的表达能力",{"2":{"1012":1}}],["复杂数据中的非线性特征交互是由模型学习而不是人工设计的",{"2":{"708":1}}],["复杂输入",{"2":{"612":1}}],["复杂性",{"2":{"273":1,"504":2}}],["复杂度是o",{"2":{"210":1}}],["靠近的词比远离的词更重要",{"2":{"765":1}}],["靠老师",{"2":{"411":1}}],["靠标准答案",{"2":{"406":1}}],["靠自己",{"2":{"406":1,"411":1}}],["靠后层的学习就会建立在错误基础上",{"2":{"401":1}}],["靠后的层是以考前层的输出为基础进行学习",{"2":{"401":1}}],["教学课件",{"0":{"1609":1},"1":{"1610":1,"1611":1,"1612":1,"1613":1,"1614":1,"1615":1,"1616":1}}],["教育",{"2":{"2097":1}}],["教育和生产环境中的高性能计算",{"2":{"1569":1}}],["教育价值",{"2":{"369":1}}],["教授学生以及为我们的同事提供实践建议的经验",{"2":{"1127":1}}],["教科书也往往回避实用指南",{"2":{"1127":1}}],["教师强制训练",{"2":{"406":1}}],["η​max​​−η​min​​",{"2":{"1243":1,"1244":1}}],["η​t​​=η​min​​+​2​​1​​",{"2":{"1243":1,"1244":1}}],["ηmax−ηmin",{"2":{"1243":1,"1244":1}}],["ηt=ηmin+12",{"2":{"1243":1,"1244":1}}],["η",{"2":{"402":1,"1045":1,"1049":1}}],["衰减方案",{"2":{"1173":1}}],["衰减方案是根据验证集性能以临时方式调整衰减方案的结果",{"2":{"1173":1}}],["衰减方案是什么",{"2":{"1171":1}}],["衰减方案的论文并不少见",{"2":{"1173":1}}],["衰减速度先快后慢",{"2":{"402":1}}],["衰减",{"2":{"402":1,"1173":1}}],["衰减器",{"2":{"224":1}}],["缓冲区",{"2":{"1590":1}}],["缓冲区默认是持久的",{"2":{"1211":1}}],["缓冲区等",{"2":{"1208":1}}],["缓存等",{"2":{"1506":1}}],["缓存",{"2":{"986":1}}],["缓存的流式传输可以重叠延迟",{"2":{"977":1}}],["缓存将成为限制推理效率的瓶颈",{"2":{"956":1}}],["缓办",{"2":{"679":1}}],["缓增到指定大小",{"2":{"401":1}}],["缓解数据稀疏性问题",{"2":{"898":2}}],["缓解了过拟合问题的发生",{"2":{"840":1}}],["缓解",{"2":{"314":1}}],["缓解退化",{"0":{"305":1},"2":{"293":1}}],["缓解稀疏",{"0":{"14":1},"2":{"0":1}}],["热身",{"2":{"401":1,"402":1}}],["找零问题",{"0":{"2136":1},"1":{"2137":1,"2138":1,"2139":1}}],["找东西",{"0":{"2100":1},"2":{"2108":1}}],["找不到返回",{"2":{"1713":1}}],["找出列表中的最小值和最大值分别输出",{"2":{"1759":1}}],["找出得出概率最大",{"2":{"1320":1}}],["找出具备语义的字符串转换成token",{"2":{"549":1}}],["找出target中为",{"2":{"399":1}}],["找到vmware的nat和dhcp服务并开启",{"2":{"2093":1}}],["找到专业中需要解决的问题",{"2":{"1598":1}}],["找到进程",{"2":{"1524":1}}],["找到本机pytorch",{"2":{"1200":1}}],["找到这个值作为max",{"2":{"1156":1}}],["找到出现最多的字节对",{"2":{"592":1}}],["找到按照注意力分数匹配的高阶向量",{"2":{"526":1}}],["找到它们之间的共同模式和结构",{"2":{"505":1}}],["找到",{"2":{"200":1,"1073":3}}],["找到输入序列自身的关系",{"2":{"200":1}}],["找到与现有数据",{"2":{"164":1}}],["找到自身的关系",{"2":{"38":1}}],["ϵ",{"2":{"1016":3,"1130":1,"1174":1}}],["ϵls",{"2":{"399":1}}],["ϵr1×2um",{"2":{"289":2}}],["损失c对w的权重有两部分",{"2":{"1450":1}}],["损失值",{"2":{"1398":1}}],["损失通常高度敏感于参数空间中的某些方向",{"2":{"1041":1}}],["损失反而越大",{"2":{"399":1}}],["损失函数我们使用mseloss",{"2":{"1386":1}}],["损失函数将此输出序列与训练数据中的目标序列进行比较",{"2":{"427":1}}],["损失函数值曲面上不再存在平缓区域",{"2":{"399":1}}],["损失函数就是要计算两者之间的差异",{"2":{"398":1}}],["损失函数通过评估模型预测值与真实值之间的差异来直观地了解模型的预测性能",{"2":{"397":1}}],["损失函数",{"0":{"397":1},"1":{"398":1,"399":1},"2":{"1202":1,"1438":1}}],["损害输入数据原本的表达能力等",{"2":{"309":1}}],["损害了其对重要特征的关注",{"2":{"212":1}}],["答案",{"2":{"1374":1}}],["答案不一",{"2":{"396":1}}],["答案是不必要",{"2":{"1157":1}}],["答案是不会的",{"2":{"307":1}}],["答案是不需要",{"2":{"72":1}}],["发生严重错误",{"2":{"1814":1,"1832":1}}],["发生歧义",{"2":{"1660":2}}],["发生栈溢出",{"2":{"1648":1}}],["发微信",{"2":{"1580":1}}],["发送端和接收端采用同一个变量命名",{"2":{"1590":1}}],["发送方会等待接收方收到消息后",{"2":{"1574":1}}],["发送消息",{"2":{"1563":2}}],["发布",{"2":{"1498":1}}],["发布截止日期",{"2":{"1173":1}}],["发射分数的生成过程如下",{"2":{"1323":1}}],["发射分数",{"0":{"1323":1},"2":{"1328":1}}],["发射这个概念是从crf里面带出来的",{"2":{"1320":1}}],["发疯",{"2":{"562":1}}],["发展脉络",{"0":{"1311":1},"1":{"1312":1,"1313":1,"1314":1,"1315":1,"1316":1,"1317":1,"1318":1}}],["发展",{"0":{"396":1,"609":1},"1":{"610":1,"611":1,"612":1,"613":1,"614":1,"615":1,"616":1,"617":1,"618":1,"619":1,"620":1,"621":1,"622":1,"623":1,"624":1,"625":1,"626":1,"627":1,"628":1,"629":1,"630":1,"631":1,"632":1,"633":1,"634":1,"635":1,"636":1,"637":1}}],["发现没了",{"2":{"2070":1}}],["发现有知己之时",{"2":{"2054":1}}],["发现寝室没人",{"2":{"2054":1}}],["发现潜在的新方向",{"2":{"1140":1}}],["发现中间两项看起来很均匀",{"2":{"764":1}}],["发现rw和hs嵌入在聚类行为和主题上有所不同",{"2":{"739":1}}],["发现训练不足能让大模型",{"2":{"562":1}}],["发现仅仅把原始词表的大小替换成预测的最优词表大小",{"2":{"561":1}}],["发现两者都以类似的方式回应提示",{"2":{"489":1}}],["发现隐状态中的关键信息就用上",{"2":{"267":1}}],["发现深度记忆模块",{"2":{"228":1}}],["发现了变形金刚",{"2":{"163":1}}],["发现了transformer架构中的知识回路",{"2":{"130":1}}],["发现ffn",{"2":{"126":1}}],["发现还是进入到了attention",{"2":{"83":1}}],["发现大部分head可以分为以下几种",{"2":{"20":1}}],["发现多个head的作用有大多数是冗余的",{"2":{"20":1}}],["率先采用个",{"2":{"1275":1}}],["率将是一个冗余超参数",{"2":{"1143":1}}],["率作为一个冗余超参数",{"2":{"1143":1}}],["率",{"2":{"392":1}}],["涌现出",{"2":{"386":1}}],["遍历过程中",{"2":{"2157":1}}],["遍历两个数组的时间复杂度是",{"2":{"2155":1}}],["遍历饼干数组",{"2":{"2152":1}}],["遍历目录",{"2":{"1930":1}}],["遍历元素",{"2":{"1714":1}}],["遍历这个数组",{"2":{"1690":1}}],["遍历二维数组",{"2":{"1631":1}}],["遍历",{"2":{"1215":1,"1807":1,"1930":1}}],["遍历模块的buffers",{"2":{"1214":1}}],["遍历模块的参数",{"2":{"1214":1}}],["遍历所有可能的y",{"2":{"908":1}}],["遍历ids",{"2":{"592":1}}],["遍历self",{"2":{"522":1}}],["遍历数据集中的每个batch",{"2":{"385":1}}],["遍历句子对列表",{"2":{"384":1,"558":1}}],["爱喝热水的lucky",{"2":{"638":1}}],["爱你我",{"2":{"457":1}}],["爱",{"2":{"381":1,"385":1,"576":1}}],["鹿",{"2":{"378":1}}],["笑书神侠倚碧鸳",{"2":{"378":1}}],["飞雪连天射白鹿",{"2":{"378":4}}],["领域纵深发展",{"2":{"1961":1}}],["领域的",{"2":{"1317":1}}],["领域的发展提供新的思路和方法",{"2":{"474":1}}],["领域出现了越来越多基于",{"2":{"1312":1}}],["领域等其他领域",{"2":{"911":1}}],["领域有许多常见的任务",{"2":{"906":1}}],["领域模型词表扩增是一个值得考虑的策略",{"2":{"560":1}}],["领域特定词汇",{"2":{"560":1}}],["领域和概念",{"2":{"505":1}}],["领域中很难做到",{"2":{"376":1}}],["领域取得了突破性进展",{"2":{"138":1}}],["德语词典",{"2":{"384":1,"558":1}}],["德语词表",{"2":{"375":2}}],["德语分词方法",{"2":{"557":1}}],["德语分词功能",{"2":{"384":1,"558":1}}],["德语分词函数",{"2":{"375":1}}],["德语分词器",{"2":{"375":1}}],["英伟达的llm",{"2":{"740":1}}],["英语",{"2":{"862":1,"1411":2,"1439":1,"1455":1,"1456":1,"1564":1,"1825":1,"1843":1}}],["英语词典",{"2":{"384":1,"558":1}}],["英语词表",{"2":{"375":2}}],["英语分词方法",{"2":{"557":1}}],["英语分词功能",{"2":{"384":1,"558":1}}],["英语分词函数",{"2":{"375":1}}],["英语分词器",{"2":{"375":1}}],["英文天然可以根据空格或者标点来分割出单词",{"2":{"564":1}}],["英文句子",{"2":{"528":1}}],["英文",{"2":{"277":1,"516":1,"528":1}}],["英文和中文明显是有对齐关系的",{"2":{"246":1}}],["帮助大家理解",{"2":{"2121":1}}],["帮助我们更好地应对每天的琐事与挑战",{"2":{"2108":1}}],["帮助我们更有效率地处理生活中的琐事",{"2":{"2103":1}}],["帮助模型更有效地学习从输入到输出的映射",{"2":{"701":1}}],["帮助网络更好地泛化新的",{"2":{"469":1}}],["帮助性",{"2":{"369":1}}],["帮助减少这些数字的方差",{"2":{"193":1}}],["评价指标",{"0":{"1359":1},"1":{"1360":1,"1361":1}}],["评估越高",{"2":{"1360":1}}],["评估",{"2":{"1169":1}}],["评估设置",{"0":{"1163":1}}],["评估改变是否有用",{"2":{"1146":1}}],["评估模型性能",{"0":{"1162":1},"1":{"1163":1,"1164":1,"1165":1},"2":{"1125":1}}],["评估模式不是一种局部禁用梯度计算的机制",{"2":{"1122":1}}],["评估模式",{"0":{"1122":1}}],["评估文档的连贯性",{"2":{"369":1}}],["评论回答",{"2":{"361":1}}],["未初始化",{"2":{"1714":1}}],["未初始化的指针可能包含随机的内存地址",{"2":{"1611":1}}],["未初始化的风险",{"2":{"1611":1}}],["未定义行为",{"2":{"1683":1}}],["未发表的研究结果显示",{"2":{"1242":1}}],["未经记录的实验可能不会得到重视",{"2":{"1167":1}}],["未概率归一化的",{"2":{"765":1}}],["未引入位置编码",{"2":{"744":1}}],["未标记",{"2":{"725":1}}],["未考虑到左侧上下文和右侧上下文之间的交互",{"2":{"720":1}}],["未知单词",{"2":{"557":1}}],["未知字符",{"2":{"363":1,"548":1}}],["未见过的数据",{"2":{"469":1}}],["未来发展等因素",{"2":{"2105":1}}],["未来随着算法和计算能力的进一步提升",{"2":{"2011":1}}],["未来版本中可能会对非完整钩子的行为进行更改",{"2":{"1214":1}}],["未来的研究可以基于此来进一步探索更复杂的模型动力学",{"2":{"507":1}}],["未来的情况下来预测下一个token",{"2":{"464":1}}],["未来词汇相关的掩码",{"2":{"382":1}}],["未被预测单词的",{"2":{"82":1}}],["弗里斯顿",{"2":{"363":1}}],["莲子",{"2":{"361":1}}],["唐翔昊",{"2":{"361":1}}],["赞同",{"2":{"361":1}}],["万能类型",{"2":{"1615":1}}],["万能的map",{"2":{"1485":1}}],["万物皆可嵌入",{"2":{"696":1}}],["万字长文全面解读llm中的分词算法与分词器",{"2":{"638":2}}],["万字长文介绍为大语言模型建立的",{"2":{"513":1}}],["万字逐行解析与实现transformer",{"2":{"361":1,"429":3}}],["万变不离其宗",{"2":{"325":1}}],["芦苇",{"2":{"361":1}}],["江小皮不皮",{"2":{"361":1}}],["α是学习率",{"2":{"1023":1}}],["α",{"2":{"1023":1,"1029":1}}],["α≈1",{"2":{"843":2}}],["αx",{"2":{"359":1}}],["αα",{"2":{"347":1}}],["αijαij",{"2":{"172":1}}],["年薪",{"2":{"1938":1,"1943":1,"1948":1,"1953":1,"1958":1}}],["年不是闰年",{"2":{"1729":3}}],["年是闰年",{"2":{"1729":3}}],["年份",{"2":{"1642":1}}],["年在贝尔实验室开发",{"2":{"1603":1}}],["年代就已经被使用",{"2":{"1016":1}}],["年即被人提出",{"2":{"844":1}}],["年",{"2":{"681":1,"775":1,"840":2,"1312":1,"1459":1,"1472":1}}],["年这样的表示",{"2":{"595":1}}],["年首次由何凯明大神在resnet论文中引入",{"2":{"470":1}}],["年提出的",{"2":{"347":1,"713":1}}],["年龄不能为负数",{"2":{"1811":1,"1829":1}}],["年龄",{"2":{"313":1}}],["回调",{"2":{"1645":1}}],["回调函数的实际应用场景",{"2":{"1645":1}}],["回调函数被调用",{"2":{"1645":2}}],["回调函数常用于处理事件的发生",{"2":{"1645":1}}],["回调函数允许我们分离操作的执行者和操作的具体内容",{"2":{"1645":1}}],["回调函数就是将一个函数的指针像参数一样传递给另一个函数",{"2":{"1645":1}}],["回调函数",{"0":{"1645":1},"2":{"1645":2}}],["回复",{"2":{"986":1}}],["回答用户提出的自然语言问题",{"2":{"906":1}}],["回归到开始",{"2":{"2120":1}}],["回归该句",{"2":{"2054":1}}],["回归",{"2":{"785":1}}],["回顾上节课内容",{"2":{"1637":1}}],["回顾与引入",{"0":{"1637":1}}],["回顾与补充",{"0":{"1634":1}}],["回顾性最佳检查点选择",{"2":{"1166":1}}],["回顾",{"0":{"664":1}}],["回人类的语言",{"2":{"460":1,"676":1}}],["回到本篇最开始图1的例子",{"2":{"455":1}}],["回到输入的数据类型",{"2":{"346":1}}],["回忆下之前提到的rnn方案和cnn方案遇到的问题",{"2":{"272":1}}],["省略大小的初始化",{"2":{"1623":1}}],["省略起始索引和结束索引来选择整个张量",{"2":{"832":1}}],["省略了归一化过程中的均值计算",{"2":{"346":1}}],["省略其它代码",{"2":{"65":2}}],["简易模板",{"2":{"1918":1}}],["简要代码示例",{"2":{"1799":1,"1800":1,"1801":1,"1802":1,"1803":1}}],["简要的说",{"2":{"762":1}}],["简洁不废话",{"2":{"2051":1}}],["简洁",{"2":{"1805":1,"1911":1}}],["简洁但安全性较差",{"2":{"1629":1}}],["简洁性",{"2":{"369":1}}],["简写为",{"2":{"1607":3}}],["简略",{"0":{"1581":1},"1":{"1582":1,"1583":1,"1584":1,"1585":1,"1586":1,"1587":1,"1588":1,"1589":1}}],["简述",{"0":{"878":1},"2":{"808":1,"809":1}}],["简介",{"0":{"789":1,"980":1,"1433":1,"1505":1,"1878":1,"1882":1,"1886":1,"1894":1},"1":{"790":1,"791":1,"1434":1,"1435":1,"1436":1}}],["简化命名空间",{"2":{"1932":1}}],["简化的嵌套命名空间定义",{"2":{"1931":1}}],["简化版",{"2":{"1887":1}}],["简化某些操作",{"2":{"1776":1}}],["简化程序逻辑",{"2":{"1729":1}}],["简化了头文件中的变量定义",{"2":{"1932":1}}],["简化了从复杂数据结构中提取数据的过程",{"2":{"1932":1}}],["简化了多线程程序的编写",{"2":{"1896":1}}],["简化了字符串操作",{"2":{"1715":1}}],["简化了运行时调度和匹配",{"2":{"986":1}}],["简化函数调用",{"0":{"1708":1}}],["简化简单的",{"2":{"1630":1}}],["简化代码",{"2":{"1606":1}}],["简化编译过程",{"2":{"1589":1,"1917":1}}],["简化伪代码",{"2":{"963":1}}],["简化",{"2":{"762":1}}],["简化为self",{"2":{"344":1}}],["简称rl",{"2":{"1472":1}}],["简称为深度神经网络",{"2":{"1464":1}}],["简称神经网络",{"2":{"1456":1}}],["简称",{"2":{"610":1}}],["简而言之",{"2":{"473":1,"627":1,"701":1,"1340":1}}],["简单总结一下",{"0":{"2108":1}}],["简单介绍",{"2":{"1673":1}}],["简单易用",{"2":{"1569":1}}],["简单易学",{"2":{"1479":1}}],["简单",{"2":{"1137":1,"2051":1}}],["简单快速",{"2":{"901":1}}],["简单一点就是",{"2":{"863":1}}],["简单直观",{"2":{"739":1,"1761":1}}],["简单的几个字",{"2":{"2051":1,"2056":1}}],["简单的一阶马尔可夫性假设可能不足以捕捉到完整的依赖关系",{"2":{"1326":1}}],["简单的说就是先根据之前累积的梯度方向模拟下一步参数更新后的值",{"2":{"1034":1}}],["简单的说就是控制每一层输入数据",{"2":{"310":1}}],["简单的串联可能导致某些信息的重复或抵消",{"2":{"739":1}}],["简单的词表截断或者使用基于规则的方法来处理领域特定词汇也可以取得不错的效果",{"2":{"560":1}}],["简单的恒等残差连接是唯一有效的方法",{"2":{"302":1}}],["简单的最近邻搜索需要比较每一对查询",{"2":{"154":1}}],["简单图解一下线性注意力机制",{"2":{"233":1}}],["简单来说",{"2":{"137":1,"299":1,"676":1,"1462":1}}],["简单而言",{"2":{"100":1}}],["含义为特征数",{"2":{"343":1}}],["γv​t+1​​+∇l",{"2":{"1190":1}}],["γvt+1+∇l",{"2":{"1190":1}}],["γ",{"2":{"343":1,"504":5,"1049":1}}],["γγ",{"2":{"343":2}}],["γx+βγx+β",{"2":{"313":1}}],["λ≈1",{"2":{"843":1}}],["λλ",{"2":{"765":3}}],["λ",{"2":{"341":1,"502":1,"503":1,"1184":1}}],["格式化标志等",{"2":{"1678":1}}],["格式化输出",{"0":{"1817":1,"1835":1},"2":{"1673":1}}],["格式化代码",{"0":{"1560":1}}],["格式的数据",{"2":{"1298":1}}],["格式的稀疏",{"2":{"1083":1}}],["格式转换",{"2":{"545":1}}],["格式",{"2":{"341":1,"1083":1,"1693":1,"1699":1,"1700":1}}],["格式为",{"2":{"315":1,"1825":1,"1843":1}}],["份",{"2":{"340":1}}],["页的小册子",{"2":{"340":1}}],["页的书平均分成",{"2":{"340":1}}],["页",{"2":{"340":1}}],["遗传算法",{"2":{"1175":1}}],["遗憾的是",{"2":{"1158":1,"1477":1}}],["遗憾",{"2":{"339":1}}],["遗忘门决定了从之前的步骤中保留哪些信息是相关的",{"2":{"869":1}}],["遗忘门",{"0":{"865":1},"2":{"864":1,"865":1}}],["遗忘问题",{"2":{"284":1}}],["遗忘机制如下图标号5",{"2":{"230":1}}],["遗忘机制",{"2":{"228":1,"230":1}}],["遗忘学习",{"2":{"123":1}}],["划分为块",{"2":{"942":1,"959":1}}],["划分为",{"2":{"567":1}}],["划分为多个",{"2":{"338":1}}],["划重点",{"2":{"256":1,"272":1}}],["失败的情况",{"2":{"1647":1}}],["失活",{"2":{"840":1}}],["失效",{"2":{"338":1}}],["失去了残差网络",{"2":{"332":1}}],["虚基类",{"2":{"1693":1}}],["虚函数与静态绑定",{"2":{"2006":1}}],["虚函数",{"2":{"1688":1,"1693":1}}],["虚继承使用",{"2":{"1662":1}}],["虚继承",{"0":{"1662":1},"2":{"1658":1,"1662":1,"1693":1}}],["虚拟网络编辑器",{"2":{"2094":1}}],["虚拟化",{"2":{"2090":1}}],["虚拟化管理工具",{"2":{"2089":1}}],["虚拟现实等领域的逼真化和互动性提供了强大的支持",{"2":{"2011":1}}],["虚拟现实等",{"2":{"2010":1}}],["虚拟现实",{"2":{"2010":1}}],["虚拟机搭建过程不再赘述",{"2":{"2088":1}}],["虚拟机",{"2":{"1435":1,"2090":1}}],["虚拟批次大小的情况",{"2":{"1168":1}}],["虚线框",{"2":{"940":1,"962":1}}],["虚线代表key与query的相关度",{"2":{"265":1}}],["虚",{"2":{"334":1,"1693":1}}],["倾向于退化",{"2":{"334":1}}],["倾向于分配较低的概率",{"2":{"191":1}}],["倾向于分配较高的概率",{"2":{"191":1}}],["知己",{"2":{"2054":1}}],["知乎",{"2":{"768":1}}],["知乎唐翔昊大神给出的答案是",{"2":{"334":1}}],["知识体系构建",{"0":{"1598":1}}],["知识存储在与参考系相关联的位置",{"2":{"754":1}}],["知识存储与模型编辑的机制",{"2":{"485":1}}],["知识存储和提取",{"2":{"156":1}}],["知识丰富性和类别相关性",{"2":{"369":1}}],["知识提取器",{"2":{"144":1}}],["知识分片与合并",{"2":{"143":1}}],["知识擦除是在模型中移除已有的知识",{"2":{"140":1}}],["知识擦除",{"2":{"140":1}}],["知识干扰",{"2":{"140":1}}],["知识修正旨在纠正这些谬误",{"2":{"140":1}}],["知识修正",{"2":{"140":1}}],["知识修改则是指改变",{"2":{"140":1}}],["知识修改",{"2":{"140":1}}],["知识修改和知识擦除",{"2":{"140":1}}],["知识插入通过赋予",{"2":{"140":1}}],["知识插入",{"2":{"140":3}}],["知识编辑任务定义及方法分类",{"2":{"156":1}}],["知识编辑领域的代表性方法",{"2":{"141":1}}],["知识编辑需要更深入地理解",{"2":{"139":1}}],["知识编辑代表了一种更精确",{"2":{"139":1}}],["知识编辑与其它技术相互交叉",{"2":{"139":1}}],["知识编辑技术针对性地定位",{"2":{"139":1}}],["知识编辑技术",{"2":{"138":1}}],["知识增强",{"2":{"139":1}}],["知识就是不完整的",{"2":{"136":1}}],["知识神经元可以捕获关系事实的语义模式",{"2":{"135":1}}],["知识神经元会不同",{"2":{"135":1}}],["知识神经元会对最后的输出做主要贡献",{"2":{"135":1}}],["知识神经元来提升定位效果",{"2":{"135":1}}],["知识神经元",{"2":{"135":1}}],["知识被定义为对事实",{"2":{"121":1}}],["知识归因",{"0":{"134":1},"2":{"96":1,"133":1}}],["知识的定位",{"0":{"132":1},"1":{"133":1,"134":1,"135":1,"136":1,"137":1},"2":{"96":1}}],["知识回路更关注信息的流动",{"2":{"130":1}}],["知识回路将语言模型看作一个由组件",{"2":{"130":1}}],["知识回路",{"0":{"130":1},"2":{"96":1,"123":1}}],["知识记忆的目的是记忆和回忆知识",{"2":{"123":1}}],["知识记忆",{"0":{"123":1},"1":{"124":1,"125":1,"126":1,"127":1,"128":1,"129":1,"130":1,"131":1},"2":{"96":1}}],["知识利用",{"0":{"121":1},"1":{"122":1,"123":1,"124":1,"125":1,"126":1,"127":1,"128":1,"129":1,"130":1,"131":1,"132":1,"133":1,"134":1,"135":1,"136":1,"137":1,"138":1,"139":1,"140":1,"141":1,"142":1,"143":1,"144":1,"145":1,"146":1,"147":1,"148":1},"2":{"96":1}}],["苏神提出了",{"2":{"542":1}}],["苏建林大神这篇文章中",{"2":{"334":1}}],["苏剑林",{"2":{"233":2,"361":5,"543":2,"638":1,"740":4,"768":1}}],["彼此之间没有连续性",{"2":{"850":1}}],["彼此交流信息",{"2":{"461":1}}],["彼此只有信息的不同",{"2":{"334":1}}],["彼此独立的假设显然不成立",{"2":{"326":1}}],["落入sigmoid",{"2":{"334":1}}],["往往追求",{"2":{"2112":1}}],["往往得不出有效的结论",{"2":{"1143":1}}],["往往从全局注意力过渡到几乎局部的注意力",{"2":{"765":1}}],["往往需要从头训练整个模型",{"2":{"624":1}}],["往往存在大量的冗余参数",{"2":{"512":1}}],["往往存在明显的性能权衡",{"2":{"222":1}}],["往往被认为是一种",{"2":{"512":1}}],["往往采用的是post",{"2":{"335":1}}],["往往更容易训练",{"2":{"334":1}}],["往往比较大",{"2":{"698":1}}],["往往比",{"2":{"333":1}}],["级别的模型中",{"2":{"333":1}}],["级别的存储开销",{"2":{"180":1}}],["削弱得越严重",{"2":{"332":1}}],["稳定性",{"2":{"1025":1}}],["稳定了前向传播的方差",{"2":{"331":1}}],["稳定训练过程",{"2":{"310":1}}],["尺度",{"2":{"326":2}}],["尺寸为大小为",{"2":{"944":1,"963":1}}],["尺寸为",{"2":{"70":1}}],["叠加或者是加法是一个比较常规的操作",{"2":{"722":1}}],["叠加其他embedding产生的",{"2":{"326":1}}],["叠加假设",{"2":{"137":1}}],["出行方案的费用分别是",{"2":{"2131":1}}],["出作用域时",{"2":{"1695":2}}],["出栈",{"2":{"1648":1}}],["出于这个目的",{"2":{"1198":1}}],["出于教育或调试目的",{"2":{"1114":1}}],["出于计算效率的考虑",{"2":{"565":1}}],["出发点有两个",{"2":{"757":1}}],["出现了",{"2":{"1472":1}}],["出现频率",{"2":{"604":1}}],["出现的越少",{"2":{"584":1}}],["出现最多",{"2":{"575":1}}],["出现训练不稳定",{"2":{"333":1}}],["出相似度",{"2":{"318":1}}],["出来的",{"2":{"318":1,"627":1}}],["什么又是互斥",{"0":{"1409":1}}],["什么情况下容易出现过拟合",{"0":{"1012":1}}],["什么是算法",{"0":{"2097":1}}],["什么是函数",{"2":{"1729":1}}],["什么是联合体",{"2":{"1728":1}}],["什么是枚举",{"2":{"1728":1}}],["什么是结构体",{"2":{"1728":1}}],["什么是构造函数参数初始化表",{"2":{"1641":1}}],["什么是常量成员",{"2":{"1640":1}}],["什么是静态成员",{"2":{"1639":1}}],["什么是并行",{"0":{"1564":1}}],["什么是持久层",{"2":{"1478":1}}],["什么是mybatis",{"0":{"1476":1}}],["什么是人工神经网络",{"0":{"1456":1}}],["什么是大模型外推性",{"2":{"1341":1}}],["什么是ring",{"2":{"974":1}}],["什么是",{"0":{"1734":1,"1810":1,"1828":1,"1963":1},"1":{"1964":1},"2":{"933":1,"1638":1,"1917":1,"2062":1}}],["什么是深度学习模型",{"0":{"785":1}}],["什么是最关键的",{"2":{"689":1}}],["什么是scale",{"2":{"233":1}}],["什么数据需要彼此比较",{"2":{"318":1}}],["认识其在程序设计中的重要性",{"2":{"1727":1}}],["认识pytorch",{"0":{"790":1}}],["认输你就真了",{"2":{"429":1}}],["认为一个非常大的max",{"2":{"1156":1}}],["认为",{"2":{"812":1}}],["认为虽然它们都与将数据表示为向量有关",{"2":{"676":1}}],["认为transformer模块的神经网络架构蓝图可以从经典统计力学中熟悉的物理自旋系统的结构中导出",{"2":{"508":1}}],["认为大模型不需要dropout的主要原因有如下几点",{"2":{"396":1}}],["认为其主要问题是在前向传播和反向传播中",{"2":{"316":1}}],["认知和行为决策过程中会选择性地关注和处理相关信息",{"2":{"257":1}}],["杯",{"2":{"316":1,"318":1}}],["香蕉",{"2":{"714":2}}],["香",{"2":{"316":1,"318":1}}],["太大",{"2":{"1302":1}}],["太",{"2":{"316":1,"318":1}}],["故在应用工作中通常应避免使用它",{"2":{"1132":1}}],["故",{"2":{"709":1,"715":2}}],["故无法捕捉单词之间的任何语义关系",{"2":{"681":1}}],["故称之为bbpe",{"2":{"605":1}}],["故此不容易训练",{"2":{"332":1}}],["故批次中的样本个数为5",{"2":{"316":1}}],["故需要对于长度不足的句子进行补齐",{"2":{"316":1}}],["故选择到目标物品的可能性也会变大",{"2":{"163":1}}],["测试越界的情况",{"2":{"2063":1}}],["测试某一位",{"2":{"2062":1}}],["测试中",{"2":{"2043":1}}],["测试支持",{"0":{"1986":1}}],["测试网络连通性",{"2":{"1526":2}}],["测试类中测试",{"2":{"1485":1}}],["测试分割时",{"2":{"1164":1}}],["测试度量中的周期性",{"2":{"1164":1}}],["测试集上运行时",{"2":{"1163":1}}],["测试",{"2":{"1122":1,"1480":1,"1486":2,"1487":1,"1488":1,"1594":1}}],["测试这三个阶段存在不一致性",{"2":{"316":1}}],["测试工程师负责项目质量保证",{"2":{"5":1}}],["验证str1未改变",{"2":{"1887":1}}],["验证输入",{"2":{"1729":1}}],["验证用户输入等功能都可以封装成函数",{"2":{"1729":1}}],["验证数据和测试数据的任何一种随机分割所产生的方差",{"2":{"1152":1}}],["验证时",{"2":{"385":1}}],["验证",{"2":{"316":1,"1122":1,"1163":1}}],["宽度",{"2":{"783":1}}],["宽度为",{"2":{"155":1,"1708":1}}],["宽",{"2":{"341":1,"683":1}}],["宽为",{"2":{"315":1}}],["卷积层通过池化层后一般要接多个全连接层进行降维",{"2":{"816":1}}],["卷积层之后再接一个线性层做分类",{"2":{"473":1}}],["卷积过程",{"2":{"782":1}}],["卷积核是具有感受野的",{"2":{"1019":1}}],["卷积核被分成前后两个组",{"2":{"775":1}}],["卷积核被分成不同的组",{"2":{"775":1}}],["卷积核尺寸为",{"2":{"774":1}}],["卷积中",{"2":{"773":1}}],["卷积",{"0":{"772":1,"774":1,"783":1},"2":{"772":1,"776":1}}],["卷积的情况呢",{"2":{"783":1}}],["卷积的作用在于能有效地减少维度",{"2":{"774":1}}],["卷积的计算图",{"2":{"770":1}}],["卷积的第一个参数",{"2":{"770":1}}],["卷积是对输入图像提取出特征",{"2":{"779":1}}],["卷积是可交换的",{"2":{"770":1}}],["卷积是一种特殊的线性运算",{"2":{"769":1}}],["卷积运算通过三个重要的思想来帮助改进机器学习系统",{"2":{"772":1}}],["卷积运算动态图",{"2":{"770":1}}],["卷积运算的数学公式如下",{"2":{"770":1}}],["卷积运算",{"0":{"770":1}}],["卷积神经网络在进行完线性变换后",{"2":{"838":1}}],["卷积神经网络的灵感来自于动物视觉皮层组织的神经连接方式",{"2":{"769":1}}],["卷积神经网络需要考量的参数更少",{"2":{"769":1}}],["卷积神经网络由一个或多个卷积层和顶端的全连通层",{"2":{"769":1}}],["卷积神经网络",{"2":{"769":1}}],["卷积神经网络一词表明该网络使用了卷积",{"2":{"769":1}}],["卷积操作在特征图的每个channel上进行了大量的叠加操作",{"2":{"314":1}}],["卷积网络是指那些至少在网络的一层中使用卷积运算来替代一般的矩阵乘法运算的神经网络",{"2":{"769":1}}],["卷积网络",{"2":{"232":1,"769":1}}],["否",{"2":{"1650":1}}],["否定了原来的一些观点",{"2":{"314":1}}],["否则会导致内存泄漏或程序崩溃",{"2":{"1668":1}}],["否则会导致无限循环",{"2":{"1620":1}}],["否则可能导致程序崩溃或未定义的行为",{"2":{"1633":1}}],["否则可能发生与时间有关的错误",{"2":{"1409":1}}],["否则返回",{"2":{"1630":1}}],["否则相比训练一个专门的模型",{"2":{"1313":1}}],["否则直接报错",{"2":{"1214":2}}],["否则直接加载词典",{"2":{"374":1}}],["否则这些统计数据在每个设备上都是不同的",{"2":{"1168":1}}],["否则您可能永远不需要使用它们",{"2":{"1123":1}}],["否则规模不一致相加后会丢失信息",{"2":{"701":1}}],["否则两子词之间添加分隔符",{"2":{"588":1}}],["否则输出某个表示token不存在的特殊符号",{"2":{"563":1}}],["否则输出为0",{"2":{"104":1}}],["否则无法预测第一个输出",{"2":{"528":1}}],["否则后面计算loss的时候就会出错",{"2":{"399":1}}],["否则为分组查询注意力",{"2":{"201":1}}],["否则为true",{"2":{"66":1}}],["否则",{"2":{"143":1,"184":1,"819":1,"1145":2}}],["否则放置0",{"2":{"62":1,"63":1}}],["否则将无法直接使用",{"2":{"36":1}}],["毕竟我连第一家老板的能力也没有",{"2":{"2051":1}}],["毕竟",{"2":{"314":1,"322":1}}],["毕竟如果有新方案取代了transformer",{"2":{"280":1}}],["薪资范围为一线城市3",{"2":{"1961":1}}],["薪资",{"2":{"313":1}}],["体现面向对象编程中代码复用和扩展的优势",{"2":{"1657":1}}],["体现了前端",{"2":{"986":1}}],["体现在如下几点",{"2":{"252":1}}],["体会卷积的作用",{"0":{"771":1}}],["体重数据往往形如63kg",{"2":{"313":1}}],["体重",{"2":{"313":1}}],["强大的脚本和配置语言",{"2":{"1964":1}}],["强大但稍有不慎就会割伤自己",{"2":{"1611":1}}],["强制处理异常",{"2":{"1761":1}}],["强制类型转换",{"2":{"1629":1}}],["强制转换为",{"2":{"1611":1,"1623":1,"1704":1}}],["强制退出",{"2":{"1544":1}}],["强制终止进程",{"2":{"1523":1}}],["强制报错",{"2":{"1083":1}}],["强制把分布限制在zero",{"2":{"313":1}}],["强迫encoder学习到良好的embedding",{"2":{"727":1}}],["强化学习不需要带标签的输入输出对",{"2":{"1472":1}}],["强化学习",{"2":{"1472":1}}],["强化学习在ai各种领域中愈发重要",{"2":{"626":1}}],["强化其他位置的特征点学习到丢失掉的位置的语义信息",{"2":{"1019":1}}],["强化模型在具体任务上的表现",{"2":{"726":1}}],["强调数据的组合",{"2":{"1728":1}}],["强调可移植性和性能",{"2":{"1569":1}}],["强调如何基于环境而行动",{"2":{"1472":1}}],["强调以有意义和结构化的方式表示数据的概念",{"2":{"676":1}}],["强调了不同实体之间的重要性",{"2":{"99":1}}],["强行施加独立同分布假设并不会有好结果",{"2":{"337":1}}],["准备调用回调函数",{"2":{"1645":1}}],["准备基础词表",{"2":{"576":1,"599":1,"602":1}}],["准备足够大的训练语料",{"2":{"576":1,"599":1,"602":1}}],["准备",{"0":{"943":1,"961":1},"2":{"313":2}}],["准确的知识库在各种下游应用中的潜力",{"2":{"138":1}}],["规定了某些表达式的求值顺序",{"2":{"1931":1}}],["规范派生类的行为",{"2":{"1693":1}}],["规范化等",{"2":{"1930":1}}],["规范化是指对文本进行标准化处理",{"2":{"552":1}}],["规范化",{"0":{"552":1},"2":{"310":1,"545":1}}],["规模模型的研究人员组成了",{"2":{"1316":1}}],["规模增加的模型训练速度要快得多",{"2":{"620":1}}],["规则进行",{"2":{"1227":1}}],["规则是这个head在90",{"2":{"20":1}}],["规约类算子",{"0":{"833":1}}],["业界将位置编码主要分为绝对位置编码",{"2":{"742":1}}],["业界有一种说法",{"2":{"696":1}}],["业界也有不同意见",{"2":{"309":1}}],["业务流程",{"0":{"537":1}}],["业务逻辑",{"0":{"536":1}}],["业务选择",{"0":{"324":1},"1":{"325":1,"326":1},"2":{"293":1}}],["白平衡",{"2":{"1015":1}}],["白化是在传统机器学习中有效缓解",{"2":{"309":1}}],["白色是表示低频词",{"2":{"185":1}}],["造成数据冗余和浪费",{"2":{"1661":1}}],["造成信息损失",{"2":{"840":1}}],["造成信息的泄露",{"2":{"307":1}}],["造成了很大的浪费",{"2":{"87":1}}],["承担了模型层间修正的作用",{"2":{"306":1}}],["打包目录",{"2":{"1535":1}}],["打包文件",{"2":{"1535":1}}],["打包和解压",{"0":{"1535":1}}],["打乱句子顺序",{"2":{"1317":1}}],["打乱的网格搜索",{"2":{"1175":1}}],["打印翻倍",{"2":{"1914":1}}],["打印容器内容",{"2":{"1914":2}}],["打印过滤后的结果",{"2":{"1883":1}}],["打印包含问候语的消息",{"2":{"1729":1}}],["打印信息",{"2":{"1708":1}}],["打印所有相关信息",{"2":{"1664":1}}],["打印",{"2":{"1664":2,"1914":1}}],["打印数字的平方",{"2":{"1645":1}}],["打印消息",{"2":{"1645":1}}],["打印出来",{"2":{"1625":1}}],["打印变量的值",{"2":{"1607":1}}],["打印变量值",{"2":{"1607":1}}],["打印每个进程的计算状态",{"2":{"1594":1}}],["打印状态信息",{"2":{"1566":1}}],["打印机",{"2":{"1413":1}}],["打印训练信息",{"2":{"1295":1}}],["打印当前学习率",{"2":{"1239":1}}],["打破对称性",{"2":{"305":1}}],["打开生成的",{"2":{"1969":1}}],["打开模式",{"2":{"1820":1,"1838":1}}],["打开文件进行读取",{"2":{"1821":1,"1839":1}}],["打开文件后",{"2":{"1820":1,"1838":1}}],["打开文件",{"2":{"1820":2,"1838":2}}],["打开的文件等",{"2":{"1676":1}}],["打开多个文件",{"0":{"1556":1}}],["打开或创建名为",{"2":{"1543":1}}],["打开或关闭的信息流",{"2":{"103":1}}],["打开图像",{"2":{"1253":1}}],["打开黑匣子的神器来了",{"2":{"513":1}}],["打开training",{"2":{"370":1}}],["打开ai黑箱的新视角",{"2":{"156":1}}],["路径追踪",{"2":{"2009":1}}],["路径操作",{"2":{"1930":1}}],["路径拼接",{"2":{"1930":1}}],["路由权重表示每个专家对最终输出的贡献",{"2":{"739":1}}],["路由分数由每个单独头的分数和与头类型相关的分数共同决定",{"2":{"42":1}}],["路",{"2":{"304":1}}],["见下图",{"2":{"709":1}}],["见下图红色部分",{"2":{"301":1}}],["见上图标号2",{"2":{"145":1}}],["−1",{"2":{"1396":4}}],["−√​​​n​^​​​i​​​​6​​​​​",{"2":{"1007":1}}],["−√​​n​i​​​​6​​​​​",{"2":{"1007":1}}],["−6n^i",{"2":{"1007":1}}],["−6ni",{"2":{"1007":1}}],["−6nj+nj+1",{"2":{"1000":1}}],["−​√​n​j​​+n​j+1​​​​​​​√​6​​​​​",{"2":{"1000":1}}],["−m",{"2":{"943":8,"961":8}}],["−log",{"2":{"899":8}}],["−3",{"2":{"839":1}}],["−8",{"2":{"765":2}}],["−k",{"2":{"763":1}}],["−",{"2":{"399":1,"899":2,"1002":2}}],["−x=0",{"2":{"301":1}}],["−xf",{"2":{"301":1}}],["−x",{"2":{"301":2}}],["−yy⊤",{"2":{"192":1}}],["−yy⊤∂y∂x=diag",{"2":{"192":1}}],["差两个位置",{"2":{"757":1}}],["差四个位置",{"2":{"744":1}}],["差一个位置",{"2":{"744":1,"757":1}}],["差分transformer",{"2":{"513":1}}],["差分",{"2":{"501":1}}],["差分注意力机制具体如下",{"2":{"502":1}}],["差分注意力",{"0":{"502":1},"2":{"500":1,"502":1}}],["差分角度",{"0":{"500":1},"1":{"501":1,"502":1,"503":1}}],["差",{"2":{"301":1}}],["恒等快捷映射",{"2":{"301":1}}],["恒等快捷连接",{"2":{"300":1}}],["恒等映射不可能永远是最优的",{"2":{"301":1}}],["恒等映射在神经网络中的作用很微妙",{"2":{"299":1}}],["恒等映射",{"0":{"299":1},"2":{"293":1}}],["灰色",{"2":{"295":1}}],["柳浩作者已同意",{"2":{"292":1}}],["极不安全",{"2":{"1629":1}}],["极其激进",{"2":{"1184":1}}],["极端激进的梯度截断本质上是一种降低学习率的奇怪方式",{"2":{"1184":1}}],["极有可能转移",{"2":{"1158":1}}],["极市平台",{"2":{"292":1}}],["极性感知线性注意力",{"2":{"233":1}}],["极性感知注意力机制根据",{"2":{"213":1}}],["极性感知注意力将",{"2":{"213":1}}],["极性感知注意力背后的核心思想是为了解决现有线性注意力机制的局限性",{"2":{"213":1}}],["胡文星",{"2":{"292":1}}],["首单词发射分数",{"2":{"1328":1}}],["首次提出了修正线性单元",{"2":{"840":1}}],["首次从理论层面证明了大语言模型",{"2":{"504":1}}],["首次公开私人邮件",{"2":{"292":1}}],["首先判断",{"2":{"1620":1}}],["首先还是看这张图",{"2":{"1405":1,"1426":1}}],["首先计算其对应的",{"2":{"1344":1}}],["首先选择一个完善且常用的模型架构来开始工作",{"2":{"1129":1}}],["首先是转换检查点",{"2":{"938":1,"954":1}}],["首先是无法计算",{"2":{"316":1}}],["首先通过",{"2":{"763":1}}],["首先直接将单向注意力机制改成双向注意力机制",{"2":{"734":1}}],["首先生成两个不同的输入流h1",{"2":{"727":1}}],["首先生成填充词对应的掩码",{"2":{"74":1,"382":1}}],["首先mask文本中的部分单词",{"2":{"721":1}}],["首先提出了具有数百万个参数的预训练语言模型",{"2":{"711":1}}],["首先将词语的内容与相对位置分离",{"2":{"1315":1}}],["首先将词嵌入维度与隐藏维度解耦以减少模型参数",{"2":{"1315":1}}],["首先将输入内容分割成句子",{"2":{"628":1}}],["首先将它们投射成查询",{"2":{"502":1}}],["首先在较高的层次上规划整体结构",{"2":{"627":1}}],["首先根据基础词表",{"2":{"582":1}}],["首先被tokenizer处理成token",{"2":{"528":1}}],["首先看序列操作的复杂度",{"2":{"511":1}}],["首先看看直接使用embedding的缺点",{"2":{"172":1}}],["首先对注意力分数进行缩放",{"2":{"464":1}}],["首先对输入x进行层归一化",{"2":{"344":1}}],["首先要提前说明下",{"2":{"524":1}}],["首先要生成一个mask",{"2":{"396":1}}],["首先要说明的是",{"2":{"198":1}}],["首先调用tgt",{"2":{"384":1,"558":1}}],["首先调用src",{"2":{"384":1,"558":1}}],["首先使用类似于bert的编码器先对输入进行随机mask",{"2":{"727":1}}],["首先使用kldivloss进行了损失计算",{"2":{"398":1}}],["首先使用",{"2":{"355":1}}],["首先使用torch",{"2":{"74":1}}],["首先针对当前batch内",{"2":{"313":1}}],["首先依据计算区域的大小将对齐函数分为局注意力",{"2":{"285":1}}],["首先我们要初始化字向量为",{"2":{"709":1}}],["首先我们把",{"2":{"582":1}}],["首先我们来看看原始论文里面的架构图",{"2":{"435":1}}],["首先我们看看归一化的目的",{"2":{"318":1}}],["首先我们给出transformer总体代码如下",{"2":{"201":1}}],["首先我们将h转置到倒数第二个维度",{"2":{"35":1}}],["首先确定哪个目标token来作自注意力机制",{"2":{"170":1}}],["首先需要声明结构体类型的变量",{"2":{"1728":1}}],["首先需要找到网络参数的梯度",{"2":{"495":1}}],["首先需要定义掩码张量的形状",{"2":{"74":1}}],["首先需要迭代计算分母的和",{"2":{"54":1,"179":1}}],["首先",{"2":{"57":1,"122":1,"145":1,"154":1,"180":1,"222":1,"225":1,"245":1,"274":1,"276":1,"301":2,"320":1,"405":1,"407":1,"426":1,"456":1,"613":1,"614":1,"649":1,"676":1,"691":1,"712":1,"776":1,"807":1,"866":1,"867":1,"868":1,"902":1,"931":1,"975":1,"1059":1,"1086":1,"2152":1}}],["尤其适用于循环次数已知或可以预先确定的情况",{"2":{"1621":1}}],["尤其在图形算法的效率优化和资源管理上",{"2":{"2010":1}}],["尤其在",{"2":{"1605":1}}],["尤其在处理非同步通信时",{"2":{"1579":1}}],["尤其在强化学习领域",{"2":{"291":1}}],["尤其喜欢在树上活动",{"2":{"713":1}}],["尤其是变换",{"2":{"2009":1}}],["尤其是对于文科生来说",{"2":{"2096":1}}],["尤其是对于复杂类型如迭代器",{"2":{"1880":1}}],["尤其是对于大型模型而言",{"2":{"542":1}}],["尤其是需要操作两个不同类对象的操作时",{"2":{"1788":1}}],["尤其是来自函数返回值或动态分配的内存时",{"2":{"1611":1}}],["尤其是多个参数时",{"2":{"1488":1}}],["尤其是transformer类",{"2":{"1183":1}}],["尤其是transformer的维度较高",{"2":{"326":1}}],["尤其是学习率和正则化超参数",{"2":{"1131":1}}],["尤其是当许多试验需要并行运行时",{"2":{"1175":1}}],["尤其是当验证集性能不会随时间持续增加而是围绕特定值波动时",{"2":{"1166":1}}],["尤其是当涉及到细微语义差异时",{"2":{"738":1}}],["尤其是当训练模型在计算上的成本很高时",{"2":{"183":1}}],["尤其是网格细胞",{"2":{"490":1}}],["尤其是黑盒子的神经网络模型",{"2":{"475":1}}],["尤其是在不清楚变量类型的情况下",{"2":{"1615":1}}],["尤其是在培训可能受到训练作业抢占",{"2":{"1164":1}}],["尤其是在开始新项目时",{"2":{"1130":1}}],["尤其是在高维数据中使用时",{"2":{"692":1}}],["尤其是在处理大量计算和对性能有极致要求的场景下",{"2":{"1602":1}}],["尤其是在处理复杂任务时",{"2":{"512":1}}],["尤其是在处理超出训练分布的数据时",{"2":{"181":1}}],["尤其是在深度较低的情况下",{"2":{"480":1}}],["尤其是在训练大规模模型时效果更明显",{"2":{"346":1}}],["尤其是在较深的网络中",{"2":{"310":1}}],["尤其是在前向传播阶段",{"2":{"148":1}}],["尤其是将其应用于",{"2":{"148":1,"484":1}}],["浅拷贝",{"2":{"1694":4}}],["浅拷贝和深拷贝",{"0":{"1694":1}}],["浅谈llm",{"2":{"513":1}}],["浅谈transformer的初始化",{"2":{"361":1}}],["浅而宽",{"2":{"334":1,"335":1}}],["浅融合模型",{"2":{"287":1}}],["浅层倾向于检测出浅层模式",{"2":{"126":1}}],["哪一个是activation呢",{"2":{"1109":1}}],["哪种优化算法最好呢",{"2":{"1057":1}}],["哪种形式用的比较多呢",{"2":{"889":1}}],["哪种方法更好",{"2":{"874":1}}],["哪个是kernel",{"2":{"773":1}}],["哪个是输出",{"2":{"773":1}}],["哪个是输入",{"2":{"773":1}}],["哪个特征向量包含对q最重要的信息",{"2":{"263":1}}],["哪怕是再资深的语言学家恐怕也会难以感到棘手",{"2":{"712":1}}],["哪些任务要先完成",{"2":{"2099":1}}],["哪些情况下可以使用栈上的自动内存分配",{"2":{"1647":1}}],["哪些超参数交互最多",{"2":{"1140":1}}],["哪些张量由特定",{"2":{"1114":1}}],["哪些参数需要设置requires",{"2":{"1109":1}}],["哪些输入元素起到了关键作用",{"2":{"512":1}}],["哪些部分在压缩中能被记忆是未知的",{"2":{"287":1}}],["眼睛",{"2":{"284":1}}],["尝试把while循环改写成for循环",{"2":{"2157":1}}],["尝试修改代码",{"2":{"2070":1}}],["尝试编写一个",{"2":{"1918":1}}],["尝试编写一个函数",{"2":{"1645":1}}],["尝试复制",{"2":{"1911":1}}],["尝试取出3000元和6000元",{"2":{"1873":1}}],["尝试用饼干去满足孩子",{"2":{"2153":1}}],["尝试用每块饼干去满足胃口最小的孩子",{"2":{"2152":1}}],["尝试用引用传递的方式实现一个链表节点的插入操作",{"2":{"1650":1}}],["尝试用迭代的方式实现计算阶乘的函数",{"2":{"1646":1}}],["尝试使用新的优化器",{"2":{"1180":1}}],["尝试对训练流程进行改进",{"2":{"1141":1}}],["尝试太小的值",{"2":{"996":1}}],["尝试外推rope至1m上下文",{"2":{"768":1}}],["尝试提供一个从数学角度研究",{"2":{"499":1}}],["尝试了一种",{"2":{"284":1}}],["尝试将残差调控因子初始化为",{"2":{"1180":1}}],["尝试将",{"2":{"209":1}}],["端到端训练一个深度学习模型",{"0":{"1215":1}}],["端到端模型",{"2":{"284":1}}],["端到端神经网络模型",{"2":{"281":1}}],["缩进选定文本",{"2":{"1559":1}}],["缩进",{"2":{"1551":1}}],["缩写为bp",{"2":{"1439":1}}],["缩写",{"2":{"769":1}}],["缩减单词间距",{"0":{"274":1}}],["缩放等基本变换",{"2":{"2009":1}}],["缩放每个参数反比于其所有梯度历史平方值总和的平方根",{"2":{"1042":1}}],["缩放版本的点积注意力",{"0":{"916":1},"1":{"917":1,"918":1,"919":1,"920":1,"921":1,"922":1,"923":1,"924":1}}],["缩放因子并对和进行重新缩放和归一化",{"2":{"357":1}}],["缩放因子",{"2":{"355":1}}],["缩放后应用",{"2":{"355":1}}],["缩放不变性",{"2":{"320":1}}],["缩放和偏移",{"2":{"313":1}}],["缩放点积注意力",{"2":{"173":1,"186":1}}],["缩放",{"0":{"186":1},"1":{"187":1,"188":1,"189":1,"190":1,"191":1,"192":1,"193":1,"194":1},"2":{"157":1,"173":1,"464":1,"702":1}}],["供后续解码器使用",{"2":{"267":1}}],["咱俩之间是否密切",{"2":{"265":1}}],["丰富的字符串操作函数",{"2":{"1803":1}}],["丰富的成员函数",{"2":{"1713":1}}],["丰富的表征",{"2":{"261":1}}],["丰富多变的数据集可能需要更大的词汇量来捕获文本的多样性",{"2":{"560":1}}],["丰富上下文理解增加模型的表达能力和学习能力",{"2":{"21":1}}],["杂乱",{"2":{"260":1}}],["像普通指针一样访问对象成员",{"2":{"1695":1}}],["像ghost",{"2":{"1136":1}}],["像下图标号1那样描述",{"2":{"739":1}}],["像",{"2":{"579":1,"838":1}}],["像cnn一样可以在每一层内实现并行",{"2":{"274":1}}],["像素中心偏移",{"2":{"2019":1}}],["像素",{"2":{"260":1}}],["像章鱼一样适应环境",{"2":{"233":1}}],["他不光学",{"2":{"2054":1}}],["他人本地没有这个模型",{"2":{"1260":1,"1264":1}}],["他",{"2":{"709":1}}],["他是一个dict",{"2":{"666":1}}],["他尝试加入几个团队",{"2":{"628":2}}],["他认为如果参加一项运动就会有所改变",{"2":{"628":2}}],["他会先快速通览",{"2":{"257":1}}],["他们都可能对数据进行一些操作",{"2":{"1469":1}}],["他们都很重要",{"2":{"1130":1}}],["他们又进入了饱和区",{"2":{"996":1}}],["他们应该早就知道三角函数编码的问题",{"2":{"759":1}}],["他们发现",{"2":{"542":1}}],["他们通过将图像和文本段落输入",{"2":{"489":1}}],["他们的点积就越大",{"2":{"692":1}}],["他们的生物物理神经元",{"2":{"489":1}}],["他们的研究发现",{"2":{"437":1}}],["他们将主要发现总结如下",{"2":{"477":1}}],["他们希望在短期内迅速上手",{"2":{"235":1}}],["他们提供了数学理论基础",{"2":{"211":1}}],["他们认为",{"2":{"144":1}}],["他们进一步将映射矩阵分解为一个输入无关的静态矩阵wbwbw",{"2":{"46":1}}],["他们设法保留了",{"2":{"20":1}}],["他们每个人可能翻译时阅读顺序和关注点都有所不同",{"2":{"13":1}}],["他们是专门用来拆解每个",{"2":{"12":1}}],["爆炸问题",{"2":{"286":1}}],["爆炸",{"2":{"255":1,"333":1}}],["消息队列",{"2":{"1951":1}}],["消息的内容可能是不同的类型",{"2":{"1728":1}}],["消息通讯的匹配",{"2":{"1590":1}}],["消息标志",{"2":{"1590":1}}],["消息标签",{"2":{"1576":1}}],["消息数据",{"2":{"1573":1}}],["消息传递接口",{"2":{"1569":1}}],["消息传递",{"2":{"1563":1}}],["消息是数据和控制信息的封装",{"2":{"1563":1}}],["消息",{"2":{"1563":1}}],["消息中间件",{"0":{"1498":1}}],["消费者异步性",{"2":{"973":1}}],["消除每个连续数据中除第一个元素之外的所有元素",{"2":{"1083":1}}],["消除由低三角形因果蒙版导致的不必要计算",{"2":{"976":1}}],["消除个别设备施加的内存限制",{"2":{"975":1}}],["消除梯度消失和爆炸",{"2":{"843":1}}],["消除中间的空格",{"2":{"582":1}}],["消除奇异性",{"2":{"305":1}}],["消除了hbm中o",{"2":{"180":1}}],["消失",{"2":{"255":1}}],["整理来源",{"2":{"2088":1}}],["整理文本",{"2":{"545":1}}],["整除的年份是闰年",{"2":{"1619":1}}],["整除但不能被",{"2":{"1619":1,"1729":2}}],["整除",{"2":{"1607":1,"1619":1,"1729":7}}],["整除时",{"2":{"1164":1}}],["整型",{"2":{"1607":1,"1642":3}}],["整数+加法",{"2":{"2026":1}}],["整数",{"2":{"1614":1,"1728":1}}],["整数除法的重要特性",{"2":{"1607":1}}],["整数和浮点计算分开",{"2":{"973":1}}],["整数的好处是连续",{"2":{"679":1}}],["整数的最大范围是词表大小",{"2":{"550":1}}],["整流线性单位函数",{"2":{"840":1}}],["整体最优包含了子问题最优",{"2":{"2119":1}}],["整体实现方式",{"2":{"2064":1}}],["整体该博客",{"0":{"2035":1}}],["整体流程图",{"0":{"1349":1}}],["整体图",{"2":{"870":1}}],["整体结构",{"0":{"863":1,"1348":1},"1":{"1349":1,"1350":1,"1351":1,"1352":1,"1353":1,"1354":1}}],["整体介绍",{"0":{"838":1},"1":{"839":1,"840":1}}],["整体完善图",{"2":{"782":1}}],["整体偏置易随相对位置大小波动",{"2":{"746":1}}],["整体推理速度和上下文长长度线性相关",{"2":{"250":1}}],["整个程序",{"2":{"1649":1}}],["整个表达式的值是",{"2":{"1630":1}}],["整个逗号表达式的值是最后一个表达式的值",{"2":{"1630":1}}],["整个过程走下来",{"2":{"1598":1}}],["整个decoding",{"2":{"904":1}}],["整个seq2seq流程可以表述如下",{"2":{"892":1}}],["整个encoder",{"2":{"890":1}}],["整个句子的表示",{"2":{"764":1}}],["整个句子的数学表示就是encoder的输出",{"2":{"445":1}}],["整个流程如下图所示",{"2":{"709":1}}],["整个词向量空间也可以编码",{"2":{"698":1}}],["整个模型由l个diff",{"2":{"501":1}}],["整个模型的秩也会很快坍缩",{"2":{"115":1}}],["整个架构从原来的序列模型变成了一个全连接图模型",{"2":{"415":1}}],["国",{"2":{"249":1}}],["北航",{"2":{"740":1}}],["北京智源人工智能研究院",{"2":{"513":1}}],["北京大学",{"2":{"156":1}}],["北",{"2":{"249":2,"253":1,"415":3}}],["北国的特产",{"2":{"249":1,"263":3,"415":1}}],["逐一解决",{"2":{"2104":1}}],["逐行读取",{"2":{"1820":1,"1838":1}}],["逐行解释执行代码",{"2":{"1604":1}}],["逐位对应相乘",{"2":{"1344":1}}],["逐位相乘",{"2":{"751":1}}],["逐参数选项",{"0":{"1222":1}}],["逐词预测",{"2":{"626":1}}],["逐渐增加词表大小所带来的分词效率收益会逐渐减少",{"2":{"561":1}}],["逐步去实现",{"2":{"2102":1}}],["逐步优化的过程",{"0":{"2102":1}}],["逐步计算出直线上其他点的坐标",{"2":{"2017":1}}],["逐步添加功能并进行改进",{"2":{"1139":1}}],["逐步减小",{"2":{"604":1}}],["逐步变大",{"2":{"604":2}}],["逐步调整采样概率分布",{"2":{"480":1}}],["逐步输出token",{"2":{"427":1}}],["逐层完全连通的水流系统",{"2":{"1466":1}}],["逐层计算梯度",{"2":{"484":1}}],["逐层提取出更高层次的特征",{"2":{"437":1}}],["逐层累积的方差如果不加以抑制",{"2":{"314":1}}],["逐级加工",{"2":{"247":1}}],["逐深度卷积的网络",{"2":{"232":1}}],["啼唱",{"2":{"246":1}}],["屋子",{"2":{"246":1}}],["屋子又低",{"2":{"246":1}}],["树等",{"2":{"1611":1,"1647":1,"1728":1}}],["树结构存储在",{"2":{"986":1}}],["树",{"2":{"246":2}}],["秋蝉",{"2":{"246":5,"247":3}}],["秋蝉的衰弱的残声",{"2":{"246":2}}],["秋枫学习笔记",{"2":{"233":1}}],["新增java",{"2":{"2043":1}}],["新增内容",{"2":{"2043":1}}],["新增贪心算法模块的几篇文章",{"2":{"2043":1}}],["新增算法模块的一些内容",{"2":{"2043":1}}],["新增ssl证书",{"2":{"2042":1}}],["新增seq",{"2":{"972":1}}],["新增",{"2":{"2042":1,"2043":1,"2048":1,"2049":2}}],["新特性",{"0":{"1903":1,"1919":1},"1":{"1904":1,"1905":1,"1906":1,"1907":1,"1908":1,"1909":1,"1910":1,"1911":1,"1912":1,"1913":1,"1914":1,"1920":1,"1921":1,"1922":1,"1923":1,"1924":1,"1925":1,"1926":1,"1927":1,"1928":1,"1929":1,"1930":1,"1931":1,"1932":1,"1933":1}}],["新特性概述",{"0":{"1875":1},"1":{"1876":1,"1877":1,"1878":1,"1879":1,"1880":1,"1881":1,"1882":1,"1883":1,"1884":1,"1885":1,"1886":1,"1887":1,"1888":1,"1889":1,"1890":1,"1891":1,"1892":1,"1893":1,"1894":1,"1895":1,"1896":1,"1897":1,"1898":1,"1899":1,"1900":1,"1901":1,"1902":1}}],["新东西学习",{"0":{"1596":1}}],["新皮质必须知道接下来的移动是什么",{"2":{"754":1}}],["新皮质的关键是参考系",{"2":{"754":1}}],["新的求值顺序保证",{"2":{"1931":1}}],["新的训练过程总共应当进行了11000次",{"2":{"1183":1}}],["新的想法经过一个完整的工作",{"2":{"1159":1}}],["新的正则化器",{"2":{"1141":1}}],["新的y",{"2":{"1098":1}}],["新的轮次被追加到",{"2":{"986":1}}],["新的轮次被追加到树中作为一个新节点",{"2":{"986":1}}],["新的细胞状态和新的隐藏状态随后传递到下一个时间步",{"2":{"868":1}}],["新的ulmfit和openai",{"2":{"718":1}}],["新的elmo也属于这类",{"2":{"718":1}}],["新的预分词器引入了将标点符号和换行符组合成新",{"2":{"553":1}}],["新",{"2":{"681":1}}],["新方法将与token级别的处理不同",{"2":{"628":1}}],["新系统将不再单纯基于下一个token预测",{"2":{"627":1}}],["新idx对应的token就是原来词汇表中两个对应字节的拼接",{"2":{"592":1}}],["新token",{"2":{"576":1}}],["新年快乐",{"2":{"679":1}}],["新年大吉",{"2":{"674":1}}],["新年乐",{"2":{"381":1}}],["新年好",{"2":{"380":1,"381":3}}],["新模型结构如下图所示",{"2":{"288":1}}],["新模型应该不仅仅简单的关注单词的频率和顺序",{"2":{"242":1}}],["新智元",{"2":{"233":1,"638":2}}],["步进主方向在",{"2":{"2018":2}}],["步进方向选择",{"2":{"2018":1}}],["步幅",{"2":{"1087":1}}],["步长为2",{"2":{"832":1}}],["步才能完成",{"2":{"511":1}}],["步内计算任何time",{"2":{"504":1}}],["步内计算任何time2",{"2":{"504":1}}],["步",{"0":{"463":1,"464":1}}],["步步错",{"2":{"239":1,"895":1}}],["步骤四",{"2":{"1594":1}}],["步骤三",{"2":{"1594":1}}],["步骤二",{"2":{"1594":1}}],["步骤一",{"2":{"1594":1}}],["步骤",{"2":{"145":2,"427":1,"1156":1}}],["步骤1",{"2":{"145":1}}],["成绩",{"2":{"1728":1}}],["成指向其第一个行",{"2":{"1705":1}}],["成员密码>",{"2":{"2070":1}}],["成员名>",{"2":{"2070":1}}],["成员在派生类中都变为",{"2":{"1855":1,"1859":1}}],["成员在派生类中保持不变",{"2":{"1851":1}}],["成员在派生类中无法访问",{"2":{"1655":1}}],["成员指针访问运算符",{"2":{"1712":1}}],["成员指针变量的处理",{"2":{"1694":1}}],["成员列表",{"2":{"1700":1}}],["成员隐藏了类的内部实现",{"2":{"1677":1}}],["成员定义了类的外部接口",{"2":{"1677":1}}],["成员",{"2":{"1655":2,"1728":1,"1853":1,"1857":1,"1861":1,"2070":2}}],["成员函数重载",{"2":{"1712":1}}],["成员函数和友元函数",{"2":{"1712":1}}],["成员函数和成员变量也可以发生覆盖",{"2":{"1663":1}}],["成员函数模板",{"0":{"1701":1}}],["成员函数",{"2":{"1640":1,"1642":1,"1674":1,"1700":1,"1728":1,"1766":1,"2006":1}}],["成员选择",{"2":{"1630":1}}],["成员访问运算符",{"2":{"1630":1,"1712":1}}],["成员变量会按照",{"2":{"1653":1}}],["成员变量的大小",{"2":{"1653":1}}],["成员变量",{"0":{"380":1},"2":{"1642":1,"1674":1,"1700":1,"1766":1}}],["成tensorboard支持的格式就行",{"2":{"1275":1}}],["成功",{"2":{"1761":1}}],["成功训练需要4万次的预热",{"2":{"1181":1}}],["成功将复杂度降为",{"2":{"216":1}}],["成多个tensor",{"2":{"1083":1}}],["成向量空间上的线性变换",{"2":{"713":1}}],["成自己的语言",{"2":{"676":1}}],["成诚",{"2":{"543":1}}],["成为",{"2":{"1214":1}}],["成为欧式空间的一个向量",{"2":{"714":1}}],["成为ai可解释性研究的一个重要方向",{"2":{"475":1}}],["成为理解神经网络乃至智能本质不可或缺的手段",{"2":{"474":1}}],["成为整个句子的概率最大",{"2":{"238":1}}],["成各种复杂关联的模式",{"2":{"386":1}}],["背景",{"2":{"816":1,"840":1,"844":1,"845":1,"935":1,"951":1}}],["背景知识",{"0":{"236":1},"1":{"237":1,"238":1,"239":1,"240":1,"241":1,"242":1}}],["背景颜色",{"2":{"3":1}}],["博采众长的旋转式位置编码",{"2":{"768":1}}],["博采众家之长",{"2":{"139":1}}],["博客迁移至新mac",{"2":{"2048":1}}],["博客指南",{"2":{"2043":1}}],["博客结构",{"2":{"2035":1,"2043":1}}],["博客作者",{"2":{"692":1}}],["博客作者认为transformer模块的正反向传播可映射为矢量自旋模型中的计算磁化",{"2":{"508":1}}],["博客",{"2":{"508":1,"513":1,"987":1}}],["博客以及其源码",{"2":{"432":1}}],["博客和代码的学习和解读",{"2":{"235":1}}],["力争融入一些比较新的或者有特色的论文或者理念",{"2":{"235":1}}],["遂有此系列",{"2":{"235":1}}],["渣b",{"2":{"233":2}}],["谈谈大模型架构的演进之路",{"2":{"233":2}}],["天美",{"2":{"1939":1}}],["天马行空",{"2":{"1338":1}}],["天然地由于其位移不变性具备更好的外推能力",{"2":{"756":1}}],["天才程序员周弈帆",{"2":{"233":1}}],["天机不可泄露",{"2":{"58":1}}],["深拷贝",{"2":{"1694":4}}],["深浅拷贝及智能指针",{"0":{"1692":1},"1":{"1693":1,"1694":1,"1695":1}}],["深刻认识内存越界和内存泄漏的危害",{"2":{"1666":1}}],["深入探讨",{"0":{"2061":1}}],["深入类和对象",{"0":{"1636":1},"1":{"1637":1,"1638":1,"1639":1,"1640":1,"1641":1}}],["深入了解一个专业",{"2":{"1598":1}}],["深入浅出完整解析stable",{"2":{"1365":2}}],["深入理解并熟练运用结构体",{"2":{"1727":1}}],["深入理解并熟练运用各类运算符",{"2":{"1627":1}}],["深入理解",{"2":{"1711":1}}],["深入理解二维数组与行指针",{"0":{"1705":1}}],["深入理解字符串常量",{"0":{"1704":1}}],["深入理解字符串",{"0":{"1703":1},"1":{"1704":1,"1705":1,"1706":1,"1707":1,"1708":1,"1709":1,"1710":1}}],["深入理解数组作为函数参数传递时的特性",{"2":{"1666":1}}],["深入理解内存与地址",{"0":{"1610":1}}],["深入理解底层原理",{"2":{"1602":1}}],["深入理解问题是至关重要的",{"2":{"1140":1}}],["深入理解nlp",{"2":{"638":2}}],["深入解析",{"2":{"740":1}}],["深入解读残差网络resnet",{"2":{"361":1}}],["深融合模型",{"2":{"287":1}}],["深海",{"2":{"233":1}}],["深度前馈网络",{"2":{"1457":1}}],["深度卷积和",{"2":{"776":1}}],["深度可分离卷积由两步组成",{"2":{"776":1}}],["深度可分离卷积",{"0":{"776":1}}],["深度神经网络识别汉字",{"2":{"1466":1}}],["深度神经网络解决问题案例",{"0":{"1466":1}}],["深度神经网络训练需要明确的几个概念",{"0":{"1441":1}}],["深度神经网络",{"2":{"492":1}}],["深度神经网络每层的输入分布和权重要相互协调",{"2":{"313":1}}],["深度网络在不同层上以不同的抽象级别对信息进行编码",{"2":{"154":1}}],["深度网络归因定义如下图所示",{"2":{"134":1}}],["深度",{"2":{"147":1}}],["深度学习与传统机器学习",{"0":{"1468":1},"1":{"1469":1,"1470":1}}],["深度学习与人工智能的关系",{"0":{"1454":1}}],["深度学习旨在通过构建和训练多层神经网络来实现人工智能任务",{"2":{"1455":1}}],["深度学习调优指南中文版",{"0":{"1124":1},"1":{"1125":1,"1126":1,"1127":1,"1128":1,"1129":1,"1130":1,"1131":1,"1132":1,"1133":1,"1134":1,"1135":1,"1136":1,"1137":1,"1138":1,"1139":1,"1140":1,"1141":1,"1142":1,"1143":1,"1144":1,"1145":1,"1146":1,"1147":1,"1148":1,"1149":1,"1150":1,"1151":1,"1152":1,"1153":1,"1154":1,"1155":1,"1156":1,"1157":1,"1158":1,"1159":1,"1160":1,"1161":1,"1162":1,"1163":1,"1164":1,"1165":1,"1166":1,"1167":1,"1168":1,"1169":1,"1170":1,"1171":1,"1172":1,"1173":1,"1174":1,"1175":1,"1176":1,"1177":1,"1178":1,"1179":1,"1180":1,"1181":1,"1182":1,"1183":1,"1184":1,"1185":1,"1186":1,"1187":1,"1188":1,"1189":1,"1190":1,"1191":1,"1192":1,"1193":1,"1194":1,"1195":1,"1196":1,"1197":1,"1198":1,"1199":1}}],["深度学习优化器是用于训练神经网络模型的算法或工具",{"2":{"1021":1}}],["深度学习的众多项任务都开始应用",{"2":{"1472":1}}],["深度学习的典型算法",{"0":{"1472":1}}],["深度学习的特点",{"0":{"1471":1}}],["深度学习的特征提取并不依靠人工",{"2":{"1470":1}}],["深度学习的概念",{"0":{"1455":1}}],["深度学习的",{"2":{"909":1}}],["深度学习预训练模型下载",{"2":{"786":1}}],["深度学习网络图中的算子有很多种类",{"2":{"785":1}}],["深度学习时代",{"2":{"711":1}}],["深度学习模型训练过程的本质是对",{"2":{"988":1}}],["深度学习模型可视化",{"2":{"787":1}}],["深度学习模型通过学习大量数据来提取和学习数据的高级特征表示",{"2":{"785":1}}],["深度学习模型通常会同时使用多种不同的正则化技术",{"2":{"692":1}}],["深度学习模型是一种机器学习模型",{"2":{"785":1}}],["深度学习模型在处理输入数据之后得到的内部表示会具有语义和压缩的数据信息",{"2":{"730":1}}],["深度学习模型要求输入数据具有固定的尺寸",{"2":{"376":1}}],["深度学习第6章",{"2":{"513":1}}],["深度学习笔记",{"2":{"361":1}}],["深度学习中的正则化方法就是",{"2":{"321":1}}],["深度学习自然语言处理",{"2":{"156":1}}],["深度学习",{"2":{"95":1,"740":1,"1455":1}}],["咚咚呛",{"2":{"233":1}}],["硅星人pro",{"2":{"233":1}}],["彻底让推理摆脱语言和模态制约",{"2":{"627":1}}],["彻底改变了深度学习transformer论文中",{"2":{"470":1}}],["彻底开源",{"2":{"233":1}}],["彻底搞懂多头注意力",{"2":{"47":1}}],["激励传播",{"2":{"1441":1}}],["激进架构",{"2":{"233":1}}],["激活很小时会使得下层mlp的weight",{"2":{"994":1}}],["激活值均为0",{"2":{"992":1}}],["激活值",{"2":{"310":1}}],["激活分布发生了变化",{"2":{"309":1}}],["激活向量并分解到各个特征",{"2":{"137":1}}],["激活的梯度",{"2":{"1441":2}}],["激活的",{"2":{"135":1}}],["激活",{"2":{"99":1,"1392":1,"1441":1}}],["激活函数z对w的偏导数",{"2":{"1450":1}}],["激活函数导数计算",{"2":{"1393":1}}],["激活函数导数范围为",{"2":{"839":1}}],["激活函数是一个固定超参数",{"2":{"1143":1}}],["激活函数是神经网络中的非线性函数",{"2":{"102":1}}],["激活函数类型",{"2":{"1129":1}}],["激活函数变换为",{"2":{"1004":1}}],["激活函数在最终准确度上比swish",{"2":{"846":1}}],["激活函数在正半轴具有与relu",{"2":{"843":1}}],["激活函数一样的优势",{"2":{"843":1}}],["激活函数保持一致",{"2":{"843":1}}],["激活函数的选择可以是一个目标超参数",{"2":{"1143":1}}],["激活函数的正半轴与relu",{"2":{"843":1}}],["激活函数的基础上将大于6",{"2":{"841":1}}],["激活函数几乎同时有一半的神经元被激活",{"2":{"841":1}}],["激活函数存在的双向饱和性仍然使得梯度弥散问题存在",{"2":{"839":1}}],["激活函数梯度弥散的问题",{"2":{"839":1}}],["激活函数非",{"2":{"839":1}}],["激活函数解决了",{"2":{"839":1}}],["激活函数求导过程计算量较大",{"2":{"839":1}}],["激活函数具有双向饱和性",{"2":{"839":1}}],["激活函数值的范围为",{"2":{"839":1}}],["激活函数复杂就会降低计算速度",{"2":{"838":1}}],["激活函数应该具有什么样的性质",{"2":{"838":1}}],["激活函数可以看作卷积神经网络模型中一个特殊的层",{"2":{"838":1}}],["激活函数综述",{"2":{"837":1,"849":1}}],["激活函数汇总",{"2":{"837":1,"849":1}}],["激活函数进行组合",{"2":{"356":1}}],["激活函数",{"0":{"102":1,"840":1,"842":1,"1447":1,"1448":1},"1":{"103":1,"104":1,"105":1,"106":1,"107":1,"108":1,"109":1,"110":1,"111":1},"2":{"96":1,"122":2,"125":1,"841":1,"844":1,"1000":1,"1202":1,"1388":1,"1389":1}}],["激活top",{"2":{"42":1}}],["雅牧",{"2":{"233":1,"513":1}}],["集成经验",{"2":{"1937":1}}],["集中注意力很重要",{"2":{"1598":1}}],["集体通信",{"2":{"1573":1}}],["集",{"2":{"623":1}}],["集智俱乐部",{"2":{"233":1}}],["集智科学家",{"2":{"233":1}}],["集多种优秀特性于一身",{"2":{"1":1}}],["导入mybatis相关",{"2":{"1481":1}}],["导入mybatis",{"2":{"1480":1}}],["导读2",{"2":{"1378":1}}],["导读",{"2":{"1378":1}}],["导数",{"2":{"1377":2}}],["导数相乘很容易造成梯度弥散",{"2":{"839":1}}],["导航软件中用来找最短路径的算法",{"2":{"2107":1}}],["导航",{"2":{"233":1}}],["导致程序在未知的状态下继续运行",{"2":{"1764":1}}],["导致程序逻辑错误",{"2":{"1670":1}}],["导致程序出错甚至安全漏洞",{"2":{"1648":1}}],["导致成员变量被构造两次甚至多次",{"2":{"1661":1}}],["导致编译错误",{"2":{"1628":1}}],["导致步间方差的最大可能的是batch的方差",{"2":{"1149":1}}],["导致反向时",{"2":{"971":1}}],["导致反向传播时计算不准确",{"2":{"661":1,"1104":1}}],["导致行业专家和搬砖工人门纷纷下岗",{"2":{"909":1}}],["导致相应参数永远不会被更新",{"2":{"840":1}}],["导致为任何两个文本计算体现出惊人的高相似性",{"2":{"711":1}}],["导致它们的点积有很大的不同",{"2":{"692":1}}],["导致效率低下",{"2":{"681":1}}],["导致数据丢失",{"2":{"1684":1}}],["导致数据依赖关系复杂",{"2":{"542":1}}],["导致数字化后的数值与词义之间缺乏联系",{"2":{"679":1}}],["导致极大的计算开销",{"2":{"618":1}}],["导致表征的成本上升",{"2":{"608":1}}],["导致在翻译任务中",{"2":{"595":1}}],["导致训练过程中出现不稳定性",{"2":{"1179":1}}],["导致训练和推理过程变得低效",{"2":{"562":1}}],["导致训练效率较低",{"2":{"254":1}}],["导致训练效果不佳",{"2":{"239":1,"405":1}}],["导致上层调用者无法获得足够的错误信息",{"2":{"1764":1}}],["导致上限不高",{"2":{"542":1}}],["导致上述情形的主要是三个问题",{"2":{"296":1}}],["导致学习速度变慢",{"2":{"407":1}}],["导致梯度爆炸",{"2":{"400":1}}],["导致较低的推理延迟",{"2":{"348":1}}],["导致其容易退化为一个",{"2":{"335":1}}],["导致其在目前的大模型中较少使用",{"2":{"331":1}}],["导致计算出的平均值和标准差无法真正代表数据的分布",{"2":{"316":1}}],["导致计算成本较高",{"2":{"183":1}}],["导致问题的根源是",{"2":{"316":1}}],["导致影响网络训练的问题",{"2":{"313":1}}],["导致模型过度依赖训练数据中的噪声和异常点",{"2":{"1012":1}}],["导致模型无法充分理解这些词的语义",{"2":{"565":1}}],["导致模型在部署中表现变差的现象叫做exposure",{"2":{"411":1}}],["导致模型的泛化能力下降的问题",{"2":{"1341":1}}],["导致模型的",{"2":{"305":1}}],["导致模型性能下降",{"2":{"247":1,"274":1}}],["导致网络权重的大幅更新",{"2":{"296":1}}],["导致网络不稳定",{"2":{"255":1}}],["导致信息遗失",{"2":{"256":1}}],["导致最近才发现好多未读消息和私信",{"2":{"235":1}}],["导致存储和计算需求增加",{"2":{"222":1}}],["导致巨大的计算成本和超长的训练时间",{"2":{"222":1}}],["导致注意力图",{"2":{"212":1}}],["导致求和结果显著发生变化",{"2":{"194":1}}],["导致",{"2":{"187":1}}],["导致整体优化效果有限",{"2":{"152":1}}],["导致被生成的概率远低于事实性知识",{"2":{"140":1}}],["套码的汉子",{"2":{"233":1}}],["南枫",{"2":{"233":1}}],["丁师兄大模型",{"2":{"233":1}}],["丁稼宇",{"2":{"156":1}}],["动物园管理系统",{"2":{"1689":1}}],["动物的中枢神经系统",{"2":{"1456":1}}],["动量",{"2":{"1143":1,"1186":1}}],["动量等",{"2":{"1135":1}}],["动量直接并入了梯度一阶矩",{"2":{"1059":1}}],["动量算法可以在一定程度缓解这些问题",{"2":{"1041":1}}],["动量算法积累了之前梯度指数级衰减的移动平均",{"2":{"1028":1}}],["动量的形式",{"2":{"1049":1}}],["动量的方法",{"2":{"1036":1}}],["动量的积累",{"2":{"1031":1}}],["动量可以平滑参数更新的路径",{"2":{"1031":1}}],["动量项",{"2":{"1059":1}}],["动量项可以帮助算法在平坦区域上获得更大的动量",{"2":{"1032":1}}],["动量项可以减少参数更新方向的震荡",{"2":{"1031":1,"1032":1}}],["动量项会逐渐增大",{"2":{"1031":1}}],["动量项会考虑前一次更新的方向和幅度",{"2":{"1031":1}}],["动量方法",{"2":{"1028":1}}],["动量后面的admw那些据估计忘了",{"2":{"645":1}}],["动作等等",{"2":{"676":1}}],["动作tokenizer",{"0":{"637":1},"2":{"638":1}}],["动词性",{"2":{"712":1}}],["动词",{"2":{"567":1}}],["动力学是一个梯度向量场",{"2":{"507":1}}],["动态规划",{"2":{"2102":1}}],["动态扩展",{"2":{"1797":1}}],["动态扩容",{"2":{"1714":1}}],["动态数组的更安全选择",{"2":{"1714":1}}],["动态数组的内存在堆上分配",{"2":{"1714":1}}],["动态数组的元素",{"2":{"1706":1}}],["动态数组",{"2":{"1714":1}}],["动态大小",{"2":{"1713":1,"1803":1}}],["动态多态",{"0":{"1688":1}}],["动态内存释放",{"0":{"1669":1}}],["动态内存管理和类的基础知识",{"2":{"1678":1}}],["动态内存管理",{"2":{"1650":1}}],["动态内存分配的必要性",{"2":{"1647":1}}],["动态内存分配",{"0":{"1668":1},"2":{"1612":1}}],["动态分配一个大小为",{"2":{"1716":1}}],["动态分配一个字符串",{"2":{"1647":1}}],["动态分配的对象通过",{"2":{"1676":1}}],["动态分配的数组",{"2":{"1668":1}}],["动态分配的数组元素",{"2":{"1647":1}}],["动态分配的内存位于堆",{"2":{"1668":1}}],["动态分配的内存",{"2":{"1648":1}}],["动态地分配一块指定类型的内存空间",{"2":{"1647":1}}],["动态地将字节分组为patch",{"2":{"611":1}}],["动态性的一个重要体现",{"2":{"1113":1}}],["动态原理图",{"2":{"1089":1}}],["动态效果展示",{"2":{"1030":1}}],["动态计算图",{"2":{"875":1,"1090":1}}],["动态展示为",{"2":{"870":1}}],["动态词向量相较于静态词向量",{"2":{"715":1}}],["动态图更灵活",{"2":{"1287":1}}],["动态图的一个典型示例是pytorch的计算图",{"2":{"1287":1}}],["动态图是在模型执行阶段构建的计算图",{"2":{"1287":1}}],["动态图机制",{"0":{"1108":1}}],["动态图如下",{"2":{"854":1}}],["动态图为",{"2":{"779":1}}],["动态图",{"2":{"661":1,"664":1,"1104":1}}],["动态学习",{"2":{"612":1}}],["动态patch划分",{"2":{"611":1}}],["动态",{"2":{"360":1}}],["动态调整自己权重",{"2":{"233":1}}],["动态调整权重",{"2":{"233":1}}],["动机与挑战",{"0":{"152":1},"2":{"96":1}}],["动机",{"0":{"45":1,"351":1,"359":1,"484":1,"488":1,"606":1,"612":1,"618":1,"627":1,"720":1,"739":1},"2":{"0":1,"293":2,"765":1}}],["活",{"2":{"233":1}}],["哈哈哈",{"2":{"2112":1,"2129":1}}],["哈佛大学",{"2":{"1124":1}}],["哈佛数据集",{"0":{"370":1}}],["哈佛的代码相对简单太多",{"2":{"364":1}}],["哈佛教程之中的实现方式是pre",{"2":{"344":1}}],["哈佛代码用generator类对这两部分进行了封装",{"2":{"471":1}}],["哈佛代码中的make",{"2":{"447":1}}],["哈佛代码中rate",{"2":{"402":1}}],["哈佛代码中使用交叉熵损失函数来比较模型的预测的概率分布",{"2":{"398":1}}],["哈佛代码中通过两个变量把两种掩码做了糅合",{"2":{"77":1}}],["哈佛代码就是用了填充和截断",{"2":{"376":1}}],["哈佛代码通过multi30k数据集来训练模型",{"2":{"370":1}}],["哈佛代码",{"0":{"113":1,"197":1,"701":1},"1":{"198":1,"199":1,"200":1},"2":{"96":1,"157":1}}],["哈工深张正团队提出polaformer视觉基础模型",{"2":{"233":1}}],["细胞状态中的值有可能被丢弃",{"2":{"867":1}}],["细胞状态与忘记向量进行逐元素相乘",{"2":{"867":1}}],["细节展示",{"2":{"1090":1}}],["细节展开图如下",{"2":{"890":1}}],["细节没有在make",{"2":{"449":1}}],["细节",{"0":{"341":1,"630":1},"1":{"631":1,"632":1,"633":1,"634":1,"635":1,"636":1},"2":{"293":1}}],["细读经典+代码解析",{"2":{"233":1}}],["细化",{"2":{"126":1}}],["浙江大学提出动态注意力机制",{"2":{"233":1}}],["浙大揭秘llm隐层之间的知识流动",{"2":{"156":1}}],["允许编译器自动推导类模板的参数",{"2":{"1925":1}}],["允许编译器根据变量的初始化值自动推断变量的类型",{"2":{"1615":1}}],["允许",{"2":{"1924":1}}],["允许你将一个结构体",{"2":{"1921":1}}],["允许使用",{"2":{"1910":1}}],["允许使用不同的激活函数",{"2":{"1143":1}}],["允许函数返回类型也可以自动推导",{"2":{"1905":1}}],["允许实现移动语义",{"2":{"1886":1}}],["允许一个类从另一个类中继承属性和方法",{"2":{"1848":1}}],["允许随机访问文件中的数据",{"2":{"1821":1,"1839":1}}],["允许通过下标访问单个字符",{"2":{"1713":1}}],["允许像访问数组一样访问对象的元素",{"2":{"1712":1}}],["允许隐式类型转换",{"2":{"1685":1}}],["允许我们定义可以参数化的变量",{"2":{"1908":1}}],["允许我们为自定义的数据类型重新定义现有运算符的行为",{"2":{"1712":1}}],["允许我们编写可以处理多种数据类型的代码",{"2":{"1698":1}}],["允许我们创建一个新类",{"2":{"1654":1}}],["允许我们存储和操作更复杂的数据结构",{"2":{"1622":1}}],["允许在头文件中定义内联变量",{"2":{"1923":1}}],["允许在",{"2":{"1922":1}}],["允许在数字字面量中插入单引号",{"2":{"1910":1}}],["允许在两端进行高效地添加和删除操作",{"2":{"1800":1}}],["允许在两端快速插入和删除元素",{"2":{"1722":1}}],["允许在任何位置快速插入和删除元素",{"2":{"1720":1}}],["允许在函数声明时为参数指定默认值",{"2":{"1708":1}}],["允许在运行时动态绑定",{"2":{"1693":1}}],["允许在创建对象时传递初始值",{"2":{"1675":1}}],["允许在不修改原有代码的情况下",{"2":{"1645":1}}],["允许在多个源文件中共享声明",{"2":{"1628":1}}],["允许绑定到即将销毁的对象",{"2":{"1629":1}}],["允许将多个表达式放在一个语句中",{"2":{"1630":1}}],["允许将任意类型的指针转换为任意其他类型的指针",{"2":{"1629":1}}],["允许将一个项目拆分成多个独立的编译单元",{"2":{"1628":1}}],["允许程序员直接控制内存的分配和释放",{"2":{"1602":1}}],["允许精确控制进程间的数据传输",{"2":{"1579":1}}],["允许计算机学习使用特征的同时",{"2":{"1455":1}}],["允许来自代价函数的信息通过网络向后流动",{"2":{"1438":1}}],["允许根据一些验证指标动态减小学习率",{"2":{"1230":1}}],["允许根据输入范围的不同进行缩放",{"2":{"360":1}}],["允许以",{"2":{"1178":1}}],["允许下游任务灵活地利用这两种表示",{"2":{"739":1}}],["允许每个位置自由地注意到整个序列",{"2":{"535":1}}],["允许信息高速无阻碍的通过深层神经网络的各层",{"2":{"298":1}}],["允许记忆忘记不再需要的信息",{"2":{"230":1}}],["允许模型通过反向传播进行优化",{"2":{"153":1}}],["允许模型利用注意力机制生成的上下文信息",{"2":{"120":1}}],["谷歌开源社区指南",{"2":{"1199":1}}],["谷歌大脑团队",{"2":{"1124":1}}],["谷歌研究",{"2":{"1124":1}}],["谷歌对开源编码器",{"2":{"540":1}}],["谷歌transformer继任者",{"2":{"233":1}}],["谷歌直接使用输入数据来更新长期记忆",{"2":{"231":1}}],["谷歌通过三个不同的",{"2":{"231":1}}],["谷歌仅仅使用了没有更新权重的前向传递",{"2":{"230":1}}],["谷歌使用了滑动窗口注意力",{"2":{"231":1}}],["谷歌使用了一种自适应遗忘机制",{"2":{"230":1}}],["谷歌使用线性层",{"2":{"230":1}}],["谷歌使用两个线性层将",{"2":{"230":1}}],["谷歌希望记忆模块可以学习键和值之间的关联",{"2":{"230":1}}],["谷歌则专注于关联",{"2":{"230":1}}],["谷歌将意外指标分解称为",{"2":{"230":1}}],["谷歌认为需要一个在线元模型来学习如何在测试时记忆或忘记数据",{"2":{"230":1}}],["受保护属性",{"2":{"1873":1}}],["受保护方法",{"2":{"1873":1}}],["受保护成员",{"2":{"1769":1}}],["受保护的成员可以在类的内部以及派生类中访问",{"2":{"1677":1}}],["受系统可用内存限制",{"2":{"1648":1}}],["受简单且可扩展的预训练方法的启发",{"2":{"1316":1}}],["受batch",{"2":{"1186":1}}],["受计算限制的和不受计算限制的",{"2":{"1154":1}}],["受人类构思交流的高层级思路启发",{"2":{"627":1}}],["受人类记忆启发",{"2":{"228":1}}],["受力角度",{"0":{"509":1}}],["受到归一化层与缩放后的tanh函数形状相似的启发",{"2":{"360":1}}],["受这一启发",{"2":{"359":1}}],["受此启发",{"2":{"230":1}}],["令起点坐标为",{"2":{"2018":1}}],["令词义的不同维度和向量不同维度进行关联",{"2":{"712":1}}],["令",{"2":{"503":1}}],["令人惊讶",{"2":{"230":1}}],["令牌的秩崩溃仍然会发生",{"2":{"92":1}}],["门控循环单元",{"0":{"872":1},"1":{"873":1,"874":1,"875":1}}],["门控函数",{"2":{"739":1}}],["门控快捷映射",{"2":{"301":1}}],["门控记忆",{"2":{"229":1}}],["门限",{"2":{"98":1,"100":1}}],["持久单元",{"2":{"1478":1}}],["持久模块",{"2":{"1478":1}}],["持久层",{"0":{"1478":1},"2":{"1478":2}}],["持久化的主要应用是将内存中的对象存储在数据库中",{"2":{"1477":1}}],["持久化是将程序数据在持久状态和瞬时状态间转换的机制",{"2":{"1477":1}}],["持久化",{"0":{"1477":1}}],["持久性缓冲区和非持久性缓冲区之间唯一的区别是后者不会成为该模块的",{"2":{"1211":1}}],["持久记忆分支",{"2":{"229":1}}],["持续学习",{"2":{"139":1}}],["惊讶程度",{"2":{"228":1}}],["违反预期",{"2":{"228":1}}],["泰坦",{"2":{"226":1}}],["少样本学习示例",{"2":{"985":1}}],["少样本适配",{"2":{"225":1}}],["少于给定字符数",{"2":{"636":1}}],["少部分头可以比较好地捕捉到各种文本信息",{"2":{"20":1}}],["专业",{"2":{"2051":1}}],["专为",{"2":{"1605":1}}],["专著于数据持久化逻辑的实现",{"2":{"1478":1}}],["专门用于评估模型生成图片的性能",{"2":{"1361":1}}],["专门用于离散索引到向量的映射",{"2":{"702":1}}],["专门为语义相似度任务训练的微调模型",{"2":{"692":1}}],["专门设计的适配性提示",{"2":{"225":1}}],["专家和新手用着表面上类似的方法",{"2":{"1127":1}}],["专家",{"2":{"224":1}}],["专家模块可以离线开发并按需增强到基础",{"2":{"222":1}}],["执行一系列步骤来解决问题的过程",{"2":{"2103":1}}],["执行一次优化步骤",{"2":{"1227":1}}],["执行结果",{"2":{"1706":1}}],["执行部分",{"2":{"1699":1}}],["执行完毕",{"2":{"1645":1}}],["执行完循环体后",{"2":{"1620":1}}],["执行效率上",{"2":{"1630":1}}],["执行递增",{"2":{"1621":1}}],["执行循环体",{"2":{"1621":1}}],["执行初始化语句",{"2":{"1621":1}}],["执行速度相对较慢",{"2":{"1602":1}}],["执行速度快",{"2":{"1602":1}}],["执行计算任务",{"2":{"1578":1}}],["执行相应操作",{"2":{"1563":1}}],["执行权限",{"2":{"1512":1}}],["执行方式",{"0":{"1281":1}}],["执行训练步骤",{"2":{"1243":1}}],["执行训练和参数更新",{"2":{"1239":1}}],["执行前向传播",{"2":{"1212":1}}],["执行",{"2":{"1083":1,"1620":1}}],["执行矩阵乘法操作",{"2":{"1086":1}}],["执行矩阵乘法和加法操作",{"2":{"1086":1}}],["执行矩阵乘法",{"2":{"973":1}}],["执行所有的计算步骤",{"2":{"944":1,"963":1}}],["执行替换操作",{"2":{"587":1}}],["执行任何可编程的任务",{"2":{"504":1}}],["执行步骤如下",{"2":{"445":1}}],["执行流程",{"0":{"445":1},"2":{"1621":1}}],["执行子层操作",{"2":{"344":1}}],["执行的操作",{"2":{"320":1}}],["执行模型并观察其测试时行为",{"2":{"225":1}}],["执行qk^t",{"2":{"199":1}}],["放在个位",{"2":{"2125":1}}],["放在十位",{"2":{"2125":1}}],["放在百位",{"2":{"2125":1}}],["放一张大纲导图",{"2":{"1502":1}}],["放大器",{"2":{"224":1}}],["放到一起",{"2":{"36":1}}],["充分",{"2":{"335":1}}],["充分训练的标准大致是一个知识要在训练语料中出现1000次以上",{"2":{"147":1}}],["充当精确的短期记忆",{"2":{"231":1}}],["充当",{"2":{"224":1}}],["某种适应性策略会怎么样呢",{"2":{"1184":1}}],["某些问题用递归解决思路清晰",{"2":{"1646":1}}],["某些情况下",{"2":{"1615":1}}],["某些模型会在非常早期的阶段显示出不稳定的情况",{"2":{"1179":1}}],["某些函数将使用其他值",{"2":{"1115":1}}],["某些神经元可能永远不会被激活",{"2":{"840":1}}],["某些神经网络很难处理长距离依赖关系",{"2":{"246":1}}],["某些",{"2":{"568":1}}],["某些词可能会被拆分成更小的单元",{"2":{"601":1}}],["某些词语",{"2":{"566":1}}],["某些词汇可能有特定的含义",{"2":{"560":1}}],["某些tokenizer会把训练语料出现的且词汇表中本来没有的token或者特殊字符加入词表",{"2":{"563":1}}],["某些参数分布偏离太大",{"2":{"309":1}}],["某些组件在语言理解和推理任务之间是共享的",{"2":{"224":1}}],["某个特定的推理操作",{"2":{"629":1}}],["某个词的含义是其与同一句中其他词",{"2":{"318":1}}],["某个单词依赖句子中很早的单词",{"2":{"242":1}}],["某个查询向量的注意力权重分布的熵可以用来衡量它对不同键向量的关注程度",{"2":{"194":1}}],["想当然的",{"2":{"2117":1}}],["想想日常生活",{"2":{"2112":1}}],["想想铁打的钢七连",{"2":{"2056":1}}],["想要和得到",{"2":{"2056":1}}],["想要加入哪怕只是一句话的新知识",{"2":{"222":1}}],["想象你在一座山上爬山",{"2":{"2115":1}}],["想象你的电脑内存是一栋栋排列整齐的房子",{"2":{"1611":1}}],["想象一下你借了别人的土地却一直不归还",{"2":{"1647":1}}],["想象一下你面前放着两面镜子互相反射",{"2":{"1646":1}}],["想象一下均匀网格棋盘上的物体",{"2":{"692":1}}],["想象一下",{"2":{"627":1,"1728":1,"1729":1}}],["想象一个机器学习系统",{"2":{"220":1}}],["想办法微调一下attention结构",{"2":{"1334":1}}],["想办法将位置信息融入到输入中",{"2":{"1334":1}}],["想知道是怎么训练的就很容易了",{"2":{"899":1}}],["想做到语义搜索就要理解语义",{"2":{"678":1}}],["想法很简单",{"2":{"185":1}}],["物联网",{"2":{"1941":1}}],["物品",{"2":{"1728":2}}],["物竞天择",{"2":{"220":1}}],["物理机",{"2":{"2090":1}}],["物理模拟",{"2":{"2009":1}}],["物理引擎",{"2":{"1936":1,"1937":1}}],["物理块会根据需要进行分配",{"2":{"982":1}}],["物理和语言空间被统一表示为embedding",{"2":{"689":1}}],["物理含义",{"2":{"687":1}}],["物理自旋系统的结构",{"0":{"508":1}}],["物理学与图形学结合",{"2":{"2009":1}}],["物理学家与数学家的含义相近",{"2":{"685":1}}],["物理学家解决了这个悬而未决的问题",{"2":{"685":1}}],["物理学家喜欢和咖啡",{"2":{"685":1}}],["物理学家跑向商店",{"2":{"685":1}}],["物理学角度",{"0":{"506":1},"1":{"507":1,"508":1,"509":1}}],["物理学和哲学等方面得到了广泛的应用",{"2":{"505":1}}],["物理上的三个大的权重矩阵会自然而然的变成逻辑上的3×h3×h3",{"2":{"26":1}}],["物理角度",{"0":{"29":1},"2":{"0":1}}],["生态持续完善",{"2":{"1960":1}}],["生态系统的基础",{"2":{"1433":1}}],["生命周期",{"2":{"1649":1}}],["生活化的例子来解释贪心思维",{"2":{"2121":1}}],["生活离不开算法思维",{"0":{"2103":1},"1":{"2104":1,"2105":1,"2106":1}}],["生活中的很多方面都可以通过算法来解释",{"2":{"2107":1}}],["生活中的",{"2":{"2101":1}}],["生活中的搜索",{"0":{"2100":1}}],["生活中我们经常在不经意间进行这样的搜索",{"2":{"2100":1}}],["生活中我们经常需要做选择",{"2":{"2099":1}}],["生活中常常有算法",{"0":{"2098":1},"1":{"2099":1,"2100":1,"2101":1,"2102":1}}],["生活中很多习惯和行为",{"2":{"2097":1}}],["生活",{"2":{"2031":1}}],["生活案例",{"2":{"1597":1}}],["生活与算法",{"0":{"2096":1},"1":{"2097":1,"2098":1,"2099":1,"2100":1,"2101":1,"2102":1,"2103":1,"2104":1,"2105":1,"2106":1,"2107":1,"2108":1},"2":{"1566":1}}],["生产者",{"2":{"973":1}}],["生僻词应该拆分成子词以共享token压缩空间",{"2":{"567":1}}],["生物学中的神经元",{"2":{"1459":1}}],["生物学上的进展也在启发ai研究人员开发新模型",{"2":{"486":1}}],["生物学角度",{"0":{"486":1},"1":{"487":1,"488":1,"489":1,"490":1}}],["生物学和数学",{"2":{"474":1}}],["生物体展现出的适应能力使得生命能够在不断变化的环境中蓬勃发展",{"2":{"220":1}}],["生成与构建",{"0":{"1967":1,"1995":1},"1":{"1968":1,"1969":1,"1996":1}}],["生成目标文件需要执行的命令",{"2":{"1917":1}}],["生成目标文件所依赖的其他文件",{"2":{"1917":1}}],["生成目标序列的解码表示",{"2":{"427":2}}],["生成订单等多个函数",{"2":{"1729":1}}],["生成旋转矩阵",{"2":{"1345":1}}],["生成发射分数",{"2":{"1320":1}}],["生成文本形式的标签",{"2":{"1317":1}}],["生成式问答",{"2":{"1317":1}}],["生成脚本",{"2":{"1291":1}}],["生成随机数据",{"2":{"1218":1}}],["生成像",{"2":{"1164":1}}],["生成自然语言文本",{"2":{"906":1}}],["生成较短的摘要",{"2":{"906":1}}],["生成符合语法和语义规则的文本",{"2":{"906":1}}],["生成等任务",{"2":{"785":1}}],["生成更加丰富的词向量表示",{"2":{"717":1}}],["生成词向量的过程是一个参数更新的过程",{"2":{"714":1}}],["生成4个独立的向量",{"2":{"676":1}}],["生成注意力掩码等",{"2":{"545":1}}],["生成最终的翻译输出",{"2":{"528":1}}],["生成输出序列的下一个",{"2":{"515":1}}],["生成预测^yy^",{"2":{"484":1}}],["生成预测输出",{"2":{"398":1}}],["生成",{"2":{"383":1,"541":1,"1345":1}}],["生成多少个batch",{"2":{"383":1}}],["生成一组输入数据",{"2":{"383":1}}],["生成一组随机数据",{"2":{"383":1}}],["生成一个",{"2":{"1912":2}}],["生成一个新网络",{"2":{"301":1}}],["生成一个下三角矩阵",{"2":{"84":1}}],["生成的是unique",{"2":{"1911":1}}],["生成的结果一定是类似于样本的数据",{"2":{"1372":1}}],["生成的数据与真实数据分布越相似",{"2":{"1361":1}}],["生成的每个单元称为token",{"2":{"545":1}}],["生成的概率分布的置信度",{"2":{"352":1}}],["生成的小批量的平均值为0和单位方差为1",{"2":{"343":1}}],["生成语义向量",{"2":{"267":1}}],["生成当前时刻的字符ytyty",{"2":{"241":1}}],["生成结果就是一些人脸",{"2":{"1371":1}}],["生成结果主要依赖于某个图像实例",{"2":{"809":1}}],["生成结果",{"2":{"173":1,"271":1}}],["生成权重",{"2":{"170":1}}],["生成填充词对应的掩码",{"2":{"74":1,"380":1}}],["生成tgt",{"2":{"74":1,"382":1}}],["生成src",{"2":{"66":1,"74":1,"382":1}}],["生成掩码有两个目的",{"2":{"382":1}}],["生成掩码",{"0":{"74":1,"382":1},"2":{"49":1}}],["章鱼能够迅速改变自身的肤色和纹理",{"2":{"220":1}}],["适用场景",{"2":{"1615":1,"1709":1}}],["适用的条件是",{"2":{"1228":1}}],["适用于很多场景",{"2":{"2121":1}}],["适用于内存敏感的场景",{"2":{"1801":1}}],["适用于需要频繁插入",{"2":{"1799":1}}],["适用于需要大规模并行计算的科学和工程应用",{"2":{"1569":1}}],["适用于函数不需要修改原始数据的情况",{"2":{"1729":1}}],["适用于大规模计算",{"2":{"1579":1}}],["适用于多核处理器的并行计算",{"2":{"1569":1}}],["适用于windows",{"2":{"1569":1}}],["适用于不同的计算需求和环境",{"2":{"1569":1}}],["适用于不同维度的输入",{"2":{"805":1}}],["适用于具有动态特性和复杂逻辑的模型",{"2":{"1287":1}}],["适用于ai算法的训练学习",{"2":{"796":1}}],["适用于一些具有非常大词汇量的神经网络",{"2":{"185":1}}],["适合用于集合操作",{"2":{"1806":1}}],["适合喜欢自定义配置的开发者",{"2":{"1605":1}}],["适合有一定基础的开发者",{"2":{"1605":1}}],["适合流水线任务",{"2":{"1578":1}}],["适合矩阵乘法",{"2":{"1578":1}}],["适合需要在不同节点之间进行通信的任务",{"2":{"1569":1}}],["适合大型集群内的多核节点",{"2":{"1568":1}}],["适合大规模分布式系统",{"2":{"1568":1}}],["适合多任务操作",{"2":{"1555":1}}],["适合生成任务",{"2":{"542":1}}],["适合处理序列数据",{"2":{"250":1}}],["适者生存",{"2":{"220":1}}],["适应的概念同样具有巨大的吸引力",{"2":{"220":1}}],["适应",{"2":{"220":1}}],["效率很高",{"2":{"1648":1}}],["效率更高",{"2":{"1612":1,"1646":1}}],["效率略低于直接访问",{"2":{"1612":1}}],["效率",{"2":{"1612":1,"1650":1,"1715":1}}],["效率高",{"2":{"1602":1}}],["效率飙升5倍",{"2":{"638":1}}],["效率以及适应性",{"2":{"559":1}}],["效率自然不同",{"2":{"217":1}}],["效果大的工作",{"2":{"2106":1}}],["效果最优的一条路径",{"2":{"1320":1}}],["效果往往并不好",{"2":{"1312":1}}],["效果很好",{"2":{"1047":1,"1275":1}}],["效果展示",{"0":{"1062":1},"2":{"1035":1}}],["效果也一般",{"2":{"907":1}}],["效果也是相似的",{"2":{"160":1}}],["效果会变差",{"2":{"807":1}}],["效果其实不太好",{"2":{"711":1}}],["效果提升不大",{"2":{"698":1}}],["效果与bpe相当",{"2":{"608":1}}],["效果与初始词表息息相关",{"2":{"603":1}}],["效果基本相当",{"2":{"346":1}}],["效果肯定更优",{"2":{"335":1}}],["效果越差",{"2":{"296":1}}],["效果更好",{"2":{"169":1,"696":1,"711":1}}],["效果",{"0":{"18":1,"511":1,"945":1,"965":1},"2":{"0":1,"333":1,"843":1,"911":1,"935":1,"951":1}}],["感兴趣的小伙伴可以自己查阅学习",{"2":{"2119":1}}],["感兴趣的token",{"2":{"158":1}}],["感悟总结",{"2":{"2051":1}}],["感知机的结构如下",{"2":{"1461":1}}],["感知机",{"2":{"1461":1}}],["感知型优化版本",{"2":{"216":1}}],["便应运而生了",{"2":{"850":1}}],["便是进行相反的操作",{"2":{"779":1}}],["便是一种线性注意力",{"2":{"216":1}}],["便于代码调试",{"2":{"1729":1}}],["便于组织和管理大型项目",{"2":{"1628":1}}],["便于开发者编译和执行",{"2":{"1589":1}}],["便于统一管理和优化",{"2":{"1479":1}}],["便于计算机进行处理",{"2":{"680":1}}],["便于机器学习模型处理",{"2":{"680":1}}],["便于后续处理",{"2":{"517":1}}],["便于解释和计算损失函数",{"2":{"180":1}}],["便可以得到下一个输出单词",{"2":{"267":1}}],["响应以降低熵",{"2":{"211":1,"213":1}}],["旨在通过逐步做出局部最优选择来寻找全局最优解",{"2":{"2114":1}}],["旨在通过纳入被忽略的负交互作用来解决先前线性注意力模型的局限性",{"2":{"211":1}}],["旨在按照https",{"2":{"1176":1}}],["旨在最大限度地洞察调优问题",{"2":{"1175":1}}],["旨在应用于凸问题时快速收敛",{"2":{"1048":1}}],["旨在加速学习",{"2":{"1028":1}}],["旨在降低通信成本",{"2":{"975":1}}],["旨在希望可以找到一个最优的激活函数",{"2":{"845":1}}],["旨在高维空间捕捉词汇间的关系",{"2":{"697":1}}],["旨在让无需归一化层的transformer",{"2":{"358":1}}],["旨在统一该领域的各种发现和观察结果",{"2":{"351":1}}],["旨在将过去的信息压缩到其参数中",{"2":{"228":1}}],["旨在纠正",{"2":{"140":1}}],["⋅",{"2":{"210":2}}],["⋅vmemorynet",{"2":{"125":2}}],["⋅v",{"2":{"71":4,"125":1}}],["ϕ",{"2":{"210":4}}],["核函数",{"2":{"210":1,"770":1}}],["核心技能要求",{"0":{"1937":1,"1942":1,"1947":1,"1952":1,"1957":1}}],["核心方向",{"0":{"1936":1,"1941":1,"1946":1,"1951":1,"1956":1}}],["核心区别在于默认的成员访问权限和继承权限",{"2":{"1728":1}}],["核心给每个进程",{"2":{"1589":1}}],["核心概念",{"0":{"1434":1},"1":{"1435":1,"1436":1},"2":{"1667":1}}],["核心公式",{"2":{"1394":1}}],["核心基础知识",{"2":{"1365":1,"1367":1}}],["核心要点",{"2":{"971":1}}],["核心改进点",{"2":{"970":1}}],["核心不同",{"2":{"969":1}}],["核心思想",{"2":{"935":1,"951":1,"1810":1,"1828":1}}],["核心在于一个序列当前的输出与前面的输出也有关",{"2":{"851":1}}],["核心贡献",{"0":{"352":1},"2":{"293":1}}],["核心分支",{"2":{"229":1}}],["核心的自注意力机制是其计算成本的重要来源",{"2":{"210":1}}],["核心创新",{"0":{"228":1},"2":{"157":1}}],["核心逻辑就是让填充词在经过softmax操作不应该有对应的输出",{"2":{"61":1}}],["平",{"2":{"2056":1}}],["平常心",{"2":{"2056":1}}],["平常最常用到的就是pca",{"2":{"1370":1}}],["平方",{"2":{"1709":1}}],["平台开发中表现优秀",{"2":{"1605":1}}],["平台的核心版本",{"2":{"1433":1}}],["平衡实验的信息量和成本",{"0":{"1145":1}}],["平衡模型的拟合能力和泛化能力",{"2":{"1012":1}}],["平衡不同级别的注意力权重",{"2":{"209":1}}],["平行语料",{"2":{"908":1}}],["平滑参数更新路径",{"2":{"1032":1}}],["平滑的激活函数允许更好的信息深入神经网络",{"2":{"846":1}}],["平滑后的标签",{"2":{"399":1}}],["平滑因子",{"2":{"399":1}}],["平移参数",{"2":{"341":1}}],["平移不变性",{"2":{"204":1,"320":1}}],["平均成绩",{"2":{"1680":1}}],["平均池化在每个池化窗口中选择特征值的平均值作为输出",{"2":{"815":1}}],["平均72",{"2":{"735":1}}],["平均token是基于训练数据的词频",{"2":{"437":1}}],["平均",{"2":{"437":1,"731":1}}],["平均字",{"2":{"340":3}}],["平均书",{"2":{"340":2}}],["平等看待序列中每个单词的顺序",{"2":{"272":1}}],["平等看待序列中每个单词",{"2":{"256":1}}],["希望你能喜欢",{"2":{"2140":1}}],["希望这些随笔能给我带来反思的同时带给你启发",{"2":{"2109":1}}],["希望能给大家起到抛砖引玉的作用",{"2":{"2097":1}}],["希望通过本节课的学习",{"2":{"1913":1,"1932":1}}],["希望大家能够静下心来跟我一块学习算法",{"2":{"2108":1}}],["希望大家通过本次课程的学习",{"2":{"1826":1,"1844":1}}],["希望大家认真学习和实践",{"2":{"1678":1}}],["希望避免产生另一个混淆",{"2":{"1185":1}}],["希望可以得到效果更好且具有强解释性的结果",{"2":{"492":1}}],["希望理解llm内部的运行机制",{"2":{"475":1}}],["希望在更复杂的高阶空间中提炼并重新表达权重相关性",{"2":{"209":1}}],["希望读者能够从其中对qkv有所理解",{"2":{"162":1}}],["位运算的高级用法",{"0":{"2061":1}}],["位运算的意义",{"2":{"2058":1}}],["位运算",{"0":{"2057":1},"1":{"2058":1,"2059":1,"2060":1,"2061":1,"2062":1}}],["位运算符介绍",{"0":{"2059":1}}],["位运算符通常用于对整数类型进行操作",{"2":{"1630":1}}],["位运算符",{"2":{"1630":1}}],["位小数",{"2":{"1817":1,"1835":1}}],["位",{"2":{"1678":1}}],["位或",{"2":{"1635":1,"1678":1}}],["位异或",{"2":{"1635":1}}],["位与",{"2":{"1635":1}}],["位左移和位右移",{"2":{"1635":1}}],["位系统",{"2":{"1611":2}}],["位系统中",{"2":{"1607":1}}],["位有效数字的精度",{"2":{"1607":2}}],["位精度内计算任何",{"2":{"504":1}}],["位于编码器中",{"2":{"442":1}}],["位于解码器的中间的橙色圈",{"2":{"436":1}}],["位于第一和第二位置",{"2":{"209":1}}],["位置的",{"2":{"1344":1}}],["位置都计算对应的旋转位置编码",{"2":{"1344":1}}],["位置完全连接的前馈网络",{"2":{"914":1}}],["位置嵌入和词嵌入不应该耦合",{"2":{"764":1}}],["位置矩阵被添加到查询键矩阵中",{"2":{"746":1}}],["位置标签",{"2":{"744":1}}],["位置最后一层的隐状态作为句子向量",{"2":{"727":1}}],["位置编码是加性的",{"2":{"1344":1}}],["位置编码是在原",{"2":{"1336":1}}],["位置编码有点相似",{"2":{"1344":1}}],["位置编码的方案",{"2":{"1344":1}}],["位置编码的加入是必不可少的",{"2":{"1334":1}}],["位置编码的操作对象是自注意力变换中的自注意力矩阵",{"2":{"745":1}}],["位置编码的操作对象是自注意力变换中的特征序列q",{"2":{"745":1}}],["位置编码的作用是告诉模型正确的语序",{"2":{"698":1}}],["位置编码分类",{"0":{"741":1},"1":{"742":1,"743":1,"744":1,"745":1,"746":1,"747":1,"748":1,"749":1,"750":1,"751":1,"752":1,"753":1,"754":1,"755":1,"756":1,"757":1,"758":1,"759":1,"760":1,"761":1,"762":1,"763":1,"764":1,"765":1,"766":1,"767":1,"768":1},"2":{"741":1}}],["位置编码在语义空间中并不具有这种可变换性",{"2":{"512":1}}],["位置编码本身就是一个妥协之举",{"2":{"512":1}}],["位置编码",{"0":{"672":1,"1336":1},"2":{"445":1,"765":1}}],["位置编码可以确保单词的顺序不会丢失",{"2":{"431":1}}],["位置编码和线性变换等转换",{"2":{"161":1}}],["位置之间的前后向注意力就会变得一样",{"2":{"172":1}}],["位置信息等",{"2":{"131":1}}],["位置时输出logits的预测排名",{"2":{"130":2}}],["位置",{"0":{"394":1,"440":1},"2":{"122":1,"535":1,"671":1,"758":4,"764":2,"930":1,"1340":7}}],["比直接使用",{"2":{"1891":1}}],["比裸指针更安全",{"2":{"1695":1}}],["比tgi高出最多3",{"2":{"980":1}}],["比赛中使用了激活函数",{"2":{"840":1}}],["比传统的向量化方式效率更高",{"2":{"696":1,"711":1}}],["比不带scale",{"2":{"647":1,"924":1}}],["比x大一点",{"2":{"399":1}}],["比较字符串",{"2":{"1715":1}}],["比较递归版本和迭代版本的代码",{"2":{"1646":1}}],["比较指向同一数组的指针",{"2":{"1633":1}}],["比较它们指向的内存地址",{"2":{"1633":1}}],["比较相同类型的指针",{"2":{"1633":1}}],["比较两个字符串",{"2":{"1713":1}}],["比较两个指针的地址",{"2":{"1633":1}}],["比较两个值之间的关系",{"2":{"1619":1}}],["比较运算符",{"2":{"1619":1}}],["比较",{"2":{"1340":1}}],["比较足够多目标超参数的不同值",{"2":{"1145":1}}],["比较batch",{"2":{"1133":1}}],["比较优化器的性能也是一项艰巨的任务",{"2":{"1130":1}}],["比较本地",{"2":{"1073":1}}],["比较了各种架构和预训练方法的组合",{"2":{"542":1}}],["比较大的时候",{"2":{"511":1}}],["比较大时",{"2":{"334":1,"647":1,"924":1}}],["比较常见的解释是分层的本质是由下往上从不同上下文中逐步构建不同层次的特征",{"2":{"437":1}}],["比较小",{"2":{"332":1}}],["比较特殊",{"2":{"326":1}}],["比较才有意义",{"2":{"318":1}}],["比较a1a1a",{"2":{"209":1}}],["比",{"2":{"318":2,"560":1,"935":1,"951":1}}],["比对",{"0":{"339":1,"604":1},"1":{"340":1,"341":1},"2":{"293":1}}],["比线性模型更有效",{"2":{"228":1}}],["比如你要规划任务",{"2":{"2118":1}}],["比如你可以用中文输入来查询英文文本内容",{"2":{"696":1}}],["比如制定学习计划",{"2":{"2102":1}}],["比如工作时",{"2":{"2099":1}}],["比如说",{"2":{"2097":1}}],["比如说编码器把x映射成中间表示z",{"2":{"267":1}}],["比如实时光线追踪",{"2":{"2010":1}}],["比如设置全局的c++",{"2":{"1982":1}}],["比如参数列表中的参数类型",{"2":{"1699":1}}],["比如猫叫声为",{"2":{"1690":1}}],["比如mpi与openmp的结合使用",{"2":{"1568":1}}],["比如mlm或者seq2seq",{"2":{"727":1}}],["比如银行账号等",{"2":{"1477":1}}],["比如样本为很多个人脸",{"2":{"1371":1}}],["比如手写数字",{"2":{"1370":1}}],["比如8～11",{"2":{"1340":1}}],["比如最大长度为512",{"2":{"1337":1}}],["比如研究中检查点的最佳性能",{"2":{"1167":1}}],["比如固定步长间隔评估一次",{"2":{"1164":1}}],["比如为",{"2":{"1110":1}}],["比如已经走了t步",{"2":{"904":1}}],["比如获得每个时间步的相对位置编码",{"2":{"746":1}}],["比如把rope单独列为旋转编码",{"2":{"742":1}}],["比如把中间层维度缩减为原来的2",{"2":{"109":1}}],["比如4096=",{"2":{"733":1}}],["比如ner",{"2":{"720":1}}],["比如no",{"2":{"377":1}}],["比如用与任务相关的缩放系数进行加权求和",{"2":{"718":1}}],["比如用于检索",{"2":{"711":1}}],["比如就像",{"2":{"709":1}}],["比如2就是",{"2":{"700":1}}],["比如是否可以跑",{"2":{"690":1}}],["比如从下面句子中我们可以知道",{"2":{"685":1}}],["比如instant这个单词就有多种词性",{"2":{"683":1}}],["比如图像",{"2":{"682":1}}],["比如计算距离和角度",{"2":{"679":1}}],["比如计算成本高",{"2":{"309":1}}],["比如词典中有4个单词",{"2":{"679":1}}],["比如词形还原",{"2":{"552":1}}],["比如大家熟悉的gpt",{"2":{"627":1}}],["比如在计算自注意力矩阵时",{"2":{"757":1}}],["比如在三角函数编码的基础之上额外学习一些其他参数",{"2":{"747":1}}],["比如在英文字典中",{"2":{"679":1}}],["比如在解决一项复杂的任务或撰写一份长篇文档时",{"2":{"627":1}}],["比如在机器翻译中",{"2":{"257":1}}],["比如在机器翻译任务中",{"2":{"165":1}}],["比如data",{"2":{"624":1}}],["比如单个中文字符被切割为多个字节表示",{"2":{"608":1}}],["比如单词",{"2":{"456":1,"553":1}}],["比如所有字符+高频ngram",{"2":{"602":1}}],["比如unigram语言模型",{"2":{"599":1}}],["比如u是对称的",{"2":{"507":1}}],["比如不同tokenizer",{"2":{"595":1}}],["比如英文中26个字母加上各种符号",{"2":{"576":1,"599":1}}],["比如有一段数据是",{"2":{"575":1}}],["比如某些特定的分词方法可以帮助模型更好地理解和处理未见过的词汇",{"2":{"568":1}}],["比如某个token",{"2":{"326":1}}],["比如某个q去捕捉语法依赖",{"2":{"5":1}}],["比如推理和训练都会带来更多的计算成本",{"2":{"566":1}}],["比如每个字符对应的嵌入向量会承载太多语义信息",{"2":{"566":1}}],["比如无法处理单词的形态",{"2":{"565":1}}],["比如那个著名问题",{"2":{"560":1}}],["比如确保文本采用统一的字符编码",{"2":{"552":1}}],["比如统一大小写和数字标准化",{"2":{"552":1}}],["比如特殊字符",{"2":{"552":1}}],["比如去除无用字符",{"2":{"552":1}}],["比如整数",{"2":{"545":1}}],["比如原始输入是",{"2":{"528":1}}],["比如粒子位置对时间的导数是两个函数的和",{"2":{"498":1}}],["比如gpt",{"2":{"490":1}}],["比如gpu",{"2":{"1":1}}],["比如如何将梯度信息有效地应用于模型的知识更新与编辑中",{"2":{"484":1}}],["比如如下",{"2":{"305":1}}],["比如该研究中有一部分是探索不同注意力头的功能",{"2":{"475":1}}],["比如我们不熟悉",{"2":{"713":1}}],["比如我们可以用向量之间的距离来度量",{"2":{"692":1}}],["比如我们可以得到",{"2":{"545":1}}],["比如我们可以简单地根据对它们进行排名",{"2":{"473":1}}],["比如我们的标签是2",{"2":{"399":1}}],["比如以下两句话的文字完全相同",{"2":{"459":1}}],["比如以某某结尾",{"2":{"127":1}}],["比如模型无法区分",{"2":{"457":1}}],["比如模型想要回答",{"2":{"118":1}}],["比如上面图中的标号1",{"2":{"449":1}}],["比如上图的每个推理步的输入都是",{"2":{"409":1}}],["比如llama",{"2":{"976":1}}],["比如linear和softmax被封装在generator类中",{"2":{"449":1}}],["比如loss",{"2":{"399":1}}],["比如集体智能来自个体的组合",{"2":{"446":1}}],["比如底层学习单词特征",{"2":{"437":1}}],["比如选取概率最高的",{"2":{"431":1}}],["比如其中一个变种是curriculum",{"2":{"411":1}}],["比如利用其它模型生成高质量的合成数据",{"2":{"369":1}}],["比如会基于知识深度和帮助性等质量指标进行采样权重调整",{"2":{"368":1}}],["比如按标点符号分词",{"2":{"363":1,"547":1}}],["比如必备的warm",{"2":{"333":1}}],["比如难以训练",{"2":{"331":1}}],["比如rnn无法并行训练",{"2":{"291":1}}],["比如隐向量是固定的",{"2":{"267":1}}],["比如softmax函数",{"2":{"264":1}}],["比如第一次是",{"2":{"532":1,"533":1}}],["比如第一个句子的",{"2":{"167":1,"259":1}}],["比如第二个英文句子中",{"2":{"260":1}}],["比如人类可以依据兴趣和需求选择关注某些信息而忽略或抑制其它信息",{"2":{"257":1}}],["比如翻译",{"2":{"256":1}}],["比如需要完整理解整个句子乃至整篇文章才能做出判断",{"2":{"255":1}}],["比如信息遗失",{"2":{"251":1}}],["比如下图给出了一个模型中的第6层和第7层之间的关系",{"2":{"437":1}}],["比如下图所示",{"2":{"247":1}}],["比如下面句子中",{"2":{"755":1}}],["比如下面句子中就有字体大小",{"2":{"3":1}}],["比如下面是把",{"2":{"702":1}}],["比如下面就是一个可能的词典形式",{"2":{"567":1}}],["比如下面中译英的例子",{"2":{"277":1}}],["比如下面两个句子中都有",{"2":{"259":1}}],["比如下面要介绍的transformer2transformer2transformer^2",{"2":{"207":1}}],["比如将",{"2":{"246":1}}],["比如机器翻译",{"2":{"245":1}}],["比如语音识别",{"2":{"245":1}}],["比如百万",{"2":{"230":1}}],["比如现在已在使用的",{"2":{"206":1}}],["比如相似度",{"2":{"174":1}}],["比如注意力",{"2":{"162":1}}],["比如当识别到和这个知识有关的所有知识神经元之后",{"2":{"143":1}}],["比如可以通过bpe来初始化",{"2":{"603":1}}],["比如可以将",{"2":{"567":1}}],["比如可以直接使用新知识的表示来训练超网络",{"2":{"142":1}}],["比如可以是从2到8的任何范围",{"2":{"100":1}}],["比如由若干描述李世民的prompts",{"2":{"135":1}}],["比如句法信息和词法信息",{"2":{"135":1}}],["比如句子的分类",{"2":{"127":1}}],["比如融入beats的信息",{"2":{"122":1}}],["比如基于",{"2":{"58":1}}],["比如填",{"2":{"53":1}}],["比如query的第一段只和key的第一段进行点积",{"2":{"33":1}}],["比如名词和动词的指向关系",{"2":{"20":1}}],["比如一些通用",{"2":{"127":1}}],["比如一个一百万维空间里的随便一万个点",{"2":{"684":1}}],["比如一个句子",{"2":{"380":1,"714":1}}],["比如一个向量列表",{"2":{"158":1}}],["比如一个",{"2":{"87":1}}],["比如一个学句法",{"2":{"20":1}}],["比如一项事物往往有多个方面",{"2":{"3":1}}],["比如都关注cls",{"2":{"20":1}}],["比如论文",{"2":{"20":1,"181":1,"396":1}}],["比如对于bert使用",{"2":{"731":1}}],["比如对于同一个句子",{"2":{"595":1}}],["比如对于句子",{"2":{"547":1}}],["比如对于对话和问答数据",{"2":{"369":1}}],["比如对于rnn来说",{"2":{"256":1}}],["比如对于上面的例子",{"2":{"4":1}}],["比如对于一个词来说",{"2":{"4":1}}],["比如",{"2":{"3":1,"17":1,"20":1,"36":1,"89":2,"111":1,"121":1,"129":1,"131":1,"169":1,"172":1,"180":2,"204":1,"206":1,"224":1,"245":1,"258":1,"261":1,"316":1,"344":1,"369":1,"384":1,"391":2,"396":1,"453":1,"496":1,"505":1,"512":3,"536":1,"553":1,"557":1,"558":1,"560":1,"567":1,"576":1,"587":1,"612":1,"620":1,"676":1,"681":1,"683":1,"684":1,"689":2,"690":1,"700":1,"709":1,"713":1,"714":1,"715":1,"744":2,"751":1,"757":2,"765":1,"1143":1,"1210":1,"1462":2,"1472":1,"1479":1,"1568":1,"1963":1,"2054":1,"2104":1,"2106":1,"2107":1}}],["硬件加速技术",{"2":{"1942":1}}],["硬件加速的低精度gemm",{"2":{"973":1}}],["硬件故障",{"2":{"1814":1,"1832":1}}],["硬件实现",{"0":{"1420":1},"1":{"1421":1,"1422":1,"1423":1,"1424":1}}],["硬件设备等",{"2":{"1407":1}}],["硬件层面上",{"2":{"206":1}}],["硬编码",{"2":{"681":1}}],["硬编码的对角线注意力",{"2":{"284":1}}],["硬核",{"2":{"47":1}}],["连续赋值",{"2":{"1629":1}}],["连续的输入和具有兼容步幅的输入可以进行重塑而无需复制",{"2":{"819":1}}],["连续块注意力用来保持llm的局部性",{"2":{"204":1}}],["连续块注意力",{"2":{"204":1}}],["连乘后使得整个秩变的更低",{"2":{"305":1}}],["连接等大量字符串操作函数",{"2":{"1803":1}}],["连接调用者和被调用者",{"2":{"1729":1}}],["连接字符串",{"2":{"1713":1,"1715":1}}],["连接两个字符串或字符串与字符",{"2":{"1713":1}}],["连接",{"2":{"689":1,"1594":2,"1713":1}}],["连接几何空间和概念空间的就是embedding",{"2":{"689":1}}],["连接mlp17的边是",{"2":{"130":1}}],["连接组件的边",{"2":{"130":1}}],["连接后加权求和",{"2":{"718":1}}],["连接后",{"2":{"41":1}}],["块来处理这些异常",{"2":{"1766":1}}],["块来捕获这个异常并进行处理",{"2":{"1647":1}}],["块参数的类型必须与抛出的异常类型相同",{"2":{"1762":1}}],["块处理不同类型的异常",{"2":{"1762":1}}],["块可以跟随多个",{"2":{"1762":1}}],["块中的剩余代码被跳过",{"2":{"1762":1}}],["块中的代码抛出一个异常时",{"2":{"1762":1}}],["块中的代码",{"2":{"1762":2}}],["块中剩余的代码",{"2":{"1762":1}}],["块中应用两次",{"2":{"468":1}}],["块内使用两次",{"2":{"470":1}}],["块内注意力",{"2":{"204":1}}],["块的中间状态进行重新缩放",{"2":{"357":1}}],["块的输入隐藏状态首先使用",{"2":{"356":1}}],["块的更新方程",{"2":{"354":1}}],["块的拟合能力",{"2":{"232":1}}],["块",{"0":{"356":1},"2":{"293":1,"354":1,"1762":7}}],["块间注意力",{"2":{"204":1}}],["块式稀疏注意力",{"2":{"204":1}}],["窗口截断的方式也可以作为长度外推的一个不错的baseline",{"2":{"204":1}}],["窗口",{"2":{"204":1}}],["空调却一直开着",{"2":{"2054":1}}],["空数组的意义",{"2":{"1704":1}}],["空指针检查",{"2":{"1672":1}}],["空格是字符串内容的一部分",{"2":{"1704":1}}],["空格字符",{"2":{"1704":1}}],["空格",{"2":{"1616":1}}],["空字符串的长度为",{"2":{"1704":1}}],["空字符串",{"2":{"1704":3}}],["空字符",{"2":{"1616":1}}],["空值",{"2":{"1612":1}}],["空类型",{"2":{"1607":1}}],["空间复杂度小",{"2":{"848":1}}],["空间可分离卷积是将卷积核分解为两项独立的核分别进行操作",{"2":{"777":1}}],["空间可分离卷积",{"0":{"777":1}}],["空间内外",{"2":{"712":1}}],["空间前后",{"2":{"712":1}}],["空间上下",{"2":{"712":1}}],["空间中的逻辑细分子空间用的",{"2":{"12":1}}],["空白patch化无法优雅地处理所有语言和领域",{"2":{"613":1}}],["空白patch",{"2":{"613":1}}],["空洞卷积",{"0":{"778":1},"2":{"778":1}}],["空洞注意力和全局注意力",{"2":{"204":1}}],["空洞自注意力",{"2":{"204":1}}],["起始梯度",{"2":{"1045":1}}],["起始从2−8",{"2":{"765":1}}],["起始层的注意力分布大致均匀",{"2":{"204":1}}],["起码每一条残差通道都是平权的",{"2":{"334":1}}],["起作用的原因",{"2":{"314":1}}],["起到信息对齐的目的",{"2":{"261":1}}],["起来",{"2":{"7":1,"689":1}}],["改动",{"2":{"2043":1}}],["改操作需要提交事务",{"2":{"1486":1}}],["改为只依赖于相对距离",{"2":{"1339":1}}],["改为只依赖于相对距离i−j的向量rki",{"2":{"759":1}}],["改为二元位置向量",{"2":{"1339":1}}],["改善验证损失",{"2":{"1154":1}}],["改造decoder",{"2":{"740":1}}],["改造的原因在于研究人员认为decoder",{"2":{"734":1}}],["改变文件所有者和所属组",{"2":{"1513":1}}],["改变",{"2":{"1099":1,"1143":1}}],["改变梯度积累为指数加权的移动平均",{"2":{"1048":1}}],["改变数据分布的均值和方差",{"2":{"807":1}}],["改变注意力中的",{"2":{"357":1}}],["改变计算单元从内存读数据的方式",{"2":{"206":1}}],["改变output的形状为",{"2":{"201":1}}],["改变xq",{"2":{"201":1}}],["改进策略",{"0":{"2028":1}}],["改进后",{"2":{"1683":2,"1684":1,"1685":1,"1687":1,"1688":1}}],["改进前",{"2":{"1683":2,"1684":1,"1685":1,"1687":1,"1688":1}}],["改进最近的研究",{"2":{"1146":1}}],["改进的方法",{"2":{"901":1}}],["改进的utf",{"2":{"607":1}}],["改进诉求",{"0":{"682":1},"1":{"683":1,"684":1,"685":1,"686":1,"687":1}}],["改进",{"0":{"40":1,"182":1,"377":1},"1":{"41":1,"42":1,"43":1,"44":1,"45":1,"46":1,"183":1,"184":1,"185":1},"2":{"0":1,"157":1}}],["负面的还是中性的",{"2":{"906":1}}],["负半轴",{"2":{"842":1}}],["负无穷",{"2":{"651":1,"933":1,"944":1}}],["负无穷大",{"2":{"201":1}}],["负责用户交互和调用其他模块",{"2":{"1916":1}}],["负责底层数据的存储和查询",{"2":{"1867":1}}],["负责管理其内部存储字符串数据的内存",{"2":{"1713":1}}],["负责查询数据库中与之最相似的",{"2":{"536":1}}],["负责对编码器的输出和解码器之前的输出做交叉注意力计算",{"2":{"533":1}}],["负责对解码器之前的输出",{"2":{"533":1}}],["负责对decoder的输出做预测",{"2":{"450":1}}],["负责特定的预测",{"2":{"476":1}}],["负责依据decoder的输出来预测下一个token",{"2":{"449":1}}],["负交互作用则完全丢失",{"2":{"212":1}}],["负和负",{"2":{"212":1}}],["负值丢失",{"2":{"212":1}}],["负载平衡损失",{"2":{"42":1}}],["批处理规范统计信息在主机之间同步",{"2":{"1169":1}}],["批次大小",{"2":{"375":1}}],["批次大小和序列长度",{"2":{"201":1}}],["批次",{"2":{"370":1}}],["批量",{"2":{"370":1}}],["批量规范化bnbn",{"2":{"343":1}}],["批量统计估算不准确导致批量变小时",{"2":{"338":1}}],["批量大小",{"2":{"66":1,"217":1}}],["批量执行从",{"2":{"36":1}}],["了吗",{"2":{"2054":1}}],["了解如何模拟光照和材质效果",{"2":{"2009":1}}],["了解平移",{"2":{"2009":1}}],["了解c++11相对于c++98的新特性",{"2":{"1876":1}}],["了解它们的使用场景和对程序设计的影响",{"2":{"1846":1}}],["了解它们的特点和应用场景",{"2":{"1711":1}}],["了解它们的优势和适用场景",{"2":{"1602":1}}],["了解智能指针",{"2":{"1647":1}}],["了解什么是",{"2":{"1646":1}}],["了解栈空间的限制",{"2":{"1646":1}}],["了解",{"0":{"1589":1},"2":{"1602":1,"1603":1,"1648":1,"1649":1,"1711":1,"1731":1}}],["了解随机元优化中的短期偏差描述了选择学习率的短视危险",{"2":{"1158":1}}],["了解特定模型超参数",{"2":{"1141":1}}],["了解即可",{"0":{"1112":1},"1":{"1113":1,"1114":1,"1115":1,"1116":1,"1117":1,"1118":1,"1119":1,"1120":1,"1121":1,"1122":1,"1123":1}}],["了解了ffn的重要性",{"2":{"115":1}}],["了一份数据",{"2":{"1072":1}}],["了",{"2":{"246":1,"334":1,"525":1,"528":1,"974":1,"1157":1,"1344":1,"2054":1}}],["了下面提到的问题",{"2":{"243":1}}],["了原有的注意力",{"2":{"194":1}}],["换行符",{"2":{"1616":1}}],["换行符等",{"2":{"552":1}}],["换个方式或者换个材料",{"2":{"1597":1}}],["换个角度想",{"2":{"194":1}}],["换成",{"2":{"1339":1}}],["换成了",{"2":{"232":1}}],["换句话说就是效果有所下降",{"2":{"935":1,"951":1}}],["换句话说",{"2":{"35":1,"41":1,"72":1,"296":1,"985":1,"1120":1,"1143":1,"1149":1,"1150":1,"1154":2,"1313":1,"2108":1}}],["尽可能使用局部变量或将数据封装在类中",{"2":{"1649":1}}],["尽可能早地在管道中删除不必要的特征和元数据",{"2":{"1161":1}}],["尽可能考虑新点和baseline的再训练方差",{"2":{"1152":1}}],["尽可能密集地对冗余超参数采样",{"2":{"1145":1}}],["尽可能多地比较目标超参数的值可以拓宽我们从实验中获得的经验的范围",{"2":{"1145":1}}],["尽可能让输入的数据",{"2":{"310":1}}],["尽量抛出和捕获具体类型的异常",{"2":{"1764":1}}],["尽量都写上",{"2":{"1488":1}}],["尽量去近似",{"2":{"1377":1}}],["尽量重用有效的模型",{"2":{"1129":1}}],["尽量减少warp间的通讯和读取shared",{"2":{"972":1}}],["尽量保持梯度的大小和方向保持一致",{"2":{"838":1}}],["尽量具备熵不变性",{"2":{"194":1}}],["尽管可能会造成混淆",{"2":{"1185":1}}],["尽管在实践中",{"2":{"1728":1}}],["尽管在更高的维度中不行",{"2":{"1176":1}}],["尽管在人为判断生成的衰减方案时这几乎不可能",{"2":{"1173":1}}],["尽管它包含相同优秀的点",{"2":{"1175":1}}],["尽管它可能很糟糕",{"2":{"1137":1}}],["尽管上面的例子的是一个很好的起点",{"2":{"1151":1}}],["尽管不完全相同",{"2":{"1143":1}}],["尽管不同应用场景的开发流程有所不同",{"2":{"1138":1}}],["尽管当资源有限或有强力的证据表明它们不影响目标超参数时",{"2":{"1143":1}}],["尽管超参数的类型取决于实验目标",{"2":{"1143":1}}],["尽管有些人认为我们会花大部分时间来提升验证集的指标",{"2":{"1140":1}}],["尽管如此",{"2":{"1122":1,"2054":2}}],["尽管每个张量都有这个标志",{"2":{"1117":1}}],["尽管学习率有时需要从建议的默认修改",{"2":{"1059":1}}],["尽管bgd可能收敛到全局最优解",{"2":{"1025":1}}],["尽管从概念上讲",{"2":{"964":1}}],["尽管由于重新计算而增加了浮点运算量",{"2":{"940":1,"962":1}}],["尽管llms在生成任务上表现出色",{"2":{"738":1}}],["尽管曼哈顿距离在高维数据中似乎可以工作",{"2":{"692":1}}],["尽管这会让我们更难解释我们的调优结果",{"2":{"1175":1}}],["尽管这是可能的",{"2":{"977":1}}],["尽管这是一种常用的距离度量",{"2":{"692":1}}],["尽管这两对向量在方向上保持一致",{"2":{"692":1}}],["尽管这取决于",{"2":{"34":1}}],["尽管通过我们上面手工制作的向量",{"2":{"690":1}}],["尽管处理的是sonar空间中的连续表示",{"2":{"636":1}}],["尽管decoder端在推理阶段的并行化存在挑战",{"2":{"413":1}}],["尽管结果尚可",{"2":{"284":1}}],["尽管线性注意力在计算效率上有所提升",{"2":{"212":1}}],["尽管已有方法尝试优化",{"2":{"152":1}}],["尽管外部知识依赖和外部知识注入这两类方法在不同任务上表现良好",{"2":{"142":1}}],["尽管包含相同的",{"2":{"135":1}}],["尽管升维有助于捕捉更多的信息",{"2":{"116":1}}],["尽管升维带来更多的特征表示",{"2":{"116":1}}],["尽管",{"2":{"41":1,"121":1,"137":1,"335":1,"1315":1}}],["熵值𝐻𝑖应该对长度𝑛不敏感",{"2":{"194":1}}],["熵不变性是指",{"2":{"194":1}}],["熵可以用来度量模型输出的不确定性",{"2":{"194":1}}],["熵的作用",{"0":{"194":1},"2":{"157":1}}],["∂loss∂w",{"2":{"1442":1}}],["∂l∂w=x⊤i⋅δi∂l∂w=xi⊤⋅δi",{"2":{"148":1,"485":1}}],["∂eo2∂outh1",{"2":{"1394":1}}],["∂eo2∂outh1=∂eo2∂outo2×∂outo2∂neto2×∂neto2∂outh1=−",{"2":{"1394":1}}],["∂eo1∂outh1=−",{"2":{"1394":1}}],["∂eo1∂outh1=∂eo1∂outo1×∂outo1∂neto1×∂neto1∂outh1=−",{"2":{"1394":1}}],["∂eo1∂outh1+∂eo2∂outh1",{"2":{"1394":1}}],["∂etotal∂w1=0",{"2":{"1394":1}}],["∂etotal∂w1=∂etotal∂outh1×∂outh1∂neth1×∂neth1∂w1=",{"2":{"1394":1}}],["∂etotal∂w7=0",{"2":{"1393":1}}],["∂etotal∂w7=−",{"2":{"1392":1}}],["∂etotal∂w7=∂etotal∂outo1×∂outo1∂neto1×∂neto1∂w7",{"2":{"1392":1}}],["∂etotal∂outo1=2×12",{"2":{"1393":1}}],["∂neto1∂w7=1×outh1+0+0+0=0",{"2":{"1393":1}}],["∂outo1∂neto1=outo1",{"2":{"1393":1}}],["∂cost∂si",{"2":{"999":2}}],["∂y∂x=diag",{"2":{"192":1}}],["⋯",{"2":{"192":3}}],["≈​2​​1​​×",{"2":{"844":1}}],["≈12×",{"2":{"844":1}}],["≈11=1softmax",{"2":{"191":2}}],["≈xσ",{"2":{"844":2}}],["≈0",{"2":{"191":2}}],["减",{"2":{"1607":1}}],["减法模块",{"2":{"1916":1}}],["减法",{"2":{"1436":1,"1607":1,"1997":1}}],["减小学习率",{"2":{"1245":1}}],["减小特征图尺寸",{"2":{"813":1}}],["减小部分参数",{"2":{"347":1}}],["减轻了梯度消失的可能性",{"2":{"333":1}}],["减均值",{"2":{"312":1,"314":1,"316":1,"322":1}}],["减去一个插值δiδi",{"2":{"191":1}}],["减少未定义行为等",{"2":{"1932":1}}],["减少冗余代码",{"2":{"1880":1}}],["减少冗余通信",{"2":{"1579":1}}],["减少重复代码",{"2":{"1848":1}}],["减少或是保持不变都有可能",{"2":{"1131":1}}],["减少开发周期的延迟",{"2":{"1131":1}}],["减少震荡现象",{"2":{"1031":1}}],["减少参数更新方向的震荡",{"2":{"1031":1}}],["减少过拟合的风险",{"2":{"1012":1}}],["减少缓存的开销",{"2":{"986":1}}],["减少内存泄漏的风险",{"2":{"1892":1}}],["减少内存占用",{"2":{"986":1}}],["减少内存需求",{"2":{"153":1}}],["减少训练时间和资源消耗",{"2":{"898":1}}],["减少短期记忆的影响",{"2":{"863":1}}],["减少信息熵是减少预测误差的一种方法",{"2":{"612":1}}],["减少稀疏性",{"2":{"547":1}}],["减少模型的偏见",{"2":{"474":1}}],["减少梯度爆炸的可能性",{"2":{"403":1}}],["减少了模板代码的冗余",{"2":{"1932":1}}],["减少了冗余",{"2":{"1911":1}}],["减少了出错的可能性",{"2":{"1715":1}}],["减少了随机性带来的波动",{"2":{"1027":1}}],["减少了网络消耗并简化了对频繁弹性扩展的依赖",{"2":{"977":1}}],["减少了要学习的位置嵌入数量",{"2":{"759":1}}],["减少了不同层之间的输入分布变化",{"2":{"310":1}}],["减少了计算复杂度",{"2":{"204":1}}],["减少不同层之间的输入分布变化",{"2":{"310":1}}],["减少其他动态路由头之间的冗余",{"2":{"42":1}}],["∑jexp",{"2":{"847":1}}],["∑fscore",{"2":{"270":2}}],["∑i=1dkqi×ki",{"2":{"189":1}}],["∑ni=1pi=1∑i=1npi=1",{"2":{"178":1}}],["拿",{"2":{"709":1}}],["拿到",{"2":{"1283":1}}],["拿到这个梯度值",{"2":{"1099":1}}],["拿到batch类的实例",{"2":{"385":1}}],["拿到相互关系后",{"2":{"265":1}}],["拿出q矩阵中单独一列qiqiq",{"2":{"189":1}}],["拿什么",{"2":{"45":1}}],["推荐",{"2":{"1628":1}}],["推荐在需要较高精度的浮点数计算中使用",{"2":{"1607":1}}],["推荐使用parallels",{"2":{"2089":1}}],["推荐使用",{"2":{"1605":1,"1695":1}}],["推荐使用1e",{"2":{"346":1}}],["推荐通过下述方式来实现",{"2":{"1344":1}}],["推测",{"2":{"1159":1}}],["推迟其的增加可能更好",{"2":{"1134":1}}],["推断出来",{"2":{"1242":1}}],["推断出每个词对应的意思",{"2":{"715":1}}],["推断模式下的计算不会被记录在反向图中",{"2":{"1121":1}}],["推断模式是无梯度模式的极端版本",{"2":{"1121":1}}],["推断模式",{"0":{"1121":1}}],["推导指引",{"2":{"1925":1}}],["推导为",{"2":{"1925":2}}],["推导返回类型",{"2":{"1905":1}}],["推导过程详解",{"0":{"919":1},"1":{"920":1,"921":1,"922":1,"923":1}}],["推导过程非常复杂",{"2":{"908":1}}],["推导",{"0":{"494":1}}],["推导如下",{"2":{"189":1}}],["推理流程图",{"0":{"1351":1}}],["推理速度上生成一个",{"2":{"935":1,"951":1}}],["推理阶段",{"2":{"706":1}}],["推理阶段是用上一次的输出拼接成下一次输入",{"2":{"426":1}}],["推理阶段是自回归模式",{"2":{"57":1}}],["推理任务不用真实目标序列来指导生成过程",{"2":{"529":1}}],["推理时也保留了此处代码和模型结构",{"2":{"525":1}}],["推理时的",{"2":{"69":1}}],["推理的本质也是串行自回归的",{"2":{"525":1}}],["推理的目的则是仅通过输入序列来产生目标序列",{"2":{"389":1}}],["推理过程的逻辑流程如下",{"2":{"427":1}}],["推理过程中每个时间步的输入是直到当前时间步所产生的整个输出序列",{"2":{"525":1}}],["推理过程中每个时间步的输入",{"2":{"81":1}}],["推理还得",{"2":{"411":1}}],["推理步骤中会出现错误",{"2":{"405":1}}],["推理成本",{"2":{"351":1}}],["推理计算量减小10倍",{"2":{"156":1}}],["推理",{"0":{"83":1,"388":1,"425":1,"529":1},"1":{"389":1,"390":1,"391":1,"392":1,"393":1,"394":1,"395":1,"396":1,"397":1,"398":1,"399":1,"400":1,"401":1,"402":1,"403":1,"404":1,"405":1,"406":1,"407":1,"408":1,"409":1,"410":1,"411":1,"412":1,"413":1,"414":1,"415":1,"416":1,"417":1,"418":1,"419":1,"420":1,"421":1,"422":1,"423":1,"424":1,"425":1,"426":2,"427":2,"428":2,"429":1},"2":{"49":1,"426":1,"427":1,"628":1}}],["推理效率提升",{"2":{"611":1}}],["推理效率更高",{"2":{"542":1}}],["推理效率",{"2":{"47":1}}],["正好凑齐",{"2":{"2139":1}}],["正好抵消了维度增加造成的点积尺度放大效应",{"2":{"187":1}}],["正真领悟了这两句话的含义之后",{"2":{"2054":1}}],["正确",{"2":{"1614":2,"1867":1,"1868":1}}],["正确预测对应的词的那一维的概率大小",{"2":{"899":1}}],["正确答案",{"2":{"411":1}}],["正所谓一步错",{"2":{"895":1}}],["正反向的结果",{"2":{"858":1}}],["正向的时候",{"2":{"1106":1}}],["正向",{"2":{"1105":1}}],["正向激活的分布和反向激活梯度的分布尽量维持一致",{"2":{"999":1}}],["正向和反向用到权重是同一份数据吗",{"2":{"858":1}}],["正向forward",{"2":{"661":1,"1104":1}}],["正则项等方法将表征转换为稀疏表征",{"2":{"733":1}}],["正则化技术引入的超参数通常是冗余超参数",{"2":{"1143":1}}],["正则化具有k",{"2":{"1016":1}}],["正则化可以限制模型的复杂度",{"2":{"1012":1}}],["正则化可以帮助减少模型参数的数量",{"2":{"1012":1}}],["正则化在数据集规模较小",{"2":{"1012":1}}],["正则化在以下情况下容易出现",{"2":{"1012":1}}],["正则化在深度学习的出现前就已经被使用了数十年",{"2":{"1011":1}}],["正则化概念",{"0":{"1011":1}}],["正则化不仅可以防止模型过拟合",{"2":{"393":1}}],["正则化",{"2":{"310":1,"393":1,"1158":1,"1202":1}}],["正是这种共同的经历使话语变得有意义",{"2":{"689":1}}],["正因为transformer存在的各种问题",{"2":{"512":1}}],["正定和对称时",{"2":{"507":1}}],["正如在序言中所讨论的",{"2":{"1196":1}}],["正如figure",{"2":{"1177":1}}],["正如之前讨论的那样",{"2":{"1174":1}}],["正如期望的那样",{"2":{"1117":1}}],["正如上面所述",{"2":{"1117":1}}],["正如上述性能结果所示",{"2":{"982":1}}],["正如图2所示",{"2":{"976":1}}],["正如我们在下一节中所展示的",{"2":{"975":1}}],["正如我们将展示的那样",{"2":{"937":1,"953":1}}],["正如预期的那样",{"2":{"507":1}}],["正如",{"2":{"279":1,"334":1}}],["正交互作用",{"2":{"212":1}}],["正比于维度",{"2":{"187":1}}],["正在执行",{"2":{"1645":1}}],["正在处理的序列",{"2":{"265":1}}],["正在对llm的大脑进行细致操作",{"2":{"221":1}}],["正在逐步被门限",{"2":{"98":1}}],["正在阅读中",{"2":{"48":1}}],["文科生都能懂的算法",{"2":{"2096":1}}],["文档",{"2":{"1501":2}}],["文档旋转等方式破坏后传给",{"2":{"1317":1}}],["文件描述如何构建您的项目",{"2":{"1965":1}}],["文件等",{"2":{"1963":1}}],["文件路径",{"2":{"1933":1}}],["文件系统开发",{"2":{"1951":1}}],["文件系统库",{"0":{"1930":1},"2":{"1920":1,"1932":1,"1933":1}}],["文件系统结构采用树形结构",{"2":{"1505":1}}],["文件系统结构",{"0":{"1504":1},"1":{"1505":1,"1506":1,"1507":1}}],["文件的规则",{"2":{"1917":1}}],["文件的随机访问",{"0":{"1821":1,"1839":1},"2":{"1826":1,"1844":1}}],["文件结尾",{"2":{"1821":1,"1839":1}}],["文件开头",{"2":{"1821":1,"1839":1}}],["文件流",{"2":{"1819":1,"1837":1}}],["文件流类",{"0":{"1819":1,"1837":1}}],["文件打开失败",{"2":{"1761":1}}],["文件中",{"2":{"1628":1}}],["文件查找",{"0":{"1531":1},"1":{"1532":1,"1533":1}}],["文件权限管理",{"0":{"1511":1},"1":{"1512":1,"1513":1}}],["文件操作步骤",{"0":{"1820":1,"1838":1}}],["文件操作",{"0":{"1510":1},"2":{"1930":1}}],["文件和目录管理",{"0":{"1509":1}}],["文件io也是一种持久化机制",{"2":{"1477":1}}],["文件",{"0":{"1818":1,"1836":1},"1":{"1819":1,"1820":1,"1821":1,"1837":1,"1838":1,"1839":1},"2":{"1407":1,"1604":1,"1812":1,"1815":1,"1826":1,"1830":1,"1833":1,"1844":1,"1969":1,"1999":1}}],["文献4",{"2":{"1010":1}}],["文献3",{"2":{"1010":1}}],["文献2",{"2":{"1010":1}}],["文献1",{"2":{"1010":1}}],["文章侧边栏配置取消",{"2":{"2043":1}}],["文章结构建立整理并排查修复",{"2":{"2043":1}}],["文章内容",{"2":{"1472":1}}],["文章中的n为10000",{"2":{"1336":1}}],["文章中使用正余弦函数生成的位置编码",{"2":{"747":1}}],["文章的大多数词都由词汇表里的少数词构成",{"2":{"185":1}}],["文化等因素形成的",{"2":{"709":1}}],["文本替换",{"2":{"1632":1}}],["文本缩进和格式调整",{"0":{"1558":1},"1":{"1559":1,"1560":1}}],["文本查看",{"0":{"1514":1},"1":{"1515":1,"1516":1}}],["文本对之间的相关性更高",{"2":{"1360":1}}],["文本摘要",{"2":{"906":2}}],["文本聚类",{"2":{"906":1}}],["文本分类就是将文本送入",{"2":{"1317":1}}],["文本分类",{"2":{"906":1}}],["文本向量都是这样的",{"2":{"718":1}}],["文本嵌入模型旨在将自然语言文本的语义内容编码为向量表示",{"2":{"711":1}}],["文本嵌入",{"0":{"710":1},"1":{"711":1,"712":1,"713":1,"714":1,"715":1,"716":1,"717":1,"718":1,"719":1,"720":1,"721":1,"722":1,"723":1,"724":1,"725":1,"726":1,"727":1,"728":1,"729":1,"730":1,"731":1,"732":1,"733":1,"734":1,"735":1,"736":1,"737":1,"738":1,"739":1}}],["文本",{"0":{"678":1},"2":{"676":1,"1360":1}}],["文本模态仍然是离散的",{"2":{"636":1}}],["文本清洗",{"2":{"552":1}}],["文本生成",{"2":{"906":1}}],["文本生成机制",{"0":{"238":1}}],["文本生成模型的交互式学习",{"2":{"233":1}}],["文字转换",{"0":{"455":1},"1":{"456":1,"457":1,"458":1,"459":1,"460":1}}],["文中通过在多个数据集上跑实验",{"2":{"20":1}}],["接触前沿技术",{"2":{"2010":1}}],["接上例",{"2":{"1729":1}}],["接受",{"2":{"1929":1}}],["接受两个",{"2":{"1706":1}}],["接受一个容器",{"2":{"1914":1}}],["接受一个",{"2":{"1645":1}}],["接受来自其他进程或节点的消息",{"2":{"1563":1}}],["接受参数是源序列和掩码",{"2":{"450":1}}],["接口继承",{"0":{"1866":1},"2":{"1868":1}}],["接口所有的普通参数",{"2":{"1488":1}}],["接口中的方法名与映射文件中的sql语句id",{"2":{"1485":1}}],["接收型号名称",{"2":{"1664":3}}],["接收型号名称和最大速度并进行初始化",{"2":{"1664":1}}],["接收回调函数指针作为参数的函数",{"2":{"1645":1}}],["接收方可以通过标签区分不同的消息",{"2":{"1576":1}}],["接收端处理收到的消息",{"2":{"1563":1}}],["接收消息",{"2":{"1563":2}}],["接收输入",{"2":{"1438":1}}],["接收一个调度器列表",{"2":{"1247":1}}],["接收key",{"2":{"975":1}}],["接收features",{"2":{"343":1}}],["接近1",{"2":{"1360":1}}],["接近",{"2":{"865":2}}],["接近零",{"2":{"301":1}}],["接近程度根据这组数的数量级有所不同",{"2":{"180":1}}],["接着从剩下的",{"2":{"2125":1}}],["接着对每个",{"2":{"1344":1}}],["接着对新的权重矩阵b使用softmax函数",{"2":{"209":1}}],["接着进行层标准化",{"2":{"914":1}}],["接着再转换为输出",{"2":{"878":1}}],["接着再通过一种递归更新的方法",{"2":{"216":1}}],["接着通过参数矩阵",{"2":{"763":1}}],["接着用",{"2":{"423":1}}],["接着应用dropout",{"2":{"344":1}}],["接下来的目标就是找到一个等价的位置编码方式",{"2":{"1342":1}}],["接下来的指南中我们做出了这些假设",{"2":{"1138":1}}],["接下来需要对下一时刻的输出",{"2":{"537":1}}],["接下来需要解决的一个重要问题是",{"2":{"231":1}}],["接下来我应该输出什么",{"2":{"536":1}}],["接下来我们讲讲把文本变成向量的方法",{"2":{"680":1}}],["接下来我们会选择最高频的字符对进行合并",{"2":{"582":1}}],["接下来我们构建基础词表",{"2":{"580":1}}],["接下来我们用一个实例来进行剖析",{"2":{"577":1}}],["接下来我们看看每个层的代码",{"2":{"201":1}}],["接下来我们来追溯一下训练时候的",{"2":{"82":1}}],["接下来我们从另一个方面来解释",{"2":{"13":1}}],["接下来通过后面的全连接层",{"2":{"510":1}}],["接下来会通过注意力机制从高维语言空间中提取各种丰富的知识和结构",{"2":{"460":1}}],["接下来会通过交换",{"2":{"31":1}}],["接下来进行施釉",{"2":{"437":1}}],["接下来就以它为源头进行分析",{"2":{"435":1}}],["接下来就要从深层次来看",{"2":{"324":1}}],["接下来模型会依据这些概率",{"2":{"397":1}}],["接下来三篇偏重于工程",{"2":{"362":1}}],["接下来对每个数据减去均值再除以标准差的操作",{"2":{"313":1}}],["接下来对",{"2":{"200":3}}],["接下来",{"2":{"145":1,"320":1,"456":1,"537":1,"758":1,"1143":1}}],["接下来每个分组内进行点积运算和加权平均",{"2":{"33":1}}],["∥∥kj∥∥∥kj∥∥k",{"2":{"176":1}}],["∥qi∥∥qi∥∥q",{"2":{"176":1}}],["区的自动分配内存不同",{"2":{"1668":1}}],["区",{"2":{"1668":1}}],["区域",{"2":{"1148":1}}],["区域进行知识编辑外",{"2":{"144":1}}],["区间进行设置效果最佳",{"2":{"765":1}}],["区别于编译时错误",{"2":{"1761":1}}],["区别于llm原有的linear映射器",{"2":{"731":1}}],["区别在于",{"2":{"765":1}}],["区别",{"0":{"743":1},"1":{"744":1,"745":1,"746":1},"2":{"741":1}}],["区别encoderlayer只有一个多头自注意力模块",{"2":{"533":1}}],["区分前缀和后缀形式需要不同的函数签名",{"2":{"1712":1}}],["区分静态数组和动态数组",{"2":{"1711":1}}],["区分基类和派生类",{"2":{"1652":1}}],["区分特性",{"2":{"679":1}}],["区分不同角色或者说角色分离",{"2":{"172":1}}],["匹配规则",{"2":{"1762":1}}],["匹配的",{"2":{"1672":1}}],["匹配子字串",{"2":{"587":1}}],["匹配失败则调整或重新选择",{"2":{"550":1}}],["匹配成功则按照词典的词进行分词",{"2":{"550":1}}],["匹配",{"2":{"172":1}}],["呢",{"2":{"172":1,"831":1,"933":1,"1016":1,"1233":1,"1601":1,"2054":1}}],["外号",{"2":{"1612":2}}],["外推性是指大模型在训练时和预测时的输入长度不一致",{"2":{"1341":1}}],["外推性差",{"2":{"749":1}}],["外部使用",{"2":{"1214":1}}],["外部知识注入",{"2":{"141":1}}],["外部知识依赖和外部知识注入都算是保留权重方法",{"2":{"141":1}}],["外部知识依赖",{"2":{"141":1}}],["外",{"2":{"1118":1}}],["外层",{"0":{"1083":1}}],["外层循环遍历k和v",{"2":{"944":1}}],["外观上没有花纹且头骨较小",{"2":{"713":1}}],["外侧",{"2":{"490":1}}],["外在没变",{"2":{"170":1}}],["苹果",{"2":{"170":5,"267":2,"277":5,"516":3,"547":1,"714":2}}],["苹果给出了",{"2":{"156":1}}],["归档与压缩",{"0":{"1534":1},"1":{"1535":1}}],["归根结底",{"2":{"1152":1,"1161":1}}],["归纳了一些可以替换余弦相似度的备选项",{"2":{"692":1}}],["归纳",{"2":{"386":1}}],["归一化有助于降低不稳定性",{"2":{"1180":1}}],["归一化应该是残差之前的最后一个操作",{"2":{"1180":1}}],["归一化嵌入与余弦相似度",{"2":{"692":1}}],["归一化形状之内的所有元素上共用一套缩放",{"2":{"341":1}}],["归一化维度为",{"2":{"341":1}}],["归一化既保留了原来的语义",{"2":{"323":1}}],["归一化操作也可以使反向传播的梯度更加稳定",{"2":{"320":1}}],["归一化前年纪大的归一化后也不会变小",{"2":{"314":1}}],["归一化前是高个子的归一化后仍然是高个子",{"2":{"314":1}}],["归一化的维度为",{"2":{"341":3}}],["归一化的时候避开batch这个维度",{"2":{"316":1}}],["归一化的目的是将具有相同性质的数据转化为标准正态分布",{"2":{"316":1,"318":1}}],["归一化的结果保持样本之间的可比较性",{"2":{"314":1}}],["归一化的作用如下",{"2":{"310":1}}],["归一化可以采用不同的方法",{"2":{"294":1}}],["归一化对残差向量进行归一化",{"2":{"294":1}}],["归一化层能够提高模型的最终性能",{"2":{"310":1}}],["归一化层能够加速训练过程",{"2":{"310":1}}],["归一化层通过对激活值进行标准化处理",{"2":{"310":1}}],["归一化层",{"2":{"201":1}}],["归一化层和输出层串联起来",{"2":{"201":1}}],["归一化",{"0":{"308":1},"1":{"309":1,"310":1,"311":1},"2":{"173":1,"178":1,"271":1,"293":1,"310":1,"313":2,"352":2,"354":1,"355":1,"356":1,"1469":1}}],["归一化就是softmax操作",{"2":{"169":1}}],["归航",{"2":{"169":3}}],["郁达夫曾任主编",{"2":{"169":1}}],["等内容",{"2":{"2048":1}}],["等3a厂商中国分部",{"2":{"1939":1}}],["等支持多线程编程",{"2":{"1894":1}}],["等可以继承",{"2":{"1866":1}}],["等级划分",{"2":{"1825":1,"1843":1}}],["等级后",{"2":{"1825":1,"1843":1}}],["等方法进行读写操作",{"2":{"1820":1,"1838":1}}],["等方法对模型的mlp层修改",{"2":{"123":1}}],["等运算符来操作你的类对象",{"2":{"1712":1}}],["等函数计算在内",{"2":{"1704":1}}],["等函数计算字符串长度时",{"2":{"1704":1}}],["等流操纵符进行更复杂的格式化输出",{"2":{"1673":1}}],["等进行条件编译",{"2":{"1632":1}}],["等平台",{"2":{"1605":1}}],["等包装器工具后",{"2":{"1589":1}}],["等编译器工具并不是真正的编译器",{"2":{"1589":1}}],["等待所有线程完成",{"2":{"1566":1}}],["等待法",{"2":{"1413":1}}],["等工具熟练使用",{"2":{"1961":1}}],["等工具",{"2":{"1435":1}}],["等效于",{"2":{"1820":1,"1838":1}}],["等效于mha",{"2":{"937":1,"953":1}}],["等效于mqa",{"2":{"937":1,"953":1}}],["等效地",{"2":{"1132":1}}],["等模块",{"2":{"1122":1}}],["等模型生成的词向量作为嵌入层的初始值",{"2":{"709":1}}],["等于",{"2":{"1619":1,"1630":1,"1635":1}}],["等于计算输出值时使用的权重系数",{"2":{"1443":1}}],["等于1",{"2":{"1003":1}}],["等于最后一个维度",{"2":{"808":1}}],["等目前业内顶尖的",{"2":{"844":1}}],["等激活函数",{"2":{"842":1}}],["等在",{"2":{"840":1}}],["等并不满足单调的条件",{"2":{"838":1}}],["等操作的激活函数更受欢迎",{"2":{"838":1}}],["等都属于pointwise",{"2":{"829":1}}],["等变表示",{"2":{"772":2}}],["等算法预训练得到",{"2":{"706":1}}],["等价于",{"2":{"1034":1,"1629":1,"1712":1}}],["等价于model",{"2":{"385":1}}],["等价变换如下",{"2":{"770":1}}],["等价处理",{"2":{"662":1}}],["等词之间的区别",{"2":{"579":1}}],["等词对",{"2":{"277":1}}],["等设计方法",{"2":{"467":1}}],["等归一化层",{"2":{"351":1}}],["等",{"2":{"334":1,"642":1,"711":1,"1132":1,"1221":1,"1436":2,"1573":1,"1589":1,"1728":1,"1729":2,"1758":3,"1913":1,"1933":1}}],["等优化器",{"2":{"333":1}}],["等参数高效的方法",{"2":{"222":1}}],["等技术进行微调",{"2":{"222":1}}],["等许多技术",{"2":{"210":1}}],["等间隔的和其他",{"2":{"204":1}}],["等等",{"2":{"169":1,"402":1,"1762":1}}],["春风沉醉的晚上",{"2":{"169":3}}],["迟桂花",{"2":{"169":3}}],["薄奠",{"2":{"169":3}}],["标签在发送和接收时需一致",{"2":{"1576":1}}],["标签",{"2":{"1324":1,"1328":1}}],["标签向量",{"2":{"1320":2}}],["标签的预处理",{"2":{"1250":1}}],["标签平滑化",{"2":{"1149":1}}],["标签平滑的优势是能够防止模型追求确切概率而不影响模型学习正确分类",{"2":{"1016":1}}],["标签平滑",{"0":{"1016":1},"2":{"1016":1}}],["标签平滑交叉熵",{"2":{"429":1}}],["标识符",{"2":{"547":1}}],["标量的梯度才能被隐式创建",{"2":{"661":1,"1104":1}}],["标量",{"2":{"503":1,"1398":1}}],["标号4利用注意力权重a和矩阵v进行计算",{"2":{"264":1}}],["标号3通过对齐层",{"2":{"264":1}}],["标号2使用矩阵k和查询向量q作为输入",{"2":{"264":1}}],["标号1是输入",{"2":{"264":1}}],["标记的实体时会发出警告",{"2":{"1909":1}}],["标记后面的成员只能在类的内部访问",{"2":{"1674":1}}],["标记后面的成员可以在类的外部访问",{"2":{"1674":1}}],["标记视为字节",{"2":{"982":1}}],["标记表示解码结束或者达到指定长度后停止",{"2":{"515":1}}],["标记表示解码结束或者已经生成所需数量的token",{"2":{"431":1}}],["标记嵌入向量的范数不受约束",{"2":{"352":1}}],["标记或者达到预定序列长度的边界",{"2":{"241":1}}],["标记",{"2":{"204":1,"725":1,"1147":1}}],["标题",{"2":{"164":1,"463":2}}],["标准协作良好",{"2":{"1964":1}}],["标准日志流",{"2":{"1811":1,"1829":1}}],["标准错误流",{"2":{"1811":1,"1829":1}}],["标准输出流",{"2":{"1811":1,"1829":1}}],["标准输入流",{"2":{"1811":1,"1829":1}}],["标准库的改进",{"0":{"1889":1},"1":{"1890":1,"1891":1,"1892":1}}],["标准库提供了一组预定义的异常类",{"2":{"1762":1}}],["标准库提供的动态数组容器",{"2":{"1714":1}}],["标准库提供的类模板",{"2":{"1695":1}}],["标准库提供的字符串处理方式",{"2":{"1624":1}}],["标准库中的字符串类",{"2":{"1803":1}}],["标准库中一个非常重要的模块",{"2":{"1734":1}}],["标准库中用于处理字符串的类",{"2":{"1713":1}}],["标准库常用算法",{"0":{"1730":1},"1":{"1731":1,"1732":1,"1733":1,"1734":1,"1735":1,"1736":1,"1737":1,"1738":1,"1739":1,"1740":1,"1741":1,"1742":1,"1743":1,"1744":1,"1745":1,"1746":1,"1747":1,"1748":1,"1749":1,"1750":1,"1751":1,"1752":1,"1753":1,"1754":1,"1755":1,"1756":1,"1757":1,"1758":1,"1759":1}}],["标准库",{"2":{"1624":1}}],["标准交叉熵损失可以用在这些非确切目标的输出上",{"2":{"1016":1}}],["标准差为",{"2":{"994":1,"995":1}}],["标准attention",{"2":{"945":1,"965":1}}],["标准attention机制的算法实现",{"0":{"941":1,"960":1}}],["标准答案",{"2":{"897":1}}],["标准化工具链",{"2":{"1589":1}}],["标准化写法",{"2":{"552":1}}],["标准化",{"2":{"326":1,"551":1}}],["标准化破坏了数据的原始分布",{"2":{"313":1}}],["标准softmax的时间复杂度都是",{"2":{"184":1}}],["标准正态分布的累积分布函数",{"2":{"106":1}}],["标准的一部分",{"2":{"1589":1}}],["标准的注意力实现将矩阵",{"2":{"941":1,"960":1}}],["标准的编码器",{"2":{"450":1}}],["标准的softmax公式涉及到了很多的求幂和除法",{"2":{"183":1}}],["标准的",{"2":{"98":1}}],["标准",{"0":{"1811":1,"1829":1},"2":{"98":2,"100":1,"1982":1}}],["李四",{"2":{"1481":1,"1680":1}}],["李新春",{"2":{"233":1,"513":1}}],["李宁鞋",{"2":{"164":1,"463":1}}],["李世民",{"2":{"135":1}}],["普通用户账户",{"0":{"2070":1}}],["普通用户的主目录",{"2":{"1506":1}}],["普通赋值操作符",{"2":{"1887":1}}],["普通情况下",{"2":{"1769":1}}],["普通模式基础操作",{"0":{"1546":1},"1":{"1547":1,"1548":1,"1549":1,"1550":1}}],["普通模式",{"2":{"1519":1,"1541":1}}],["普通的",{"2":{"1476":1}}],["普通的字典查找是精确匹配",{"2":{"164":1,"265":1}}],["普通attention",{"2":{"964":1}}],["普通卷积网络kernel大小固定",{"2":{"781":1}}],["普遍认为这种归一化有助于缓解与内部协变量偏移相关的问题",{"2":{"468":1}}],["普林斯顿大学",{"2":{"156":1}}],["问号",{"2":{"1616":1}}],["问答系统",{"2":{"906":1}}],["问答场景等",{"2":{"722":1}}],["问的就是q和k两个词之间的紧密程度",{"2":{"164":1}}],["问题描述",{"0":{"2147":1}}],["问题就被彻底解决了",{"2":{"2104":1}}],["问题就转化成为了找最相似的",{"2":{"204":1}}],["问题场景",{"2":{"1849":1}}],["问题说明",{"0":{"1482":1}}],["问题制定",{"2":{"1128":1}}],["问题合集",{"0":{"665":1}}],["问题解决",{"0":{"272":1},"1":{"273":1,"274":1,"275":1,"276":1,"277":1,"278":1}}],["问题推导",{"0":{"188":1},"1":{"189":1,"190":1,"191":1,"192":1,"193":1},"2":{"157":1}}],["问题及优化",{"2":{"95":1}}],["问题所在",{"0":{"54":1,"58":1},"2":{"49":2}}],["问题转变为",{"2":{"17":1}}],["问题",{"0":{"3":1,"296":1,"309":1,"316":1,"405":1,"626":1,"715":1,"880":1,"881":1,"882":1,"2123":1,"2129":1,"2133":1,"2137":1},"2":{"0":1,"89":1,"204":1,"293":3,"565":1,"878":1,"880":1,"883":1,"895":1,"976":1,"1374":1,"1462":3,"2003":1,"2004":1,"2005":1,"2006":5,"2043":1,"2064":1,"2088":1}}],["乘法",{"2":{"213":1,"1436":1,"1607":1,"1635":1,"1997":1}}],["乘法方案还有一个劣势是",{"2":{"175":1}}],["乘",{"2":{"163":1,"1607":1}}],["乘以",{"2":{"1729":1}}],["乘以nbest",{"2":{"1330":1}}],["乘以embedding",{"2":{"701":1}}],["乘以vv",{"2":{"71":1}}],["乘以一个mask矩阵",{"2":{"71":1}}],["盐",{"2":{"163":1}}],["盐和变形金刚",{"2":{"163":1}}],["吸引了",{"2":{"163":1}}],["吸收新知识的能力至关重要",{"2":{"140":1}}],["詹姆斯在19世纪90年代提出的非自主性提示",{"2":{"163":1}}],["尚未找到确切的理论分析",{"2":{"162":1}}],["量化系统开发工程师",{"2":{"1948":1}}],["量化交易平台",{"2":{"1946":1}}],["量化",{"2":{"162":1}}],["量子位",{"2":{"47":1}}],["网关",{"2":{"2090":1}}],["网易伏羲采用最新的3d面部重建技术",{"2":{"2011":1}}],["网易伏羲团队开发的雅可比预处理非线性共轭梯度方法在处理复杂的自碰撞场景时表现卓越",{"2":{"2011":1}}],["网易雷火",{"2":{"1939":1}}],["网格搜索也是可以接受的",{"2":{"1176":1}}],["网站链接",{"2":{"974":1}}],["网上一个非常恰当的例子是",{"2":{"626":1}}],["网上对编码器和解码器的关系有一个比较恰当的通俗比喻",{"2":{"524":1}}],["网页内容和编程代码等",{"2":{"367":1}}],["网",{"2":{"316":1,"318":1}}],["网址之中也缩写为q",{"2":{"162":1}}],["网络同步",{"2":{"1936":1}}],["网络管理",{"0":{"1525":1},"1":{"1526":1,"1527":1,"1528":1}}],["网络连线与记忆体区块等",{"2":{"1411":1}}],["网络延迟问题等影响的情况下",{"2":{"1164":1}}],["网络层数通常是一个目标或固定的超参数",{"2":{"1143":1}}],["网络层数的增加会加剧梯度爆炸的风险",{"2":{"296":1}}],["网络的出口也是若干管道开口",{"2":{"1466":1}}],["网络的入口是若干管道开口",{"2":{"1466":1}}],["网络的输出信号",{"2":{"1443":1}}],["网络的最终性能与收敛得到的最优解直接相关",{"2":{"990":1}}],["网络的权重会变得极大",{"2":{"255":1}}],["网络会对前面的信息进行记忆并应用于当前输出的计算中",{"2":{"851":1}}],["网络架构和数据特征进行评估和实验",{"2":{"838":1}}],["网络各个层的参数就相对固化",{"2":{"709":1}}],["网络推理计算的每一步都足够小时",{"2":{"494":1}}],["网络需要不断适应新的输入数据分布",{"2":{"309":1}}],["网络y=f",{"2":{"301":1}}],["网络退化",{"2":{"296":1}}],["网络和数据易出现过拟合",{"2":{"296":1}}],["网络上也有一些讨论",{"2":{"175":1}}],["网络中长距离依赖关系之间的路径长度",{"2":{"160":1}}],["网络由两层组成",{"2":{"98":1}}],["网络结构的搭建",{"2":{"1202":1}}],["网络结构",{"0":{"98":1,"300":1},"1":{"99":1,"100":1,"101":1,"102":1,"103":1,"104":1,"105":1,"106":1,"107":1,"108":1,"109":1,"110":1,"111":1,"301":1,"302":1},"2":{"96":1,"293":1}}],["源代码和测试用例",{"2":{"1999":1}}],["源文件",{"2":{"1916":1,"1917":1}}],["源词典和目标词典大小都为11",{"2":{"428":1}}],["源词表",{"2":{"375":2}}],["源码了解",{"0":{"1592":1}}],["源码一样吗",{"2":{"1073":1}}],["源码",{"0":{"395":1},"2":{"1073":1,"1596":1}}],["源码中使用batch类承载了batch概念",{"2":{"379":1}}],["源句子列表",{"2":{"451":1}}],["源句子2",{"2":{"384":1,"558":1}}],["源句子1",{"2":{"384":1,"558":1}}],["源语句的嵌入向量或者前一个编码器的输出",{"2":{"523":1}}],["源语句的掩码对应的变量叫做src",{"2":{"382":1}}],["源语言和目标语言的句子往往并没有相同的长度",{"2":{"883":1}}],["源语言文本嵌入层",{"2":{"454":1}}],["源语言input",{"2":{"450":1}}],["源语言的word",{"2":{"450":1}}],["源语言词表中单词数目",{"2":{"448":1}}],["源语言词典",{"2":{"423":1}}],["源语言序列",{"2":{"427":2}}],["源语言分词器",{"2":{"423":1}}],["源语言句子相关的成员变量只有src一个",{"2":{"381":1}}],["源语言句子的掩码",{"2":{"380":1}}],["源语言句子列表",{"2":{"66":1,"380":2}}],["源自序列中item的表示",{"2":{"209":1}}],["源隐状态之中哪几个token比较相关",{"2":{"200":1}}],["源隐状态",{"2":{"200":1}}],["源隐状态会作为参数memory传给解码器",{"2":{"200":1}}],["源隐状态torch",{"2":{"200":1}}],["源序列的掩码",{"2":{"532":1,"533":1}}],["源序列的每个元素会把自己融合其它单词提供的信息之后得到的真实数据放到一个向量中",{"2":{"265":1}}],["源序列的每个元素都会把自己的特征总结到一个向量key之中",{"2":{"265":1}}],["源序列的每个单词的实际值",{"2":{"265":1}}],["源序列掩码",{"2":{"450":3}}],["源序列会输入给编码器",{"2":{"391":1}}],["源序列",{"2":{"391":1,"450":2}}],["源序列中的每个单词都可能会对输出造成影响",{"2":{"277":1}}],["源序列每个元素转化为",{"2":{"265":1}}],["源序列所有元素的value构成了值矩阵v",{"2":{"265":1}}],["源序列之中",{"2":{"200":1}}],["源序列首先通过嵌入和位置编码层",{"2":{"161":1}}],["奇异值微调",{"0":{"224":1},"2":{"157":1}}],["案例演示",{"2":{"1594":1}}],["案例",{"0":{"208":1,"1294":1},"1":{"209":1,"210":1,"211":1,"212":1,"213":1,"214":1,"215":1,"216":1,"217":1,"218":1,"219":1,"220":1,"221":1,"222":1,"223":1,"224":1,"225":1,"226":1,"227":1,"228":1,"229":1,"230":1,"231":1,"232":1,"1295":1,"1296":1,"1297":1},"2":{"157":1,"1466":1,"1566":1}}],["必先利其器",{"0":{"1605":1}}],["必须以",{"2":{"1917":1}}],["必须以某种形式的记忆保存所有单词",{"2":{"488":1}}],["必须与函数定义时的名称完全一致",{"2":{"1729":1}}],["必须实现",{"2":{"1693":1}}],["必须成对使用",{"2":{"1668":2}}],["必须使用",{"2":{"1647":1}}],["必须确保数组的大小足够容纳字符串的所有字符以及结尾的空字符",{"2":{"1624":1}}],["必须手动添加",{"2":{"1624":1}}],["必须在某个地方捕获并处理抛出的异常",{"2":{"1761":1}}],["必须在类定义外部进行初始化",{"2":{"1639":1}}],["必须在声明时初始化",{"2":{"1612":1}}],["必须在所有mpi函数之前调用",{"2":{"1575":1}}],["必须初始化",{"2":{"1612":1,"1615":1}}],["必须写上",{"2":{"1488":1}}],["必须一致",{"2":{"1484":1}}],["必须先调用父类的",{"2":{"1206":1}}],["必须优化才能公平比较不同目标超参数值的参数",{"2":{"1143":1}}],["必须是",{"2":{"1087":1}}],["必须被淘汰",{"2":{"986":1}}],["必须考虑任务的具体要求",{"2":{"692":1}}],["必须导入或者自己实现",{"2":{"668":1}}],["必须往后看到",{"2":{"277":1}}],["必须要存在的原因",{"2":{"117":1}}],["必要性",{"0":{"180":1},"2":{"157":1}}],["潘梓正",{"2":{"156":1}}],["去了不同的两家店",{"2":{"2051":1}}],["去噪",{"2":{"1469":1}}],["去噪器通过迭代去噪潜高斯隐变量来预测下一个句子嵌入",{"2":{"635":1}}],["去除vj=",{"2":{"762":1}}],["去除自注意力变换中的校正系数",{"2":{"761":1}}],["去除了减掉均值的部分",{"2":{"346":1}}],["去除了计算过程中的平移",{"2":{"346":1,"812":1}}],["去预测",{"2":{"714":1}}],["去",{"2":{"714":2}}],["去设计能够高效完成计算的提示",{"2":{"504":1}}],["去embedding矩阵中查找第token",{"2":{"458":1}}],["去重作用",{"2":{"1083":1}}],["去重",{"2":{"364":1}}],["去归一化",{"2":{"337":1}}],["去掉了章节文件夹",{"2":{"2049":1}}],["去掉了就开始摆烂",{"2":{"156":1}}],["去掉的对应的文件夹",{"2":{"1404":1}}],["去掉的这些概率均分给其它人",{"2":{"399":1}}],["去掉一个dropout影响不大",{"2":{"396":1}}],["去掉tgt的第一个词",{"2":{"380":1}}],["去掉tgt的最后一个单词",{"2":{"380":1}}],["去掉softmax的attention的复杂度可以降到最理想的线性级别o",{"2":{"180":1}}],["去表达这个事实",{"2":{"135":1}}],["揭秘transformer",{"2":{"292":1}}],["揭秘wise如何带来终生学习新突破",{"2":{"156":1}}],["揭示了transformer中注意力机制的基本动力学特性",{"2":{"507":1}}],["揭示了一个属性提取的内部机制",{"2":{"122":1}}],["揭示",{"2":{"484":1}}],["揭示llm的认知机制",{"2":{"156":1}}],["芝士ai吃鱼",{"2":{"156":1}}],["顿数ai",{"2":{"156":2}}],["袁焱",{"2":{"156":1}}],["样例",{"0":{"690":1}}],["样本量",{"2":{"1165":1}}],["样本之间仍然具有可比较性",{"2":{"314":1,"322":1}}],["样本有多少个句子",{"2":{"198":1}}],["样本的",{"2":{"88":1}}],["样条函数在低维函数上非常准确",{"2":{"155":1}}],["向史班长学习为人",{"2":{"2056":1}}],["向三多学习精神",{"2":{"2056":1}}],["向前移动",{"2":{"1633":1}}],["向后移动",{"2":{"1633":1}}],["向左",{"2":{"1557":1}}],["向下滚动半屏",{"2":{"1547":1}}],["向下一个位置的标签",{"2":{"1324":1}}],["向上滚动半屏",{"2":{"1547":1}}],["向上取整",{"2":{"1087":1}}],["向",{"2":{"1324":1,"1797":1}}],["向module",{"2":{"1214":1}}],["向解码器提供目标序列实际上是给了模型一个正确指导",{"2":{"406":1}}],["向多个层添加记忆可能有助于模型以更通用的方式使用其记忆",{"2":{"154":1}}],["向量和矩阵运算",{"2":{"2009":1}}],["向量和位置信息的纠缠",{"2":{"756":1}}],["向量相关性2",{"2":{"921":1}}],["向量相关性1",{"2":{"921":1}}],["向量现在不仅代表随机分配",{"2":{"709":1}}],["向量开始稳定下来",{"2":{"709":1}}],["向量通常被设置为正交的",{"2":{"709":1}}],["向量维度越来越大",{"2":{"708":1}}],["向量转换为高维向量了",{"2":{"700":1}}],["向量转换回词表对应的10000",{"2":{"473":1}}],["向量空间具有标准的数学运算",{"2":{"689":1}}],["向量空间是向量的集合",{"2":{"689":1}}],["向量可能满足表达人类概念所需的所有特性",{"2":{"689":1}}],["向量来表示",{"2":{"687":1}}],["向量来相应地调制权重",{"2":{"225":1}}],["向量就是把概念映射到某个空间",{"2":{"691":1}}],["向量就是是大模型的底层数据结构",{"2":{"680":1}}],["向量就有多少维",{"2":{"681":1}}],["向量是标量值",{"2":{"680":1}}],["向量分为两组",{"2":{"502":1}}],["向量分解为它们的正部和负部",{"2":{"213":1}}],["向量时",{"2":{"475":1}}],["向量化的结果不一定是稠密向量",{"2":{"680":1}}],["向量化的数据库",{"2":{"164":1}}],["向量化可以是简单的规则转换",{"2":{"680":1}}],["向量化主要是将原始数据表示为可以直接输入模型的数值向量",{"2":{"680":1}}],["向量化主要存在以下两种情况",{"2":{"676":1}}],["向量化是一种将数据转换为向量形式的过程",{"2":{"680":1}}],["向量化就是把其它格式的数据转换为向量形式",{"2":{"676":1}}],["向量化",{"2":{"431":1,"676":1}}],["向量乘法就可以看作是表示在",{"2":{"352":1}}],["向量集",{"2":{"225":1}}],["向量z",{"2":{"224":1}}],["向量方向和尺度",{"2":{"222":1}}],["向量x分别乘以三个不同的权重矩阵wt",{"2":{"172":1}}],["向量雅可比乘积",{"2":{"148":1,"485":1}}],["向量的元素按照两两一组应用旋转变换",{"2":{"1344":1}}],["向量的每个维度可以代表相应对象的某个特征",{"2":{"688":1}}],["向量的每个维度对应语义的某个方面",{"2":{"458":1}}],["向量的维度就等于词表的大小",{"2":{"681":1}}],["向量的维度取决于模型",{"2":{"458":1}}],["向量的加权平均而已",{"2":{"117":1}}],["向量的角度看是从",{"2":{"28":1}}],["向量",{"0":{"680":1},"1":{"681":1,"682":1,"683":1,"684":1,"685":1,"686":1,"687":1},"2":{"12":1,"83":1,"213":1,"225":3,"473":1,"679":1,"680":2,"682":1,"691":1,"943":1,"961":1,"1342":2,"1344":1,"1398":1}}],["收单词转移分数",{"2":{"1328":1}}],["收到这两个函数的影响",{"2":{"844":1}}],["收敛趋势对比图",{"0":{"1066":1}}],["收敛过程对比",{"0":{"1057":1}}],["收敛行为可能导致模型崩溃",{"2":{"507":1}}],["收敛速度也相当快",{"2":{"1057":1}}],["收敛速度也得以大幅提升",{"2":{"411":1}}],["收敛速度快",{"2":{"104":1}}],["收敛起来会比较难",{"2":{"335":1}}],["收敛更快",{"2":{"329":1}}],["收集",{"2":{"1573":1}}],["收集指标",{"2":{"1163":1}}],["收集相关信息以理解解决当前问题所需的技能",{"2":{"225":1}}],["收集与其自身索引部分相对应的部分嵌入",{"2":{"154":1}}],["查阅资料",{"2":{"1649":1}}],["查阅运算符优先级表",{"2":{"1630":1}}],["查阅",{"2":{"1596":1}}],["查表得到一个向量",{"2":{"694":1}}],["查看输入流中的下一个字符",{"2":{"1813":1,"1831":1}}],["查看",{"2":{"1607":1}}],["查看日志",{"0":{"1539":1}}],["查看监听的端口和服务",{"2":{"1527":1}}],["查看监听端口",{"2":{"1527":1}}],["查看网络端口",{"0":{"1527":1}}],["查看网络接口信息",{"2":{"1526":1}}],["查看网络配置",{"0":{"1526":1}}],["查看所有进程",{"2":{"1524":1}}],["查看系统所有进程",{"2":{"1522":1}}],["查看系统配置文件目录",{"2":{"1507":1}}],["查看当前终端的进程",{"2":{"1522":1}}],["查看当前用户进程",{"2":{"1522":1}}],["查看当前子词是否是w的子字符串",{"2":{"587":1}}],["查看进程",{"0":{"1522":1}}],["查看后",{"2":{"1515":1}}],["查看前",{"2":{"1515":1}}],["查看文本文件",{"0":{"1515":1}}],["查看文件内容",{"2":{"1510":1}}],["查看权限",{"2":{"1513":1}}],["查看和修改权限",{"0":{"1513":1}}],["查看整个文件内容",{"2":{"1510":1}}],["查看基本命令目录",{"2":{"1507":1}}],["查看根目录内容",{"2":{"1507":1}}],["查看帮助文档",{"2":{"1481":2}}],["查看特征图",{"0":{"1283":1}}],["查看graph",{"0":{"1282":1}}],["查看您当前存档的协议或签署新协议",{"2":{"1197":1}}],["查看自己电脑的driver",{"2":{"792":1}}],["查看最终预测的变化情况",{"2":{"475":1}}],["查询属性",{"2":{"1930":1}}],["查询电话号码",{"2":{"1808":1}}],["查询元素",{"2":{"1719":1,"1720":1,"1721":1,"1722":1,"1724":1,"1725":1}}],["查询用户",{"2":{"1485":1}}],["查询全部用户",{"2":{"1485":1}}],["查询矩阵",{"2":{"265":1}}],["查询模型",{"2":{"263":1}}],["查询+聚合",{"2":{"166":1}}],["查询的相关性",{"2":{"164":1,"173":1}}],["查询",{"2":{"154":1,"162":1,"535":1}}],["查找唯一的数字",{"2":{"2061":1}}],["查找其他库或包",{"2":{"1977":1}}],["查找元素",{"2":{"1806":1,"1807":1}}],["查找迅速",{"2":{"1806":1}}],["查找算法",{"0":{"1753":1},"1":{"1754":1,"1755":1,"1756":1},"2":{"1732":1}}],["查找键为",{"2":{"1725":1}}],["查找值为",{"2":{"1724":1}}],["查找字符串中第一个与指定字符集合中任何字符匹配的字符的位置",{"2":{"1713":1}}],["查找子串",{"2":{"1713":1}}],["查找子串最后一次出现的位置",{"2":{"1713":1}}],["查找子串第一次出现的位置",{"2":{"1713":1}}],["查找资料",{"2":{"1646":1,"1647":1}}],["查找上一个匹配",{"2":{"1553":1}}],["查找下一个匹配",{"2":{"1553":1}}],["查找可执行文件",{"0":{"1533":1}}],["查找大于",{"2":{"1532":1}}],["查找文件",{"0":{"1532":1},"2":{"1532":1}}],["查找与替换",{"0":{"1552":1},"1":{"1553":1,"1554":1},"2":{"1520":1}}],["查找替换等操作",{"2":{"1519":1}}],["查找指定字符串",{"2":{"1516":1}}],["查找内容",{"0":{"1516":1}}],["查找机制",{"2":{"164":1}}],["查找表可以在训练过程中不断优化",{"2":{"153":1}}],["查找",{"0":{"1553":1},"2":{"45":1,"1520":1,"1803":1}}],["支持latex渲染",{"2":{"2045":1}}],["支持https协议",{"2":{"2042":1}}],["支持缓冲",{"2":{"1810":1,"1828":1}}],["支持截取",{"2":{"1803":1}}],["支持随机访问",{"2":{"1800":1}}],["支持任何类型的元素",{"2":{"1797":1}}],["支持编译分离",{"2":{"1628":1}}],["支持编写动态sql",{"2":{"1479":1}}],["支持模块化编程",{"2":{"1628":1}}],["支持分布式数据处理和mapreduce并行计算模式",{"2":{"1569":1}}],["支持高性能计算",{"2":{"1569":1}}],["支持多种网络协议和多种平台",{"2":{"1569":1}}],["支持同时编辑多个文件和窗口",{"2":{"1555":1}}],["支持原地操作",{"2":{"1123":1}}],["支持将",{"2":{"1083":1}}],["支持",{"2":{"1083":1,"1605":1}}],["支持各种高性能模型",{"2":{"983":1}}],["支持超过200种语言和语音输入",{"2":{"629":1}}],["支持动态参数扩展",{"2":{"621":1}}],["支持逐步扩展模型参数量等",{"2":{"618":1}}],["支持端到端训练",{"2":{"153":1}}],["支持变长序列",{"2":{"90":1}}],["技术心得",{"2":{"2109":1}}],["技术趋势分析",{"0":{"1960":1}}],["技术没有高低之分",{"2":{"1479":1}}],["技术寻找更好的激活函数",{"2":{"845":1}}],["技术",{"2":{"298":1,"739":1,"1315":1,"1957":1}}],["技术挑战在于重新设计flashattention",{"2":{"973":1}}],["技术挑战",{"0":{"244":1},"1":{"245":1,"246":1}}],["技术微佬",{"2":{"233":1}}],["技术集成到llm中",{"2":{"150":1}}],["技术解决这个问题",{"2":{"88":1}}],["许多子类继承自一个父类",{"2":{"1849":1}}],["许多流行的深度学习框架",{"2":{"1602":1}}],["许多复杂的",{"2":{"1173":1}}],["许多正则化方法通过对目标函数j",{"2":{"1014":1}}],["许多策略显式地被设计来减少测试误差",{"2":{"1011":1}}],["许多研究提出了序列并行性",{"2":{"977":1}}],["许多研究都对仅解码器架构和编码器",{"2":{"542":1}}],["许多研究都集中在将混合专家",{"2":{"150":1}}],["许多机器学习的库实现的是互相关函数但是称之为卷积",{"2":{"770":1}}],["许多",{"2":{"696":1}}],["许多特征保持不变或随机出现",{"2":{"477":1}}],["许多早期层神经元处于叠加状态",{"2":{"477":1}}],["许多应用也需要在训练后不断调整模型",{"2":{"138":1}}],["偏差",{"2":{"762":1}}],["偏离全局的running",{"2":{"316":1}}],["偏移通过输入特征卷积得到",{"2":{"781":1}}],["偏移阶段",{"2":{"485":1}}],["偏移参数",{"2":{"343":1}}],["偏移",{"2":{"148":1,"1086":1,"1633":1}}],["偏置初始化为",{"2":{"1003":1}}],["偏置项趋近于负无穷",{"2":{"765":1}}],["偏置编码",{"0":{"766":1},"2":{"741":1}}],["偏置",{"0":{"8":1},"2":{"0":1}}],["印记阶段",{"2":{"485":1}}],["印记",{"2":{"148":2,"485":1}}],["印记与偏移",{"2":{"148":2,"485":1}}],["δθ",{"2":{"1052":2}}],["δθt",{"2":{"1045":1}}],["δy=y1−y0",{"2":{"2018":1}}],["δy​l​​=f​",{"2":{"1004":1}}],["δy​l​​",{"2":{"1004":3}}],["δyl=f",{"2":{"1004":1}}],["δyl",{"2":{"1004":3}}],["δx=x1−x0",{"2":{"2018":1}}],["δx​l+1​​",{"2":{"1004":1}}],["δx​l​​",{"2":{"1004":4}}],["δx​l​​=​w​l​​​^​​δy​l​​",{"2":{"1004":1}}],["δxl+1",{"2":{"1004":1}}],["δxl",{"2":{"1004":4}}],["δxl=wl^δyl",{"2":{"1004":1}}],["δx^⊤",{"2":{"485":1}}],["δ",{"2":{"763":2,"1004":1,"1443":1}}],["δ的外积来计算梯度矩阵的过程",{"2":{"485":1}}],["δiδi",{"2":{"148":2,"485":3}}],["δ𝑊",{"2":{"142":1}}],["梯度求解",{"0":{"1449":1}}],["梯度有一个非常重要的性质",{"2":{"1440":1}}],["梯度计算为例",{"2":{"1391":1}}],["梯度计算的结果更加平滑",{"2":{"1027":1}}],["梯度清0",{"2":{"1202":1,"1215":1}}],["梯度范数与步骤数量的关系图",{"2":{"1184":1}}],["梯度范数",{"2":{"1184":1}}],["梯度截断可以修复早期训练中出现的不稳定性",{"2":{"1184":1}}],["梯度截断会变得非常有用",{"2":{"1184":1}}],["梯度截断纠正早期训练不稳定性的图示",{"2":{"1184":1}}],["梯度截断",{"0":{"1184":1}}],["梯度模式",{"0":{"1118":1},"1":{"1119":1,"1120":1,"1121":1,"1122":1}}],["梯度会计算",{"2":{"1106":1}}],["梯度累加和清0",{"0":{"1096":1}}],["梯度累加也有可能",{"2":{"659":1}}],["梯度估计的抖动减少",{"2":{"1027":1}}],["梯度下降算法简述",{"0":{"1440":1}}],["梯度下降法代码展示",{"2":{"1440":1}}],["梯度下降法效果展示",{"2":{"1440":1}}],["梯度下降法三个变种",{"0":{"1024":1},"1":{"1025":1,"1026":1,"1027":1}}],["梯度下降法被广泛应用于训练模型",{"2":{"1023":1}}],["梯度下降法",{"2":{"1023":1,"1440":1}}],["梯度下降法概念",{"0":{"1023":1}}],["梯度下降等优化算法在大型模型上可能会遇到局部最优",{"2":{"512":1}}],["梯度",{"2":{"1004":1}}],["梯度缺变成了0",{"2":{"996":1}}],["梯度为",{"2":{"840":1}}],["梯度为0",{"2":{"104":1}}],["梯度传播等",{"2":{"1214":1}}],["梯度传播",{"2":{"838":1}}],["梯度将会被频率",{"2":{"702":1}}],["梯度流",{"2":{"499":1}}],["梯度的计算会受到很大的影响",{"2":{"403":1}}],["梯度误差以及权重的更新也可以中心化",{"2":{"298":1}}],["梯度值可能会变得非常大",{"2":{"403":1}}],["梯度值就会变得非常大",{"2":{"296":1}}],["梯度值呈指数级增长",{"2":{"296":1}}],["梯度需要通过多层进行反向传播",{"2":{"296":1}}],["梯度爆炸指的是在反向传播过程中",{"2":{"296":1}}],["梯度爆炸",{"2":{"296":1}}],["梯度越大",{"2":{"230":1}}],["梯度消失或梯度爆炸等问题",{"2":{"512":1}}],["梯度消失也不全是",{"2":{"333":1}}],["梯度消失了",{"2":{"304":1}}],["梯度消失意味着我们没有很好的信号去调整优化前面的层",{"2":{"296":1}}],["梯度消失是在反向传播过程中使用的链式法则引起的",{"2":{"296":1}}],["梯度消失是指在训练深度神经网络时",{"2":{"296":1}}],["梯度消失",{"0":{"192":1,"304":1},"2":{"157":1,"293":1,"296":1,"333":1}}],["梯度在更新过程中的表现可以表示为",{"2":{"148":1,"485":1}}],["近亲",{"2":{"1611":1}}],["近义词等情况时更具优势",{"2":{"696":1}}],["近来",{"2":{"561":1}}],["近似随机搜索",{"2":{"1150":1}}],["近似等效于一个更宽的",{"2":{"334":1}}],["近似过程中的信息丢失",{"2":{"212":1}}],["近8年后",{"2":{"233":1}}],["近期偏见",{"2":{"204":1}}],["近期的可解释性研究已提出了多种方法",{"2":{"148":1}}],["近年来最火的生成模型莫过于gan和vae",{"2":{"1371":1}}],["近年来",{"2":{"138":1,"227":1,"475":1,"747":1,"756":1}}],["近年来有一些研究工作试图引入某种形式的注意力头间的交互",{"2":{"44":1}}],["排队和优先处理",{"0":{"2099":1}}],["排水量",{"2":{"1664":1}}],["排序的时间复杂度是",{"2":{"2155":1}}],["排序与查找算法",{"2":{"1758":1}}],["排序",{"2":{"1734":1}}],["排序算法",{"0":{"1748":1},"1":{"1749":1,"1750":1,"1751":1,"1752":1},"2":{"1732":1,"2099":1}}],["排序算法定制",{"2":{"1645":1}}],["排序数组",{"2":{"1729":1}}],["排序前",{"2":{"1645":1}}],["排序后的",{"2":{"1087":1}}],["排除被mask",{"2":{"1330":1}}],["排名和分类",{"2":{"696":1}}],["排名最短的pair都不在merge之中",{"2":{"592":1}}],["排名越高",{"2":{"130":2}}],["排在前面的tokens说明这个neuron",{"2":{"147":1}}],["处的变分或函数的变分被定义为它们之差",{"2":{"1377":1}}],["处都会填入相同的文本",{"2":{"731":1}}],["处处都有较大的梯度指向各个类中心",{"2":{"399":1}}],["处",{"2":{"145":1,"995":1,"1115":1}}],["处理除零错误",{"2":{"1999":1}}],["处理文件内容",{"2":{"1761":1}}],["处理文本数据",{"0":{"1624":1}}],["处理数值范围的算法",{"2":{"1758":1}}],["处理两个",{"2":{"1707":1}}],["处理两个整数",{"2":{"1707":1}}],["处理三个整数",{"2":{"1707":1}}],["处理指针数组或指针的指针",{"2":{"1650":1}}],["处理错误情况",{"2":{"1611":1}}],["处理以",{"2":{"1604":1}}],["处理以产生神经元的输出",{"2":{"1459":1}}],["处理请求异常",{"2":{"1566":1}}],["处理请求与响应",{"2":{"1493":1}}],["处理消息",{"2":{"1563":2}}],["处理同一任务的不同部分",{"2":{"1563":1}}],["处理商业问题的机器学习工程师很少有时间回顾并概括他们的调参过程",{"2":{"1127":1}}],["处理它",{"2":{"986":1}}],["处理人机对话",{"2":{"906":1}}],["处理流程非常简单",{"2":{"628":1}}],["处理",{"2":{"614":1,"1340":1,"1667":1,"1764":1}}],["处理多语言或噪声数据的效率低下",{"2":{"610":1}}],["处理未匹配的子字符串",{"2":{"587":1}}],["处理或转换特殊字符和符号",{"2":{"552":1}}],["处理输入",{"2":{"445":1}}],["处理长序列时",{"2":{"255":1}}],["处理每个词之后会产生一个隐状态",{"2":{"241":1}}],["处理序列转换的神经网络模型大多是编码器",{"2":{"241":1}}],["处理在内积计算过程中维度的同号和异号交互作用",{"2":{"213":1}}],["处理和运用已学习的信息",{"2":{"121":1}}],["处理之后",{"2":{"66":1}}],["又或者说把结束最早的最先安排",{"2":{"2118":1}}],["又或者输入很多的手写数字",{"2":{"1371":1}}],["又无法让我真正感受到",{"2":{"2097":1}}],["又保证了原始数据的安全",{"2":{"1729":1}}],["又保证了所有输出值之和为1",{"2":{"180":1}}],["又有函数的类型安全",{"2":{"1632":1}}],["又加入了许多现代编程的特性",{"2":{"1603":1}}],["又加上编码器和解码器两个模块的排列组合",{"2":{"77":1}}],["又会给我们带来什么新的体验和挑战呢",{"2":{"1601":1}}],["又称",{"2":{"1312":1,"1317":1}}],["又称自回归",{"2":{"1312":1}}],["又称自编码",{"2":{"1312":1}}],["又称权重初始化",{"2":{"988":1}}],["又称修正线性单元",{"2":{"840":1}}],["又不是不能跑",{"2":{"895":1}}],["又并非十分妥当",{"2":{"765":1}}],["又重新变回了原来那几个词",{"2":{"582":1}}],["又符合语言模型的内在规律",{"2":{"412":1}}],["又要不偏不倚",{"2":{"369":1}}],["又要对聚合的结果负责",{"2":{"288":1}}],["又可以有效地解决对齐问题",{"2":{"275":1}}],["又能够用来构造其它词",{"2":{"567":1}}],["又能够有效防止值过大导致的梯度问题",{"2":{"323":1}}],["又能生成输出序列",{"2":{"540":1}}],["又能大大增强表达能力",{"2":{"273":1}}],["又能很好地逼近单变量函数",{"2":{"155":1}}],["又能避免灾难性遗忘等知识变价的副作用",{"2":{"143":1}}],["既可以读取也可以写入数据",{"2":{"1819":1,"1823":1,"1837":1,"1841":1}}],["既可以实现模型高效的更新",{"2":{"143":1}}],["既避免了拷贝开销",{"2":{"1729":1}}],["既有宏的效率",{"2":{"1632":1}}],["既能够编码输入序列",{"2":{"540":1}}],["既包括编码器",{"2":{"519":1}}],["既要丰富详实",{"2":{"369":1}}],["既要对查询的结果负责",{"2":{"288":1}}],["既不增加额外的参数也不增加计算复杂度",{"2":{"301":1}}],["既然纯构建的方式不可行",{"2":{"712":1}}],["既然解码新token的过程是把最终的hidden",{"2":{"482":1}}],["既然自回归模式的全靠自身预测结果和teacher",{"2":{"411":1}}],["既然是融合全局信息",{"2":{"323":1}}],["既然明确了问题",{"2":{"316":1}}],["既然隐向量长度固定",{"2":{"252":1}}],["既然k和q都是一样维度",{"2":{"172":1}}],["既然可以定位到对某些事实或者知识影响较大的神经元",{"2":{"143":1}}],["既然提到了ffn是用来存储知识的",{"2":{"121":1}}],["既然ffn是存储知识的模块",{"2":{"118":1}}],["元硬币",{"2":{"2139":1}}],["元的硬币",{"2":{"2137":1,"2139":2}}],["元预算",{"2":{"2131":1}}],["元参数",{"2":{"1185":2}}],["元",{"2":{"508":1,"2131":9,"2137":3,"2139":5}}],["元素只能通过向前遍历访问",{"2":{"1801":1}}],["元素类型",{"2":{"1705":1}}],["元素是否含有true",{"2":{"1086":1}}],["元素是否全是true",{"2":{"1086":1}}],["元素",{"2":{"642":1}}],["元素的差值变大会导致",{"2":{"188":1}}],["元素间差值变大",{"0":{"190":1},"2":{"157":1}}],["元素间的互动完全靠自注意力",{"2":{"101":1,"466":1}}],["元学习并非直接更新模型权重",{"2":{"142":1}}],["元学习",{"2":{"142":1}}],["属于类而不是类的某个特定对象",{"2":{"1649":1}}],["属于掌握阶段的研究大多在使用专门针对特定知识的方法来对模型参数进行更新",{"2":{"142":1}}],["属性在",{"2":{"1874":1}}],["属性",{"0":{"1226":1,"1909":1},"2":{"1214":1,"1629":1,"1904":1,"1909":1,"1913":1}}],["属性设置为",{"2":{"1211":1}}],["属性详解",{"0":{"1208":1}}],["属性是进入这个图的入口",{"2":{"1113":1}}],["属性还包含",{"2":{"1111":1}}],["属性提取",{"2":{"122":1}}],["微分几何",{"2":{"2009":1}}],["微秒级延迟优化技术",{"2":{"1947":1}}],["微服务架构",{"2":{"1500":1}}],["微小的不相关token的注意力之和可能超过对少数相关token的注意力",{"2":{"500":1}}],["微软开发的集成开发环境",{"2":{"1605":1}}],["微软提出直接用上一层的注意力权重生成当前层的注意力权重",{"2":{"429":1}}],["微软",{"2":{"156":1}}],["微调训练的约束更少",{"2":{"1313":1}}],["微调模型所需的时间",{"2":{"1313":1}}],["微调模型是编辑内在知识最直接的方式",{"2":{"142":1}}],["微调时只需要很少的数据量就可以达到不错的性能",{"2":{"1313":1}}],["微调的方式可以在任务特定的数据上进行更好的参数优化",{"2":{"898":1}}],["微调的数据集就是专业领域的数据集",{"2":{"670":1}}],["微调则类似借助新语料和手段对模型内部的相关参数进行微扰",{"2":{"363":1}}],["微调",{"2":{"222":1,"711":1,"733":1}}],["微调框架从简单性的角度来看是理想的",{"2":{"222":1}}],["微调技术通常容易出现灾难性遗忘和过拟合现象",{"2":{"142":1}}],["微妙的知识存储和处理能力",{"2":{"139":1}}],["意不尽悟",{"2":{"2052":1,"2054":1}}],["意为误差反向传播",{"2":{"1439":1}}],["意为输出",{"2":{"10":1}}],["意指是一个电脑系统中",{"2":{"1411":1}}],["意在降低层参数的训练敏感性",{"2":{"1180":1}}],["意图识别",{"0":{"860":1}}],["意义为是否需要保持图",{"2":{"1110":1}}],["意义",{"0":{"753":1,"1880":1,"1884":1,"1888":1,"1892":1,"1896":1,"1899":1},"1":{"754":1,"755":1,"756":1},"2":{"741":1,"1853":1,"1857":1,"1861":1}}],["意义上已经不再是",{"2":{"334":1}}],["意味着在相等元素间保持相对顺序",{"2":{"1750":1}}],["意味着尽可能避免花里胡哨的东西",{"2":{"1137":1}}],["意味着我们可以将llm视为一个通用计算器",{"2":{"504":1}}],["意味着这些方法是否能在支持一次同时编辑多个案例",{"2":{"141":1}}],["意外指标可以导致在重大意外时刻之后出现重要信息缺失",{"2":{"230":1}}],["意思是指的",{"2":{"175":1}}],["识别不稳定的训练任务",{"0":{"1179":1}}],["识别错误的搜索空间边界",{"0":{"1147":1}}],["识别目标超参数",{"0":{"1143":1}}],["识别",{"2":{"141":1,"1455":1}}],["识别其知识神经元的完整过程描述如下",{"2":{"135":1}}],["非知音",{"2":{"2054":1}}],["非知己",{"2":{"2054":1}}],["非成员友元函数既不是类的成员",{"2":{"1772":1}}],["非传递性",{"2":{"1768":1,"1781":1}}],["非修改算法",{"2":{"1758":1}}],["非虚函数",{"2":{"1688":1}}],["非左值引用",{"2":{"1629":1}}],["非安全",{"2":{"1629":1}}],["非持久性缓冲区不会被保存为模型的状态字典",{"2":{"1208":1}}],["非自适应地采样搜索空间可以在不重新运行实验的情况下更改性能指标",{"2":{"1175":1}}],["非恒定的",{"2":{"1171":1}}],["非正式地",{"2":{"1152":1}}],["非叶张量",{"2":{"1117":1}}],["非叶子节点才有",{"2":{"1110":1}}],["非叶子节点对应函数的inputs",{"2":{"661":1,"1104":1}}],["非叶子",{"2":{"661":1,"1104":1}}],["非叶子结点中途可能会计算grad",{"2":{"659":1}}],["非中心的",{"2":{"1059":2}}],["非零中心激活函数的不足也可以在一定程度上被缓解",{"2":{"838":1}}],["非零中心的激活函数",{"2":{"838":1}}],["非饱和性",{"2":{"838":1}}],["非常适用于需要位运算的场景",{"2":{"2062":1}}],["非常适合快速原型开发和脚本编写",{"2":{"1602":1}}],["非常适合用于序列化输入",{"2":{"808":1}}],["非常重要",{"2":{"1764":1}}],["非常难以理解和编写",{"2":{"1603":1}}],["非常有用",{"2":{"1214":1,"1620":1}}],["非常大的风险",{"2":{"661":1,"1104":1}}],["非结构化数据变得越来越多",{"2":{"682":1}}],["非打印字符等",{"2":{"552":1}}],["非transformer架构的进展与猜想",{"2":{"292":1}}],["非负性",{"2":{"178":1}}],["非线性变换",{"2":{"1388":1}}],["非线性以及可微性",{"2":{"848":1}}],["非线性",{"2":{"838":1}}],["非线性之前单独处理各个矩阵",{"2":{"334":1}}],["非线性的输入相关门控来提高记忆层的训练性能",{"2":{"154":1}}],["非线性激活函数的引入打破了线性模型的限制",{"2":{"117":1}}],["非故意造成的谬误",{"2":{"140":1}}],["确认",{"2":{"1164":1}}],["确实能带来一定的远程衰减性",{"2":{"1344":1}}],["确实",{"2":{"1185":1}}],["确实可以",{"2":{"904":1}}],["确实没有确凿证据证明一种架构在最终性能上优于另一种架构",{"2":{"542":1}}],["确实将一些",{"2":{"126":1}}],["确切的说",{"2":{"436":1}}],["确定最大位移方向",{"2":{"2023":1}}],["确定了目标和冗余超参数之后",{"2":{"1144":1}}],["确定了资源分配的原则之后就可以进行信息交换",{"2":{"261":1}}],["确定哪个优化器在给定的步数中产生最低的验证错误",{"2":{"1143":1}}],["确定更深的模型是否会减少验证集错误",{"2":{"1143":1}}],["确定无效的方向并将其删除",{"2":{"1140":1}}],["确定验证集效果对哪些超参数最敏感",{"2":{"1140":1}}],["确定此初始配置将需要一些手动配置的训练运行和反复试验",{"2":{"1137":1}}],["确定可行的batch",{"0":{"1132":1}}],["确定每次训练运行的步数",{"0":{"1154":1},"1":{"1155":1,"1156":1,"1157":1,"1158":1,"1159":1},"2":{"1125":1}}],["确定是否采用此训练工作流更改或超参数配置",{"0":{"1152":1},"2":{"1125":1}}],["确定",{"2":{"1085":1}}],["确定期望的子词表大小",{"2":{"599":1,"602":1}}],["确定合适的词汇表大小是一个关键步骤",{"2":{"559":1}}],["确定损失大小",{"2":{"381":1}}],["确保清理后的单词非空",{"2":{"1933":1}}],["确保独占所有权",{"2":{"1911":1}}],["确保以空字符结尾",{"2":{"1821":1,"1839":1}}],["确保你清楚地知道当前存储在联合体中的数据类型",{"2":{"1728":1}}],["确保每个",{"2":{"1671":1,"1672":1}}],["确保访问在已分配的范围内",{"2":{"1670":1}}],["确保指向有效的内存地址",{"2":{"1633":1}}],["确保可以无密码登录",{"2":{"1594":1}}],["确保使用最佳实践",{"2":{"1180":1}}],["确保在运行评估或检查点之前",{"2":{"1169":1}}],["确保采样集与完整数据集之间没有偏差",{"2":{"1165":1}}],["确保定期作业使用的采样数据集的性能与整个离线评估集的性能相似",{"2":{"1165":1}}],["确保填充的数据被正确地加权",{"2":{"1164":1}}],["确保我们能从实验中获得足够多的经验",{"2":{"1145":1}}],["确保任何改进都有据可循",{"2":{"1139":1}}],["确保前缀正确插入树中",{"2":{"986":1}}],["确保对位置的预测",{"2":{"915":1}}],["确保语言建模只能引用前缀来预测下一个令牌",{"2":{"732":1}}],["确保生成过程的正确性",{"2":{"535":1}}],["确保解码器在每个步骤只关注已经生成的上下文",{"2":{"525":1}}],["确保解码时能够参考源句子的结构和语义",{"2":{"525":1}}],["确保它们满足特定的数学和逻辑规则",{"2":{"505":1}}],["确保模型的行为符合预期",{"2":{"505":1}}],["确保模型始终产生准确的",{"2":{"140":1}}],["确保激活的均值和方差一致",{"2":{"468":1}}],["确保generator的输出维度和词典大小一致",{"2":{"399":1}}],["确保所有的神经元都参与到计算中",{"2":{"392":1}}],["确保所有源元素贡献的特征总量保持一定",{"2":{"269":1}}],["确保了文本嵌入的通用性",{"2":{"725":1}}],["确保了解码器可以",{"2":{"536":1}}],["确保了对特征进行非对称的压缩和还原",{"2":{"515":1}}],["确保了合成数据的质量和相关性",{"2":{"369":1}}],["确保了输出值为正",{"2":{"178":1}}],["确保高质量内容的优先性",{"2":{"368":1}}],["确保神经元至少有时处于活动状态",{"2":{"305":1}}],["确保能够捕捉关键的全局上下文",{"2":{"217":1}}],["确保cache",{"2":{"201":2}}],["确保旋转矩阵和输入h位于同一设备",{"2":{"201":1}}],["∅",{"2":{"140":2}}],["仅主机模式",{"2":{"2090":1}}],["仅可以访问子类的受保护成员",{"2":{"1784":1}}],["仅在必要时使用异常",{"2":{"1764":1}}],["仅在必要时将请求分发到sp组",{"2":{"977":1}}],["仅返回值类型不同",{"2":{"1687":1}}],["仅使用于个人的专英课堂翻转",{"2":{"2072":1}}],["仅使用两个阶段可以获得更好的结果",{"2":{"1242":1}}],["仅使用约",{"2":{"1168":1}}],["仅针对目标超参数的某些值存在的超参数称为条件超参数",{"2":{"1143":1}}],["仅仅用一个整数表示学号",{"2":{"1728":1}}],["仅仅是类型声明的一部分",{"2":{"1611":1}}],["仅仅是训练统计数据的线性组合",{"2":{"1168":1}}],["仅仅是近似计算对应的平均值",{"2":{"1052":1}}],["仅仅使用shifted",{"2":{"409":1}}],["仅浪费不到4",{"2":{"982":1}}],["仅基于attention机制并完全避免循环",{"2":{"911":1}}],["仅是特征的机械化表示",{"2":{"676":1}}],["仅与source",{"2":{"649":1,"931":1}}],["仅编码器的模型逐渐开始逐渐消失",{"2":{"540":1,"541":1}}],["仅解码器模型的灵活性和多功能性似乎使谷歌对这一方向的坚持不那么有希望",{"2":{"540":1}}],["仅解码器模型经历了显著的繁荣",{"2":{"540":1}}],["仅解码器模型不如仅编码器和编码器",{"2":{"540":1}}],["仅解码器模型逐渐主导了llm的发展",{"2":{"540":1}}],["仅解码器",{"2":{"539":1}}],["仅依赖注意力权重会忽略它们所操作的向量的大小",{"2":{"478":1}}],["仅需少量计算步骤即可完成任务",{"2":{"294":1}}],["仅需要对推理代码进行修改",{"2":{"204":1}}],["仅",{"2":{"137":1}}],["仅逻辑上每个注意力头对应于",{"2":{"29":1}}],["层可以更好地建模邻近词语对的依赖关系",{"2":{"1315":1}}],["层宽度",{"2":{"1129":1}}],["层面上是经过多层的函数调用",{"2":{"1097":1}}],["层面归一化",{"2":{"337":1}}],["层次分解位置编码",{"2":{"768":1}}],["层引入了一个额外的维度",{"2":{"621":1}}],["层及",{"2":{"535":1}}],["层来生成概率",{"2":{"515":1}}],["层之前添加了绝对位置嵌入",{"2":{"1315":1}}],["层之后添加了一个交叉注意力层",{"2":{"614":1}}],["层之间的数据流和变换可以被视为态射",{"2":{"505":1}}],["层之和",{"2":{"334":1}}],["层进行处理",{"2":{"466":1}}],["层数更加",{"2":{"335":1}}],["层数越多",{"2":{"334":1}}],["层模型是否优于最好的",{"2":{"1143":1}}],["层模型",{"2":{"334":1,"1143":1}}],["层出来时候那样",{"2":{"334":1}}],["层放在多头自注意力层或者全连接层之前",{"2":{"329":1}}],["层是放在残差连接之后的",{"2":{"329":1}}],["层只针对最后一个维度",{"2":{"326":1}}],["层在训练和推理时的行为可能不一致",{"2":{"316":1}}],["层归一化的实例",{"2":{"532":1}}],["层归一化",{"2":{"294":1,"467":1,"517":1}}],["层归一化等设计",{"2":{"294":1}}],["层和交叉注意力层将这些表征转换为patch",{"2":{"614":1}}],["层和块",{"0":{"354":1},"2":{"293":1}}],["层和中上层注意力层密切相关",{"2":{"136":1}}],["层间修正",{"0":{"306":1},"2":{"293":1}}],["层层抽象",{"2":{"247":1}}],["层式记忆",{"2":{"229":1}}],["层保留传统的",{"2":{"217":1}}],["层使用局部块因果注意力掩码",{"2":{"614":1}}],["层使用训练过程中数据均值和方差得到的参数",{"2":{"316":1}}],["层使用",{"2":{"217":1}}],["层",{"2":{"214":1,"485":1,"535":1,"614":2,"620":1,"976":1,"1478":1}}],["层级越高",{"2":{"204":1}}],["层的路由权重连接起来",{"2":{"739":1}}],["层的自回归",{"2":{"614":1}}],["层的梯度可以表示为正向传播的输入向量和反向传播的",{"2":{"485":1}}],["层的梯度可以表示为正向传递的输入向量和反向传递的",{"2":{"148":1}}],["层的权重",{"2":{"485":1}}],["层的前向与反向传播过程",{"2":{"485":1}}],["层的前馈网络",{"2":{"154":1}}],["层的输出",{"2":{"394":1}}],["层的维度大小",{"2":{"344":1}}],["层的模型与",{"2":{"334":1}}],["层的",{"2":{"334":2,"361":1}}],["层中存储信息",{"2":{"485":1}}],["层中存储信息的复杂机制",{"2":{"485":1}}],["层中",{"2":{"217":1}}],["层中有效地存储和调整信息",{"2":{"148":1}}],["层中最后一个主语token索引",{"2":{"145":1}}],["层对于给定输入的",{"2":{"148":1}}],["层时",{"2":{"148":1,"302":1,"485":1}}],["层前向与反向过程中对模型更新的影响",{"2":{"148":1}}],["层非常小",{"2":{"137":1}}],["层很可能使用叠加来表示比它的神经元更多的特征",{"2":{"137":1}}],["层也类似",{"2":{"17":1}}],["缺乏类型安全检查",{"2":{"1632":1}}],["缺乏字法知识等",{"2":{"612":1}}],["缺少对于token间相对方向的刻画",{"2":{"761":1}}],["缺少语义信息",{"2":{"681":1}}],["缺少任何一个",{"2":{"136":1}}],["缺省设置为√nn",{"2":{"621":1}}],["缺点是扩展性不强",{"2":{"749":1}}],["缺点是",{"2":{"746":2}}],["缺点如下",{"2":{"608":1}}],["缺点",{"0":{"181":1,"251":1,"1045":1},"1":{"252":1,"253":1,"254":1,"255":1},"2":{"157":1,"692":2,"840":1,"1607":1,"1632":1,"1646":1,"1761":1,"1776":1}}],["缺一不可但却各司其职",{"2":{"510":1}}],["缺一不可",{"2":{"115":1}}],["包裹起来",{"2":{"1729":1}}],["包裹",{"2":{"1704":1,"1728":2}}],["包测试",{"2":{"1481":1}}],["包",{"2":{"1481":1}}],["包含内容",{"2":{"2035":1,"2039":1}}],["包含内容如下",{"2":{"1599":1}}],["包含代码结构",{"2":{"1999":1}}],["包含至少两个源文件和一个头文件",{"2":{"1918":1}}],["包含至少一个纯虚函数的类",{"2":{"1693":1}}],["包含至少一个纯虚函数的类称为抽象类",{"2":{"1693":1}}],["包含空终止符",{"2":{"1821":1,"1839":1}}],["包含在",{"2":{"1817":1,"1835":1}}],["包含头文件",{"2":{"1729":1,"1916":1}}],["包含函数要执行的代码块",{"2":{"1729":1}}],["包含函数要执行的代码",{"2":{"1729":1}}],["包含一个",{"2":{"1704":1}}],["包含一个虚函数",{"2":{"1690":2}}],["包含私有成员",{"2":{"1678":1}}],["包含默认构造函数和参数化构造函数",{"2":{"1675":1}}],["包含以下类",{"2":{"1664":1}}],["包含以下内容",{"2":{"1427":1}}],["包含员工的共同属性",{"2":{"1657":1}}],["包含自定义头文件",{"2":{"1628":1}}],["包含系统头文件",{"2":{"1628":1}}],["包含已经编译好的函数和代码",{"2":{"1604":1}}],["包含编译器",{"2":{"1435":1}}],["包含所有参数组的列表",{"2":{"1227":1}}],["包含了输入输出流的库",{"2":{"1729":1}}],["包含了消息传递接口的支持库",{"2":{"1589":1}}],["包含了整个输入序列的信息",{"2":{"888":1}}],["包含了丰富的词汇语义关系",{"2":{"709":1}}],["包含了词的语义信息",{"2":{"490":1}}],["包含31014个德语翻译的英语描述和155070个独立收集的德语描述",{"2":{"370":1}}],["包含有",{"2":{"151":1}}],["包含",{"2":{"135":1,"315":1,"1364":1,"1435":1,"1635":1,"1704":1,"1766":1}}],["包括我这里所写的这些",{"2":{"2052":1}}],["包括光线反射",{"2":{"2009":1}}],["包括phong模型",{"2":{"2009":1}}],["包括空格",{"2":{"1813":1,"1831":1}}],["包括空白字符",{"2":{"1813":1,"1831":1}}],["包括内置类型",{"2":{"1797":1}}],["包括存款",{"2":{"1766":1}}],["包括变量声明",{"2":{"1729":1}}],["包括返回值",{"2":{"1727":1}}],["包括编译器和运行程序",{"2":{"1589":1}}],["包括点对点通信",{"2":{"1573":1}}],["包括单核单处理器",{"2":{"1566":1}}],["包括档案",{"2":{"1411":1}}],["包括来自",{"2":{"1315":1}}],["包括来自项目成员的提交",{"2":{"1198":1}}],["包括参数和持久性缓冲区",{"2":{"1214":1}}],["包括参数高效的微调",{"2":{"139":1}}],["包括输入张量",{"2":{"1208":1}}],["包括新的超参数配置",{"2":{"1152":1}}],["包括尽可能多的有冗余超参数",{"2":{"1145":1}}],["包括贝叶斯优化或进化算法等方法",{"2":{"1144":1}}],["包括",{"2":{"1143":1,"1817":1,"1826":1,"1835":1,"1844":1}}],["包括对于获得良好结果至关重要的所有实用细节",{"2":{"1127":1}}],["包括个人和团队",{"2":{"1126":1}}],["包括偏置修正",{"2":{"1059":1}}],["包括mha",{"2":{"976":1}}],["包括比如transfomrer",{"2":{"766":1}}],["包括隐藏单词之前和之后的上下文",{"2":{"734":1}}],["包括但不限于",{"2":{"724":1}}],["包括但不限于语言学知识",{"2":{"121":1}}],["包括文档",{"2":{"696":1}}],["包括文本信息特征空间与位置信息特征空间",{"2":{"460":1}}],["包括距离和角度",{"2":{"689":1}}],["包括相似性的计算",{"2":{"689":1}}],["包括主谓宾的语义结构",{"2":{"676":1}}],["包括token",{"2":{"618":1}}],["包括固定的词汇表",{"2":{"610":1}}],["包括预训练数据的构成和模型架构本身",{"2":{"560":1}}],["包括通过大量文本数据",{"2":{"554":1}}],["包括linear层和softmax层",{"2":{"450":1}}],["包括embebddings和positionalencoding",{"2":{"450":2}}],["包括了四个主体模块",{"2":{"436":1}}],["包括交替的自注意力",{"2":{"354":1}}],["包括检测",{"2":{"338":1}}],["包括模型权重",{"2":{"295":1}}],["包括最频繁的单词和表示聚类的向量",{"2":{"185":1}}],["包括提高数值性能和梯度优化等",{"2":{"183":1}}],["包括gpt系列中使用的因果掩码",{"2":{"93":1}}],["包括多个注意力头和一个路由器",{"2":{"42":1}}],["积分梯度法有个重要的性质",{"2":{"134":1}}],["显式声明",{"2":{"1715":1}}],["显式使用",{"2":{"1638":1}}],["显式类型转换",{"2":{"1629":1}}],["显卡对应的就是显卡驱动",{"2":{"794":1}}],["显卡驱动的作用就是用来驱动显卡的",{"2":{"794":1}}],["显卡驱动",{"0":{"794":1}}],["显存不足问题",{"2":{"775":1}}],["显存占用量会减少",{"2":{"665":1}}],["显然只能使用自回归迭代操作来预测所有的单词",{"2":{"413":1}}],["显然第二个损失大",{"2":{"398":1}}],["显然",{"2":{"316":2,"1460":1}}],["显然被mask掉",{"2":{"71":1}}],["显著降低了预填节点的mfu",{"2":{"977":1}}],["显著降低了长序列处理的计算负担",{"2":{"210":1}}],["显著降低了计算复杂度和资源需求",{"2":{"151":1}}],["显示动物的基本信息",{"2":{"1690":1}}],["显示转换",{"0":{"1683":1}}],["显示隐藏文件",{"2":{"1509":1}}],["显示详细信息",{"2":{"1509":1}}],["显示出对子词结构和字符级信息的更好理解",{"2":{"611":1}}],["显示",{"2":{"131":2}}],["→x⋅→y=n∑i=1xi×yix→⋅y→=∑i=1nxi×yi",{"2":{"692":1}}],["→",{"2":{"130":3,"140":3,"845":1,"1157":1,"1159":1}}],["共约600m样本",{"2":{"1363":1}}],["共170m样本",{"2":{"1363":1}}],["共轭转置将矩阵的每个元素取复共轭",{"2":{"1082":1}}],["共轭转置操作将实部保持不变",{"2":{"1082":1}}],["共用设备或是共用存储器",{"2":{"1413":1}}],["共用的范围就可以越大",{"2":{"1340":1}}],["共用",{"2":{"658":1}}],["共同决定了对应的英文单词应该是",{"2":{"277":1}}],["共同组成的一个计算图",{"2":{"130":1}}],["共享内存的特殊数据类型",{"2":{"1728":1}}],["共享内存模型",{"2":{"1568":1}}],["共享进程资源",{"2":{"1563":1}}],["共享资源",{"0":{"1412":1}}],["共享了同一个wt",{"2":{"172":1}}],["共享记忆",{"2":{"154":1}}],["共享",{"2":{"135":1,"935":1,"951":1}}],["共享头",{"2":{"42":1}}],["求泛函极值的方法称为变分法",{"2":{"1377":1}}],["求导操作针对整个反向图来进行",{"2":{"1107":1}}],["求导如下",{"2":{"192":1}}],["求梯度",{"2":{"1096":1}}],["求解过程",{"2":{"1448":1}}],["求解线性最小二乘问题",{"2":{"1083":1}}],["求解器",{"2":{"498":1}}],["求解器可能简单地将多个非线性连接的权重推向零来接近恒等映射",{"2":{"301":1}}],["求得的",{"2":{"1004":1}}],["求方差的时候用到",{"2":{"1003":1}}],["求逆",{"2":{"473":1}}],["求每个小册子的",{"2":{"340":1}}],["求标准差时也是同理",{"2":{"340":1}}],["求通道",{"2":{"315":1}}],["求",{"2":{"200":3}}],["求内积得到相似度",{"2":{"175":1}}],["求和之后",{"2":{"394":1}}],["求和",{"2":{"129":1}}],["求出自注意力的q",{"2":{"36":1}}],["移动饼干指针",{"2":{"2153":1}}],["移动孩子指针",{"2":{"2153":1}}],["移动",{"2":{"1930":1}}],["移动捕获",{"2":{"1907":1}}],["移动赋值",{"2":{"1887":1}}],["移动赋值操作符",{"2":{"1887":1}}],["移动构造",{"2":{"1887":1}}],["移动构造函数",{"2":{"1887":1}}],["移动迭代器到第二个元素",{"2":{"1720":1,"1721":1,"1722":1}}],["移动了",{"2":{"1705":1}}],["移动两个元素",{"2":{"1633":1}}],["移动位数有other",{"2":{"1085":1}}],["移动指数平均是啥忘了",{"2":{"666":1}}],["移动到下一行",{"2":{"1705":1}}],["移动到下一个元素",{"2":{"1633":1}}],["移动到下一个单词的开头",{"2":{"1547":1}}],["移动到文件中间",{"2":{"1821":1,"1839":1}}],["移动到文件末尾之前的",{"2":{"1821":1,"1839":1}}],["移动到文件末尾",{"2":{"1547":1}}],["移动到文件开头",{"2":{"1547":1}}],["移动到前一个单词的开头",{"2":{"1547":1}}],["移动到行尾",{"2":{"1547":1}}],["移动到行首",{"2":{"1547":1}}],["移动到最后一个token",{"2":{"130":1}}],["移动到残差流的最终位置",{"2":{"122":1}}],["移除钩子函数",{"2":{"1212":1,"1213":1}}],["移除权重衰减和学习率预热",{"2":{"357":1}}],["移除所有归一化层",{"2":{"357":1}}],["移去尾部词",{"2":{"127":1}}],["趋于一致吗",{"2":{"1396":1}}],["趋向于捕捉抽象的语义模式",{"2":{"127":1}}],["趋近于无穷大时",{"2":{"108":1}}],["低延迟交易引擎开发",{"2":{"1946":1}}],["低效性",{"2":{"1025":1}}],["低精度要求小心处理以最小化量化误差",{"2":{"973":1}}],["低精度",{"2":{"973":1}}],["低维表征映射",{"2":{"733":1}}],["低维空间",{"2":{"4":1}}],["低地热带森林",{"2":{"713":1}}],["低",{"2":{"676":1,"1650":1}}],["低楼层发生一个较小的倾斜",{"2":{"309":1}}],["低秩分解和基于核的线性注意力",{"2":{"210":1}}],["低秩压缩等等",{"2":{"162":1}}],["低熵的决策",{"2":{"612":1}}],["低熵分布倾向于选择性地提取信息",{"2":{"194":1}}],["低熵表示模型的注意力集中",{"2":{"194":1}}],["低频词无法得到充分训练",{"2":{"565":1}}],["低频词非常多",{"2":{"185":1}}],["低频",{"2":{"185":4}}],["低级炼丹师",{"2":{"156":1}}],["低层键向量趋向于捕捉浅显的模式",{"2":{"127":1}}],["重排范围使得第",{"2":{"1752":1}}],["重载运算符不能改变其优先级和结合性",{"2":{"1712":1}}],["重载运算符的行为应该与内置类型的对应运算符保持一致",{"2":{"1712":1}}],["重载运算符的两种方式",{"2":{"1712":1}}],["重载的",{"2":{"1707":3}}],["重载的优势",{"2":{"1707":1}}],["重载的原理",{"2":{"1707":1}}],["重载发生在同一个类域中",{"2":{"1663":1}}],["重载",{"2":{"1663":1,"1712":1,"1788":1,"1789":2}}],["重写",{"2":{"1657":2,"1664":4,"1763":1}}],["重做撤销的操作",{"2":{"1550":1}}],["重做",{"2":{"1520":1}}],["重命名或移动文件",{"2":{"1510":1}}],["重置所有被优化的",{"2":{"1227":1}}],["重置所有模型参数的梯度",{"2":{"1214":1}}],["重置门是另一个门",{"2":{"874":1}}],["重置门",{"2":{"874":1}}],["重用",{"2":{"985":1}}],["重计算",{"0":{"946":1,"966":1}}],["重量",{"2":{"683":1}}],["重点",{"0":{"661":1,"1104":1,"1424":1},"2":{"1486":1,"1487":1,"1488":1,"1620":1,"2022":1}}],["重磅",{"2":{"513":1}}],["重新判断条件",{"2":{"1620":1}}],["重新运行模型",{"2":{"1297":1}}],["重新加载模型",{"2":{"1297":1}}],["重新命名可以让后续代码比较简洁",{"2":{"449":1}}],["重新堆叠成ffn层的输出",{"2":{"419":1}}],["重要提示",{"2":{"1667":1,"1668":1}}],["重要",{"2":{"840":1}}],["重要的任务可能来不及完成",{"2":{"2134":1}}],["重要的两个部分是平移不变性和缩放不变性",{"2":{"812":1}}],["重要的两个特性是缩放不变性",{"2":{"320":1}}],["重要的是要注意",{"2":{"1117":1}}],["重要的是",{"2":{"504":1,"623":1}}],["重要信息不会被淹没",{"2":{"260":1}}],["重叠的输入权重",{"2":{"305":1}}],["重叠奇点",{"2":{"305":1}}],["重叠会使",{"2":{"41":1}}],["重构整个博客",{"2":{"2048":1}}],["重构等强大功能",{"2":{"1605":1}}],["重构了注意力计算过程",{"2":{"940":1,"959":1}}],["重构的表示可以表达为如下公式",{"2":{"169":1}}],["重构词向量角度",{"0":{"166":1},"1":{"167":1,"168":1,"169":1},"2":{"157":1}}],["重复元素",{"2":{"1724":1}}],["重复执行一段代码块",{"2":{"1620":1}}],["重复执行直到条件不满足",{"0":{"1620":1}}],["重复此过程",{"2":{"1173":1}}],["重复多次",{"2":{"776":1}}],["重复第5步直到达到第2步设定的subword词表大小或概率增量低于某一阈值",{"2":{"599":1}}],["重复直到无法匹配",{"2":{"587":1}}],["重复上述过程",{"2":{"576":1}}],["重复",{"2":{"576":1}}],["重复步骤3到5",{"2":{"602":1}}],["重复步骤",{"2":{"515":1}}],["重复词句n",{"2":{"127":1}}],["重复三次以上",{"2":{"127":1}}],["要凑出",{"2":{"2137":1}}],["要表达的话我觉得可以理解成",{"2":{"2054":1}}],["要生成的文件或执行的操作",{"2":{"1917":1}}],["要调用的函数的名称",{"2":{"1729":1}}],["要努力学习",{"2":{"1728":1}}],["要么等于yi+1y",{"2":{"2023":1}}],["要么等于yiy",{"2":{"2023":1}}],["要么将其置为",{"2":{"1694":1}}],["要么申请新的内存地址并将内容拷贝",{"2":{"1694":1}}],["要么走了很多弯路才找到",{"2":{"1057":1}}],["要计算的最大值",{"2":{"1594":1}}],["要爬取的url列表",{"2":{"1566":1}}],["要做到这一点",{"2":{"1464":1}}],["要构建一个优化器",{"2":{"1221":1}}],["要使用结构体",{"2":{"1728":1}}],["要使用",{"2":{"1221":1}}],["要使深度神经网络在实践中正常运行",{"2":{"1127":1}}],["要使这种方法既可扩展又具有组合性",{"2":{"222":1}}],["要冻结模型的某些部分",{"2":{"1117":1}}],["要在代码的整个块中禁用梯度",{"2":{"1116":1}}],["要点总结",{"0":{"1103":1,"1375":1},"1":{"1104":1,"1105":1,"1106":1,"1107":1,"1108":1,"1109":1,"1110":1,"1111":1}}],["要点",{"0":{"1206":1},"2":{"1046":1,"1653":1,"1654":1,"1655":1,"1656":1}}],["要丢掉有效特征的方法就是将有效特征点与相邻点都丢掉",{"2":{"1019":1}}],["要等到所有句子",{"2":{"904":1}}],["要比",{"2":{"838":1}}],["要求如下",{"2":{"1873":1}}],["要求用户输入",{"2":{"1716":1}}],["要求用户输入一个整数",{"2":{"1716":1}}],["要求",{"2":{"1664":1,"1791":1,"1933":1,"1997":1}}],["要求赋值号左边的变量已经声明",{"2":{"1629":1}}],["要求为",{"2":{"986":1}}],["要求输入为2维张量",{"2":{"805":1}}],["要求输入为3维张量",{"2":{"805":1}}],["要求模式必须满足",{"2":{"127":1}}],["要确保看不到自身token",{"2":{"727":1}}],["要",{"2":{"714":2}}],["要将每个词不同维度的意义赋予有效的数值",{"2":{"712":1}}],["要低",{"2":{"601":1}}],["要更新词频统计表",{"2":{"582":1}}],["要实现",{"2":{"542":1}}],["要堆叠的编码器层",{"2":{"522":1}}],["要对网络进行求解",{"2":{"495":1}}],["要被翻译的句子",{"2":{"426":1}}],["要被预测的所有token",{"2":{"385":1,"398":1}}],["要经过",{"2":{"394":1}}],["要算作句子成分",{"2":{"382":1}}],["要用的时候才去标准化",{"2":{"334":1}}],["要理解这个",{"2":{"221":1}}],["要理解llm的底层实现原理",{"2":{"162":1}}],["要通过计算hiℎiℎ",{"2":{"172":1}}],["要是模型能未卜先知地知道自己下一步将要输出什么",{"2":{"58":1}}],["学不是学习书本知识",{"2":{"2054":1}}],["学",{"2":{"2054":5}}],["学而时习",{"2":{"2054":2}}],["学而时习之",{"0":{"2053":1},"1":{"2054":1,"2055":1},"2":{"2043":1,"2053":1,"2054":1}}],["学而篇",{"0":{"2052":1},"1":{"2053":1,"2054":1,"2055":1}}],["学生的成绩",{"2":{"1825":1,"1843":1}}],["学生成绩",{"2":{"1728":1}}],["学生学号",{"2":{"1728":1}}],["学生姓名",{"2":{"1728":1}}],["学生也积极学习",{"2":{"897":1}}],["学号",{"2":{"1728":2}}],["学会使用工具和编程技巧避免这些问题",{"2":{"1666":1}}],["学会使用",{"2":{"1652":1}}],["学前须知",{"2":{"1561":1}}],["学到的线性映射",{"2":{"926":1}}],["学到的记忆有一定的可解释性",{"2":{"126":1}}],["学术上的计算过程如下",{"2":{"870":1}}],["学术头条",{"2":{"156":1}}],["学者提出了gelu激活函数",{"2":{"844":1}}],["学习图形学可以培养将理论转化为实践的能力",{"2":{"2010":1}}],["学习图形学会锻炼数学建模和复杂算法实现的能力",{"2":{"2010":1}}],["学习glsl",{"2":{"2009":1}}],["学习如何通过opengl",{"2":{"2009":1}}],["学习如何学习",{"2":{"1455":1}}],["学习正交投影和透视投影的概念",{"2":{"2009":1}}],["学习基本几何图形的表示方法",{"2":{"2009":1}}],["学习了构造函数",{"2":{"1678":1}}],["学习目标",{"2":{"1666":1}}],["学习一些更高级的函数使用技巧",{"2":{"1644":1}}],["学习",{"2":{"1602":1,"2031":1,"2109":1}}],["学习策略与建议",{"0":{"1595":1},"1":{"1596":1,"1597":1,"1598":1}}],["学习框架图",{"2":{"1561":1}}],["学习前要自己配置一个linux虚拟机",{"2":{"1561":1}}],["学习前一定要自己配置一个linux虚拟机",{"2":{"1502":1}}],["学习前需要掌握",{"2":{"1474":1}}],["学习ai算法技术",{"2":{"1403":1}}],["学习方差取决于试验次数和搜索空间",{"2":{"1152":1}}],["学习轨迹可能穿过了很多不同的结构",{"2":{"1048":1}}],["学习过程的目标是让共享上下文的单词也共享相似的向量",{"2":{"709":1}}],["学习过程和意外指标",{"2":{"230":1}}],["学习低维稠密语义表示",{"2":{"676":1}}],["学习到于源文本中与当前输出的",{"2":{"536":1}}],["学习到历史译文的所有信息",{"2":{"536":1}}],["学习到的主要是两种语言之间的对应关系",{"2":{"908":1}}],["学习到的z向量使transformer²能够适应各种新的下游任务",{"2":{"224":1}}],["学习到的z向量可能是",{"2":{"224":1}}],["学习到的参数矩阵又是不同的",{"2":{"99":1}}],["学习的过程显然困难也会更大",{"2":{"679":1}}],["学习的过程",{"2":{"472":1}}],["学习率可以同时被其他操作器在此调度器之外修改",{"2":{"1243":1}}],["学习率是控制模型参数更新幅度的超参数",{"2":{"1229":1}}],["学习率是一个冗余超参数",{"2":{"1143":1}}],["学习率调度器",{"0":{"1232":1}}],["学习率调度应该在优化器更新之后应用",{"2":{"1231":1}}],["学习率调度是根据训练的迭代次数或验证误差的变化动态地调整学习率",{"2":{"1229":1}}],["学习率调度",{"2":{"1229":1}}],["学习率调度参数",{"2":{"1143":1}}],["学习率调整是一种重要的技术",{"2":{"1229":1}}],["学习率的控制",{"2":{"1202":1}}],["学习率的选择需要根据具体任务和模型结构进行调整",{"2":{"400":1}}],["学习率预热对解决训练不稳定性的有益影响",{"2":{"1183":1}}],["学习率预热",{"0":{"1181":1},"1":{"1182":1,"1183":1}}],["学习率预热策略具体如下图所示",{"2":{"402":1}}],["学习率预热策略",{"2":{"402":1}}],["学习率衰减计划",{"2":{"1158":1}}],["学习率也可以设置为",{"2":{"1052":1}}],["学习率就会收缩并最终会变得非常小使得训练提前结束",{"2":{"1045":1}}],["学习率决定了模型参数更新的步长",{"2":{"400":1}}],["学习率",{"0":{"400":1},"1":{"401":1,"402":1},"2":{"1049":1,"1135":1,"1150":1,"1186":1}}],["学习在参数空间中沿退化方向大幅减慢",{"2":{"305":1}}],["学习和提高的过程",{"2":{"235":1}}],["学习知识",{"0":{"146":1},"1":{"147":1,"148":1},"2":{"96":1}}],["学习模式",{"2":{"20":1}}],["学习不同的信息",{"2":{"12":1}}],["借以实现语义搜索",{"2":{"692":1}}],["借助词表把token映射为数字",{"2":{"431":1}}],["借此预测下一个单词",{"2":{"526":1}}],["借此来表示句子整体语义",{"2":{"289":1}}],["借此可以对公式有更加深深刻的理解",{"2":{"176":1}}],["借此可以在神经网络中添加记忆模块",{"2":{"125":1}}],["借鉴了很多网上朋友的文章",{"2":{"235":1}}],["借鉴并非所有注意力头都具有同等重要性的观点",{"2":{"42":1}}],["祛毒",{"2":{"123":1}}],["王五",{"2":{"1486":1,"1680":1}}],["王立威",{"2":{"513":1}}],["王兆洋",{"2":{"233":1}}],["王半仙",{"2":{"156":1}}],["王云鹤",{"2":{"156":1}}],["王坚院士称",{"2":{"123":1}}],["王庆法",{"2":{"47":1,"233":2,"513":3}}],["获得各个位置的标签向量之后",{"2":{"1320":1}}],["获得",{"2":{"1153":1}}],["获得整个序列的嵌入",{"2":{"735":1}}],["获得玻尔兹曼奖的物理学家霍菲尔德也曾在一次访谈中提到",{"2":{"506":1}}],["获得更稳定的深层网络结构",{"2":{"334":1}}],["获得粗略的知识神经元集",{"2":{"135":1}}],["获得的loss的期望还是比较小的",{"2":{"122":1}}],["获取变量或表达式类型",{"2":{"1897":1}}],["获取文件大小",{"2":{"1821":1,"1839":1}}],["获取长度",{"2":{"1715":1}}],["获取大小和容量",{"2":{"1714":1}}],["获取内部裸指针",{"2":{"1695":1}}],["获取字节大小",{"2":{"1635":1}}],["获取字符串长度",{"2":{"1624":1}}],["获取个位数字",{"2":{"1607":1}}],["获取进程总数",{"2":{"1594":1}}],["获取当前进程的编号",{"2":{"1594":1}}],["获取当前进程的id",{"2":{"1575":1}}],["获取当前学习率",{"2":{"1243":1}}],["获取总的进程数",{"2":{"1575":1}}],["获取网页内容",{"2":{"1528":1}}],["获取sqlsession连接",{"2":{"1481":1,"1485":1}}],["获取实际的实体",{"2":{"1331":1}}],["获取预测出来的实体",{"2":{"1331":1}}],["获取每条句子长度",{"2":{"1330":1}}],["获取",{"2":{"1227":1,"1715":1}}],["获取额外状态",{"2":{"1214":1}}],["获取更多信息",{"2":{"1198":1}}],["获取经过掩模后的矩阵",{"2":{"1086":1}}],["获取稀疏矩阵中非0元素的",{"2":{"1086":1}}],["获取稀疏矩阵的索引矩阵",{"2":{"1086":1}}],["获取嵌入向量",{"2":{"702":1}}],["获取输出",{"2":{"428":1}}],["获取的是batch",{"2":{"385":1}}],["获取全部的",{"2":{"78":1}}],["获取目标句子",{"2":{"65":1}}],["获取batch",{"2":{"36":1}}],["做出一个看似合理的决定",{"2":{"2105":1}}],["做出准确判断",{"2":{"260":1}}],["做好关注选择的优化器的",{"2":{"1130":1}}],["做自注意力计算",{"2":{"533":1}}],["做单独归一化",{"2":{"323":1}}],["做标准化",{"2":{"318":1,"323":1,"326":1}}],["做成计算和存储一体的",{"2":{"206":1}}],["做全部的注意力计算",{"2":{"204":1}}],["做注意力计算",{"2":{"204":4}}],["做多分类分解成多个sigmoid",{"2":{"184":1}}],["做了",{"2":{"694":1}}],["做了平滑",{"2":{"694":1}}],["做了注解",{"2":{"419":1}}],["做了不同的投影",{"2":{"172":1}}],["做了进一步研究",{"2":{"122":1}}],["做个比喻来说",{"2":{"13":1}}],["传递",{"2":{"1891":1}}],["传递复杂的数据结构",{"2":{"1728":1}}],["传递了打印次数",{"2":{"1708":1}}],["传递了宽度参数",{"2":{"1708":1}}],["传递的是地址",{"2":{"1667":1}}],["传递大型数据结构",{"2":{"1650":1}}],["传递简单数据类型",{"2":{"1650":1}}],["传递内容",{"2":{"1650":1}}],["传递给它",{"2":{"1729":1}}],["传递给函数的实际值或表达式",{"2":{"1729":1}}],["传递给交叉注意力",{"2":{"416":1}}],["传递给语言模型",{"2":{"145":1}}],["传输相关的网络资源争用",{"2":{"977":1}}],["传给架构图中最上方的线性层",{"2":{"526":1}}],["传给sublayerconnection",{"2":{"523":1}}],["传给collate",{"2":{"384":1,"558":1}}],["传入字符串字面量",{"2":{"1929":1}}],["传入sql语句的参数类型",{"2":{"1485":1}}],["传入crf之后",{"2":{"1320":1}}],["传入的参数criterion是损失函数",{"2":{"398":1}}],["传入的三个参数分别为",{"2":{"385":1}}],["传入的分别是self",{"2":{"344":1}}],["传入",{"2":{"344":1,"1929":1}}],["传入参数",{"2":{"344":2}}],["传播回所有输出信号作为该神经元的输入的神经元中",{"2":{"1443":1}}],["传播时激活值的分布情况如下图所示",{"2":{"994":1}}],["传播",{"2":{"304":1}}],["传播关系",{"2":{"122":1}}],["传统机器学习的特征提取主要依赖人工",{"2":{"1470":1}}],["传统思想启发的注意力算法",{"2":{"981":1}}],["传统",{"2":{"839":1}}],["传统transformer中的前馈网络被替换为两个连续的pattention块",{"2":{"622":1}}],["传统构造词表的方法会先对各个句子进行分词",{"2":{"565":1}}],["传统的错误处理方式的局限性",{"2":{"1761":1}}],["传统的错误处理方式及其局限性",{"2":{"1761":1}}],["传统的jdbc操作",{"2":{"1479":1}}],["传统的transformer模型通常采用多头注意力机制",{"2":{"956":1}}],["传统的",{"2":{"785":1}}],["传统的神经网络使用矩阵乘法来建立输入与输出的连接关系",{"2":{"772":1}}],["传统的attention是基于source端和target端的隐变量",{"2":{"649":1,"931":1}}],["传统的语言模型依赖于",{"2":{"610":1}}],["传统的lstm只有两个输出",{"2":{"287":1}}],["传统的lstm在处理输入时把句子视为单词序列",{"2":{"287":1}}],["传统的rnn架构仅适用于输入和输出等长的任务",{"2":{"241":1}}],["传统的自注意力模型可能不足以捕获序列推荐场景中复杂的item依赖关系",{"2":{"209":1}}],["传统上",{"2":{"222":1}}],["传参等过程中的构造和析构行为",{"2":{"1680":1}}],["传参方式",{"2":{"1650":1}}],["传参",{"2":{"122":2}}],["巴黎",{"2":{"122":1,"485":1}}],["残差链接",{"2":{"446":1}}],["残差的作用会比post",{"2":{"334":1}}],["残差的最高预测词是模型的最终预测词",{"2":{"306":1}}],["残差使得模型的输出被逐步改善",{"2":{"306":1}}],["残差网络解决了什么",{"2":{"740":1}}],["残差网络可以被看作是求解常微分方程的欧拉方法的离散化版本",{"2":{"496":1}}],["残差网络与常微分方程",{"2":{"496":1}}],["残差网络的前世今生与原理",{"2":{"361":1}}],["残差网络会减轻神经网络的退化",{"2":{"305":1}}],["残差网络将恒等映射结构h",{"2":{"301":1}}],["残差网络和归一化",{"0":{"293":1},"1":{"294":1,"295":1,"296":1,"297":1,"298":1,"299":1,"300":1,"301":1,"302":1,"303":1,"304":1,"305":1,"306":1,"307":1,"308":1,"309":1,"310":1,"311":1,"312":1,"313":1,"314":1,"315":1,"316":1,"317":1,"318":1,"319":1,"320":1,"321":1,"322":1,"323":1,"324":1,"325":1,"326":1,"327":1,"328":1,"329":1,"330":1,"331":1,"332":1,"333":1,"334":1,"335":1,"336":1,"337":1,"338":1,"339":1,"340":1,"341":1,"342":1,"343":1,"344":1,"345":1,"346":1,"347":1,"348":1,"349":1,"350":1,"351":1,"352":1,"353":1,"354":1,"355":1,"356":1,"357":1,"358":1,"359":1,"360":1,"361":1},"2":{"293":1}}],["残差",{"0":{"307":1,"344":1},"2":{"293":2,"332":1}}],["残差连接等",{"2":{"529":1}}],["残差连接对应上图中的",{"2":{"517":1}}],["残差连接对于权重退化的解决思路非常简单",{"2":{"304":1}}],["残差连接使注意力机制中产生的新数据和最开始输入的原始数据合并在一起",{"2":{"517":1}}],["残差连接在每个",{"2":{"470":1}}],["残差连接之前",{"2":{"394":1}}],["残差连接确实会添加到应用掩码之后的输出上",{"2":{"307":1}}],["残差连接至少保证了不会丢失上一层学习到的东西",{"2":{"304":1}}],["残差连接就是把网络的输入和输出相加",{"2":{"300":1,"470":1}}],["残差连接的思想起源于中心化",{"2":{"298":1}}],["残差连接和归一化层的结合使得transformer能够更有效地进行训练",{"2":{"294":1}}],["残差连接把网络的输入和输出相加",{"2":{"294":1}}],["残差连接",{"0":{"295":1,"470":1},"1":{"296":1,"297":1,"298":1,"299":1,"300":1,"301":1,"302":1,"303":1,"304":1,"305":1,"306":1,"307":1},"2":{"293":1,"294":2,"302":1,"344":1,"467":1,"470":1}}],["残差连接原理详细解析和代码论证",{"2":{"47":1}}],["残差流",{"2":{"130":1}}],["残差流中解码出的高概率token会由国家过渡到首都名字",{"2":{"122":1}}],["融入到当前处理的单词中",{"2":{"158":1,"274":1}}],["融入信息",{"2":{"122":1}}],["融合结果会进入ffn",{"2":{"445":1}}],["融合信息",{"2":{"274":1}}],["融合了源句子的结构和目标序列的部分结构",{"2":{"525":1}}],["融合了上下文信息得到的在隐空间的语义矩阵",{"2":{"168":1}}],["融合了下一个token",{"2":{"58":1}}],["融合记忆",{"0":{"231":1},"2":{"157":1}}],["融合四部分",{"2":{"16":1}}],["融合每个头的z",{"0":{"35":1},"2":{"0":1}}],["融合方式",{"0":{"19":1},"2":{"0":1}}],["融合",{"0":{"13":1},"2":{"0":1,"16":1,"1330":1}}],["主页页脚添加公安备案信息",{"2":{"2046":1}}],["主页🏠",{"0":{"2037":1}}],["主页",{"0":{"2033":1}}],["主目录的",{"0":{"1980":1}}],["主程序文件",{"2":{"1916":1}}],["主函数",{"2":{"1606":1}}],["主进程打印结果",{"2":{"1594":1}}],["主成分分析法",{"2":{"1370":1}}],["主机系统",{"2":{"2089":1}}],["主机2",{"2":{"975":1}}],["主机1",{"2":{"975":1}}],["主题分类等",{"2":{"906":1}}],["主动",{"2":{"712":1}}],["主语宾语的位置",{"2":{"676":1}}],["主语𝑠",{"2":{"122":1}}],["主谓宾",{"2":{"510":1}}],["主体类",{"0":{"450":1}}],["主体模块",{"0":{"436":1}}],["主要就业方向与技术能力分析报告",{"0":{"1934":1},"1":{"1935":1,"1936":1,"1937":1,"1938":1,"1939":1,"1940":1,"1941":1,"1942":1,"1943":1,"1944":1,"1945":1,"1946":1,"1947":1,"1948":1,"1949":1,"1950":1,"1951":1,"1952":1,"1953":1,"1954":1,"1955":1,"1956":1,"1957":1,"1958":1,"1959":1,"1960":1,"1961":1}}],["主要功能",{"2":{"1930":1}}],["主要体现在以下几个方面",{"2":{"1913":1}}],["主要操作",{"2":{"1824":1,"1842":1}}],["主要优点",{"2":{"1810":1,"1828":1}}],["主要内容",{"2":{"1658":1,"1697":1}}],["主要包含",{"2":{"1912":1}}],["主要包含函数声明",{"2":{"1628":1}}],["主要包括以下几个方面",{"2":{"552":1}}],["主要完成self",{"2":{"1227":1}}],["主要在imagenet上",{"2":{"1168":1}}],["主要区别在于输入的构造",{"2":{"1086":1}}],["主要思想是将输入的",{"2":{"942":1,"959":1}}],["主要思路是以当前位置qtqtq",{"2":{"759":1}}],["主要用于检查或处理数据",{"2":{"1758":1}}],["主要用于以下两种情况",{"2":{"1607":1}}],["主要用于教学和研究",{"2":{"1569":1}}],["主要用于向上采样",{"2":{"779":1}}],["主要用于将离散的词汇索引",{"2":{"702":1}}],["主要分为以下5种策略",{"2":{"731":1}}],["主要猎食麋鹿",{"2":{"713":1}}],["主要探讨了一种革新性的基于参数token化的高效可扩展的transformer架构设计方案",{"2":{"616":1}}],["主要贡献",{"0":{"611":1,"617":1}}],["主要定理与证明",{"2":{"507":1}}],["主要目标是以注意力头和ffn作为基本单元",{"2":{"475":1}}],["主要目标是理解llm的信息流机制",{"2":{"475":1}}],["主要目的是引入transformer概念和其相关背景",{"2":{"235":1}}],["主要负责记忆",{"2":{"437":1}}],["主要起到两个作用",{"2":{"382":1}}],["主要的mpi实现有",{"2":{"1569":1}}],["主要的差别在于",{"2":{"598":1}}],["主要的非线性部分在",{"2":{"334":1}}],["主要的区别在于增加了两个线性映射层",{"2":{"204":1}}],["主要作用是使得整个损失函数的",{"2":{"314":1}}],["主要可以从如下方面进行思考",{"2":{"172":1}}],["主要是针对batch",{"2":{"810":1}}],["主要是因为在token",{"2":{"618":1}}],["主要是取决于高相似度的几个",{"2":{"204":1}}],["主要是由于可训练参数和相关优化器状态的数量庞大导致的",{"2":{"154":1}}],["主要是为了重置事实",{"2":{"140":1}}],["主要是为了综合编码器和解码器中的padding",{"2":{"84":1}}],["主要关注邻居的位置头",{"2":{"20":1}}],["定期评估作业的运行速度可能不够快",{"2":{"1165":1}}],["定期评估应在固定步长间隔进行",{"2":{"1164":1}}],["定期评估是最实际和最经济的选择",{"2":{"1163":1}}],["定期评估",{"2":{"1163":1}}],["定理6",{"2":{"507":1}}],["定理5",{"2":{"507":1}}],["定理4",{"2":{"507":1}}],["定理3",{"2":{"507":1}}],["定位",{"2":{"121":1,"142":1}}],["定义编译选项",{"0":{"1984":1}}],["定义项目的名称和版本号",{"2":{"1972":1}}],["定义变量",{"2":{"1917":1}}],["定义函数",{"2":{"1916":1}}],["定义函数接收的输入参数",{"2":{"1729":1}}],["定义匿名函数对象的能力",{"2":{"1882":1}}],["定义友元类",{"0":{"1778":1},"2":{"1768":1}}],["定义友元函数的方法如下",{"2":{"1772":1}}],["定义友元函数",{"0":{"1772":1},"2":{"1768":1}}],["定义联合体c++union",{"2":{"1728":1}}],["定义枚举类型",{"2":{"1728":1}}],["定义结构体",{"2":{"1728":1}}],["定义和初始化",{"2":{"1714":1}}],["定义从",{"2":{"1685":1}}],["定义了一个字符数组",{"2":{"1715":1}}],["定义了一个名为",{"2":{"1674":1,"1729":1}}],["定义了如何将问题分解为更小的",{"2":{"1646":1}}],["定义它的函数",{"2":{"1649":1}}],["定义它的代码块",{"2":{"1649":1}}],["定义时创建",{"2":{"1649":1}}],["定义时需要指定数组的大小",{"2":{"1634":1}}],["定义宏函数",{"2":{"1632":1}}],["定义常量",{"2":{"1632":2}}],["定义并初始化",{"2":{"1624":1}}],["定义优化器和损失函数",{"2":{"1296":1}}],["定义训练函数",{"2":{"1295":1}}],["定义数据集",{"2":{"1250":1,"1295":1}}],["定义自己的数据集",{"0":{"1250":1}}],["定义参数",{"2":{"1243":1}}],["定义学习率调度器",{"2":{"1239":1}}],["定义输入数据和目标标签",{"2":{"1213":1}}],["定义模型和优化器",{"2":{"1239":1}}],["定义模型参数",{"2":{"1216":1}}],["定义模型",{"2":{"1213":1,"1295":1}}],["定义我们自己的module",{"0":{"1204":1},"1":{"1205":1,"1206":1}}],["定义的操作",{"2":{"1114":1}}],["定义右侧取幂运算符",{"2":{"1083":1}}],["定义右侧取模运算符",{"2":{"1083":1}}],["定义tensor",{"2":{"1083":2}}],["定义对象的字符串表示形式",{"2":{"1083":1,"1214":1}}],["定义源序列掩码",{"2":{"428":1}}],["定义构建batch功能",{"2":{"375":1}}],["定义前向传播函数",{"2":{"343":1,"344":1}}],["定义一个伪目标",{"2":{"1994":1}}],["定义一个简单的lambda表达式",{"2":{"1883":1}}],["定义一个",{"2":{"1766":1}}],["定义一个函数",{"2":{"1729":2}}],["定义一个可以存储",{"2":{"1704":1}}],["定义一个模板函数sum",{"2":{"1702":1}}],["定义一个模型",{"0":{"1257":1}}],["定义一个层归一化",{"2":{"344":1}}],["定义一个大小为features的一维张量",{"2":{"343":2}}],["定义一个dropout层",{"2":{"113":1,"344":1}}],["定义一个继承自nn",{"2":{"113":1}}],["定义为在某时刻",{"2":{"494":1}}],["定义为",{"2":{"106":1,"108":1}}],["定义w^q",{"2":{"23":1}}],["定义",{"0":{"23":1,"88":1,"178":1,"310":1,"571":1,"885":1},"2":{"0":1,"49":1,"157":1,"293":1,"769":1,"885":1,"1594":1,"1612":1,"1614":3,"1628":1,"1700":1,"1701":1,"1766":1,"1848":1,"1851":1,"1855":1,"1859":1,"1916":2,"1923":1}}],["具象的物理空间和语言都被表达为embedding",{"2":{"689":1}}],["具备语义性的稠密多维向量就是embedding",{"2":{"688":1}}],["具备隐式的位置编码功能",{"2":{"542":1}}],["具备这些特征的系统可以用来模拟任何其他图灵完备的系统",{"2":{"504":1}}],["具备因果关系",{"2":{"443":1}}],["具备如下优点",{"2":{"346":1}}],["具有独占所有权",{"2":{"1911":1}}],["具有内部状态标志",{"2":{"1814":1,"1832":1}}],["具有引用计数功能",{"2":{"1695":1}}],["具有公共成员函数",{"2":{"1664":2}}],["具有构造函数",{"2":{"1664":4}}],["具有私有成员",{"2":{"1664":3}}],["具有局部作用域",{"2":{"1649":1}}],["具有全局作用域",{"2":{"1649":1}}],["具有类型检查",{"2":{"1632":1}}],["具有短路特性",{"2":{"1630":1}}],["具有灵活的架构",{"2":{"1569":1}}],["具有灵活性",{"2":{"616":1}}],["具有复杂分段学习率",{"2":{"1173":1}}],["具有固定动量的",{"2":{"1130":1}}],["具有损失最大偏导的参数相应地有一个快速下降的学习率",{"2":{"1042":1}}],["具有较好的语言表示能力",{"2":{"898":1}}],["具有了某种能力",{"2":{"898":1}}],["具有相似模式的词语就会把这些相似更新累积到可观的程度",{"2":{"709":1}}],["具有相同的特征可以被分到同一个group中",{"2":{"338":1}}],["具有显式的分层结构模型更适合创建长篇输出",{"2":{"627":1}}],["具有一个新的token参数注意力",{"2":{"620":1}}],["具有ld≪lgld≪lg",{"2":{"614":1}}],["具有",{"2":{"614":1,"1117":1,"1316":1,"1317":1}}],["具有非线性表达能力",{"2":{"320":1}}],["具有注意力和前馈网络块的扩展视图",{"2":{"295":1}}],["具有更好的外推性",{"2":{"1341":1}}],["具有更好的信息理论可解释性",{"2":{"183":1}}],["具有更多超参数的优化器可能需要更多的调优工作才能找到最佳配置",{"2":{"1130":1}}],["具有更强表达能力",{"2":{"766":1}}],["具有更高的计算效率和可扩展性",{"2":{"291":1}}],["具有选择性地在序列步骤间传递信息的能力",{"2":{"248":1}}],["具有建模高阶转换的潜力",{"2":{"209":1}}],["具有巨大的潜力",{"2":{"121":1}}],["具体理解还是要看个人的悟性",{"2":{"2052":1}}],["具体视企业规模和个人能力浮动",{"2":{"1961":1}}],["具体要求",{"2":{"1914":1}}],["具体行为取决于编译器实现",{"2":{"1630":1}}],["具体模型结构",{"0":{"1355":1},"1":{"1356":1,"1357":1,"1358":1}}],["具体过程可展示如下",{"2":{"1344":1}}],["具体配置请查看",{"2":{"1196":1}}],["具体请阅读",{"2":{"1186":1}}],["具体安排如下",{"2":{"1183":1}}],["具体原因和干预措施将高度依赖于任务",{"2":{"1161":1}}],["具体指的是在网络模型训练之前",{"2":{"988":1}}],["具体实施",{"2":{"972":2}}],["具体实现有不同的厂商或者社区实现",{"2":{"1569":1}}],["具体实现有多种",{"2":{"1568":1}}],["具体实现原理",{"0":{"975":1}}],["具体实现细节上完全相同",{"2":{"538":1}}],["具体实现",{"0":{"327":1},"2":{"293":1}}],["具体实现就是两个mask相加作为attn",{"2":{"78":1}}],["具体取决于序列长度",{"2":{"964":1}}],["具体flashattention的算法",{"0":{"944":1}}],["具体什么是对齐方式alignment呢",{"2":{"908":1}}],["具体见4",{"2":{"894":1}}],["具体见下图",{"2":{"427":1}}],["具体见下图上的公式",{"2":{"185":1}}],["具体计算过程",{"0":{"1393":1}}],["具体计算过程为",{"2":{"870":1}}],["具体计算",{"0":{"870":1}}],["具体计算流程如下",{"2":{"264":1}}],["具体公式是qiktj→qiktj−λ|i−j|qikjt→qikjt−λ|i−j|q",{"2":{"765":1}}],["具体公式如下",{"2":{"70":1,"103":1}}],["具体公式如下图",{"2":{"46":1}}],["具体技术细节如下",{"2":{"759":1}}],["具体细节如下",{"2":{"745":1,"760":1}}],["具体细节如下图所示",{"2":{"125":1}}],["具体做法就是直接将位置编码当作可训练参数",{"2":{"749":1}}],["具体做法如下",{"2":{"727":1}}],["具体做法时在计算注意力时加入一个掩码",{"2":{"409":1}}],["具体向量如下图所示",{"2":{"681":1}}],["具体定义可以不一致",{"2":{"668":1}}],["具体的加噪过程如下",{"2":{"1350":1}}],["具体的映射代码",{"2":{"1340":1}}],["具体的glorot条件如下",{"2":{"999":1}}],["具体的原因是",{"2":{"701":1}}],["具体的做法是",{"2":{"651":1,"933":1}}],["具体的代码逻辑如下",{"2":{"449":1}}],["具体方式是将xixix",{"2":{"613":1}}],["具体到每一种模型",{"2":{"568":1}}],["具体到某一层",{"2":{"147":1}}],["具体到某一个token上",{"2":{"36":1}}],["具体又包括如下几步",{"2":{"545":1}}],["具体流程解读如下",{"2":{"538":1}}],["具体流程说明如下",{"2":{"537":1}}],["具体流程如下",{"2":{"455":1,"515":1,"545":1}}],["具体训练时候的输入输出如下图",{"2":{"528":1}}],["具体推导如下图所示",{"2":{"494":1}}],["具体地",{"2":{"485":1,"1320":1}}],["具体意义同上",{"2":{"451":1}}],["具体作用是对填充符号进行掩码",{"2":{"450":1}}],["具体含义类似src",{"2":{"450":1}}],["具体操作是将解码器llm的因果注意力掩码",{"2":{"734":1}}],["具体操作是softmax",{"2":{"519":1}}],["具体操作是应用",{"2":{"473":1}}],["具体操作是通过一个线性层把特征向量升维到词表维度",{"2":{"431":1}}],["具体操作是torch",{"2":{"200":3}}],["具体运作流程如下",{"2":{"431":1}}],["具体对应图上的",{"2":{"517":1}}],["具体对应下图所示",{"2":{"407":1}}],["具体对应如下代码片段",{"2":{"394":1}}],["具体使用方式如下",{"2":{"402":1}}],["具体调用损失函数的精简代码如下",{"2":{"399":1}}],["具体分为如下",{"2":{"733":1}}],["具体分为三步",{"2":{"582":1}}],["具体分为以下几个步骤",{"2":{"455":1,"698":1}}],["具体分为四种",{"2":{"394":1}}],["具体分析如下",{"2":{"116":1}}],["具体是在pre",{"2":{"554":1}}],["具体是基于一些简单的规则",{"2":{"553":1}}],["具体是把输入序列切分为多个块",{"2":{"420":1}}],["具体是指在训练开始阶段",{"2":{"401":1}}],["具体是利用分词器tgt",{"2":{"384":1,"558":1}}],["具体是利用分词器src",{"2":{"384":1,"558":1}}],["具体是通过修改实体或关系知识来确定这些变化是否会产生一致的结果",{"2":{"136":1}}],["具体信息可以参见",{"2":{"373":1}}],["具体信息参加下图",{"2":{"347":1}}],["具体代码为",{"2":{"1223":1}}],["具体代码流程如下",{"2":{"529":1}}],["具体代码如下",{"2":{"364":1,"383":1,"394":1,"398":1,"402":1,"449":1,"557":1,"723":1}}],["具体代码是",{"2":{"79":1}}],["具体怎么映射由batchnorm的可学习参数自己学习得到",{"2":{"313":1}}],["具体由add和norm两部分组成",{"2":{"294":1}}],["具体思路如下图",{"2":{"287":1}}],["具体我们又可以把这两步拆解为5个详细步骤",{"2":{"267":1}}],["具体逻辑在前面给出的batch类的成员变量函数make",{"2":{"382":1}}],["具体逻辑在make",{"2":{"74":1}}],["具体逻辑为",{"2":{"241":2}}],["具体也可以参见下图",{"2":{"213":1}}],["具体线性转换是在后续会介绍的",{"2":{"199":1}}],["具体图例如下",{"2":{"179":1}}],["具体算法如下",{"2":{"179":1}}],["具体表现为从",{"2":{"148":1,"485":1}}],["具体表现为梯度",{"2":{"148":1,"485":1}}],["具体而言是将每个位置的隐状态通过mean",{"2":{"734":1}}],["具体而言",{"2":{"148":1,"153":1,"216":1,"224":1,"298":1,"628":1,"633":1,"975":1,"981":1}}],["具体步骤如下",{"2":{"145":1}}],["具体发生在知识关联阶段",{"2":{"141":1}}],["具体发生在知识识别阶段",{"2":{"141":1}}],["具体可以从如下几个角度来思考",{"2":{"694":1}}],["具体可以参见下图",{"2":{"72":1}}],["具体可分为两类",{"2":{"140":1}}],["具体特点如下",{"2":{"128":1}}],["具体包括mha",{"2":{"205":1}}],["具体包括以下",{"2":{"143":1}}],["具体包括",{"2":{"121":1}}],["具体包含如下几种方式",{"2":{"90":1}}],["具体来说是如下几点",{"2":{"612":1}}],["具体来说",{"2":{"91":1,"131":1,"135":1,"145":1,"161":1,"210":1,"216":1,"300":1,"315":1,"349":1,"406":1,"485":1,"496":1,"500":1,"621":1,"684":1,"735":1,"739":1,"933":1,"1291":1,"1361":1,"1589":1}}],["具体来说就是transformer",{"2":{"706":1}}],["具体来说就是",{"2":{"53":1,"376":1}}],["具体会生成一个shape为",{"2":{"74":1}}],["具体会通过view",{"2":{"29":1}}],["具体掩码如下",{"2":{"74":1,"382":1}}],["具体如上面padding",{"2":{"74":1}}],["具体如下图",{"2":{"302":1}}],["具体如下图标号2",{"2":{"230":1}}],["具体如下图所示",{"2":{"103":1,"158":1,"161":1,"181":1,"185":1,"241":1,"271":1,"439":1,"445":1,"453":1,"498":1,"529":1}}],["具体如下",{"2":{"36":1,"85":1,"175":1,"183":1,"260":1,"267":1,"316":1,"451":1,"520":1,"530":1,"734":1,"736":1,"762":1,"1336":1}}],["具体应用掩码矩阵的代码位于",{"2":{"67":1}}],["具体参见下图右侧部分",{"2":{"734":1}}],["具体参见下图中间部分",{"2":{"734":1}}],["具体参见下图",{"2":{"53":1,"210":1,"323":1,"348":1}}],["具体就是求出q",{"2":{"36":1}}],["具体多少要视模型规模",{"2":{"20":1}}],["针对特定简单任务的时候人工提取特征会简单有效",{"2":{"1470":1}}],["针对性能",{"2":{"1228":1}}],["针对性地扩充词表",{"2":{"560":1}}],["针对大型模型的标准分片将单个键头和值头复制了模型分区的数量",{"2":{"937":1,"953":1}}],["针对原始attention层中第i个token的query向量和第j个token的key向量进行相乘操作",{"2":{"765":1}}],["针对绝对位置编码的公式的四项",{"2":{"764":1}}],["针对这个弊端",{"2":{"727":1}}],["针对word",{"2":{"716":1}}],["针对推理架构进行优化",{"2":{"629":1}}],["针对稀有字符",{"2":{"608":1}}],["针对当前词表",{"2":{"602":1}}],["针对当前向量",{"2":{"158":1}}],["针对每一个单词",{"2":{"587":1}}],["针对每个样本自己算均值和方差",{"2":{"322":1}}],["针对本例",{"2":{"445":1}}],["针对分层中的每一层可能都会起到不同的作用这点",{"2":{"437":1}}],["针对第二种目的的掩码",{"2":{"382":1}}],["针对第一种目的的掩码",{"2":{"382":1}}],["针对单个训练样本进行上计算均值和方差",{"2":{"322":1}}],["针对数学",{"2":{"221":1}}],["针对同一块内的标记处理",{"2":{"204":1}}],["针对",{"2":{"121":2,"130":1,"140":1,"341":1,"477":1}}],["针对上方句子的例子",{"2":{"5":1}}],["常心",{"2":{"2056":1}}],["常相守是个考验",{"2":{"2056":1}}],["常相居在山谷丛林中",{"2":{"713":1}}],["常规构造函数",{"2":{"1887":1}}],["常量引用传递",{"2":{"1729":1}}],["常量成员函数",{"2":{"1640":1}}],["常量成员变量",{"2":{"1640":1}}],["常量成员的值在初始化后不能被修改",{"2":{"1640":1}}],["常量成员是指在声明时使用",{"2":{"1640":1}}],["常量成员",{"0":{"1640":1}}],["常量可以使用",{"2":{"1628":1}}],["常量整数",{"2":{"1614":2}}],["常量指针",{"2":{"1614":1}}],["常量指针和指针常量",{"0":{"1614":1}}],["常量用于表示程序中不应该被改变的值",{"2":{"1613":1}}],["常量的值在声明后不能被修改",{"2":{"1613":1}}],["常量",{"2":{"1613":1,"1632":1,"1642":1}}],["常将神经网络层",{"2":{"785":1}}],["常用操作",{"2":{"2062":1}}],["常用特殊目标",{"2":{"1917":1}}],["常用方法",{"0":{"1813":1,"1816":1,"1831":1,"1834":1}}],["常用方法有指数衰减",{"2":{"402":1}}],["常用来实现元素整体的转换",{"2":{"1741":1}}],["常用转义字符",{"0":{"1616":1}}],["常用于电影和高质量渲染",{"2":{"2009":1}}],["常用于",{"2":{"1868":1}}],["常用于以下场景",{"2":{"1650":1}}],["常用于循环中",{"2":{"1607":1}}],["常用于压缩算法",{"2":{"637":1}}],["常用的修改算法",{"0":{"1740":1},"1":{"1741":1,"1742":1,"1743":1,"1744":1},"2":{"1732":1}}],["常用的非修改算法",{"0":{"1735":1},"1":{"1736":1,"1737":1,"1738":1,"1739":1},"2":{"1732":1}}],["常用的异步通信函数如mpi",{"2":{"1574":1}}],["常用的函数包括mpi",{"2":{"1573":1}}],["常用的subword分词算法有如下三种",{"2":{"596":1}}],["常用目录",{"0":{"1506":1}}],["常用设计模式",{"2":{"1500":1}}],["常用进程同步原语实现数据同步",{"2":{"1408":1}}],["常用功能",{"0":{"1209":1},"1":{"1210":1,"1211":1,"1212":1,"1213":1,"1214":1}}],["常用且较为完善的优化器包括",{"2":{"1130":1}}],["常用算子dropout和bachnorm",{"2":{"662":1}}],["常用词应该保持原状",{"2":{"567":1}}],["常用normmalization",{"2":{"361":1}}],["常数距离特点让自注意力机制没有了递归的限制",{"2":{"274":1}}],["常数距离特点也让自注意力机制没有",{"2":{"274":1}}],["常识",{"2":{"121":1}}],["常见网络问题",{"0":{"2091":1}}],["常见命令总结",{"0":{"1996":1}}],["常见不稳定模式的潜在修复方式",{"0":{"1180":1}}],["常见技巧",{"2":{"1161":1}}],["常见问题",{"2":{"1161":1}}],["常见问题的回答",{"0":{"1170":1},"1":{"1171":1,"1172":1,"1173":1,"1174":1,"1175":1,"1176":1,"1177":1,"1178":1,"1179":1,"1180":1,"1181":1,"1182":1,"1183":1,"1184":1,"1185":1,"1186":1,"1187":1,"1188":1,"1189":1,"1190":1,"1191":1,"1192":1,"1193":1},"2":{"1125":1}}],["常见tokenizer",{"0":{"569":1}}],["常见数据集",{"0":{"367":1}}],["常见的智能指针",{"2":{"1695":1}}],["常见的编译器有",{"2":{"1605":1}}],["常见的神经网络是形如下图所示的层级结构",{"2":{"1464":1}}],["常见的评估方法可能会因为评估不够频繁而错过这些问题",{"2":{"1179":1}}],["常见的正则化方法",{"0":{"1013":1},"1":{"1014":1,"1015":1,"1016":1,"1017":1,"1018":1,"1019":1,"1020":1}}],["常见的因素如下",{"2":{"568":1}}],["常见的权重初始化方法包括xavier初始化",{"2":{"403":1}}],["常见的做法是把",{"2":{"399":1}}],["常见的解释是layer",{"2":{"320":1}}],["常见的对softmax改进方法可以大致区分为两类",{"2":{"185":1}}],["常见的相似度计算有点积",{"2":{"175":1}}],["常见的mask操作有两种",{"2":{"50":1}}],["常见函数",{"0":{"103":1},"2":{"96":1}}],["稠密连接意味着参数量的增加",{"2":{"119":1}}],["反正学校他们是免费的嘛",{"2":{"2054":1}}],["反过来调用",{"2":{"1645":1}}],["反斜杠",{"2":{"1616":2}}],["反缩进",{"2":{"1559":1}}],["反缩进选择内容",{"2":{"1551":1}}],["反之亦然",{"2":{"1324":1}}],["反之则沿用",{"2":{"141":1}}],["反序列化时触发",{"2":{"1227":1}}],["反映了损失函数在当前状态下沿着梯度方向的变化程度",{"2":{"1179":1}}],["反映了神经科学和计算生物学中一个公认的原理",{"2":{"222":1}}],["反余弦",{"2":{"1087":1}}],["反卷积",{"0":{"779":1},"2":{"779":3}}],["反向过程为",{"2":{"1392":1}}],["反向钩子函数展示",{"0":{"1213":1}}],["反向求导原理",{"0":{"1107":1}}],["反向",{"2":{"1105":2}}],["反向时变为",{"2":{"1004":1}}],["反向时句子顺序需要倒序吗",{"2":{"858":1}}],["反向推导过程",{"0":{"1004":1}}],["反向公式推导类似",{"2":{"1000":1}}],["反向执行以上过程即可将压缩的编码复原",{"2":{"575":1}}],["反向传播总结",{"0":{"1450":1}}],["反向传播目的确认",{"0":{"1445":1}}],["反向传播数学推导",{"0":{"1444":1},"1":{"1445":1,"1446":1,"1447":1,"1448":1,"1449":1}}],["反向传播的目的是求",{"2":{"1441":1}}],["反向传播的定义",{"0":{"1439":1}}],["反向传播的梯度会变为",{"2":{"188":1}}],["反向传播过程",{"0":{"1390":1},"1":{"1391":1,"1392":1,"1393":1,"1394":1}}],["反向传播过程中",{"2":{"1208":2}}],["反向传播过程中的梯度逐渐变小",{"2":{"296":1}}],["反向传播和优化",{"2":{"1295":1}}],["反向传播钩子函数",{"2":{"1213":1}}],["反向传播用到的函数",{"2":{"1107":1}}],["反向传播算法才被提出",{"2":{"1443":1}}],["反向传播算法",{"0":{"1105":1}}],["反向传播算法通过计算每一层的梯度来更新模型中的权重",{"2":{"484":1}}],["反向传播算法通过计算每一层的梯度",{"2":{"148":1}}],["反向传播是的线性变换为",{"2":{"1004":1}}],["反向传播是将链式法则应用于计算导数并更新深度学习网络模型权重的过程",{"2":{"484":1}}],["反向传播比正向传播更简单",{"2":{"964":1}}],["反向传播应用了平铺",{"2":{"964":1}}],["反向传播还实现了2",{"2":{"964":1}}],["反向传播中",{"2":{"964":1}}],["反向传播通常需要矩阵",{"2":{"946":1,"966":1}}],["反向传播角度",{"0":{"483":1},"1":{"484":1,"485":1}}],["反向传播也需要执行额外的逻辑操作等",{"2":{"396":1}}],["反向传播",{"0":{"148":1},"2":{"96":1,"994":1,"1096":1,"1213":1,"1438":1,"1439":1,"1441":1}}],["反应了该token与序列中其它token的综合关系",{"2":{"437":1}}],["反时限衰减",{"2":{"402":1}}],["反而降低性能",{"2":{"1709":1}}],["反而就越小",{"2":{"692":1}}],["反而增加了模型的复杂性和训练的难度",{"2":{"512":1}}],["反而更加突出残差分支",{"2":{"335":1}}],["反而会使优化过程不稳定",{"2":{"333":1}}],["反而是稠密连接的fnn有更大的优势",{"2":{"119":1}}],["反推或者猜测",{"2":{"235":1}}],["反事实观念在",{"2":{"140":1}}],["捕获只能捕获当前作用域中已存在的变量",{"2":{"1907":1}}],["捕获表达式",{"0":{"1907":1},"2":{"1904":1}}],["捕获外部变量",{"2":{"1883":1}}],["捕获其他所有类型的异常",{"2":{"1762":1}}],["捕获",{"2":{"1762":1}}],["捕获模型的执行轨迹",{"2":{"1292":1}}],["捕获更复杂的模式和细微差别",{"2":{"119":1}}],["捕捉长期依赖",{"2":{"250":1}}],["捕捉所有标记的直接依赖关系",{"2":{"227":1}}],["捕捉注意力的能力就会受限",{"2":{"172":1}}],["捕捉丰富的特征和依赖关系",{"2":{"1":1}}],["参见",{"2":{"1130":1,"1131":1,"1137":1}}],["参见下图",{"2":{"418":1}}],["参照系在新皮质中无处不在",{"2":{"754":1}}],["参与运算的所有文本序列头尾相接",{"2":{"377":1}}],["参与归一化的数据分别是所有的身高数据和体重数据",{"2":{"316":1}}],["参与注意力的包括局部注意力",{"2":{"204":1}}],["参数包展开等",{"2":{"1912":1}}],["参数就像函数的",{"2":{"1729":1}}],["参数名2",{"2":{"1729":1}}],["参数名1",{"2":{"1729":1}}],["参数类型2",{"2":{"1729":1}}],["参数类型1",{"2":{"1729":1}}],["参数类型为map",{"2":{"1485":1}}],["参数传递",{"2":{"1729":1}}],["参数传递等关键要素",{"2":{"1727":1}}],["参数传param的时候的传递和打包方式",{"2":{"666":1}}],["参数调用",{"2":{"1701":1}}],["参数列表",{"2":{"1693":1,"1699":2,"1712":1,"1729":1}}],["参数列表不同",{"2":{"1663":1,"1675":1}}],["参数化构造函数",{"2":{"1675":2}}],["参数化与标准化",{"2":{"361":1}}],["参数数量或类型不同",{"2":{"1663":1}}],["参数等信息会被放入栈中",{"2":{"1648":1}}],["参数直接传递map",{"2":{"1485":1}}],["参数是用户传递给",{"2":{"1227":1}}],["参数是正在使用的优化器实例",{"2":{"1227":2}}],["参数id可能看起来像索引",{"2":{"1227":1}}],["参数需要以具有确定性顺序且在运行之间保持一致的集合形式进行指定",{"2":{"1225":1}}],["参数用于指定一个元类",{"2":{"1082":1}}],["参数更新",{"2":{"1227":1}}],["参数更新方向就是将两者矢量相加的方向",{"2":{"1036":1}}],["参数更新的幅度也会很大",{"2":{"991":1}}],["参数范数惩罚",{"0":{"1014":1}}],["参数难以被更新",{"2":{"995":1}}],["参数初始化必要条件二",{"2":{"998":1}}],["参数初始化必要条件一",{"2":{"998":1}}],["参数初始化要确保信息能够顺利传递",{"2":{"998":1}}],["参数初始化的必要条件",{"0":{"998":1}}],["参数初始化的重要性",{"0":{"989":1},"1":{"990":1,"991":1}}],["参数初始化",{"2":{"988":1}}],["参数初始化概念",{"0":{"988":1}}],["参数控制着函数的斜率变化",{"2":{"843":1}}],["参数少",{"2":{"838":1}}],["参数共享的特殊形式使得神经网络层具有对平移等变",{"2":{"772":1}}],["参数共享",{"2":{"772":1}}],["参数共享等技术来减少模型的参数量",{"2":{"512":1}}],["参数矩阵中每一个单独的参数都描述了一个输入单元与一个输出单元间的交互",{"2":{"772":1}}],["参数token的数量",{"2":{"621":1}}],["参数token化方法",{"2":{"617":1}}],["参数注意力块的query来自自注意力块的输出",{"2":{"620":1}}],["参数注意力块的键和值部分",{"2":{"620":1}}],["参数注意力块替代了ffn",{"2":{"620":1}}],["参数注意力的新组件",{"2":{"620":1}}],["参数代表token",{"2":{"620":1}}],["参数交互",{"2":{"618":1}}],["参数的类型",{"2":{"1707":1}}],["参数的函数的指针",{"2":{"1706":1}}],["参数的模型验证了这些预测",{"2":{"561":1}}],["参数的共享可能不会带来优势",{"2":{"119":1}}],["参数过多的另一个副作用就是模型无法学习到更高层级的有效特征",{"2":{"512":1}}],["参数阈值的牢笼",{"2":{"512":1}}],["参数会推动自旋系统集合的级联反应行为",{"2":{"508":1}}],["参数为词表大小和词嵌入维度",{"2":{"701":1}}],["参数为",{"2":{"450":1}}],["参数为tensor倒数第二维的长度",{"2":{"315":1}}],["参数为tensor最里面一维的长度",{"2":{"315":1}}],["参数x是输入张量",{"2":{"343":1}}],["参数memory",{"2":{"200":1}}],["参数大小的扩展直接与计算和能量需求的增加相关",{"2":{"154":1}}],["参数应用即",{"2":{"122":1}}],["参数",{"0":{"448":1},"2":{"122":3,"384":1,"558":1,"935":1,"951":1,"1208":1,"1441":1}}],["参数量大大减少",{"2":{"698":1}}],["参数量从",{"2":{"698":1}}],["参数量比例接近",{"2":{"119":1}}],["参数量对于大模型至关重要",{"2":{"119":1}}],["参数占据了模型参数的绝大部分",{"2":{"119":1}}],["参考点",{"2":{"1821":1,"1839":1}}],["参考代码",{"0":{"1665":1}}],["参考实现",{"2":{"1625":1,"1914":1}}],["参考实现如下",{"2":{"1608":1,"1710":1,"1729":1,"1902":1}}],["参考资料",{"0":{"1425":1,"1501":1},"2":{"2045":1}}],["参考3",{"2":{"1378":1}}],["参考4",{"2":{"1365":1}}],["参考torch",{"2":{"1083":2}}],["参考网站",{"2":{"910":1}}],["参考下一课时",{"2":{"817":1}}],["参考文献3",{"2":{"876":1,"910":1,"1473":1}}],["参考文献2",{"2":{"876":1,"910":1,"1473":1}}],["参考文献1",{"2":{"876":1,"910":1,"1473":1}}],["参考文献",{"0":{"910":1,"1010":1,"1067":1,"1378":1,"1473":1},"2":{"784":1}}],["参考链接",{"0":{"784":1,"837":1,"849":1,"876":1,"950":1,"987":1,"1365":1,"1367":1},"2":{"949":1,"950":1,"979":1,"987":1,"1035":1,"1088":1,"1090":1,"1325":1,"1326":1,"1345":1,"1365":1}}],["参考系是人脑中的重要部分",{"2":{"754":1}}],["参考系是一种信息在大脑中的存储结构",{"2":{"754":1}}],["参考系还能衍生到一些抽象的概念",{"2":{"754":1}}],["参考系不仅仅为实物建模",{"2":{"754":1}}],["参考系与思考",{"2":{"754":1}}],["参考系与建模",{"2":{"754":1}}],["参考系与存储",{"2":{"754":1}}],["参考系与新皮质",{"2":{"754":1}}],["参考将在各个篇幅的具体部分中给出",{"2":{"432":1}}],["参考人类学习的方式",{"2":{"143":1}}],["参考",{"0":{"47":1,"95":1,"156":1,"233":1,"292":1,"361":1,"387":1,"429":1,"513":1,"543":1,"638":1,"740":1,"768":1},"2":{"0":1,"49":1,"96":1,"157":1,"293":1,"741":1,"1089":2,"1723":2,"2043":1}}],["没太懂",{"2":{"662":1}}],["没什么意义",{"2":{"582":1}}],["没法直接做矩阵乘法",{"2":{"172":1}}],["没法加速",{"2":{"118":1}}],["没有追赶的",{"2":{"2134":1}}],["没有其他点可以更高",{"2":{"2115":1}}],["没有具体的模板",{"2":{"2115":1}}],["没有错误发生",{"2":{"1814":1,"1832":1}}],["没有动态分配开销",{"2":{"1802":1}}],["没有函数体的虚函数",{"2":{"1693":1}}],["没有定义",{"2":{"1685":1}}],["没有参数",{"2":{"1676":1}}],["没有参数的构造函数",{"2":{"1675":1}}],["没有返回类型",{"2":{"1675":2,"1676":1}}],["没有作用域",{"2":{"1632":1}}],["没有类型检查",{"2":{"1632":1}}],["没有小数部分的数字",{"2":{"1607":1}}],["没有顺序要求",{"2":{"1485":1}}],["没有任何第三方依赖",{"2":{"1479":1}}],["没有任何联系",{"2":{"681":1}}],["没有开源",{"2":{"1316":1}}],["没有发现一致的优势",{"2":{"1152":1}}],["没有在搜索空间中采样足够的点",{"0":{"1148":1}}],["没有一个优化器是适用于所有类型的机器学习问题和模型架构的",{"2":{"1130":1}}],["没有实际存储",{"2":{"1110":1}}],["没有残差",{"2":{"1001":1}}],["没有softmax重新缩放",{"2":{"964":1}}],["没有考虑位置编码的原始分数",{"2":{"758":1}}],["没有考虑向量的大小",{"2":{"692":1}}],["没有上下文参照",{"2":{"715":1}}],["没有相似性或上下文的概念",{"2":{"681":1}}],["没有词汇表外的标记",{"2":{"606":1}}],["没有明确的",{"2":{"542":1}}],["没有真正的",{"2":{"525":1}}],["没有填充词",{"2":{"428":1}}],["没有能量的多少",{"2":{"334":1}}],["没有layernorm",{"2":{"320":1}}],["没有layernorm情况下",{"2":{"320":1}}],["没有展示还原这一步",{"2":{"313":1}}],["没有把编码器",{"2":{"282":1}}],["没有被压缩",{"2":{"273":1}}],["没有被校准",{"2":{"128":1}}],["没有",{"2":{"212":1,"381":1,"1117":1,"1639":1}}],["没有冗余计算",{"2":{"90":1}}],["没有区分不同",{"2":{"90":1}}],["没有重叠",{"2":{"41":1}}],["没有偏置",{"2":{"8":4}}],["真心希望大家都能够好好静下心来问问自己",{"2":{"2056":1}}],["真诚待客",{"2":{"2051":1}}],["真诚靠谱",{"2":{"2051":1}}],["真诚",{"2":{"2051":1}}],["真变假",{"2":{"1619":1}}],["真",{"2":{"1607":1}}],["真实的实体个数",{"2":{"1331":1}}],["真实的标签序列",{"2":{"423":1}}],["真实路径得分计算",{"0":{"1328":1}}],["真实数据举例",{"2":{"922":1}}],["真实特征",{"2":{"118":1}}],["真值标签是目标序列",{"2":{"407":1}}],["真值",{"2":{"405":1,"407":1,"427":1}}],["真正可怕的或者说真正可贵的",{"2":{"2056":1}}],["真正能够参悟孔子的大部分思想",{"2":{"2054":1}}],["真正例个数",{"2":{"1331":1}}],["真正例",{"2":{"1331":1}}],["真正乘以自注意力分布上的却是指数的衰减",{"2":{"765":1}}],["真正",{"2":{"284":1}}],["真正的突破发生在dzmitry",{"2":{"284":1}}],["真正学到的知识或者信息大多都存储在",{"2":{"118":1}}],["稀疏矩阵",{"2":{"1086":2}}],["稀疏张量",{"2":{"1086":1}}],["稀疏性再述",{"2":{"841":1}}],["稀疏性允许模型消除叠加现象带来的歧义",{"2":{"118":1}}],["稀疏连接图示",{"2":{"841":1}}],["稀疏连接",{"2":{"772":1}}],["稀疏交互",{"2":{"772":1}}],["稀疏表征映射",{"2":{"733":1}}],["稀疏向量",{"2":{"711":1}}],["稀疏探测是定位此类神经元",{"2":{"477":1}}],["稀疏探测",{"0":{"477":1}}],["稀疏注意力",{"2":{"204":1}}],["稀疏激活的记忆层补充了计算量大的密集前馈层",{"2":{"154":1}}],["稀疏自编码器特征可用于干预和引导transformer的内容生成",{"2":{"137":1}}],["稀疏自编码器能产生相对通用的特征",{"2":{"137":1}}],["稀疏自编码器能产生可解释的特征",{"2":{"137":1}}],["稀疏自编码器能提取相对单一的语义特征",{"2":{"137":1}}],["稀疏自编码器是一种特殊的字典学习方法",{"2":{"137":1}}],["稀疏就意味着我们用较小的矩阵就可以来合较大的稀疏矩阵",{"2":{"14":1}}],["给我们一种感觉",{"2":{"2051":1}}],["给室友基本说了一下",{"2":{"2051":1}}],["给你一个",{"2":{"1634":1}}],["给文件所有者增加执行权限",{"2":{"1513":1}}],["给数据库增加一个用户",{"2":{"1486":1}}],["给到父类",{"2":{"1208":1}}],["给这个新token设置对应的索引",{"2":{"592":1}}],["给模型进行前向传播",{"2":{"538":1}}],["给序列中的每个位置添加一个位置编码",{"2":{"455":1,"698":1}}],["给出了文本embedding的历史",{"2":{"711":1}}],["给出了ffn高效设计的方法",{"2":{"150":1}}],["给出",{"2":{"485":1}}],["给出矩阵形式以简化计算",{"2":{"209":1}}],["给出的一些思考点是",{"2":{"175":1}}],["给体现出来了",{"2":{"170":1}}],["给定四个数字",{"2":{"2123":1}}],["给定直线的两个端点p0",{"2":{"2018":1}}],["给定输入的",{"2":{"942":1,"959":1}}],["给定输入序列",{"2":{"906":1,"941":1,"960":1}}],["给定输入",{"2":{"502":1}}],["给定输入张量x",{"2":{"360":1}}],["给定输入x和键k",{"2":{"125":1}}],["给定下一个",{"2":{"230":1}}],["给定一个文本文件",{"2":{"1933":1}}],["给定一个整数列表",{"2":{"1759":1}}],["给定一个句子输入x",{"2":{"727":2}}],["给定一个归一化的动作块",{"2":{"637":1}}],["给定一个神经层",{"2":{"309":1}}],["给定一个输入序列",{"2":{"501":1}}],["给定一个输入",{"2":{"230":1}}],["给定一个包含",{"2":{"178":1}}],["给定一个查询",{"2":{"154":1}}],["给定一个关系事实",{"2":{"135":1}}],["给定足够的规模",{"2":{"154":1}}],["给定层的原始输入和新目标",{"2":{"148":1}}],["给定任意的激活空间中的向量",{"2":{"118":1}}],["给各个head加个权值",{"2":{"20":1}}],["甚至是自定义的结构体类型",{"2":{"1729":1}}],["甚至可以在不同的程序中复用",{"2":{"1729":1}}],["甚至可能会对下游任务效果产生负面影响",{"2":{"560":1}}],["甚至可能更好",{"2":{"14":1}}],["甚至传递函数本身",{"2":{"1706":1}}],["甚至没有",{"2":{"1675":1}}],["甚至进行二次开发",{"2":{"1602":1}}],["甚至为通用人工智能",{"2":{"1318":1}}],["甚至会导致过拟合",{"2":{"1154":1}}],["甚至会导致梯度弥散或爆炸",{"2":{"990":1}}],["甚至1m",{"2":{"977":1}}],["甚至",{"2":{"506":1}}],["甚至略有提升",{"2":{"346":1}}],["甚至本任务也有一定程度的下降",{"2":{"222":1}}],["甚至即使去掉mlp层也是如此",{"2":{"147":1}}],["甚至使模型难以有效传递信息",{"2":{"116":1}}],["限制其访问范围",{"2":{"1655":1}}],["限制其运算能力的任何实体或是虚拟的组成元件",{"2":{"1411":1}}],["限制全局变量的作用域",{"2":{"1649":1}}],["限制因素通常是加速器",{"2":{"1132":1}}],["限制模型",{"2":{"1014":1}}],["限制输出的多样性",{"2":{"507":1}}],["限制计算复杂度",{"2":{"116":1}}],["限制了指针的使用",{"2":{"1611":1}}],["限制了网络几何变换建模的能力",{"2":{"781":1}}],["限制了大语言模型的最大序列长度n的大小",{"2":{"279":1}}],["限制了模型的上下文长度",{"2":{"227":1}}],["限制了模型的表达能力和进一步利用模型深度的能力",{"2":{"91":1}}],["限制了模型的理解和表达能力",{"2":{"3":1}}],["限制了灵活性和表达能力",{"2":{"45":1}}],["若在给定随机变量序列的",{"2":{"1322":1}}],["若没进行xxx",{"2":{"659":1}}],["若干堆叠的transformer",{"2":{"431":1}}],["若p",{"2":{"199":2}}],["若注意力为多头自注意力",{"2":{"199":2}}],["若注意力为单头自注意力",{"2":{"199":2}}],["若存在掩码矩阵",{"2":{"173":1}}],["若仅使用线性基",{"2":{"116":1}}],["若无意义则为true",{"2":{"66":1}}],["记住一个人的好",{"2":{"2056":1}}],["记住该记住的",{"2":{"863":1}}],["记得还要起点",{"2":{"2023":1}}],["记得释放动态分配的内存",{"2":{"1706":1}}],["记为memory",{"2":{"537":1}}],["记录日志",{"2":{"2056":1}}],["记录当前idx",{"2":{"1330":2}}],["记录对应位置相应标签结尾的路径中",{"2":{"1330":1}}],["记录正确预测样本的数量",{"2":{"1165":1}}],["记录语义关系的数据也都需要向量化",{"2":{"680":1}}],["记录处理过的token数",{"2":{"385":1}}],["记录样本数量",{"2":{"385":1}}],["记录step次数",{"2":{"385":1}}],["记录了人类海量的语言实例",{"2":{"116":1}}],["记作memory",{"2":{"428":1}}],["记作",{"2":{"313":1}}],["记忆技巧",{"2":{"1612":1,"1614":1}}],["记忆力",{"2":{"850":1}}],["记忆作为层",{"2":{"231":1}}],["记忆作为门",{"2":{"231":1}}],["记忆作为上下文",{"2":{"231":1}}],["记忆所有",{"2":{"231":1}}],["记忆模块是一个元模型",{"2":{"230":3}}],["记忆的检索",{"2":{"228":1}}],["记忆的结构",{"2":{"228":1}}],["记忆的获取",{"2":{"228":1}}],["记忆的偏好",{"2":{"123":1}}],["记忆值在嵌入维度上进行分片",{"2":{"154":1}}],["记忆层是记忆密集型的",{"2":{"154":1}}],["记忆层在键和值的数量方面通常具有更大的规模",{"2":{"154":1}}],["记忆层中的键和值是可训练参数",{"2":{"154":1}}],["记忆层与注意力层之间存在两个区别",{"2":{"154":1}}],["记忆层",{"2":{"154":1}}],["记忆系数",{"2":{"129":1}}],["记忆就是重塑神经元之间的连接",{"2":{"123":1}}],["记忆",{"2":{"121":1,"230":1,"248":1,"863":1}}],["记忆只能隐式地表达在参数当中",{"2":{"118":1}}],["记忆网络的值向量表示的是输出词汇的分布",{"2":{"128":1}}],["记忆网络使用",{"2":{"125":1}}],["记忆网络和注意力机制很类似",{"2":{"125":1}}],["记忆网络",{"0":{"125":1},"2":{"96":1,"126":1}}],["架构设计模式",{"2":{"1937":1}}],["架构性能的位置编码方式",{"2":{"1341":1}}],["架构和迁移学习",{"2":{"1316":1}}],["架构更高效",{"2":{"1315":1}}],["架构参数以及我们在深度学习中调整的所有其他参数",{"2":{"1185":1}}],["架构改进",{"0":{"730":1},"1":{"731":1,"732":1,"733":1}}],["架构没有此问题",{"2":{"542":1}}],["架构网络不是均匀对称的",{"2":{"542":1}}],["架构选择",{"0":{"542":1}}],["架构的两个模块",{"2":{"1317":1}}],["架构的两个关键组成部分",{"2":{"499":1}}],["架构的主要区别是在每个",{"2":{"614":1}}],["架构的三驾马车",{"2":{"115":1}}],["架构是自回归的",{"2":{"414":1}}],["架构是现代语言模型的基础",{"2":{"351":1}}],["架构",{"0":{"619":1,"698":1,"714":1,"887":1},"1":{"620":1,"621":1,"622":1},"2":{"151":1,"215":1,"541":2,"542":1,"909":1}}],["架构下的ffn在形式上高度类似于记忆神经网络",{"2":{"126":1}}],["架构图",{"0":{"7":1},"1":{"8":1,"9":1,"10":1},"2":{"0":1}}],["代替",{"2":{"1083":1,"1330":1}}],["代数可以用来描述模型的参数和前向传播",{"2":{"505":1}}],["代表企业",{"0":{"1939":1,"1944":1,"1949":1,"1954":1,"1959":1}}],["代表函数期望接收的输入值",{"2":{"1729":1}}],["代表我们期望的那个值",{"2":{"1398":1}}],["代表一共有3个标签",{"2":{"1323":1}}],["代表一种学好的",{"2":{"623":1}}],["代表相应的标签序列",{"2":{"1323":1}}],["代表相关向量",{"2":{"265":1}}],["代表输入变量",{"2":{"1323":1}}],["代表负无穷",{"2":{"933":1}}],["代表作是论文",{"2":{"747":2}}],["代表工作为prompteol",{"2":{"731":1}}],["代表这个词应该紧跟在前面的那个词之后来组成一个完整的词",{"2":{"567":1}}],["代表自注意力",{"2":{"542":1}}],["代表",{"2":{"540":1,"621":1,"845":1,"899":1}}],["代表encoder",{"2":{"540":1}}],["代表decoder",{"2":{"540":1}}],["代表把若干具有相同结构的层堆叠起来",{"2":{"436":1}}],["代表什么呢",{"2":{"246":1}}],["代表句子语言特点和含义的特征信息",{"2":{"241":1}}],["代表方案是增加参数",{"2":{"141":1}}],["代表方案是提示工程和知识检索",{"2":{"141":1}}],["代表的向量进行了加权平均",{"2":{"117":1}}],["代表来自上一层的输出",{"2":{"113":1}}],["代码结构",{"2":{"1999":1}}],["代码规模",{"2":{"1916":1}}],["代码更简洁",{"2":{"1911":1}}],["代码组织",{"2":{"1848":1}}],["代码重用",{"2":{"1848":1}}],["代码冗余",{"2":{"1761":1}}],["代码解释",{"2":{"1729":1}}],["代码的组织者和复用者",{"0":{"1729":1}}],["代码的关键",{"2":{"1607":1}}],["代码块结束时销毁",{"2":{"1649":1}}],["代码简洁",{"2":{"1646":1}}],["代码复用",{"2":{"1632":1}}],["代码复现步骤",{"0":{"1347":1}}],["代码示例",{"0":{"1691":1},"2":{"1659":1,"1660":1,"1661":1,"1662":1,"1663":1,"1667":1,"1668":1,"1670":1,"1671":1,"1672":1,"1673":1,"1674":1,"1675":1,"1676":1,"1677":1,"1693":3,"1694":2,"1695":4,"1698":1,"1699":1,"1700":1,"1701":1,"1718":1,"1719":6,"1720":4,"1721":4,"1722":4,"1723":3,"1724":3,"1725":4,"1761":1,"1762":1,"1763":1,"1797":1,"1805":1,"1806":1,"1807":1}}],["代码示例解释",{"2":{"1607":1}}],["代码示例分析",{"2":{"1607":1}}],["代码翻译成汇编语言",{"2":{"1604":1}}],["代码需要经过一个",{"2":{"1604":1}}],["代码在运行时逐行解释执行",{"2":{"1602":1}}],["代码在运行前会被完整地编译成机器可以直接执行的二进制代码",{"2":{"1602":1}}],["代码优化",{"2":{"1500":1}}],["代码演示",{"0":{"1481":1}}],["代码和手动设置参数以及获取结果集的过程",{"2":{"1476":1}}],["代码介绍",{"0":{"1346":1}}],["代码详解",{"0":{"1327":1},"1":{"1328":1,"1329":1,"1330":1,"1331":1}}],["代码生成和优化等步骤",{"2":{"1288":1}}],["代码展示",{"0":{"1296":1,"1297":1,"1398":1},"2":{"1221":1}}],["代码案例2",{"2":{"1205":1}}],["代码案例1",{"2":{"1205":1}}],["代码案例",{"0":{"1205":1}}],["代码审核",{"0":{"1198":1}}],["代码实践之",{"0":{"1079":1,"1080":1}}],["代码实现",{"0":{"1039":1,"1091":1,"1345":1,"2153":1},"1":{"1092":1,"1093":1,"1094":1,"1095":1,"1096":1,"1097":1,"1098":1,"1099":1,"1100":1,"1101":1,"1102":1},"2":{"933":1,"1933":1}}],["代码实现非常简单",{"2":{"113":1}}],["代码仓库",{"2":{"1346":1}}],["代码仓中找到这些方法",{"2":{"1248":1}}],["代码仓",{"2":{"1073":1}}],["代码地址",{"2":{"946":1,"958":1}}],["代码作是论文",{"2":{"747":1}}],["代码逻辑",{"0":{"538":1}}],["代码中是memory变量",{"2":{"533":1}}],["代码中把label命名为tgt",{"2":{"381":1}}],["代码是否能编译",{"2":{"2006":2}}],["代码是按照下图这个顺序实现的",{"2":{"344":1}}],["代码是view",{"2":{"36":1}}],["代码",{"0":{"199":1,"421":1,"428":1,"723":1},"1":{"422":1,"423":1,"424":1},"2":{"157":1,"950":1,"987":1,"1666":1,"1932":1}}],["代码依然使用同一套",{"2":{"69":1}}],["代码如下",{"2":{"39":1}}],["代码上更清晰一点",{"2":{"32":1}}],["代码之中没有切分线性层权重wq",{"2":{"24":1}}],["置换内外循环位置",{"2":{"972":1}}],["置零比率",{"2":{"533":1}}],["置0比率",{"2":{"113":1,"523":1}}],["置为",{"2":{"71":1}}],["置为负无穷",{"2":{"71":1}}],["隐空间对应相同标签的点离得比较近",{"2":{"1375":1}}],["隐空间中对应不同标签的点不会离得很远",{"2":{"1375":1}}],["隐空间随便拿个点解码之后",{"2":{"1375":1}}],["隐空间",{"2":{"1375":1}}],["隐式",{"2":{"1924":1}}],["隐式转换",{"0":{"1684":1}}],["隐式类型转换",{"2":{"1629":2}}],["隐式创建",{"2":{"661":1,"1104":1}}],["隐式增加batch",{"2":{"659":1}}],["隐式函数到底是怎么被应用的",{"2":{"122":1}}],["隐式函数",{"2":{"122":3}}],["隐私擦除",{"2":{"364":1}}],["隐状态会输入到全连接层",{"2":{"450":1}}],["隐状态",{"2":{"273":1,"524":1}}],["隐状态长度固定",{"2":{"256":1}}],["隐状态的每一个维度都可以作为一个抽象出的特征",{"2":{"137":1}}],["隐向量将力有不逮",{"2":{"252":1}}],["隐向量还可以存储散文的全部语义信息",{"2":{"252":1}}],["隐变量模型是一种引入隐变量来表示过去信息的模型",{"2":{"240":1}}],["隐变量自回归模型",{"0":{"240":1}}],["隐藏的",{"0":{"1638":1}}],["隐藏层数",{"2":{"1149":1}}],["隐藏层中有一半是黑色的",{"2":{"860":1}}],["隐藏神经元的一个子集变得线性相关",{"2":{"305":1}}],["隐藏状态还用于进行预测",{"2":{"868":1}}],["隐藏状态包含了先前输入的信息",{"2":{"868":1}}],["隐藏状态按照下图传递",{"2":{"853":1}}],["隐藏状态在其中进行连续转换",{"2":{"496":1}}],["隐藏状态",{"2":{"227":1}}],["隐藏看不到或者不应该看到的",{"2":{"71":1}}],["隐层的神经元数量",{"2":{"113":1}}],["名词歧义",{"0":{"1660":1},"2":{"1658":1}}],["名词性",{"2":{"712":1}}],["名字",{"2":{"1485":1}}],["名为torch",{"2":{"1105":1}}],["名为positionwisefeedforward的类来实现前馈全连接层",{"2":{"113":1}}],["名存实亡",{"2":{"332":1}}],["名称与类名相同",{"2":{"1676":1}}],["名称相同即可",{"2":{"1663":1}}],["名称和元素的个数",{"2":{"1623":1}}],["名称",{"2":{"113":1,"535":1,"604":1}}],["ℎ",{"2":{"109":1,"141":1,"727":1}}],["∗valueiattention",{"2":{"170":2,"271":2}}],["∗",{"2":{"109":2}}],["控制对象间的赋值行为",{"2":{"1712":1}}],["控制流语句",{"2":{"1729":1}}],["控制流语句总结",{"0":{"1631":1}}],["控制流语句的原因",{"2":{"1113":1}}],["控制流",{"2":{"1491":1}}],["控制符",{"2":{"1316":1}}],["控制forward的调度",{"2":{"1214":1}}],["控制梯度计算",{"0":{"1095":1}}],["控制参数更新的步长",{"2":{"1023":1}}],["控制了候选结果的数量",{"2":{"904":1}}],["控制",{"2":{"352":1}}],["控制生成文本的随机性和创造性",{"2":{"187":1}}],["控制函数的形状",{"2":{"108":1}}],["控制注意力聚焦程度的另一种方法是通过温度项dqkdqkd",{"2":{"93":1}}],["σ​2​​",{"2":{"1361":1}}],["σ​1​​",{"2":{"1361":1}}],["σ2",{"2":{"1361":1}}],["σ1",{"2":{"1361":1}}],["σiy",{"2":{"313":1}}],["σiyik=",{"2":{"313":1}}],["σiσi",{"2":{"313":1}}],["σ",{"2":{"108":1,"109":1,"332":1}}],["βi",{"2":{"1340":1}}],["β​i",{"2":{"1340":1}}],["β​2​​",{"2":{"1130":1}}],["β​1​​",{"2":{"1130":1}}],["β2",{"2":{"1130":1}}],["β1",{"2":{"1130":1,"1174":2}}],["β",{"2":{"343":1,"845":2}}],["β来弥补归一化过程中损失掉的信息",{"2":{"319":1}}],["βx",{"2":{"108":2,"845":2}}],["ββ",{"2":{"107":2,"108":2,"343":1,"347":1}}],["且只构造一次animal",{"2":{"1869":1}}],["且只依赖于位置编号k",{"2":{"748":1}}],["且所有张量都是本地的并在",{"2":{"1228":1}}],["且假设更新的过程中权重的均值一直是0",{"2":{"1003":1}}],["且均值都为0",{"2":{"1000":1}}],["且weight正态分布时",{"2":{"843":1}}],["且模型训练的时间复杂度较高",{"2":{"839":1}}],["且其导数范围为",{"2":{"839":1}}],["且容易出现过拟合的情况",{"2":{"816":1}}],["且容易是稀疏矩阵",{"2":{"9":1}}],["且每个位置相互独立",{"2":{"745":1}}],["且每个子层使用的不用的参数",{"2":{"419":1}}],["且它们之间的相关性较低",{"2":{"739":1}}],["且计算结果也在同一个空间内",{"2":{"689":1}}],["且训练难以收敛",{"2":{"566":1}}],["且可以减少参数间的相互依赖",{"2":{"840":1}}],["且可以训练完全",{"2":{"334":1}}],["且可能导致词表有关参数的欠拟合",{"2":{"561":1}}],["且输入",{"2":{"504":1}}],["且使用到了大量的多领域的数据进行预训练",{"2":{"396":1}}],["且不能被继承",{"2":{"1793":1}}],["且不在",{"2":{"839":1}}],["且不包含可学习的仿射变换",{"2":{"359":1}}],["且不同图像",{"2":{"325":1}}],["且大多数情况下无需调整超参数",{"2":{"358":1}}],["且layernorm可以稳定训练",{"2":{"348":1}}],["且对于不同的",{"2":{"338":1}}],["且词序可能并不影响我们对句子的理解",{"2":{"323":1}}],["且概率值越来越大",{"2":{"306":1}}],["且需要学习额外的残差函数",{"2":{"301":1}}],["且具备参数",{"2":{"301":1}}],["且都用rnn来实现",{"2":{"282":1}}],["且从第二轮开始",{"2":{"239":1}}],["且由于仅微调奇异值尺度本身",{"2":{"221":1}}],["且如果模型的维度是512维",{"2":{"173":1}}],["且dk=dq=dvdk=dq=dvd",{"2":{"173":1}}],["且向量之间有一定的关系",{"2":{"158":1}}],["且有助于限制对机密或私人数据的回放",{"2":{"140":1}}],["且去掉偏置的glu变体",{"2":{"107":1}}],["φenc",{"2":{"727":2}}],["φ",{"2":{"106":4,"504":6,"727":1}}],["$^",{"2":{"1917":1}}],["$pythonpath",{"2":{"1309":1,"1332":1}}],["$$var",{"2":{"1003":1}}],["$$",{"2":{"908":1,"1003":1}}],["$$argmax",{"2":{"908":1}}],["$来学习一个函数",{"2":{"230":1}}],["$",{"2":{"106":1,"109":1,"230":1,"1082":1,"1547":1,"1607":1,"1917":16,"1976":1,"1987":1,"1989":1,"1993":1}}],["高中的恩师田老师曾经告诉我们",{"2":{"2054":1}}],["高级渲染技术",{"2":{"2009":1}}],["高级用法",{"0":{"1983":1},"1":{"1984":1,"1985":1,"1986":1,"1987":1}}],["高级语言",{"2":{"1603":1}}],["高性能部署之cmake工程管理",{"0":{"1962":1},"1":{"1963":1,"1964":1,"1965":1,"1966":1,"1967":1,"1968":1,"1969":1,"1970":1,"1971":1,"1972":1,"1973":1,"1974":1,"1975":1,"1976":1,"1977":1,"1978":1,"1979":1,"1980":1,"1981":1,"1982":1,"1983":1,"1984":1,"1985":1,"1986":1,"1987":1,"1988":1,"1989":1,"1990":1,"1991":1,"1992":1,"1993":1,"1994":1,"1995":1,"1996":1,"1997":1,"1998":1,"1999":1}}],["高性能网络编程",{"2":{"1952":1}}],["高性能计算等",{"2":{"1602":1}}],["高手的必经之路",{"2":{"1611":1}}],["高扩展性",{"2":{"1579":1}}],["高学习率不再能很好地进行训练",{"2":{"1178":1}}],["高梯度方差可能是由以下原因造成的",{"2":{"1154":1}}],["高度",{"2":{"783":1}}],["高度适应性",{"2":{"512":1}}],["高",{"2":{"683":1,"1650":1}}],["高维特征空间中的线性模型往往有很大的参数空间",{"2":{"1012":1}}],["高维稀疏",{"2":{"681":1}}],["高维度导致点积qktqktqk^t值变大",{"2":{"189":1}}],["高昂的计算成本阻碍了它在实践中的广泛部署或使用",{"2":{"606":1}}],["高压缩率",{"2":{"563":1}}],["高温入窑烧造",{"2":{"437":1}}],["高层学习语义特征等",{"2":{"437":1}}],["高层倾向于检测语义模式",{"2":{"126":1}}],["高为",{"2":{"315":1}}],["高速带宽内存",{"2":{"206":1}}],["高熵分布可以捕捉更广泛的上下文信息",{"2":{"194":1}}],["高熵表示模型的注意力分散",{"2":{"194":1}}],["高频交易系统",{"2":{"1946":1}}],["高频的连续token对",{"2":{"575":1}}],["高频词还是低频词分类",{"2":{"185":1}}],["高频",{"2":{"122":1,"185":2}}],["高效完成任务",{"0":{"2106":1}}],["高效和安全",{"2":{"1932":1}}],["高效和可扩展的新架构dcformer",{"2":{"43":1}}],["高效插入",{"2":{"1801":1}}],["高效的内存共享",{"2":{"983":1}}],["高效的全局池化操作",{"2":{"267":1}}],["高效",{"2":{"696":1}}],["高效性",{"2":{"542":1}}],["高效率生成",{"2":{"233":1}}],["高效处理局部关系",{"2":{"217":1}}],["高效地改进",{"2":{"121":1}}],["高斯误差线性单元",{"2":{"106":1}}],["⊗",{"2":{"105":5,"109":1,"1344":1}}],["⊗σ",{"2":{"105":2,"109":2}}],["神经机器翻译",{"2":{"907":1}}],["神经常微分方程不拘于对已有架构的修修补补",{"2":{"493":1}}],["神经常微分方程",{"0":{"493":1},"1":{"494":1,"495":1,"496":1},"2":{"496":1}}],["神经记忆模型可以类比为代码段",{"2":{"231":1}}],["神经记忆能够不断从数据中学习并存储在其权重中",{"2":{"231":1}}],["神经网络通过将线性分类器进行组合叠加",{"2":{"1465":1}}],["神经网络通过存储多个特征的线性组合的方式来表示比其神经元更多的独立的特征",{"2":{"118":1}}],["神经网络通常是通过一个基于数学统计学类型的学习方法",{"2":{"1456":1}}],["神经网络由大量的人工神经元联结进行计算",{"2":{"1456":1}}],["神经网络训练流程概述",{"0":{"1438":1}}],["神经网络案例展示",{"0":{"1385":1}}],["神经网络研究员早就意识到学习率肯定是难以设置的超参数之一",{"2":{"1041":1}}],["神经网络需要用数据来训练",{"2":{"1009":1}}],["神经网络需要对前面",{"2":{"246":1}}],["神经网络模型一般依靠随机梯度下降进行模型训练和参数更新",{"2":{"990":1}}],["神经网络理论研究的物理学思想",{"2":{"513":1}}],["神经网络的奥秘正是在于高维的权重空间",{"2":{"506":1}}],["神经网络的高维损失函数构成的损失曲面",{"2":{"314":1}}],["神经网络的层数越多",{"2":{"296":1}}],["神经网络语言模型只有一个输出向量",{"2":{"288":1}}],["神经网络就越容易学习到这种长距离依赖关系",{"2":{"246":1}}],["神经网络架构中固定的节点连接权重",{"2":{"172":1}}],["神经网络利用高维空间中几乎正交方向的存在",{"2":{"137":1}}],["神经网络无论有多少层",{"2":{"102":1}}],["神经元之间不存在同层连接",{"2":{"1464":1}}],["神经元接收到的总输入值",{"2":{"1459":1}}],["神经元接收到来自",{"2":{"1459":1}}],["神经元",{"0":{"1459":1},"2":{"1461":1}}],["神经元模型",{"0":{"1458":1},"1":{"1459":1,"1460":1},"2":{"1459":1}}],["神经元输出",{"2":{"1000":1}}],["神经元处于饱和状态",{"2":{"995":1}}],["神经元坏死问题",{"2":{"848":1}}],["神经元坏死",{"2":{"842":1}}],["神经元坏死现象",{"2":{"840":1}}],["神经元只对输入信号的少部分选择性响应",{"2":{"841":1}}],["神经元的技术或概念",{"2":{"477":1}}],["神经元的特点",{"2":{"135":1}}],["神经元内部的激活解开为一小组可解释的特征",{"2":{"137":1}}],["神经元存储下一个词的输出概率",{"2":{"126":1}}],["神经元都会做出反应",{"2":{"118":1}}],["神经元死亡",{"2":{"104":1}}],["过日子就是问题叠着问题",{"2":{"2056":1}}],["过了几年我又是另一种理解",{"2":{"2054":1}}],["过度内联可能导致代码膨胀",{"2":{"1709":1}}],["过度使用友元会降低代码的可维护性和可读性",{"2":{"1793":1}}],["过度使用",{"2":{"1615":1}}],["过小的验证集",{"2":{"1149":1}}],["过小的权值初始值",{"2":{"991":1}}],["过短",{"2":{"976":1}}],["过程中发生了什么",{"2":{"1079":1}}],["过程才结束吗",{"2":{"904":1}}],["过程如下图所示",{"2":{"896":1}}],["过程为",{"2":{"890":1}}],["过程",{"0":{"782":1}}],["过分相信邻近生成的内容",{"2":{"746":1}}],["过滤噪声",{"2":{"684":1}}],["过滤停用词",{"2":{"545":1}}],["过大的词汇表可能导致某些token训练不足",{"2":{"562":1}}],["过高的学习率会使模型在更新参数时过于激进",{"2":{"400":1}}],["过拟合和训练时间长",{"2":{"393":1}}],["过拟合",{"2":{"393":1}}],["过拟合+遗忘",{"2":{"222":1}}],["过去意外",{"2":{"230":1}}],["过多的隐藏维度可能导致信息瓶颈和过拟合",{"2":{"116":1}}],["过渡到使用",{"2":{"103":1}}],["过激活",{"2":{"101":1}}],["降序排序",{"2":{"1883":1}}],["降序排序后",{"2":{"1645":1}}],["降噪耳机的设计也基于类似的想法",{"2":{"502":1}}],["降维操作将升维后的结果映射回原始维度",{"2":{"117":1}}],["降维操作通过将高维表示映射回较低维空间",{"2":{"116":1}}],["降维可以减少模型的参数数量",{"2":{"684":1}}],["降维可以限制计算复杂度",{"2":{"116":1}}],["降维可以浓缩特征",{"2":{"116":1}}],["降维可以将维度还原",{"2":{"116":1}}],["降维",{"0":{"684":1},"2":{"116":1,"682":1,"1469":1}}],["降回原来维度",{"2":{"101":1}}],["降低可读性和可维护性",{"2":{"1631":1}}],["降低了计算成本",{"2":{"2011":1}}],["降低了开发者的心智负担",{"2":{"1602":1}}],["降低了网络训练速度",{"2":{"816":1}}],["降低审查标准punsafe=0",{"2":{"1363":1}}],["降低学习率",{"2":{"1180":1}}],["降低网络优化难度",{"2":{"813":1}}],["降低模型计算量",{"2":{"813":1}}],["降低信息冗余",{"2":{"813":1}}],["降低计算的复杂度",{"2":{"774":1}}],["降低计算复杂度",{"2":{"153":1}}],["降低大模型幻觉的必由之路",{"2":{"513":1}}],["降低回答的自信度",{"2":{"437":1}}],["降低测试集上的信息熵",{"2":{"363":1}}],["降低各维度数据的方差",{"2":{"310":1,"321":1}}],["降低过拟合的风险",{"2":{"250":1}}],["降低到次二次方级别的水平",{"2":{"942":1,"959":1}}],["降低到",{"2":{"212":1}}],["降低",{"2":{"90":1}}],["精通函数的定义与调用",{"2":{"1727":1}}],["精讲",{"2":{"1365":1}}],["精细面部表情模拟",{"2":{"2011":1}}],["精细的文本生成任务可能需要较大的词汇量以覆盖更多细节",{"2":{"560":1}}],["精细再加工",{"2":{"101":1}}],["精度为",{"2":{"1817":1,"1835":1}}],["精度比",{"2":{"1607":1}}],["精度和范围不同",{"2":{"1607":1}}],["精度有限",{"2":{"1607":1}}],["精度改进",{"0":{"938":1,"954":1}}],["精度复杂性",{"2":{"504":1}}],["精度都比较稳定",{"2":{"338":1}}],["精炼",{"2":{"437":1}}],["精炼神经元",{"0":{"135":1},"2":{"96":1,"133":1}}],["混合模型",{"2":{"1568":1}}],["混合搜索",{"2":{"678":1}}],["混合精度训练",{"2":{"657":1}}],["混合了字节和patch信息",{"2":{"612":1}}],["混合操作具体分为两步",{"2":{"116":1}}],["混合",{"2":{"101":1}}],["建筑设计",{"2":{"2010":1}}],["建议根据自己的编程语言和变成习惯来实现",{"2":{"2146":1}}],["建议根据具体情况进行评估和实验",{"2":{"560":1}}],["建议看代码里的注释",{"2":{"2146":1}}],["建议将所有友元函数集中到一起声明",{"2":{"1773":1}}],["建议将相关的声明放在同一个头文件中",{"2":{"1628":1}}],["建议始终使用花括号",{"2":{"1619":1}}],["建议安装无gui的系统",{"2":{"1582":1}}],["建议改用",{"2":{"1214":1}}],["建议您在训练模型时始终使用",{"2":{"1122":1}}],["建议您在不需要自动求导跟踪的代码部分",{"2":{"1121":1}}],["建议设定",{"2":{"1049":1}}],["建立世界模型",{"2":{"754":1}}],["建立关系",{"2":{"689":1}}],["建立中文词表和英文词表",{"2":{"545":1}}],["建立了语义信息和位置信息之间沟通的桥梁",{"2":{"746":1}}],["建立了一个平均场相互作用的粒子系统",{"2":{"499":1}}],["建立了神经元",{"2":{"489":1}}],["建立分别用于训练数据和验证数据加载的数据加载器",{"2":{"364":1}}],["建立场景的内部表示",{"2":{"260":1}}],["建立mask",{"0":{"66":1},"2":{"49":1}}],["建模方式",{"2":{"745":2}}],["建模方面",{"2":{"287":1}}],["建模难度就越大",{"2":{"246":1}}],["建模为一个逐步预测每个新元素的条件概率的过程",{"2":{"239":1}}],["建模只考虑单独位置",{"2":{"101":1}}],["取款金额超过余额",{"2":{"1766":1}}],["取款等功能",{"2":{"1766":1}}],["取模运算符",{"2":{"1630":1}}],["取模",{"2":{"1630":1}}],["取地址运算符",{"2":{"1635":1}}],["取地址",{"2":{"1612":1}}],["取址运算符",{"2":{"1611":1}}],["取余运算的应用",{"2":{"1607":1}}],["取余运算符",{"2":{"1083":1}}],["取余",{"2":{"1607":1}}],["取得了重大成功",{"2":{"1472":1}}],["取得成功重要的是缩放不变性",{"2":{"812":1}}],["取的数据不在任何椭圆里",{"2":{"1374":1}}],["取绝对值",{"2":{"1085":1}}],["取代了传统的",{"2":{"986":1}}],["取出结果",{"2":{"1298":1}}],["取出embedding矩阵的第3行",{"2":{"700":1}}],["取出预测结果",{"2":{"428":1}}],["取出数值最大的那个token",{"2":{"428":1}}],["取名为",{"2":{"101":1}}],["取个好名字真难",{"2":{"47":1}}],["依旧能够对你的生活有启发作用",{"2":{"2146":1}}],["依赖管理时最常见的方式",{"2":{"1991":1}}],["依赖",{"2":{"1917":1}}],["依赖非负特征映射",{"2":{"212":1}}],["依次点开",{"2":{"2070":1}}],["依次打印",{"2":{"1912":1}}],["依次类推",{"2":{"428":1,"878":1,"1323":1}}],["依然可以保持",{"2":{"1315":1}}],["依然只能与仅含输出层的单层神经网络等价",{"2":{"838":1}}],["依然只是对",{"2":{"117":1}}],["依此可以洞察智能的本质",{"2":{"506":1}}],["依此类推",{"2":{"437":1}}],["依据上文的讨论及激活函数的发展规律",{"2":{"848":1}}],["依据其不同形式",{"2":{"766":1}}],["依据token",{"2":{"700":1}}],["依据johnson",{"2":{"684":1}}],["依据频率对词语进行排序",{"2":{"557":1}}],["依据这些概率",{"2":{"431":1}}],["依据配置来设定warmup参数",{"2":{"423":1}}],["依据解码器的输出预测下一个token",{"2":{"398":1}}],["依据基础学习率会预热3000次",{"2":{"372":1}}],["依据一定的更新规则对这个隐藏状态进行变换",{"2":{"273":1}}],["依据商品id来获取对应的values",{"2":{"164":1}}],["依据模型特点进行选择",{"2":{"100":1}}],["依存关系是词语和词语之间的依赖关系",{"2":{"20":1}}],["二进制表示的",{"2":{"1910":1}}],["二进制字面量",{"2":{"1910":1}}],["二进制字面量和数字分隔符",{"0":{"1910":1},"2":{"1904":1}}],["二进制文件",{"2":{"1506":1}}],["二级指针是指向指针的指针",{"2":{"1650":1}}],["二级指针的应用场景",{"2":{"1650":1}}],["二级指针",{"2":{"1611":1}}],["二字",{"2":{"1426":1,"2112":1}}],["二维数组与行指针",{"2":{"1709":1}}],["二维数组在内存中按行优先存储",{"2":{"1705":1}}],["二维数组的数组名",{"2":{"1705":1}}],["二维数组的元素是连续存储的",{"2":{"1705":1}}],["二维数组的内存模型",{"2":{"1705":1}}],["二维数组可以看作是数组的数组",{"2":{"1705":1}}],["二维数组",{"2":{"1634":1,"1705":1}}],["二维平面的中心就是图片的二维高斯分布的",{"2":{"1373":1}}],["二维坐标",{"2":{"680":1}}],["二次训练",{"2":{"1313":1}}],["二阶矩估计可能在训练初期有很高的偏置",{"2":{"1059":1}}],["二阶矩估计",{"2":{"1059":1}}],["二阶矩的估计",{"2":{"1059":1}}],["二者意义截然不同",{"2":{"579":1}}],["二者中任何一组数据都具有明确且相同的物理意义",{"2":{"316":1}}],["二者比较的粒度不相同",{"2":{"99":1}}],["二则rnn并不能很好地处理句子中的结构化信息",{"2":{"290":1}}],["二来可以将有限的计算资源用来处理更重要的信息",{"2":{"260":1}}],["二是计算机底层就是二值逻辑",{"2":{"2058":1}}],["二是注意力模块帮助长期记忆只存储来自当前上下文的有用信息",{"2":{"231":1}}],["二是用近似模型产生准确的概率分布",{"2":{"185":1}}],["二是在训练过程中直接喂入解码器正确的结果而不是上一时刻的预测值",{"2":{"57":1}}],["二",{"0":{"655":1,"1540":1,"1567":1,"1603":1,"1612":1,"1638":1,"1646":1,"1685":1,"1694":1,"1705":1,"1917":1},"1":{"656":1,"657":1,"658":1,"659":1,"660":1,"661":1,"662":1,"663":1,"664":1,"665":1,"666":1,"667":1,"668":1,"669":1,"670":1,"671":1,"1541":1,"1542":1,"1543":1,"1544":1,"1545":1,"1546":1,"1547":1,"1548":1,"1549":1,"1550":1,"1551":1,"1552":1,"1553":1,"1554":1,"1555":1,"1556":1,"1557":1,"1558":1,"1559":1,"1560":1,"1568":1,"1569":1},"2":{"156":2,"361":1,"429":1}}],["列出文件和目录",{"2":{"1509":1}}],["列数",{"2":{"700":1,"1705":1}}],["列表每个元素就是一个token",{"2":{"547":1}}],["列表大小为batch",{"2":{"384":1,"558":1}}],["列表的内存也急剧增加",{"2":{"273":1}}],["列表中所有的上下文共同构成了统一的隐状态",{"2":{"273":1}}],["列",{"2":{"99":1,"419":1,"1634":1}}],["行为",{"2":{"1866":1}}],["行指针",{"2":{"1705":2}}],["行指针的概念",{"2":{"1705":1}}],["行业未来的方向是什么",{"2":{"1598":1}}],["行业的自动化程度高么",{"2":{"1598":1}}],["行业中占据主导地位",{"2":{"1315":1}}],["行业做法",{"0":{"366":1},"1":{"367":1,"368":1,"369":1}}],["行间互不干扰",{"2":{"700":1}}],["行数",{"2":{"700":1}}],["行表示第",{"2":{"172":1}}],["行与行之间无交错",{"2":{"99":1,"419":1}}],["行",{"2":{"99":1,"128":1,"161":3,"340":1,"419":1,"1515":2,"1634":1}}],["行向量中某一个元素是xixix",{"2":{"54":1}}],["考虑局部",{"2":{"2119":1}}],["考虑资源管理",{"2":{"1764":1}}],["考虑返回值类型",{"2":{"1712":1}}],["考虑使用智能指针来管理动态分配的内存",{"2":{"1672":1}}],["考虑单边即可",{"2":{"1322":1}}],["考虑进行一次性离线预处理并保存",{"2":{"1161":1}}],["考虑运行最佳试验n次",{"2":{"1152":1}}],["考虑是否上线新的最佳配置",{"2":{"1139":1}}],["考虑",{"2":{"1115":1}}],["考虑batch",{"2":{"1087":1}}],["考虑反向传方差相等时公式",{"2":{"1006":1,"1007":1}}],["考虑前向传播方差相等时公式",{"2":{"1006":1,"1007":1}}],["考虑两种语言之间的各种对齐方式",{"2":{"908":1}}],["考虑到符号",{"2":{"764":1}}],["考虑到句子中不同单词的语义和依赖关系",{"2":{"498":1}}],["考虑到layernorm放置的位置不同",{"2":{"329":1}}],["考虑到在以张量表示的样本批次要求样本必须具有相同长度",{"2":{"316":1}}],["考虑以下两个句子",{"2":{"261":1}}],["考虑句子中每个单词与其他所有单词的依赖关系",{"2":{"158":1}}],["考虑顺序",{"2":{"142":1}}],["考虑注意力机制可能对复杂过程的拟合程度不够",{"2":{"97":1}}],["考虑了注意力掩码和层归一化",{"2":{"91":1}}],["功过格",{"2":{"2048":1}}],["功能独立的函数",{"2":{"1729":1}}],["功能完善的",{"2":{"1727":1,"1729":1}}],["功能强大",{"2":{"1605":1}}],["功能展示",{"0":{"1211":1}}],["功能",{"0":{"140":1},"2":{"96":1,"248":1,"557":1}}],["功用",{"0":{"91":1,"303":1},"1":{"92":1,"93":1,"94":1,"304":1,"305":1,"306":1,"307":1},"2":{"49":1,"293":1}}],["字面量",{"2":{"1629":1}}],["字段也不会被更新",{"2":{"1117":1}}],["字段中",{"2":{"1117":1}}],["字段",{"2":{"1116":1}}],["字向量就是one",{"2":{"694":1}}],["字向量表",{"2":{"694":1}}],["字节跳动基础架构部",{"2":{"1954":1}}],["字节的内存",{"2":{"1671":1}}],["字节对齐",{"2":{"1653":1}}],["字节对token",{"2":{"591":1}}],["字节",{"2":{"1607":1,"1633":2,"1667":1,"1671":1}}],["字节数",{"2":{"1087":1}}],["字节潜在transformer",{"2":{"638":1}}],["字节表示作为键",{"2":{"615":1}}],["字节化预训练模型",{"2":{"611":1}}],["字节集作为词汇表是解决此问题的潜在方法",{"2":{"606":1}}],["字符数组或字符串字面量",{"2":{"1929":1}}],["字符数量少",{"2":{"566":1}}],["字符个数",{"2":{"1713":1}}],["字符与字符串的本质",{"2":{"1704":1}}],["字符串转大写",{"2":{"1914":1}}],["字符串流用法",{"0":{"1824":1,"1842":1}}],["字符串流类",{"0":{"1823":1,"1841":1}}],["字符串流提供了一种在内存中操作字符串的",{"2":{"1822":1,"1840":1}}],["字符串流",{"0":{"1822":1,"1840":1},"1":{"1823":1,"1824":1,"1841":1,"1842":1},"2":{"1823":1,"1826":1,"1841":1,"1844":1}}],["字符串的总长度",{"2":{"1914":1}}],["字符串的长度是",{"2":{"1729":1}}],["字符串的长度可以根据需要动态增长",{"2":{"1713":1}}],["字符串的操作",{"2":{"1713":1}}],["字符串常量以",{"2":{"1704":1}}],["字符串常量是由双引号",{"2":{"1704":1}}],["字符串是由多个字符组成的序列",{"2":{"1704":1}}],["字符串拼接",{"2":{"1624":1,"1824":1,"1842":1}}],["字符串",{"0":{"1624":1},"2":{"1703":1,"1704":3,"1728":1,"1795":1}}],["字符型",{"2":{"1607":1}}],["字符级别分词",{"2":{"564":1}}],["字符",{"2":{"564":1,"1563":1,"1715":1}}],["字数",{"2":{"380":2}}],["字典形式",{"2":{"1226":1}}],["字典",{"2":{"1225":1}}],["字典和模型",{"2":{"371":1}}],["字典里面有什么样的信息",{"2":{"265":1}}],["字典学习的目标是将",{"2":{"137":1}}],["字典学习是一种常用的特征提取方法",{"2":{"137":1}}],["字典学习",{"2":{"137":1}}],["字典学习和稀疏自编码器",{"0":{"137":1},"2":{"96":1}}],["字体和颜色就是两个不同的子空间",{"2":{"4":1}}],["字体颜色",{"2":{"3":1}}],["关键点总结",{"2":{"1704":1,"1705":1,"1706":1,"1707":1,"1708":1}}],["关键点",{"2":{"1698":1,"1699":1,"1700":1,"1701":1}}],["关键",{"2":{"1646":1}}],["关键字进行声明的",{"2":{"1793":1}}],["关键字在类中声明一个函数为友元函数",{"2":{"1772":1}}],["关键字用于自动推导变量类型",{"2":{"1905":1}}],["关键字用于定义受保护的成员",{"2":{"1655":1}}],["关键字用来指定友元关系",{"2":{"1770":1}}],["关键字以及异常处理流程是使用异常处理的基础",{"2":{"1765":1}}],["关键字限制了在函数内部修改形参的值",{"2":{"1729":1}}],["关键字来显式控制成员的访问权限",{"2":{"1728":1}}],["关键字表明我们正在定义一个联合体",{"2":{"1728":1}}],["关键字表明我们正在定义一个枚举类型",{"2":{"1728":1}}],["关键字表明我们正在定义一个结构体",{"2":{"1728":1}}],["关键字表示该函数不会修改对象的状态",{"2":{"1674":1}}],["关键字建议编译器将函数内联",{"2":{"1709":1}}],["关键字和尖括号",{"2":{"1699":1,"1700":1,"1701":1}}],["关键字声明函数不会抛出异常",{"2":{"1764":1}}],["关键字声明类型参数",{"2":{"1698":1}}],["关键字声明的成员函数",{"2":{"1693":1}}],["关键字声明的变量",{"2":{"1649":2}}],["关键字修饰继承方式",{"2":{"1662":1}}],["关键字修饰的成员变量或成员函数",{"2":{"1640":1}}],["关键字可以用于阻止类被继承",{"2":{"1656":1}}],["关键字控制成员访问和继承",{"2":{"1652":1}}],["关键字的作用",{"2":{"1649":2}}],["关键字的使用",{"2":{"1649":1}}],["关键字是",{"2":{"1615":1}}],["关键字",{"0":{"1656":1,"1770":1},"2":{"1607":1,"1639":2,"1640":2,"1709":1,"1762":2,"1768":1}}],["关键之处是看模型能否正确预测到下一个单词",{"2":{"397":1}}],["关闭文件",{"2":{"1820":2,"1838":2}}],["关闭当前分屏",{"2":{"1557":1}}],["关闭",{"2":{"301":1}}],["关联容器",{"0":{"1804":1},"1":{"1805":1,"1806":1,"1807":1}}],["关联的输出流对象",{"2":{"1673":1}}],["关联的输入流对象",{"2":{"1673":1}}],["关联的一种方式",{"2":{"691":1}}],["关联度最大",{"2":{"170":1}}],["关联和掌握",{"2":{"141":1}}],["关系运算符重载",{"2":{"1712":1}}],["关系运算符",{"2":{"1630":1,"1635":3}}],["关系型数据库",{"2":{"1492":1}}],["关系总结如下",{"2":{"773":1}}],["关系是在语境中根据语义",{"2":{"709":1}}],["关系及更抽象的责任等等",{"2":{"689":1}}],["关系距离问题",{"2":{"256":2}}],["关系",{"2":{"213":1}}],["关系或属性",{"2":{"140":1}}],["关系知识与较高的",{"2":{"136":1}}],["关系知识存储在哪里",{"2":{"136":1}}],["关系和实体知识在知识三元组中是否同等重要",{"2":{"136":1}}],["关系和实体",{"2":{"122":1}}],["关系的定位",{"0":{"136":1},"2":{"96":1}}],["关于时间复杂度的详细计算",{"2":{"2155":1}}],["关于随笔",{"2":{"2109":1}}],["关于ethan博客介绍",{"2":{"2032":1,"2036":1}}],["关于",{"2":{"1693":1,"2054":1}}],["关于如何使用pull",{"2":{"1198":1}}],["关于训练管道的额外补充",{"0":{"1160":1},"1":{"1161":1,"1162":1,"1163":1,"1164":1,"1165":1,"1166":1,"1167":1,"1168":1,"1169":1}}],["关于训练工作流的额外补充",{"2":{"1125":1}}],["关于贡献",{"0":{"1196":1},"1":{"1197":1,"1198":1,"1199":1},"2":{"1125":1}}],["关于默认模式最重要的是",{"2":{"1119":1}}],["关于绝对位置编码的工作大多数是以不同的方法生成绝对位置编码为主",{"2":{"747":1}}],["关于维度公式",{"2":{"740":1}}],["关于词表大小",{"2":{"561":1}}],["关于反向传播的梯度如何影响模型学习和知识存储的探讨仍然较为稀缺",{"2":{"484":1}}],["关于反向传递的梯度如何影响模型学习和知识存储的探讨仍然较为稀缺",{"2":{"148":1}}],["关于数据时代的意义",{"2":{"156":1}}],["关于一个物体的知识会分布在成千上万根皮质柱中",{"2":{"129":1}}],["关于key的模式",{"2":{"127":1}}],["关于多义性的成因",{"2":{"118":1}}],["关于矩阵",{"2":{"101":1}}],["关注有限的上下文窗口",{"2":{"229":1}}],["关注到的点越多",{"2":{"20":1}}],["关注罕见词的能力了",{"2":{"20":1}}],["增量公式",{"2":{"2018":1}}],["增量调整策略",{"0":{"1139":1},"2":{"1125":1}}],["增",{"2":{"1486":1}}],["增维",{"0":{"683":1},"2":{"682":1}}],["增大网络感受野",{"2":{"813":1}}],["增大时",{"2":{"698":1,"1155":1}}],["增大输入",{"2":{"347":1}}],["增大信息含量",{"0":{"273":1}}],["增大cos",{"2":{"176":1}}],["增大模长∥∥kj∥∥∥kj∥∥k",{"2":{"176":1}}],["增强泛型编程能力",{"2":{"1913":1}}],["增强代码的表达力和安全性",{"2":{"1899":1}}],["增强的位左移和位右移",{"2":{"1635":1}}],["增强的位异或和位或",{"2":{"1635":1}}],["增强的取模和位与",{"2":{"1635":1}}],["增强的乘法和除法",{"2":{"1635":1}}],["增强的加法和减法",{"2":{"1635":1}}],["增强了代码的可读性和安全性",{"2":{"1932":1}}],["增强了函数作为参数的使用场景",{"2":{"1884":1}}],["增强了长度外推",{"2":{"759":1}}],["增强了机器人执行语言指令的能力",{"2":{"637":1}}],["增强了模型的可扩展性",{"2":{"616":1}}],["增强了模型的表达能力",{"2":{"1":1}}],["增强了模型对输入数据的理解",{"2":{"172":1}}],["增强模型的鲁棒性和降低泛化误差",{"2":{"393":1}}],["增强某些成分的信号",{"2":{"221":1}}],["增强非线性可以有效地缓解特征坍塌的问题",{"2":{"117":1}}],["增加计数",{"2":{"1933":1}}],["增加引用计数",{"2":{"1891":1}}],["增加特有属性",{"2":{"1657":2}}],["增加命名冲突的风险",{"2":{"1649":1}}],["增加",{"2":{"1134":4,"2050":1,"2070":1}}],["增加batch",{"2":{"1131":2,"1133":2}}],["增加关系表达能力",{"2":{"682":1}}],["增加词汇表大小意味着平均token更大",{"2":{"612":1}}],["增加词表大小可以提高标记化分词的效率",{"2":{"561":1}}],["增加到四倍2048",{"2":{"466":1}}],["增加了类之间的耦合度",{"2":{"1776":1}}],["增加了面向对象特性",{"2":{"1603":1}}],["增加了代码的复杂性",{"2":{"1143":1}}],["增加了两个全局可学习的变量u",{"2":{"760":1}}],["增加了文本表征的成本",{"2":{"566":1}}],["增加了建模难度",{"2":{"566":1}}],["增加了后续单词成功预测的几率",{"2":{"407":1}}],["增加了模型的拟合能力",{"2":{"172":1}}],["增加训练数据量",{"2":{"363":1}}],["增加线性变换操作",{"2":{"309":1}}],["增加灵活性",{"2":{"172":1}}],["增加拟合能力",{"2":{"172":1}}],["增加自编码器的大小时",{"2":{"137":1}}],["增加参数量",{"0":{"119":1},"2":{"96":1}}],["增加表达能力",{"0":{"117":1},"2":{"96":1,"115":1,"172":1}}],["数值微分法",{"0":{"2016":1},"1":{"2017":1,"2018":1,"2019":1,"2020":1}}],["数值算法",{"0":{"1745":1},"1":{"1746":1,"1747":1},"2":{"1732":1,"1758":1}}],["数值越高",{"2":{"130":1}}],["数值越低",{"2":{"130":2}}],["数组或元组的成员直接绑定到多个变量上",{"2":{"1921":1}}],["数组指针",{"2":{"1705":2}}],["数组传参的本质",{"0":{"1667":1}}],["数组作为函数参数传递时",{"2":{"1634":1}}],["数组与指针的关系",{"2":{"1634":1}}],["数组越界",{"2":{"1634":1}}],["数组元素的平均值为",{"2":{"1623":1}}],["数组中的每个元素都有一个唯一的索引",{"2":{"1623":1}}],["数组下标",{"2":{"1623":1,"1635":1}}],["数组的大小必须在编译时已知",{"2":{"1714":1}}],["数组的元素也是数组",{"2":{"1634":1}}],["数组的定义",{"2":{"1634":1}}],["数组的初始化",{"2":{"1623":1,"1634":1}}],["数组的声明",{"2":{"1623":1}}],["数组大小",{"2":{"1623":1}}],["数组名的本质",{"0":{"2006":1}}],["数组名可以退化为指向第一行的行指针",{"2":{"1705":1}}],["数组名可以隐式转换为指向数组第一个元素的常量指针",{"2":{"1704":1}}],["数组名作为常量指针",{"2":{"1704":1}}],["数组名与指针的紧密联系",{"2":{"1704":1}}],["数组名退化为指针",{"2":{"1667":1}}],["数组名在很多情况下可以视为指向数组首元素的常量指针",{"2":{"1704":1}}],["数组名在很多情况下会被解释为其首元素的地址",{"2":{"1667":1}}],["数组名在大多数情况下可以隐式转换为指向数组首元素的指针",{"2":{"1634":1}}],["数组名代表数组首元素的地址",{"2":{"1634":1}}],["数组名",{"2":{"1623":1}}],["数组是一种可以存储相同数据类型的多个元素的线性数据结构",{"2":{"1623":1}}],["数组",{"0":{"1623":1,"1634":1,"1703":1},"1":{"1704":1,"1705":1,"1706":1,"1707":1,"1708":1,"1709":1,"1710":1},"2":{"1083":1,"1491":1,"1703":1,"1706":1}}],["数字分隔符等特性让代码更加简洁易懂",{"2":{"1913":1}}],["数字分隔符",{"2":{"1910":1}}],["数字和下划线组成",{"2":{"1729":1}}],["数字的平方为",{"2":{"1645":1}}],["数字为",{"2":{"1645":1}}],["数字携带的信息量太少",{"2":{"679":1}}],["数字",{"0":{"679":1}}],["数十亿",{"2":{"508":1}}],["数学过程",{"0":{"2125":1,"2131":1,"2135":1,"2139":1}}],["数学挂钩",{"2":{"2096":1}}],["数学",{"2":{"1825":1,"1843":1}}],["数学函数定义域错误",{"2":{"1762":1}}],["数学模型",{"2":{"1462":1}}],["数学表达式为",{"2":{"999":1}}],["数学表达为",{"2":{"999":1}}],["数学表示",{"0":{"99":1},"2":{"96":1}}],["数学公式为",{"0":{"918":1}}],["数学公式",{"2":{"839":1}}],["数学家解决了这个悬而未决的问题",{"2":{"685":1}}],["数学家喜欢喝咖啡",{"2":{"685":1}}],["数学家跑向商店",{"2":{"685":1}}],["数学家和物理学家之间语义关联很大",{"2":{"685":1}}],["数学框架",{"2":{"513":1}}],["数学角度",{"0":{"491":1},"1":{"492":1,"493":1,"494":1,"495":1,"496":1,"497":1,"498":1,"499":1,"500":1,"501":1,"502":1,"503":1,"504":1,"505":1}}],["数学与物理相辅相成",{"2":{"474":1}}],["数量或顺序不同",{"2":{"1707":1}}],["数量也为",{"2":{"1364":1}}],["数量分别为",{"2":{"1364":1}}],["数量特征",{"2":{"712":1}}],["数量",{"2":{"201":1}}],["数据可视化等众多领域都有广泛应用",{"2":{"2010":1}}],["数据从一端",{"2":{"1810":1,"1828":1}}],["数据丢失",{"2":{"1684":1}}],["数据损坏",{"2":{"1670":1}}],["数据大小",{"2":{"1590":1}}],["数据收集",{"2":{"1575":1}}],["数据收集和抽样方差",{"2":{"1152":1}}],["数据散播",{"2":{"1575":1}}],["数据取出时的封装",{"2":{"1479":1}}],["数据持久功能大多是必不可少的组成部分",{"2":{"1478":1}}],["数据持久化往往也就意味着将内存中的数据保存到磁盘上加以固化",{"2":{"1478":1}}],["数据访问对象",{"2":{"1478":1}}],["数据标签",{"2":{"1469":1}}],["数据清洗",{"2":{"1469":1}}],["数据清理等基本工作已经完成",{"2":{"1128":1}}],["数据结构设计",{"2":{"1947":1}}],["数据结构等高级主题打下坚实的基础",{"2":{"1610":1}}],["数据结构",{"0":{"1428":1}}],["数据同步指一个数据集的多份拷贝一致以维护完整性",{"2":{"1408":1}}],["数据生成",{"0":{"1371":1}}],["数据压缩与数据生成的关系",{"0":{"1372":1}}],["数据压缩也可以成为数据降维",{"2":{"1370":1}}],["数据压缩",{"0":{"1370":1}}],["数据压缩和数据生成",{"0":{"1369":1},"1":{"1370":1,"1371":1,"1372":1,"1373":1,"1374":1,"1375":1}}],["数据准备和预处理方面",{"2":{"1469":1}}],["数据准备",{"2":{"1202":1,"1215":1}}],["数据未与训练进程存放在同一位置",{"2":{"1161":1}}],["数据增强方法",{"2":{"1158":1}}],["数据增强操作的模式和并行运算的顺序",{"2":{"1152":1}}],["数据类型和标签等信息",{"2":{"1573":1}}],["数据类型",{"0":{"1607":1},"2":{"1087":1,"1436":1,"1491":1,"1576":1,"1614":3,"1729":1}}],["数据类型转化为",{"2":{"1087":1}}],["数据类型转化为float",{"2":{"1085":1}}],["数据类型转化",{"0":{"1075":1},"2":{"1075":1}}],["数据类型转换也是一样的操作",{"2":{"662":1}}],["数据并行模式",{"2":{"1578":1}}],["数据并行",{"2":{"976":1}}],["数据",{"2":{"773":1,"1202":1,"1313":1}}],["数据的性质以及所使用的模型架构",{"2":{"692":1}}],["数据向量化和embedding两者关系如下表所示",{"2":{"676":1}}],["数据向量化的逻辑中",{"2":{"676":1}}],["数据向量化是一种数值转化的过程",{"2":{"676":1}}],["数据向量化",{"2":{"676":1}}],["数据也连续",{"2":{"658":1}}],["数据变成",{"2":{"575":1}}],["数据足够多时",{"2":{"542":1}}],["数据相当于一种初始化",{"2":{"506":1}}],["数据合成",{"2":{"369":1}}],["数据格式优化",{"2":{"369":1}}],["数据质量增强",{"2":{"369":1}}],["数据治理",{"0":{"369":1}}],["数据混合策略对于训练至关重要",{"2":{"368":1}}],["数据混合等",{"2":{"364":1}}],["数据源比率",{"0":{"368":1}}],["数据集上进行了预训练",{"2":{"1316":1}}],["数据集下载",{"0":{"1302":1},"2":{"1309":1}}],["数据集增强",{"0":{"1015":1}}],["数据集规模相对较小",{"2":{"1012":1}}],["数据集中文本的复杂性和多样性也影响词汇表的设置",{"2":{"560":1}}],["数据集用到了两次",{"2":{"370":1}}],["数据集由三个文件组成",{"2":{"370":1}}],["数据集介绍参见",{"2":{"370":1}}],["数据集",{"0":{"365":1,"725":1},"1":{"366":1,"367":1,"368":1,"369":1,"370":1}}],["数据和数据处理决定了大模型的上限",{"2":{"363":1}}],["数据处理也异常繁琐",{"2":{"908":1}}],["数据处理",{"0":{"362":1},"1":{"363":1,"364":1,"365":1,"366":1,"367":1,"368":1,"369":1,"370":1,"371":1,"372":1,"373":1,"374":1,"375":1,"376":1,"377":1,"378":1,"379":1,"380":1,"381":1,"382":1,"383":1,"384":1,"385":1,"386":1,"387":1}}],["数据分析",{"2":{"1564":1,"1602":1,"2043":1}}],["数据分析学习与实践",{"2":{"233":1}}],["数据分布影响的问题",{"2":{"322":1}}],["数据信息的重要性决定了归一化的方式",{"2":{"321":1}}],["数据驱动的变换",{"2":{"170":1}}],["数据对就是数据库的元素",{"2":{"164":1}}],["数据对",{"2":{"164":1}}],["数据库内核开发",{"2":{"1953":1}}],["数据库开发",{"2":{"1951":1}}],["数据库优化",{"2":{"1500":1}}],["数据库基础",{"0":{"1492":1}}],["数据库的建立连接等等",{"2":{"1479":1}}],["数据库中每个元素由地址key和值value组成",{"2":{"164":1}}],["数据库角度",{"0":{"164":1},"2":{"157":1}}],["数据流",{"0":{"77":1},"1":{"78":1,"79":1,"80":1,"81":1,"82":1,"83":1,"84":1,"85":1},"2":{"49":1}}],["初步感受贪心",{"2":{"2043":1}}],["初步建站并测试",{"2":{"2042":1}}],["初步了解类的概念",{"2":{"1666":1}}],["初探类",{"0":{"1674":1}}],["初值问题的解",{"2":{"494":1}}],["初始score",{"2":{"1330":1}}],["初始分数",{"2":{"1329":1}}],["初始分割",{"0":{"579":1}}],["初始学习率",{"2":{"1243":1}}],["初始权重直接影响模型的输出",{"2":{"990":1}}],["初始的大词表要足够好",{"2":{"603":1}}],["初始时",{"2":{"530":1}}],["初始化和设置位",{"2":{"2062":1}}],["初始化和终止",{"2":{"1575":1}}],["初始化时机",{"2":{"1649":1}}],["初始化没有默认构造函数的类类型的成员变量",{"2":{"1641":1}}],["初始化引用类型的成员变量",{"2":{"1641":1}}],["初始化常量成员变量",{"2":{"1641":1}}],["初始化为空指针",{"2":{"1672":1}}],["初始化为",{"2":{"1634":1}}],["初始化为全0",{"2":{"343":1}}],["初始化为全1",{"2":{"343":1}}],["初始化数组的",{"2":{"1623":1}}],["初始化语句",{"2":{"1621":2}}],["初始化mpi环境",{"2":{"1575":1}}],["初始化父类",{"2":{"1205":1}}],["初始化策略选择",{"0":{"1008":1}}],["初始化的时候令权重的均值是0",{"2":{"1003":1}}],["初始化输出矩阵",{"2":{"944":1}}],["初始化一个形状为",{"2":{"749":1}}],["初始化一个很大的词表",{"2":{"602":1}}],["初始化一个词典",{"2":{"576":1}}],["初始化方法接受两个参数",{"2":{"522":1}}],["初始化ys为",{"2":{"428":1}}],["初始化",{"0":{"403":1},"2":{"576":1,"701":1,"709":1,"1179":1,"1180":1,"1328":1,"1594":1,"1612":1,"1639":1,"1640":1,"1672":1,"1932":1}}],["初始化模型参数",{"2":{"449":2}}],["初始化模型",{"2":{"372":1}}],["初始化函数有两个参数",{"2":{"532":1}}],["初始化函数有5个参数",{"2":{"450":1}}],["初始化函数接受如下参数",{"2":{"523":1}}],["初始化函数",{"2":{"343":1,"344":1,"701":1}}],["初始化注意力权重",{"2":{"23":1}}],["初始状态",{"2":{"273":1}}],["初夏",{"2":{"95":1}}],["|=",{"2":{"1630":1}}],["|∣g∣",{"2":{"1184":1}}],["|j=1",{"2":{"613":2}}],["|i=1",{"2":{"613":2}}],["|set",{"2":{"572":2}}],["|start",{"2":{"571":1}}],["|>",{"2":{"571":1}}],["|eot",{"2":{"571":2}}],["|endoftext|>",{"2":{"591":1}}],["|end",{"2":{"571":4}}],["|reserved",{"2":{"571":6}}],["|begin",{"2":{"571":2}}],["|v|",{"2":{"184":1}}],["||^",{"2":{"1361":1}}],["||",{"2":{"233":2,"395":2,"692":2,"1361":1,"1619":1,"1630":3,"1729":3,"1902":1}}],["||q||",{"2":{"175":1}}],["||k||",{"2":{"175":3}}],["|",{"2":{"95":1,"156":4,"233":3,"429":1,"513":7,"571":4,"613":2,"740":1,"908":2,"1184":3,"1308":67,"1630":1,"1902":2,"2018":2,"2059":3,"2062":2}}],["表现出训练不稳定性的模型的超参数轴图示例",{"2":{"1182":1}}],["表现为l层的堆叠效果未能达到预期",{"2":{"335":1}}],["表达式满足",{"2":{"1924":1}}],["表达式被声明为",{"2":{"1924":1}}],["表达式指定参数类型",{"2":{"1906":1}}],["表达式已经很强大",{"2":{"1906":1}}],["表达式分隔符",{"2":{"1635":1}}],["表达式2",{"2":{"1630":2}}],["表达式1",{"2":{"1630":2}}],["表达式的返回类型可能比较复杂或不直观",{"2":{"1615":1}}],["表达式",{"0":{"1881":1,"1906":1,"1924":1},"1":{"1882":1,"1883":1,"1884":1},"2":{"1603":1,"1904":1,"1920":1}}],["表达式等现代特性",{"2":{"1603":1}}],["表达关系",{"2":{"688":1}}],["表达能力",{"2":{"504":1}}],["表达能力有限",{"2":{"273":1}}],["表达能力缺失",{"0":{"252":1}}],["表达能力也更差",{"2":{"180":1}}],["表征概念",{"0":{"689":1},"2":{"688":1}}],["表征序列",{"2":{"614":1}}],["表征pjpj",{"2":{"614":1}}],["表征",{"2":{"614":4}}],["表征解码回字节",{"2":{"614":1}}],["表征进行操作的大型全局自回归语言模型以及两个较小的局部模型组成",{"2":{"614":1}}],["表征一个向量在另一个向量上的投影",{"2":{"176":1,"692":1}}],["表来重建原始数据",{"2":{"576":1}}],["表中方法对应的具体论文请参考论文",{"2":{"141":1}}],["表明你声明的是一个指针变量",{"2":{"1611":1}}],["表明这个variable是node",{"2":{"1110":1}}],["表明是",{"2":{"1110":1}}],["表明此variable实例是否是个view",{"2":{"1110":1}}],["表明此variable实例是否需要grad",{"2":{"1110":1}}],["表明加入合并后的新字词",{"2":{"582":3}}],["表明",{"2":{"334":1}}],["表明尽管c组件对数学任务的贡献较小",{"2":{"224":1}}],["表明如果一个逐元素计算的函数具有正的一阶和二阶导数",{"2":{"211":1,"213":1}}],["表明在layernorm和适当选择值矩阵的情况下",{"2":{"94":1}}],["表明对于某些类别的值矩阵",{"2":{"94":1}}],["表示可以满足的小孩的最大数量",{"2":{"2150":1}}],["表示可以通过一个固定深度的电路解决的计算问题",{"2":{"480":1}}],["表示他们至少需要多大的饼干才能满足",{"2":{"2147":1}}],["表示信用额度",{"2":{"1873":1}}],["表示利率",{"2":{"1873":1}}],["表示账户余额",{"2":{"1873":1}}],["表示发生了某种错误",{"2":{"1762":1}}],["表示满足其中一个条件",{"2":{"1729":1}}],["表示同时满足两个条件",{"2":{"1729":1}}],["表示是闰年",{"2":{"1729":1}}],["表示年份",{"2":{"1729":1}}],["表示选项或模式",{"2":{"1728":1}}],["表示状态码或错误码",{"2":{"1728":1}}],["表示具有多个相关属性的实体",{"2":{"1728":1}}],["表示访问",{"2":{"1728":1}}],["表示数组的大小",{"2":{"1716":1}}],["表示动物发出的声音",{"2":{"1690":1}}],["表示小狗的行为",{"2":{"1674":1}}],["表示指针指向的内存地址的偏移",{"2":{"1633":1}}],["表示指针不指向任何有效的内存地址",{"2":{"1611":1}}],["表示字符串的结尾",{"2":{"1616":1}}],["表示字节",{"2":{"614":1}}],["表示函数不返回任何值",{"2":{"1611":1}}],["表示该类型不存储符号位",{"2":{"1607":1}}],["表示该单词及其上下文信息的表示",{"2":{"267":1}}],["表示没有值",{"2":{"1607":1}}],["表示程序正常结束",{"2":{"1606":1}}],["表示所有进程组成的通信域",{"2":{"1577":1}}],["表示椭圆的中心",{"2":{"1373":1}}],["表示欧几里得范数",{"2":{"1361":1}}],["表示矩阵的迹运算",{"2":{"1361":1}}],["表示生成图像的分布",{"2":{"1361":1}}],["表示真实图像的分布",{"2":{"1361":1}}],["表示成下面的式子",{"2":{"1342":1}}],["表示复数的实部",{"2":{"1342":1}}],["表示复杂语义概念需要很高的query和key维度",{"2":{"189":1}}],["表示非常低的训练损失",{"2":{"1155":1}}],["表示执行浮点数除法",{"2":{"1083":1}}],["表示执行整数除法或向下取整除法",{"2":{"1083":1}}],["表示d个通道",{"2":{"1004":1}}],["表示损失函数对其求导",{"2":{"1004":1}}],["表示网络下一层的输入通道数等于上一层的输出通道数",{"2":{"1003":1}}],["表示前一层的输出经过激活函数变成下一层的输入",{"2":{"1003":1}}],["表示激活函数relu",{"2":{"1003":1}}],["表示被卷积的输入",{"2":{"1003":1}}],["表示某个位置的输出值",{"2":{"1003":1}}],["表示kv压缩维度",{"2":{"957":1}}],["表示key与query越相关",{"2":{"265":1}}],["表示将右侧的值赋给左侧的变量或表达式",{"2":{"943":1,"961":1}}],["表示重要",{"2":{"866":1}}],["表示不存在",{"2":{"1927":1}}],["表示不是闰年",{"2":{"1729":1}}],["表示不能通过",{"2":{"1614":1}}],["表示不指向任何有效的内存地址",{"2":{"1612":1}}],["表示不重要",{"2":{"866":1}}],["表示不需要额外训练的方法",{"2":{"141":1}}],["表示第j个饼干的大小",{"2":{"2149":1}}],["表示第i个小孩的需求",{"2":{"2149":1}}],["表示第几层",{"2":{"1003":1}}],["表示第t",{"2":{"855":1}}],["表示第3个和第4个为空格",{"2":{"399":1}}],["表示能力",{"2":{"838":1}}],["表示要扩大内核的范围",{"2":{"778":1}}],["表示任意两个相距一个位置的标记之间的相对距离",{"2":{"762":1}}],["表示对key而言由绝对位置编码换成相对于qtqtq",{"2":{"760":1}}],["表示对序列中的每个元素",{"2":{"101":1}}],["表示全局的位置偏差",{"2":{"758":1}}],["表示全局的内容偏差",{"2":{"758":1}}],["表示隐藏状态",{"2":{"739":1}}],["表示隐藏状态的大小",{"2":{"335":1}}],["表示论的妙用在于能将抽象代数问题转为较容易解决的线性代数问题",{"2":{"713":1}}],["表示论将抽象代数结构中的元素",{"2":{"713":1}}],["表示两个向量的方向越相似",{"2":{"692":1}}],["表示掩码",{"2":{"650":1,"932":1}}],["表示比较级和最高级后缀",{"2":{"567":1}}],["表示时态的后缀",{"2":{"567":1}}],["表示每个单词的位置",{"2":{"515":1}}],["表示每个可能的目标语言单词作为当前输出单词的概率",{"2":{"267":1}}],["表示一个可能存在也可能不存在的值",{"2":{"1927":1}}],["表示一个编译时整数序列",{"2":{"1912":1}}],["表示一个指向包含",{"2":{"1705":1}}],["表示一个包含",{"2":{"1705":1}}],["表示一个不包含任何字符的字符串",{"2":{"1704":1}}],["表示一个子字符串或令牌序列",{"2":{"986":1}}],["表示一个新",{"2":{"689":1}}],["表示一个来自小批量bb",{"2":{"343":1}}],["表示一步就可以完成",{"2":{"511":1}}],["表示注意力头的数量",{"2":{"503":1}}],["表示梯度矩阵中的单个神经元",{"2":{"485":1}}],["表示稀疏性随着模型规模的增加而增加",{"2":{"477":1}}],["表示和模型组件",{"2":{"476":1}}],["表示词表中每个单词作为下一个输出单词的概率",{"2":{"445":1}}],["表示相关模块的输出",{"2":{"330":1}}],["表示越相关",{"2":{"277":1}}],["表示为",{"2":{"739":1}}],["表示为嵌入xixi",{"2":{"614":1}}],["表示为样条曲线",{"2":{"155":1}}],["表示为其激活空间中的方向",{"2":{"137":1}}],["表示在每个单独层中进行更新的神经元总数",{"2":{"141":1}}],["表示transformers中隐藏层的维数",{"2":{"141":2}}],["表示需要更新的层数",{"2":{"141":1}}],["表示编辑时需要更新的参数数目",{"2":{"141":1}}],["表示模型中某些能力的减少或删除",{"2":{"139":1}}],["表示技术中存在特定特征",{"2":{"139":1}}],["表示逐元素乘法",{"2":{"105":1,"109":2}}],["表示的是序列的长度",{"2":{"74":1,"380":1}}],["表示它们是",{"2":{"70":1}}],["表示子空间",{"2":{"5":1}}],["表示当前是在序列的第",{"2":{"5":1,"17":1}}],["表示",{"2":{"5":2,"17":2,"141":1,"445":1,"614":2,"713":1,"736":2,"1342":1,"1512":1,"1574":1,"1611":1,"1614":1}}],["值向下取整",{"2":{"2018":1}}],["值向量表示的是分布",{"0":{"128":1},"2":{"96":1}}],["值显示为",{"2":{"1817":2,"1835":2}}],["值传递后",{"2":{"1650":1}}],["值传递函数内部",{"2":{"1650":2}}],["值传递",{"2":{"1650":5,"1729":1}}],["值越低代表两个分布越相似",{"2":{"1361":1}}],["值越大",{"2":{"1360":1,"1398":2}}],["值越接近1",{"2":{"692":1}}],["值太大的时候梯度信息传递过去了",{"2":{"996":1}}],["值块发送到下一个主机",{"2":{"975":1}}],["值块通过主机环进行遍历",{"2":{"975":1}}],["值块",{"2":{"975":2}}],["值可以一起存储",{"2":{"970":1}}],["值和额外的统计信息",{"2":{"944":1,"963":1}}],["值域为",{"2":{"701":1}}],["值是对应的单词频次统计",{"2":{"557":1}}],["值后对输出的影响",{"2":{"485":1}}],["值后输出的影响",{"2":{"148":1}}],["值得指出的是",{"2":{"1343":1}}],["值得注意的是",{"2":{"359":1,"504":1,"879":1,"937":1,"953":1,"1183":1}}],["值得信赖的人工智能应用",{"2":{"140":1}}],["值矩阵",{"2":{"265":1}}],["值的方式选择",{"2":{"1149":1}}],["值的差异",{"2":{"178":1}}],["值的增大",{"2":{"107":1}}],["值对的数量扩展到数百万",{"2":{"154":1}}],["值",{"2":{"118":1,"463":1,"615":2,"647":1,"924":1,"1143":1}}],["值会使可达令牌之间的注意力分配更加均匀",{"2":{"93":1}}],["值为0",{"2":{"692":1}}],["值为",{"2":{"62":1}}],["秩坍塌",{"2":{"446":2}}],["秩崩溃与通用逼近能力的权衡",{"2":{"93":1}}],["秩崩溃越慢",{"2":{"93":1}}],["秩崩溃应该较慢",{"2":{"93":1}}],["秩为1的子空间崩溃仍然会指数级发生",{"2":{"91":1}}],["全损失梯度的",{"2":{"1179":1}}],["全为真则为真",{"2":{"1087":1}}],["全0或常量初始化",{"0":{"992":1}}],["全收集和reduce",{"2":{"976":1}}],["全球变暖",{"2":{"629":1}}],["全球变暖导致海平面上升",{"2":{"629":1}}],["全称为字节对编码",{"2":{"575":1}}],["全秩意味着理论上更强的表达能力",{"2":{"542":1}}],["全新视角理解",{"2":{"513":1}}],["全连接层计算梯度",{"2":{"1098":1}}],["全连接层或cnn只是对数据做仿射变换",{"2":{"838":1}}],["全连接层会接上一个softmax得到预测词的概率",{"2":{"450":1}}],["全连接层的输出大小是词典的大小",{"2":{"450":1}}],["全连接层的计算复杂度和内存需求呈指数级增长",{"2":{"152":1}}],["全连接层",{"2":{"347":1,"1096":1}}],["全连接层进行比对分析",{"2":{"276":1}}],["全局最优",{"2":{"2115":1}}],["全局最优解",{"2":{"1025":1}}],["全局光照",{"2":{"2009":1}}],["全局对象在程序结束时",{"2":{"1676":1}}],["全局静态变量",{"2":{"1649":1}}],["全局变量的使用注意事项",{"2":{"1649":1}}],["全局变量的定义通常放在一个",{"2":{"1628":1}}],["全局变量",{"0":{"1649":1},"2":{"1649":4}}],["全局变量声明",{"2":{"1628":1}}],["全局替换整个文件的文本",{"2":{"1554":1}}],["全局替换",{"2":{"1520":1,"1554":1}}],["全局设置",{"2":{"1095":1}}],["全局信息作为指导进一步增强网络性能",{"2":{"816":1}}],["全局信息查询",{"2":{"258":1}}],["全局平均池化是在整个特征图上计算特征值的平均值",{"2":{"816":1}}],["全局模型使用块因果注意力掩码",{"2":{"614":1}}],["全局统计所有相邻字符对的共现频率",{"2":{"576":1}}],["全局自注意力层",{"0":{"442":1},"2":{"442":1}}],["全局自注意力主要用于处理单个序列内元素之间的关系",{"2":{"439":1}}],["全局自注意力",{"2":{"439":1}}],["全局的关系更好的完成了对序列信息的处理",{"2":{"276":1}}],["全局",{"2":{"204":1}}],["全局注意力模式在编码器每一步中都会基于所有词计算出隐状态",{"2":{"285":1}}],["全局注意力点",{"2":{"204":1}}],["全局注意力就是对于一些特定位置的",{"2":{"204":1}}],["全局注意力",{"2":{"93":1}}],["全面解读andrej",{"2":{"47":1}}],["证明全局最优",{"2":{"2119":1}}],["证明了",{"2":{"204":1}}],["证明了layernorm在某些情况下可以有效缓解令牌的秩崩溃问题",{"2":{"92":1}}],["证明其头部分类重要性的最好方法是修剪其他类别",{"2":{"20":1}}],["现已合并到open",{"2":{"1569":1}}],["现代c++掌握",{"2":{"1961":1}}],["现代",{"2":{"1715":1}}],["现代编译器可能会对",{"2":{"1630":1}}],["现代神经网络是一种非线性统计性数据建模工具",{"2":{"1456":1}}],["现代物理知识杂志",{"2":{"513":1}}],["现时的",{"2":{"683":1}}],["现象的一种手段",{"2":{"309":1}}],["现实生活中得到的训练数据是存在噪声的",{"2":{"399":1}}],["现实中的语料库会大很多",{"2":{"585":1}}],["现实中",{"2":{"268":1}}],["现实世界在不断发展变化",{"2":{"138":1}}],["现有系统浪费了60",{"2":{"981":1}}],["现有llm的主流架构在事实上大致可分为三种主要类型",{"2":{"541":1}}],["现有的系统也无法自动适应所有场景",{"2":{"985":1}}],["现有的嵌入方法通常依赖于静态架构",{"2":{"738":1}}],["现有的llm几乎完全端到端训练",{"2":{"612":1}}],["现有的研究通常会混合几个现成的数据集",{"2":{"367":1}}],["现有的注意力权重操作围绕计算item",{"2":{"209":1}}],["现有的方法主要集中在研究前向传播的隐状态和权重的映射上",{"2":{"147":1}}],["现有的关于秩崩溃的文献大多忽略了transformer中的其他关键组件",{"2":{"91":1}}],["现有工作表明",{"2":{"140":1}}],["现有范围之外的新知识来实现这一点",{"2":{"140":1}}],["现有大量研究工作致力于揭示",{"2":{"123":1}}],["现在我也并不会觉得我当时的理解有错",{"2":{"2054":1}}],["现在我们对类有了初步了解",{"2":{"1678":1}}],["现在我们应该有足够的信息来计算细胞状态",{"2":{"867":1}}],["现在我们新的的子词表如下",{"2":{"582":1}}],["现在我们已经做出了正确分类",{"2":{"320":1}}],["现在看来以前我真的挺无知的",{"2":{"2054":1}}],["现在存储",{"2":{"1926":3}}],["现在流行用下面的方式",{"2":{"1628":1}}],["现在可以用任意的大小来初始化bitset了",{"2":{"2063":1}}],["现在可以解引用了",{"2":{"1611":1}}],["现在可以通过一条简单的命令尝试vllm",{"2":{"980":1}}],["现在让我们深入了解",{"2":{"1607":1}}],["现在有一个预测后一步位置梯度的步骤",{"2":{"1034":1}}],["现在token列表为",{"2":{"585":1}}],["现在编码器",{"2":{"473":1}}],["现在的bert",{"2":{"1337":1}}],["现在的最高频率是",{"2":{"582":1}}],["现在的大模型都是decoder",{"2":{"396":1}}],["现在的神经网络很少有将",{"2":{"172":1}}],["现在又提到数据加载器会利用collate",{"2":{"375":1}}],["现在恢复更新",{"2":{"235":1}}],["现在",{"2":{"193":1,"1175":1,"1683":1}}],["现在是变量",{"2":{"1612":1}}],["现在是",{"2":{"185":1}}],["现在被拼接成一个更长的单个序列",{"2":{"88":1}}],["现在思考一个问题",{"2":{"72":1}}],["现在用几乎同样的参数量和算力",{"2":{"43":1}}],["现在还需要拼回去",{"2":{"35":1}}],["现在每个新单词都融入了其它单词的部分信息",{"2":{"261":1}}],["现在每个",{"2":{"29":1}}],["与第三方依赖管理",{"0":{"1988":1},"1":{"1989":1,"1990":1,"1991":1}}],["与现代",{"2":{"1964":1}}],["与类的访问权限",{"2":{"1775":1}}],["与类名相同",{"2":{"1675":1}}],["与结构体结合使用",{"2":{"1728":1}}],["与函数模板类似",{"2":{"1700":1}}],["与栈",{"2":{"1668":1}}],["与成员函数和静态成员变量无关",{"2":{"1653":1}}],["与本节课讨论的内存管理有所区别",{"2":{"1648":1}}],["与原问题结构相同的子问题",{"2":{"1646":1}}],["与空指针比较",{"2":{"1633":1}}],["与通信后并协调它们的行为而形成的系统",{"2":{"1563":1}}],["与系统其他部分相对而言",{"2":{"1478":1}}],["与硬盘",{"2":{"1477":1}}],["与目标值",{"2":{"1443":1}}],["与从头训练相比",{"2":{"1313":1}}],["与我们进行了有益的讨论",{"2":{"1194":1}}],["与我们现有的配置相比",{"2":{"1152":1}}],["与对20个试验进行抽样时的四分位间距有多大的区别",{"2":{"1177":1}}],["与自定义命令",{"0":{"1992":1},"1":{"1993":1,"1994":1}}],["与自适应算法相比",{"2":{"1175":1}}],["与自注意力机制在序列中彼此交流不同",{"2":{"466":1}}],["与更复杂的黑盒优化工具",{"2":{"1175":1}}],["与初始学习率的关系",{"2":{"1147":1}}],["与batch",{"2":{"1135":1}}],["与bpe相比",{"2":{"597":1}}],["与无梯度模式和推断模式完全无关",{"2":{"1122":1}}],["与无梯度模式类似",{"2":{"1121":1}}],["与批量梯度下降",{"2":{"1026":1}}],["与dropout不同的是",{"2":{"1018":1}}],["与正常的反向传播推导不一样",{"2":{"1004":1}}],["与典型的树不同",{"2":{"986":1}}],["与传统方法不同",{"2":{"985":1}}],["与传统的注意力算法不同",{"2":{"981":1}}],["与传统的序列并行",{"2":{"977":1}}],["与传统的基于token的模型不同",{"2":{"612":1}}],["与gemm的异步wgmma指令重叠",{"2":{"973":1}}],["与没有因果掩码的注意力相比",{"2":{"970":1}}],["与经典rnn结构不同的是",{"2":{"886":1}}],["与cuda",{"2":{"792":1}}],["与生成一个向量的偏差更小呢",{"2":{"736":1}}],["与encoder的multi",{"2":{"650":1,"932":1}}],["与基础lcm相似",{"2":{"633":1}}],["与基线",{"2":{"354":1}}],["与单词相比",{"2":{"628":1}}],["与这些",{"2":{"621":1}}],["与局部编码器类似",{"2":{"614":1}}],["与固定词汇表的token化不同",{"2":{"614":1}}],["与tokenize",{"2":{"557":1}}],["与编码器",{"2":{"541":1}}],["与编码器类似",{"2":{"525":1,"915":1}}],["与编码器不同",{"2":{"515":1}}],["与总体架构那章的流程图相比较",{"2":{"515":1}}],["与此相对",{"2":{"512":1}}],["与此同时",{"2":{"211":1,"213":1,"540":1,"765":1,"1127":1,"1312":1}}],["与输入",{"2":{"504":1}}],["与具体的函数",{"2":{"504":1}}],["与业界公司通过实验来探索不同",{"2":{"498":1}}],["与上面类似",{"2":{"457":1}}],["与互联网上可以获取的其他transformer的模型实现相比较",{"2":{"432":1}}],["与sgd相比",{"2":{"1027":2}}],["与seq2seq模型的区别在于",{"2":{"427":1}}],["与softmax",{"2":{"183":1}}],["与所有归一化层中使用的参数相同",{"2":{"360":1}}],["与所有键",{"2":{"158":1}}],["与归一化层不同",{"2":{"359":1}}],["与均方根归一化",{"2":{"346":1}}],["与lstm非常相似",{"2":{"874":1}}],["与leakyrelu",{"2":{"843":1}}],["与ln相比",{"2":{"346":1}}],["与layernorm和softmax不同",{"2":{"327":1}}],["与ft+1",{"2":{"334":1}}],["与矩阵计算相比",{"2":{"327":1}}],["与线性运算相比",{"2":{"327":1}}],["与节点的消除相对应的消除奇异性",{"2":{"305":1}}],["与距离无关的依赖",{"2":{"284":1}}],["与计算复杂度类似",{"2":{"279":1}}],["与之对照",{"2":{"407":1}}],["与之对应",{"2":{"279":1}}],["与之前的函数声明冲突",{"2":{"1687":1}}],["与之前的简单提及不同",{"2":{"1678":1}}],["与之前的",{"2":{"976":1}}],["与之前",{"2":{"50":1}}],["与rnn相比",{"2":{"290":1}}],["与rnn",{"0":{"871":1},"2":{"273":1}}],["与relu相比",{"2":{"107":1}}],["与历史输入",{"2":{"239":1}}],["与其本质没有太大区别",{"2":{"2112":1}}],["与其余的cuda核心执行逻辑",{"2":{"973":1}}],["与其它激活函数有何不同",{"2":{"847":1}}],["与其他的标签和输入没有关系",{"2":{"1322":1}}],["与其他",{"2":{"624":1}}],["与其他一些魔法数",{"2":{"343":1}}],["与其从头开始进行预训练",{"2":{"541":1}}],["与其试图一次性训练",{"2":{"222":1}}],["与其宾语",{"2":{"131":1}}],["与部署在环境中的静态",{"2":{"220":1}}],["与序列长度成线性关系",{"2":{"210":1}}],["与item集合的乘积相当于对值进行求和",{"2":{"209":1}}],["与附近的几个",{"2":{"204":1}}],["与以往工作的不同在于",{"2":{"185":1}}],["与以前的工作相比",{"2":{"154":1}}],["与argmax操作中暴力地选出一个最大值",{"2":{"180":1}}],["与attention中token之间的语义关联权重不是一个意思",{"2":{"172":1}}],["与可训练的路由模块一起使用",{"2":{"150":1}}],["与事实性知识相比",{"2":{"140":1}}],["与时俱进的信息",{"2":{"140":1}}],["与简单地对现有方法进行修改不同",{"2":{"139":1}}],["与人类可解释的概念进行匹配",{"2":{"137":1}}],["与压缩感知一样",{"2":{"118":1}}],["与",{"0":{"845":1,"1602":1},"2":{"90":1,"170":1,"217":1,"332":1,"346":1,"406":1,"498":1,"553":1,"601":2,"623":1,"702":1,"713":2,"975":1,"1086":1,"1119":1,"1143":2,"1315":1,"1604":1,"1605":1,"1653":1,"1705":2,"1713":1,"1715":2,"1728":2,"1802":1}}],["几种模式简介",{"0":{"1289":1},"1":{"1290":1,"1291":1,"1292":1,"1293":1}}],["几种模式概览",{"0":{"1285":1},"1":{"1286":1,"1287":1,"1288":1}}],["几种normalization",{"2":{"806":1}}],["几何图元",{"2":{"2009":1}}],["几何和微积分不再适用",{"2":{"689":1}}],["几何向量",{"2":{"680":1}}],["几乎很快就找到了正确的方向并前进",{"2":{"1057":1}}],["几乎所有的值集中在",{"2":{"995":1}}],["几乎所有的输出激活值都很接近0",{"2":{"994":1}}],["几乎所有现有的机器翻译模型都建立在基于字符的词汇表之上",{"2":{"606":1}}],["几乎将测得的tflops",{"2":{"973":1}}],["几乎也会更新",{"2":{"714":1}}],["几乎是连在一起用的",{"2":{"714":1}}],["几乎没有冗余计算",{"2":{"90":1}}],["几部分",{"2":{"576":1}}],["几次随机输出会导致随后的训练很难学到任何东西",{"2":{"239":1}}],["几点说明",{"2":{"235":1}}],["几个变压器从电线杆上摔下来",{"2":{"259":1}}],["几个有趣的思路如下",{"2":{"218":1}}],["几个经典模型如下",{"2":{"204":1}}],["几个典型做法如下",{"2":{"204":1}}],["几个注意点是",{"2":{"74":1}}],["随时随地",{"2":{"2056":1}}],["随时随地地见习",{"2":{"2054":1}}],["随手记得关一下",{"2":{"2054":1}}],["随便给的初始值",{"2":{"1398":1}}],["随意复制生成的衰减方案通常不是一个好主意",{"2":{"1173":1}}],["随后的一系列工作对",{"2":{"1315":1}}],["随后出现恢复",{"2":{"1179":1}}],["随后发现使用梯度下降算法优化resnets得到的平滑性",{"2":{"497":1}}],["随后可以将隐藏层状态的导数作为一个参数",{"2":{"495":1}}],["随后又除以batch",{"2":{"398":1}}],["随后query会根据两个词之间的亲密关系来决定从v中提取出多少信息出来融入到自身",{"2":{"265":1}}],["随后将ff2ff2ff",{"2":{"148":1}}],["随机访问",{"2":{"1797":1}}],["随机变量序列",{"2":{"1322":1}}],["随机数生成器",{"2":{"1169":1}}],["随机梯度下降",{"2":{"1026":1}}],["随机色彩处理和许多其他技术来更改图像的色彩空间",{"2":{"1015":1}}],["随机生成的值集中在距离原点位置为",{"2":{"995":1}}],["随机生成的值都较小",{"2":{"994":1}}],["随机生成的网络结构最具多样性",{"2":{"396":1}}],["随机mask掉其中一部分token得到",{"2":{"727":1}}],["随机mask掉其中一小部分token后得到",{"2":{"727":1}}],["随机初始化其实很难的",{"2":{"996":1}}],["随机初始化是很多人经常使用的方法",{"2":{"993":1}}],["随机初始化",{"0":{"993":1},"1":{"994":1,"995":1,"996":1},"2":{"709":1}}],["随机注意力",{"2":{"204":1}}],["随机",{"2":{"90":1,"1059":1}}],["随机采样",{"2":{"90":2}}],["随着自己的能力的提升后续尝试自己证明",{"2":{"2117":1}}],["随着自己的成长和认知的提升",{"2":{"2097":1}}],["随着人工智能的发展",{"2":{"2107":1}}],["随着科技的进步",{"2":{"2107":1}}],["随着慢慢的成长",{"2":{"2054":1}}],["随着项目的规模扩大",{"2":{"1916":1}}],["随着程序规模的增大",{"2":{"1729":1}}],["随着程序的运行",{"2":{"1647":1,"1671":1}}],["随着多核处理器和分布式计算系统的发展",{"2":{"1564":1}}],["随着隐层层数的增多",{"2":{"1465":1}}],["随着隐层的增加",{"2":{"1465":1}}],["随着我们的探索",{"2":{"1139":1}}],["随着batch",{"2":{"1132":1}}],["随着网络层数的加深",{"2":{"999":1}}],["随着网络层数的增加",{"2":{"309":1}}],["随着层数的增加",{"2":{"994":1}}],["随着层数继续加深",{"2":{"122":1}}],["随着生成新标记",{"2":{"982":1}}],["随着模型维度的增加而扩展",{"2":{"937":1,"953":1}}],["随着模型规模的扩大",{"2":{"152":1}}],["随着细胞状态的旅程进行",{"2":{"863":1}}],["随着序列长度的增加",{"2":{"765":1,"904":1}}],["随着机器学习技术的进步",{"2":{"711":1}}],["随着你不断重复这个过程",{"2":{"709":1}}],["随着优化算法",{"2":{"709":1}}],["随着深度学习的成熟并对世界产生更大的影响",{"2":{"1127":1}}],["随着深度学习的出现",{"2":{"708":1}}],["随着深度学习技术的出现",{"2":{"711":1}}],["随着深度学习模型进行训练",{"2":{"437":1}}],["随着数据维数的增加",{"2":{"692":1}}],["随着数据在rnn中的流动",{"2":{"248":1,"249":1}}],["随着合并的次数增加",{"2":{"576":1,"582":1}}],["随着改变游戏规则的llm",{"2":{"540":1}}],["随着输入长度的增加",{"2":{"500":1}}],["随着上下文变长",{"2":{"500":1}}],["随着上下文长度的增加",{"2":{"273":1}}],["随着神经网络的诞生及后续的辉煌发展",{"2":{"486":1}}],["随着吸收基础知识增多",{"2":{"402":1}}],["随着向量维度的增大",{"2":{"175":1}}],["随着各个新兴领域和实体的涌现与发展",{"2":{"140":1}}],["随着",{"2":{"107":1}}],["随着时间的推移",{"2":{"98":1,"103":1,"540":1}}],["随着训练的进行",{"2":{"26":1,"686":1}}],["随着所在层数变大而减少",{"2":{"20":1}}],["存钱买房",{"2":{"2102":1}}],["存入1000元",{"2":{"1873":1}}],["存入",{"2":{"1873":1}}],["存款金额为负数",{"2":{"1766":1}}],["存内计算",{"2":{"206":1}}],["存在问题",{"2":{"1661":1}}],["存在分支",{"2":{"1442":1}}],["存在着临界资源",{"2":{"1412":1}}],["存在着重叠的元素和不相交的元素",{"2":{"143":1}}],["存在",{"2":{"1145":1}}],["存在两个主要原因限制了in",{"2":{"1123":1}}],["存在的短期记忆问题",{"2":{"861":1}}],["存在的问题",{"0":{"861":1}}],["存在的不足",{"2":{"138":1}}],["存在有限个点处不可微",{"2":{"838":1}}],["存在大量低频词语占据词表空间",{"2":{"565":1}}],["存在一定的稀疏性",{"2":{"848":1}}],["存在一个相应的有限提示πφπφπ",{"2":{"504":1}}],["存在一个固定大小的transformer",{"2":{"504":1}}],["存在一些问题",{"2":{"331":1}}],["存在一些",{"2":{"320":1}}],["存在一些缺点",{"2":{"155":1}}],["存在严重的维数灾难问题",{"2":{"155":1}}],["存在冗余计算",{"2":{"90":1}}],["存储系统专家",{"2":{"1953":1}}],["存储引擎开发",{"2":{"1952":1}}],["存储关联数据",{"2":{"1805":1}}],["存储字符串值",{"2":{"1728":1}}],["存储浮点数值",{"2":{"1728":1}}],["存储整数值",{"2":{"1728":1}}],["存储不同的动物对象",{"2":{"1690":1}}],["存储不同group",{"2":{"1226":1}}],["存储账户余额",{"2":{"1677":1}}],["存储内容",{"2":{"1648":1}}],["存储内存地址的变量",{"2":{"1612":1}}],["存储相同类型数据的集合",{"0":{"1623":1}}],["存储的是内存地址",{"2":{"1612":1}}],["存储的是指针",{"2":{"1611":1}}],["存储的数据类型",{"2":{"1083":1}}],["存储了变量",{"2":{"1611":1}}],["存储了上下文信息的h要远大于e",{"2":{"698":1}}],["存储范围比",{"2":{"1607":2}}],["存储线程的列表",{"2":{"1566":1}}],["存储完整的爬取内容",{"2":{"1566":1}}],["存储优化器的全局超参数",{"2":{"1226":1}}],["存储子模块",{"2":{"1208":1}}],["存储模块中的非持久性缓冲区",{"2":{"1208":1}}],["存储当前variable实例的梯度",{"2":{"1110":1}}],["存储在连续的内存位置",{"2":{"1634":1}}],["存储在",{"2":{"942":1,"959":1}}],["存储在长期记忆中的单个概念向量可以通过不同的函数进行投影",{"2":{"4":1}}],["存储我们反向函数的地方",{"2":{"661":1,"1104":1}}],["存储事实之间的关联信息",{"2":{"145":1}}],["存储subject信息",{"2":{"145":1}}],["存储和管理事实知识的机制",{"2":{"121":1}}],["存储",{"2":{"121":1,"1226":7,"1926":3,"1928":2}}],["存储知识",{"0":{"118":1},"2":{"96":1,"115":1}}],["方便定位和解决问题",{"2":{"1764":1}}],["方便性",{"2":{"1715":1}}],["方便项目管理",{"2":{"1605":1}}],["方便地在一段代码或函数中禁用梯度",{"2":{"1120":1}}],["方便预测第一个token",{"2":{"528":1}}],["方面进行",{"2":{"941":1,"960":1}}],["方法和",{"2":{"1874":1}}],["方法关闭文件",{"2":{"1820":1,"1838":1}}],["方法打开文件",{"2":{"1820":1,"1838":1}}],["方法应该返回清晰易懂的错误描述",{"2":{"1764":1}}],["方法应用这些函数对象来计算图的求值结果",{"2":{"1113":1}}],["方法获取异常信息",{"2":{"1762":1}}],["方法二",{"2":{"1481":1}}],["方法一",{"2":{"1481":1}}],["方法中的后置钩子函数",{"2":{"1226":1}}],["方法中的前置钩子函数",{"2":{"1226":1}}],["方法中的",{"2":{"1226":1}}],["方法中只会保留一个单独的key",{"2":{"938":1,"954":1}}],["方法会被编译",{"2":{"1214":1}}],["方法与几种类似机制之间的比较",{"2":{"1214":1}}],["方法内部调用",{"2":{"1214":1}}],["方法全解",{"0":{"1214":1}}],["方法是查找以",{"2":{"1114":1}}],["方法是目前llm的主流切词粒度",{"2":{"567":1}}],["方法来提供自定义的错误信息",{"2":{"1763":1}}],["方法来获取其字符串表示",{"2":{"1227":1}}],["方法来获取对象的字符串表示",{"2":{"1083":1}}],["方法来访问",{"2":{"1110":1}}],["方法调用",{"2":{"1085":1,"1227":1}}],["方法汇总",{"0":{"1083":1,"1084":1},"1":{"1085":1,"1086":1,"1087":1}}],["方法简介",{"0":{"908":1}}],["方法可以支持多种语言",{"2":{"696":1}}],["方法能够保留上下文信息",{"2":{"568":1}}],["方法通常取决于多个因素",{"2":{"568":1}}],["方法",{"0":{"1074":1,"1227":1},"1":{"1075":1,"1076":1},"2":{"209":1,"224":1,"485":1,"1214":1,"1223":1,"1226":3,"1241":1,"1242":1,"1312":1,"1491":1,"1709":1,"1713":2,"1715":1}}],["方法将",{"2":{"204":1}}],["方差计算数学基础",{"0":{"1002":1}}],["方差为",{"2":{"309":1}}],["方差为dkdkd",{"2":{"189":1}}],["方差为1的独立同分布随机变量",{"2":{"189":1}}],["方差为1",{"2":{"189":2,"517":1}}],["方差会变大",{"2":{"188":1}}],["方差变大会导致向量之间元素的差值变大",{"2":{"188":1,"190":1}}],["方差变大",{"0":{"189":1},"2":{"157":1}}],["方案很重要并且调整它很重要",{"2":{"1171":1}}],["方案",{"0":{"485":1,"489":1}}],["方案也属于动态调整学习率的一种",{"2":{"401":1}}],["方案选择",{"0":{"175":1},"2":{"157":1}}],["方案以及它们的影响",{"2":{"90":1}}],["方程",{"2":{"145":3}}],["方向的差值更大",{"2":{"2018":2}}],["方向等去计算不同格式数据之间的相关性",{"2":{"692":1}}],["方向移动",{"2":{"485":1}}],["方向",{"2":{"137":1}}],["方式二",{"2":{"1299":1}}],["方式一",{"2":{"1299":1}}],["方式2",{"2":{"1099":1,"1223":1,"1273":1}}],["方式1",{"2":{"1099":1,"1223":1,"1273":1}}],["方式及与",{"2":{"90":1}}],["方式",{"2":{"89":1}}],["方式把8个子进程的结果串联起来",{"2":{"10":1}}],["按频率降序排序",{"2":{"1933":1}}],["按频率从高到低排序",{"2":{"1933":1}}],["按钮点击事件的处理函数就是一个回调函数",{"2":{"1645":1}}],["按回车接受默认设置",{"2":{"1594":1}}],["按以下快捷键可以快速进入插入模式",{"2":{"1545":1}}],["按文件名查找",{"2":{"1532":1}}],["按",{"2":{"1519":3,"1541":3,"1553":1,"1559":1}}],["按序",{"2":{"1404":1}}],["按序列展开形式如下",{"2":{"887":1}}],["按优先顺序列出",{"2":{"1242":1}}],["按设备和数据类型对张量的列表进行分组",{"2":{"1227":1}}],["按转移概率的降序排列",{"2":{"1158":1}}],["按行优先的方式排列",{"2":{"1705":1}}],["按行",{"2":{"941":1,"960":1}}],["按时间步展开如下",{"0":{"854":1}}],["按子词粒度分词",{"2":{"567":1}}],["按子词粒度",{"0":{"567":1}}],["按字符粒度",{"0":{"566":1},"2":{"564":1}}],["按单词粒度",{"0":{"565":1},"2":{"564":1}}],["按切分文本的颗粒度",{"2":{"564":1}}],["按位逻辑运算",{"2":{"2062":1}}],["按位取反",{"2":{"1630":1,"2059":1}}],["按位取反操作符",{"2":{"1085":1}}],["按位异或",{"2":{"1630":1,"2059":1}}],["按位或",{"2":{"1630":1,"2059":1}}],["按位与",{"2":{"1630":1,"2059":1}}],["按位置进行变换",{"2":{"419":1}}],["按位相乘",{"2":{"71":1}}],["按论文中的描述",{"2":{"347":1}}],["按照代码行顺序编译",{"2":{"1653":1}}],["按照提示",{"2":{"1594":1}}],["按照实现方式划分",{"0":{"1569":1}}],["按照处理机划分",{"0":{"1568":1}}],["按照逻辑先后顺序反向传播算法",{"2":{"1441":1}}],["按照other",{"2":{"1087":1}}],["按照另一个tensor的",{"2":{"1083":1}}],["按照某个维度对张量进行反转操作",{"2":{"1083":1}}],["按照前向传播最后的示例",{"2":{"1004":1}}],["按照高斯分布来初始化的话",{"2":{"1000":1}}],["按照glorot条件",{"2":{"1000":1}}],["按照我们之前encoder",{"2":{"909":1}}],["按照对齐方式a翻译成x的概率",{"2":{"908":1}}],["按照device属性分发",{"2":{"662":1}}],["按照深度优先遍历sub",{"2":{"662":1}}],["按照词语频率将词语写入词汇表",{"2":{"557":1}}],["按照上述流程进行训练会过于缓慢",{"2":{"405":1}}],["按照上述思路",{"2":{"267":1}}],["按照一定的采样规则来采样下一个token",{"2":{"397":1,"431":1}}],["按照batchnorm的思路",{"2":{"316":1}}],["按照序列顺序一个一个token来整合信息",{"2":{"287":1}}],["按照",{"2":{"210":1}}],["按照这种方式可以保证和",{"2":{"89":1}}],["红点是embdding",{"2":{"890":1}}],["红圈对应就是generator类",{"2":{"473":1}}],["红框",{"2":{"89":1}}],["红色表示已被淘汰的节点",{"2":{"986":1}}],["红色箭头",{"2":{"940":1,"962":1}}],["红色圈是解码器层",{"2":{"436":1}}],["红色",{"2":{"305":1}}],["红色图说明该头更关注局部信息",{"2":{"18":1}}],["红色和蓝色虚线框部分",{"2":{"17":1}}],["特效等功能",{"2":{"2009":1}}],["特有的特性",{"2":{"1612":1}}],["特别注意",{"2":{"1607":1}}],["特别适用于需要高性能的计算应用",{"2":{"1569":1}}],["特别是数学相关的",{"2":{"2117":1}}],["特别是对于含有大量资源的对象",{"2":{"1888":1}}],["特别是大脑",{"2":{"1456":1}}],["特别是如果它们在设计时未考虑神经网络超参数调整",{"2":{"1175":1}}],["特别是如果有许多张量引用相同的存储",{"2":{"1123":1}}],["特别是处理高曲率",{"2":{"1028":1}}],["特别是当训练数据较少时",{"2":{"898":1}}],["特别是通过自注意力机制",{"2":{"719":1}}],["特别是针对低频词的词表征",{"2":{"561":1}}],["特别是在实时模拟和虚拟人物表情渲染等技术上取得了显著突破",{"2":{"2011":1}}],["特别是在光线追踪和蒙特卡洛方法中",{"2":{"2009":1}}],["特别是在宏函数中使用自增自减运算符时",{"2":{"1632":1}}],["特别是在调整学习率衰减计划时",{"2":{"1154":1}}],["特别是在存在噪声或不稳定梯度的情况下",{"2":{"1031":1}}],["特别是在迭代次数较少的情况下",{"2":{"1026":1}}],["特别是在llms的异常特征的情况下",{"2":{"973":1}}],["特别是在密集段落检索和问答任务中",{"2":{"692":1}}],["特别是在自注意力和ffn中",{"2":{"413":1}}],["特别是在处理大规模数据时",{"2":{"204":1}}],["特别是",{"2":{"93":1,"123":1}}],["特定项目或工作流的偶然案例研究",{"2":{"1127":1}}],["特定任务相关的注意力头al",{"2":{"122":1}}],["特色",{"2":{"765":1}}],["特殊token",{"2":{"591":1}}],["特殊token添加",{"2":{"555":1}}],["特殊字符",{"2":{"379":1}}],["特点和使用",{"2":{"1797":1,"1805":1,"1806":1,"1807":1}}],["特点",{"0":{"1031":1,"1044":1},"2":{"1025":1,"1026":1,"1027":1,"1569":6,"1639":1,"1640":1,"1675":1,"1676":1,"1799":1,"1800":1,"1801":1,"1802":1,"1803":1}}],["特点如下",{"2":{"322":2}}],["特点的激活函数",{"2":{"107":1}}],["特",{"2":{"253":1,"263":3}}],["特性",{"2":{"89":1,"1612":1}}],["特征学习",{"2":{"1455":1}}],["特征维度较高和模型复杂度较高等情况下容易出现",{"2":{"1012":1}}],["特征维度较高",{"2":{"1012":1}}],["特征维度大小",{"2":{"343":1}}],["特征维数为d",{"2":{"210":1}}],["特征图上",{"2":{"1019":1}}],["特征图",{"2":{"321":1}}],["特征变得独立同分布",{"2":{"310":1}}],["特征模型可能是rnn",{"2":{"263":1}}],["特征模型",{"2":{"263":1}}],["特征向量f和查询向量q是注意力模型的输入",{"2":{"263":1}}],["特征向量",{"2":{"169":1}}],["特征会",{"2":{"137":1}}],["特征与输出结果有因果关系",{"2":{"118":1}}],["特征提取能力或者表示学习的能力及其有限",{"2":{"172":1}}],["特征提取",{"2":{"12":1}}],["特征",{"2":{"3":1,"321":1,"1648":1}}],["长远规划",{"0":{"2102":1}}],["长整型",{"2":{"1607":1}}],["长的像的图片离得近",{"2":{"1375":1}}],["长的样本和极短的样本出现在同一个",{"2":{"88":1}}],["长短期记忆",{"2":{"862":1}}],["长短期记忆网络",{"0":{"862":1},"1":{"863":1,"864":1,"865":1,"866":1,"867":1,"868":1,"869":1,"870":1,"871":1}}],["长颈鹿版",{"2":{"768":1}}],["长颈鹿骑着鲨鱼",{"2":{"156":2}}],["长文本压测结果",{"2":{"768":1}}],["长文本训练在批大小大于一的情况下可能会因为",{"2":{"87":1}}],["长",{"2":{"683":1}}],["长度为",{"2":{"1713":1}}],["长度外推与长度和位置有很强的相关性",{"2":{"756":1}}],["长度外推",{"0":{"756":1},"2":{"741":1}}],["长度外推性是一个训练和预测的不一致问题",{"2":{"204":1}}],["长度可能会略长于bpe",{"2":{"608":1}}],["长度",{"2":{"607":1,"692":2,"1329":1}}],["长度不同的序列属于不同的统计对象",{"2":{"316":1}}],["长依赖",{"0":{"246":1}}],["长期记忆分支",{"2":{"229":1}}],["长期记忆",{"0":{"230":1},"2":{"157":1}}],["长序列长度",{"2":{"976":1}}],["长序列",{"2":{"95":1}}],["三维建模",{"2":{"1956":1}}],["三元条件运算符",{"2":{"1635":1}}],["三目运算符更简洁",{"2":{"1630":1}}],["三目运算符",{"2":{"1630":1}}],["三级指针",{"2":{"1611":1}}],["三项中将绝对位置编码改为相对位置编码",{"2":{"763":1}}],["三角函数位置编码",{"2":{"747":1}}],["三角函数式",{"0":{"750":1,"1336":1},"2":{"741":1,"1336":1}}],["三思而后行",{"2":{"746":1}}],["三原色",{"2":{"687":1}}],["三种继承方式的对比",{"0":{"1862":1},"1":{"1863":1,"1864":1}}],["三种传参方式的区别",{"2":{"1650":1}}],["三种变量的作用域和生命周期",{"2":{"1649":1}}],["三种embedding的相加",{"2":{"722":1}}],["三种方法切分结果如下",{"2":{"564":1}}],["三种注意力模块在transformer网络对应位置如下图所示",{"2":{"440":1}}],["三大变体之decoder",{"2":{"543":1}}],["三者自然也不会涉及比较的问题",{"2":{"316":1}}],["三者是相同的",{"2":{"72":1}}],["三个函数",{"2":{"1710":1}}],["三个矩阵的形状均为",{"2":{"944":1,"963":1}}],["三个地方见过",{"2":{"667":1}}],["三个注意力模块的作用如下",{"2":{"535":1}}],["三个维度",{"2":{"420":1}}],["三个维度做归一化",{"2":{"325":1}}],["三个句子是独立处理的",{"2":{"316":1}}],["三个权重矩阵都是可训练的",{"2":{"172":1}}],["三个考虑因素我们具体解析如下",{"2":{"160":1}}],["三个输入",{"2":{"158":1}}],["三个样本头尾相接成一个序列",{"2":{"88":1}}],["三",{"0":{"1570":1,"1604":1,"1613":1,"1639":1,"1647":1,"1686":1,"1695":1,"1706":1},"1":{"1571":1,"1572":1,"1573":1,"1574":1,"1575":1,"1576":1,"1577":1,"1578":1,"1579":1,"1580":1,"1687":1,"1688":1,"1690":1,"1691":1},"2":{"156":1,"429":1}}],["补丁列表记录了模块中所有可导操作的详细信息",{"2":{"1208":1}}],["补丁",{"2":{"610":1}}],["补语和各种从句等附加方式",{"2":{"252":1}}],["补更",{"2":{"233":1}}],["补全到最大句子长度",{"2":{"376":1}}],["补全到",{"2":{"87":1}}],["补充",{"0":{"1715":1},"2":{"80":1}}],["返回得到满足的孩子数量",{"2":{"2153":1}}],["返回字符串流中当前包含的字符串的副本",{"2":{"1824":1,"1842":1}}],["返回字符串的最后一个字符的引用",{"2":{"1713":1}}],["返回字符串的第一个字符的引用",{"2":{"1713":1}}],["返回字符串的长度",{"2":{"1713":1}}],["返回上一次非格式化输入操作读取的字符数",{"2":{"1813":1,"1831":1}}],["返回上一级目录",{"2":{"1509":1}}],["返回当前已分配的存储空间大小",{"2":{"1713":1}}],["返回当前用户的主目录",{"2":{"1509":1}}],["返回等",{"2":{"1709":1}}],["返回指向该元素的迭代器",{"2":{"1724":1,"1725":1}}],["返回指向字符串开头之前一个位置的反向迭代器",{"2":{"1713":1}}],["返回指向字符串最后一个字符的反向迭代器",{"2":{"1713":1}}],["返回指向字符串末尾之后一个位置的迭代器",{"2":{"1713":1}}],["返回指向字符串第一个字符的迭代器",{"2":{"1713":1}}],["返回指向动态分配内存的指针",{"2":{"1706":1}}],["返回指定范围的子串",{"2":{"1713":1}}],["返回指针的函数",{"2":{"1706":1}}],["返回动态分配的",{"2":{"1706":1}}],["返回结果",{"2":{"1706":1}}],["返回类型推导也可以用于递归函数",{"2":{"1905":1}}],["返回类型",{"2":{"1693":1}}],["返回类实例的名称",{"2":{"1214":1}}],["返回变量或类型的大小",{"2":{"1630":1}}],["返回布尔值",{"2":{"1630":1,"1642":1}}],["返回步骤",{"2":{"1621":1}}],["返回两个整数相除后的余数",{"2":{"1607":1}}],["返回值本身可能就是合法的结果",{"2":{"1761":1}}],["返回值歧义",{"2":{"1761":1}}],["返回值判断",{"2":{"1761":1}}],["返回值的类型必须与函数定义的返回值类型一致",{"2":{"1729":1}}],["返回值类型",{"2":{"1699":1,"1729":1}}],["返回值",{"2":{"1606":1,"1729":1}}],["返回状态",{"2":{"1590":1}}],["返回该对象的属性和方法的名称列表",{"2":{"1214":1}}],["返回的是",{"2":{"1678":1}}],["返回的是指针本身的大小",{"2":{"1667":1}}],["返回的是整个数组占用的内存大小",{"2":{"1667":1}}],["返回的对象是一个浅拷贝",{"2":{"1214":1}}],["返回的张量将是输入的视图",{"2":{"819":1}}],["返回反向传播pre",{"2":{"1214":1}}],["返回用于在调用函数中使用的反向传播钩子",{"2":{"1214":1}}],["返回排序的索引",{"2":{"1087":1}}],["返回拉平后最小值的索引",{"2":{"1087":1}}],["返回最大值的索引",{"2":{"1087":1}}],["返回最终结果",{"2":{"113":1}}],["返回输入张量在给定维度",{"2":{"1087":2}}],["返回输入张量的唯一元素",{"2":{"1083":1}}],["返回共轭张量的视图",{"2":{"1087":1}}],["返回稀疏tensor",{"2":{"1086":1}}],["返回tensor",{"2":{"1086":1,"1087":1}}],["返回给定张量的矩阵范数或向量范数",{"2":{"1083":1}}],["返回与张量关联的底层数据存储对象",{"2":{"1083":1}}],["返回原始数据的不同shape",{"2":{"820":1}}],["返回英语token列表",{"2":{"557":1}}],["返回德语token列表",{"2":{"557":1}}],["返回德语词典和英语词典",{"2":{"557":1}}],["返回",{"2":{"557":1,"944":1,"1083":1,"1087":1,"1646":1,"1761":1,"1814":4,"1832":4}}],["返回平均损失",{"2":{"385":1}}],["返回平均损失和训练状态",{"2":{"385":1}}],["返回一个临时对象",{"2":{"1887":1}}],["返回一个指向以",{"2":{"1713":1}}],["返回一个装饰器",{"2":{"1227":1}}],["返回一个迭代器",{"2":{"1214":8}}],["返回一个包含模块整个状态的字典",{"2":{"1214":1}}],["返回一个由整数组成的元组",{"2":{"1083":1}}],["返回一个矩阵",{"2":{"1082":1}}],["返回一个self张量的新视图",{"2":{"827":1}}],["返回一个具有与输入相同的数据和元素数量",{"2":{"819":1}}],["返回一个token列表",{"2":{"557":1}}],["返回一个vocab对象",{"2":{"557":1}}],["返回一个batch对象",{"2":{"383":1}}],["返回一个等大的布尔张量",{"2":{"66":1}}],["返回z和权重p",{"2":{"199":1}}],["返回每一行的最大值",{"2":{"83":1}}],["yw",{"2":{"1549":1}}],["y0",{"2":{"1323":1,"1377":1,"2018":1}}],["y|",{"2":{"2018":1}}],["y|∣δx∣",{"2":{"2018":1}}],["y|x",{"2":{"1322":2}}],["y||",{"2":{"692":1}}],["yn",{"2":{"1322":2,"1323":1}}],["yn^",{"2":{"899":1}}],["year",{"2":{"1195":1,"1729":22}}],["yet",{"2":{"428":1}}],["yl",{"2":{"1003":5,"1004":2}}],["yl−12",{"2":{"1003":1}}],["yl−1",{"2":{"1003":6}}],["yly",{"2":{"1003":1}}],["yl=wlxl+bly",{"2":{"1003":1}}],["y的数学表达式",{"2":{"1000":1}}],["y∣x",{"2":{"908":4,"1322":4}}],["y​0​​",{"2":{"1323":1,"2018":1}}],["y​n​​",{"2":{"1322":2,"1323":1}}],["y​l−1​2​​",{"2":{"1003":1}}],["y​l−1​​",{"2":{"1003":6}}],["y​l​​",{"2":{"1003":6,"1004":2}}],["y​l​​=w​l​​x​l​​+b​l​​",{"2":{"1003":1}}],["y​i​​",{"2":{"1322":2,"1323":2,"1324":2,"2018":1}}],["y​i​​∣x",{"2":{"1322":2}}],["y​i​​∣y​1​​",{"2":{"903":2}}],["y​i+1​​",{"2":{"1322":3,"1324":1,"2018":1,"2023":1}}],["y​i−1​​",{"2":{"903":2,"1322":3}}],["y​2​​",{"2":{"903":2,"1322":1}}],["y​t​​",{"2":{"903":1}}],["y​1​​",{"2":{"903":1,"1322":2,"1323":1,"2018":1}}],["yt",{"2":{"903":1}}],["yi直线光栅化",{"0":{"2012":1},"1":{"2013":1,"2014":1,"2015":1,"2016":1,"2017":1,"2018":1,"2019":1,"2020":1,"2021":1,"2022":1,"2023":1,"2024":1,"2025":1,"2026":1,"2027":1,"2028":1,"2029":1,"2030":1,"2031":1}}],["yi",{"2":{"1323":1,"2018":1}}],["yiy",{"2":{"1322":2,"1323":1,"1324":2}}],["yiyiy",{"2":{"614":1}}],["yi+1=yi+ystepy",{"2":{"2018":1}}],["yi+1y",{"2":{"1322":1,"1324":1}}],["yi+1",{"2":{"1322":2,"2018":1}}],["yi∣x",{"2":{"1322":2}}],["yi∣y1",{"2":{"903":2}}],["yi−1",{"2":{"903":2,"1322":3}}],["yi^",{"2":{"899":1}}],["yield",{"2":{"383":1,"557":9}}],["yield一个batch对象",{"2":{"383":1}}],["yasg",{"2":{"2070":1}}],["yaml",{"2":{"1088":1}}],["yao",{"2":{"513":1}}],["yanbo",{"2":{"95":1}}],["yoshua",{"2":{"429":1,"543":1,"999":1}}],["young",{"2":{"370":1,"557":2}}],["young等人",{"2":{"370":1}}],["yourmodel",{"2":{"1239":1}}],["yourself",{"2":{"1039":1}}],["your",{"2":{"292":1,"688":1,"737":1,"740":2}}],["you",{"0":{"911":1},"2":{"115":1,"233":1,"235":1,"239":3,"245":4,"385":1,"398":2,"399":1,"428":2,"429":1,"432":1,"446":2,"513":1,"528":9,"557":2,"722":1,"747":1,"757":2,"935":1,"951":1,"986":1,"1010":1,"1305":1,"1312":1,"2075":2,"2076":1,"2078":1}}],["y中的有效token数",{"2":{"398":1}}],["y中每句话的第一个单词是开始符号的编码",{"2":{"71":1}}],["y是目标语言",{"2":{"908":1}}],["y是",{"2":{"398":1}}],["y`则为`我",{"2":{"385":1}}],["y则是",{"2":{"381":1}}],["y进行比对",{"2":{"381":1}}],["y内容就是",{"2":{"380":1}}],["y存储的是希望预测的结果",{"2":{"380":1}}],["y作为真值",{"2":{"380":1}}],["yum",{"2":{"1537":2,"1584":1}}],["yuki",{"2":{"638":1}}],["yuxin",{"2":{"361":1}}],["yunhao",{"2":{"361":1}}],["y图中的一条直线",{"2":{"359":1}}],["ykiyiky",{"2":{"313":1}}],["yki=",{"2":{"313":1}}],["y2^",{"2":{"899":1}}],["y2",{"2":{"267":3,"903":2,"1322":1,"1331":4}}],["y=​√​var",{"2":{"640":1}}],["y=x−e",{"2":{"640":1}}],["y=multihead",{"2":{"510":1}}],["y=f",{"2":{"301":3,"1000":3,"1442":2}}],["y=",{"2":{"241":1,"399":1,"1322":2,"1323":3,"1440":1}}],["ym",{"2":{"241":2}}],["y1^",{"2":{"899":1}}],["y1",{"2":{"241":2,"903":1,"1322":2,"1323":1,"2018":1}}],["yyyy",{"2":{"1642":1}}],["yy",{"2":{"192":1,"1520":1,"1549":1}}],["y≈",{"2":{"192":1}}],["y",{"2":{"168":1,"192":4,"230":1,"241":3,"263":1,"361":1,"380":3,"381":3,"385":2,"398":4,"399":1,"410":3,"423":1,"446":1,"472":2,"484":1,"510":9,"530":26,"543":2,"564":2,"692":5,"768":1,"820":2,"879":3,"899":4,"903":10,"908":25,"912":2,"1000":1,"1002":8,"1003":12,"1004":7,"1083":1,"1087":1,"1093":2,"1094":2,"1095":1,"1097":4,"1098":7,"1099":7,"1114":11,"1116":5,"1218":2,"1254":1,"1299":2,"1308":8,"1322":15,"1323":4,"1331":5,"1345":3,"1398":6,"1438":2,"1440":15,"1442":4,"1443":1,"1549":1,"1551":1,"1615":2,"1630":3,"1650":12,"1699":3,"1709":3,"1788":7,"1905":2,"1906":2,"2018":17,"2019":1,"2021":4,"2059":3,"2060":4}}],["ystep=1ystep",{"2":{"2018":1}}],["ystep=k=​δx​​δy​​",{"2":{"2018":1}}],["ystep=k=δyδxystep",{"2":{"2018":1}}],["ystepy​i+1​​=y​i​​+ystep",{"2":{"2018":1}}],["ys就是decoder之前的所有输出",{"2":{"428":1}}],["ys",{"2":{"83":8,"428":6,"472":5,"529":6}}],["941715",{"2":{"1396":3}}],["945864",{"2":{"1396":3}}],["912934=0",{"2":{"1393":2}}],["912934+0",{"2":{"1389":3}}],["912934",{"2":{"1388":2,"1393":4}}],["912934out",{"2":{"1388":1}}],["9216",{"2":{"1215":1,"1257":1}}],["9米多远",{"2":{"713":1}}],["9315",{"2":{"702":1}}],["9哪一个更大",{"2":{"560":1}}],["9b",{"2":{"429":1}}],["9c78e56ef49d",{"2":{"429":1}}],["95",{"2":{"429":1,"1233":1,"1234":1,"1623":1,"1728":1}}],["9a",{"2":{"429":1}}],["99−0",{"2":{"1389":2}}],["995275=2",{"2":{"1389":3}}],["995275",{"2":{"1388":2}}],["995275out",{"2":{"1388":1}}],["999",{"2":{"595":1}}],["99",{"2":{"399":1,"1029":1,"1102":1,"1202":1,"1205":1,"1386":1,"1389":1,"1398":1,"1623":3}}],["9947",{"2":{"315":1}}],["977675",{"2":{"1396":3}}],["979164+0",{"2":{"1389":3}}],["979164",{"2":{"1388":2}}],["979164out",{"2":{"1388":1}}],["97",{"2":{"1315":1}}],["9706",{"2":{"399":1}}],["9742",{"2":{"315":1}}],["9691",{"2":{"315":1}}],["908195",{"2":{"1396":3}}],["904330",{"2":{"1389":5}}],["904330out",{"2":{"1389":1}}],["90",{"2":{"119":1,"1235":1,"1623":2,"1680":1,"1825":3,"1843":3}}],["987654",{"2":{"1481":1}}],["9897",{"2":{"702":1}}],["9896",{"2":{"315":1}}],["98",{"2":{"83":1,"402":1,"423":1,"424":1,"429":1,"1363":1}}],["9",{"0":{"289":1,"736":1,"741":1,"764":1,"777":1,"829":1,"847":1,"935":1,"967":1,"977":1,"1009":1,"1058":1,"1059":1,"1060":1,"1061":1,"1062":1,"1100":1,"1241":1,"1473":1,"1499":1,"1531":1,"1675":1,"1693":1,"1694":1,"1913":1,"1916":1,"1929":1,"2060":1,"2155":1},"1":{"742":1,"743":1,"744":1,"745":1,"746":1,"747":1,"748":1,"749":1,"750":1,"751":1,"752":1,"753":1,"754":1,"755":1,"756":1,"757":1,"758":1,"759":1,"760":1,"761":1,"762":1,"763":1,"764":1,"765":1,"766":1,"767":1,"768":1,"1060":1,"1061":1,"1062":1,"1532":1,"1533":1},"2":{"83":3,"402":2,"423":2,"424":3,"428":2,"429":1,"453":2,"472":2,"529":2,"560":2,"595":4,"741":2,"816":3,"832":3,"834":1,"986":1,"1029":1,"1039":1,"1049":1,"1184":1,"1205":1,"1215":1,"1221":1,"1222":2,"1231":4,"1239":1,"1241":1,"1242":1,"1245":1,"1246":1,"1247":2,"1389":1,"1393":2,"1395":1,"1474":2,"1523":1,"1608":3,"1629":2,"1635":1,"1645":1,"1704":1,"1749":1,"1751":1,"1752":1,"1883":1,"1990":1,"2053":1,"2062":1}}],["预编译阶段处理",{"2":{"1632":1}}],["预编译后变为",{"2":{"1632":1}}],["预钩子可以就地修改",{"2":{"1227":1}}],["预处理表达式",{"2":{"1931":1}}],["预处理指令防止头文件重复包含",{"2":{"1916":1}}],["预处理指令包含头文件",{"2":{"1628":1}}],["预处理连接运算符",{"2":{"1712":1}}],["预处理运算符",{"2":{"1712":1}}],["预处理器指令",{"2":{"1632":1}}],["预处理的输出结果是一个包含了所有必要代码的源文件",{"2":{"1604":1}}],["预处理",{"2":{"1604":1}}],["预处理方法等",{"2":{"1141":1}}],["预处理和数据集索引",{"2":{"95":1,"387":1}}],["预算",{"2":{"1133":1,"1174":1}}],["预算下训练",{"2":{"561":1}}],["预先训练好的向量上",{"2":{"715":1}}],["预分词是指在将文本分割成",{"2":{"553":1}}],["预分词",{"0":{"553":1},"2":{"545":1,"551":1}}],["预热可以独立于现有的衰减计划进行调整",{"2":{"1183":1}}],["预热的过程涉及了预先安排一个学习率计划",{"2":{"1183":1}}],["预热期间不稳定的示例",{"2":{"1181":1}}],["预热",{"2":{"333":1}}],["预训练时模型很可能已经见过与我们任务类似的数据集",{"2":{"1313":1}}],["预训练是一种从头开始训练模型的方式",{"2":{"1313":1}}],["预训练的语言模型通常是在大规模的无监督数据上进行预训练",{"2":{"898":1}}],["预训练的语言模型通常经过大规模的数据和计算资源训练得到",{"2":{"898":1}}],["预训练的语言模型在大规模数据上学习到了丰富的语义信息和语言规律",{"2":{"898":1}}],["预训练的嵌入向量会结合模型的损失函数和训练任务",{"2":{"709":1}}],["预训练的均值和方差并不能真实反映测试集",{"2":{"316":1}}],["预训练嵌入",{"2":{"709":1}}],["预训练+微调",{"2":{"670":1}}],["预训练",{"2":{"612":1,"711":1}}],["预训练阶段量学习的目标就是最小化各领域的信息熵",{"2":{"397":1}}],["预训练就是在不同尺度上提炼语料数据中的信息概率分布",{"2":{"363":1}}],["预训练好的",{"2":{"333":1,"1312":1}}],["预训练已经使模型充分具备了解答问题的能力",{"2":{"221":1}}],["预训练模型下载",{"2":{"1309":1}}],["预训练模型",{"2":{"156":1}}],["预训练语言模型通常在其参数中编码大量信息",{"2":{"154":1}}],["预训练语料",{"2":{"95":1,"387":1}}],["预训练通常包含很多不同的数据集",{"2":{"89":1}}],["预测和决策等任务",{"2":{"1455":1}}],["预测出的实体个数",{"2":{"1331":1}}],["预测为正例的个数",{"2":{"1331":1}}],["预测到会跨过山谷时",{"2":{"1034":1}}],["预测的正例个数",{"2":{"1331":1}}],["预测的时候我们只能得到前一时刻预测出的输出",{"2":{"934":1}}],["预测的输出",{"2":{"427":1}}],["预测或推理时的流程图如下",{"2":{"894":1}}],["预测时不需要",{"2":{"934":1}}],["预测时",{"2":{"894":1}}],["预测时流程",{"0":{"894":1}}],["预测时候没有目标语言句子",{"2":{"380":1}}],["预测下一词得到表征",{"2":{"733":1}}],["预测下一个输出词的分布",{"2":{"126":1}}],["预测上下文的词",{"2":{"714":1}}],["预测问题",{"2":{"613":1}}],["预测大多数单词的结尾不需要大型transformer",{"2":{"612":1}}],["预测",{"2":{"561":1,"692":1}}],["预测结果",{"2":{"526":1}}],["预测错误",{"2":{"405":3,"407":2}}],["预测正确的实体个数",{"2":{"1331":1}}],["预测正确",{"2":{"405":2,"407":3}}],["预测阶段",{"2":{"172":1}}],["预测句子的初始化",{"2":{"83":1}}],["预测过程的",{"2":{"83":1}}],["预知",{"2":{"59":1}}],["类或变量的具体实现",{"2":{"1916":1}}],["类或变量的存在",{"2":{"1916":1}}],["类或其子类来实现",{"2":{"1763":1}}],["类或其子类来创建自己的异常类型",{"2":{"1762":1}}],["类中是",{"2":{"1874":1}}],["类提供了一个虚函数",{"2":{"1763":1}}],["类提供了方便安全的字符串处理方式",{"2":{"1715":1}}],["类是",{"2":{"1713":1,"1779":1}}],["类是面向对象编程的核心概念",{"2":{"1674":1}}],["类详解",{"0":{"1711":1,"1713":1},"1":{"1712":1,"1713":1,"1714":1,"1715":1}}],["类模板参数推导",{"0":{"1925":1},"2":{"1920":1,"1932":1}}],["类模板的成员函数也可以是模板函数",{"2":{"1701":1}}],["类模板的成员函数一般需要在类外定义",{"2":{"1700":1}}],["类模板",{"0":{"1700":1},"2":{"1700":1,"1701":1}}],["类被",{"2":{"1656":1}}],["类外部编译顺序",{"2":{"1653":1}}],["类内部编译顺序",{"2":{"1653":1}}],["类成员的构造顺序",{"2":{"1653":1}}],["类的私有成员",{"2":{"1779":2}}],["类的私有属性和方法是不能被外部直接访问的",{"2":{"1769":1}}],["类的友元类",{"2":{"1779":1}}],["类的使用以及静态数组和动态数组的概念",{"2":{"1715":1}}],["类的常用操作函数",{"2":{"1713":1}}],["类的常用构造函数",{"2":{"1713":1}}],["类的引入和优势",{"2":{"1713":1}}],["类的例子",{"2":{"1712":1}}],["类的特性和常用成员函数",{"2":{"1711":1}}],["类的定义",{"2":{"1680":1}}],["类的一个对象实例所占用的内存大小",{"2":{"1678":1}}],["类的一个对象",{"2":{"1678":1}}],["类的成员变量",{"2":{"1674":1}}],["类的继承",{"0":{"1654":1}}],["类的大小是所有非静态成员变量大小之和",{"2":{"1653":1}}],["类的大小与其对象的大小一致",{"2":{"1653":1}}],["类的大小指的是该类型对象所占用的内存空间大小",{"2":{"1653":1}}],["类的大小",{"0":{"1652":1}}],["类的实例",{"2":{"82":1}}],["类和对象的基本概念",{"2":{"1637":1}}],["类定义通常放在头文件中",{"2":{"1628":1}}],["类定义",{"2":{"1628":1}}],["类声明",{"2":{"1628":1}}],["类与对象",{"2":{"1491":1}}],["类有哪些常用的方法",{"2":{"1248":1}}],["类",{"0":{"1680":1},"2":{"1210":1,"1603":1,"1624":2,"1642":1,"1657":2,"1712":1,"1762":1,"1763":1,"1766":1,"1825":1,"1843":1,"1909":1,"1916":2,"1921":1}}],["类别",{"2":{"829":1}}],["类别生成器对象",{"2":{"450":1}}],["类相同的精度复杂性",{"2":{"504":1}}],["类相同的",{"2":{"504":1}}],["类函数宏",{"2":{"1632":1}}],["类函数",{"2":{"504":3}}],["类比到学习率调整中",{"2":{"1242":1}}],["类比推理",{"2":{"689":1}}],["类比为一摞书",{"2":{"340":1}}],["类比",{"0":{"340":1},"2":{"293":1,"386":1}}],["类型安全",{"2":{"1810":1,"1828":1,"1926":1}}],["类型支持",{"2":{"1797":1}}],["类型双关",{"2":{"1728":1}}],["类型数组",{"2":{"1715":1}}],["类型信息运算符",{"2":{"1712":1}}],["类型是",{"2":{"1705":1}}],["类型元素的数组",{"2":{"1705":1}}],["类型和顺序必须与函数定义中的形式参数列表匹配",{"2":{"1729":1}}],["类型和",{"2":{"1700":1}}],["类型调用",{"2":{"1698":1}}],["类型参数",{"2":{"1698":1}}],["类型参数的函数的指针",{"2":{"1645":1}}],["类型识别运算符",{"2":{"1630":1}}],["类型转换运算符",{"2":{"1635":1}}],["类型转换",{"0":{"1681":1,"1682":1},"1":{"1682":1,"1683":2,"1684":2,"1685":1,"1686":1,"1687":1,"1688":1},"2":{"1629":1}}],["类型不明确",{"2":{"1615":1}}],["类型名过长或复杂",{"2":{"1615":1}}],["类型变量的内存地址",{"2":{"1611":1}}],["类型存储",{"2":{"1608":1}}],["类型推断",{"2":{"1291":1}}],["类型的",{"2":{"1797":1}}],["类型的异常",{"2":{"1762":1}}],["类型的值返回",{"2":{"1729":1}}],["类型的参数",{"2":{"1729":2}}],["类型的指针",{"2":{"1667":1}}],["类型的数组",{"2":{"1647":1,"1690":1,"1700":1}}],["类型的内存空间",{"2":{"1647":1}}],["类型的变量一样",{"2":{"1728":1}}],["类型的变量",{"2":{"1611":1,"1728":1}}],["类型的tensor",{"2":{"1086":2}}],["类型的非线性函数进行变换",{"2":{"838":1}}],["类型运算",{"2":{"1081":1}}],["类型",{"0":{"311":1},"2":{"293":1,"1087":1,"1099":1,"1214":5,"1221":1,"1623":1,"1647":4,"1698":1,"1707":1}}],["类系统高效管理",{"2":{"222":1}}],["类似人类的视觉原理",{"2":{"1472":1}}],["类似神经元信号传播",{"2":{"841":1}}],["类似多音字或者多义字给人们带来的麻烦",{"2":{"689":1}}],["类似一个命题结构的词典",{"2":{"510":1}}],["类似的问题在更复杂的场景中也存在",{"2":{"692":1}}],["类似的单词会被关联到接近的数字",{"2":{"691":1}}],["类似的",{"2":{"398":1}}],["类似于函数模板的参数推导",{"2":{"1925":1}}],["类似于内联函数",{"2":{"1923":1}}],["类似于指针",{"2":{"1718":1}}],["类似于抽象语法树",{"2":{"1290":1}}],["类似于统计中的冗余参数",{"2":{"1143":1}}],["类似于进程共享物理页面",{"2":{"983":1}}],["类似于训练中的流水线并行性",{"2":{"977":1}}],["类似于前面我们写的第t步的交叉熵损失的负数",{"2":{"903":1}}],["类似于为输入序列的每个元素打一个",{"2":{"744":1}}],["类似于一个查找表",{"2":{"702":1}}],["类似于一组专家分析复杂问题的各个方面",{"2":{"1":1}}],["类似于lora技术",{"2":{"623":1}}],["类似于tanh函数",{"2":{"359":1}}],["类似于x",{"2":{"359":1}}],["类似于在一个句子中找到一个",{"2":{"318":1}}],["类似于",{"2":{"204":1,"230":1,"1750":1,"1928":1}}],["类似可以得到矩阵k∈rl×dkk∈rl×dk",{"2":{"161":1}}],["类似",{"0":{"871":1},"2":{"10":1,"557":1}}],["类似消息传递的架构具有通用性",{"2":{"1":1}}],["吗",{"2":{"80":1,"934":1}}],["好好活就是做有意义的事",{"2":{"2056":1}}],["好好做人",{"2":{"2056":1}}],["好的",{"2":{"1145":1,"1606":1,"1749":1,"1864":1}}],["好的激活函数应有的性质",{"0":{"848":1}}],["好读书",{"2":{"755":1}}],["好久没有写博客了",{"2":{"235":1}}],["好像",{"2":{"168":1}}],["好",{"2":{"80":1,"1148":1,"1153":1}}],["掩模",{"2":{"1328":1}}],["掩蔽的作用是",{"2":{"78":1}}],["掩码和dropout见附录b",{"2":{"944":1,"963":1}}],["掩码多头自注意力的计算会使用tgt",{"2":{"538":1}}],["掩码多头自注意力的输出",{"2":{"535":1}}],["掩码多头自注意力",{"2":{"535":2}}],["掩码多头自注意力模块",{"2":{"533":1}}],["掩码多头注意力与全局自注意力的不同之处在于在对序列中位置的处理上",{"2":{"525":1}}],["掩码多头注意力模块会对",{"2":{"525":1}}],["掩码多头注意力",{"2":{"525":2}}],["掩码逻辑是为了训练来特殊打造",{"2":{"525":1}}],["掩码的作用是确保解码器只能关注到它之前已经生成的词",{"2":{"525":1}}],["掩码操作",{"2":{"525":1}}],["掩码自我注意力",{"0":{"464":1}}],["掩码自注意力只能保证解码器学习到历史译文的内容",{"2":{"536":1}}],["掩码自注意力机制依然会起到限制作用",{"2":{"525":1}}],["掩码自注意力层会通过自注意力机制计算输出序列中",{"2":{"525":1}}],["掩码自注意力层或者说因果自注意力层",{"2":{"443":1}}],["掩码自注意力负责基于解码器的输入向量来建模每个解码器的输出向量",{"2":{"515":1}}],["掩码自注意力允许模型通过关注输入的相关部分来生成序列",{"2":{"464":1}}],["掩码自注意力和交叉注意力",{"2":{"439":1}}],["掩码自注意力",{"0":{"71":1,"443":1},"2":{"49":1,"439":1}}],["掩码将只有一个中心节点",{"2":{"94":1}}],["掩码形状",{"2":{"80":1}}],["掩码就相当于把中心词给遮掩住",{"2":{"50":1}}],["掩码注意力",{"0":{"93":1},"2":{"49":1}}],["掩码矩阵有不同实现方式",{"2":{"62":1}}],["掩码矩阵",{"0":{"62":1,"70":1},"2":{"49":2,"199":1}}],["掩码",{"0":{"49":1,"307":1,"409":1},"1":{"50":1,"51":1,"52":1,"53":1,"54":1,"55":1,"56":1,"57":1,"58":1,"59":1,"60":1,"61":1,"62":1,"63":1,"64":1,"65":1,"66":1,"67":1,"68":1,"69":1,"70":1,"71":1,"72":1,"73":1,"74":1,"75":1,"76":1,"77":1,"78":1,"79":1,"80":1,"81":1,"82":1,"83":1,"84":1,"85":1,"86":1,"87":1,"88":1,"89":1,"90":1,"91":1,"92":1,"93":1,"94":1,"95":1},"2":{"49":1,"50":1,"173":1,"293":1,"394":1,"522":1,"523":1}}],["任意语种都可以被编码到字节进行表示",{"2":{"608":1}}],["任意两个位置之间都可以直接联系",{"2":{"511":1}}],["任意两个词都可以直接建模",{"2":{"274":1}}],["任何程序在运行时都可能遇到错误或意外情况",{"2":{"1761":1}}],["任何时候只能有一个成员存储有效的值",{"2":{"1728":1}}],["任何连结到电脑系统中的装置",{"2":{"1411":1}}],["任何训练任务都会变得不稳定",{"2":{"1179":1}}],["任何让训练变得更糟的事情都可以作为正则化器",{"2":{"1149":1}}],["任何标有🤖表情符号的地方是我们要进一步调查的地方",{"2":{"1127":1}}],["任何特定向量的意义都不能孤立地确定",{"2":{"689":1}}],["任何颜色都可以用一个",{"2":{"687":1}}],["任何高维数据集均可以被随机投影到一个较低维度的欧氏空间",{"2":{"684":1}}],["任何需要输入到模型的数据都需要向量化",{"2":{"680":1}}],["任何一个词都有可能放在初始位置",{"2":{"323":1}}],["任何key向量都可能得到最高的注意力分数",{"2":{"320":1}}],["任何线性运算",{"2":{"145":1}}],["任何位置的信息都可以被任何位置的单词获取",{"2":{"78":1}}],["任务的开始结束时间都不同",{"2":{"2118":1}}],["任务描述",{"0":{"1873":1}}],["任务要求",{"0":{"1690":1}}],["任务间通过网络通信",{"2":{"1568":1}}],["任务间通过共享内存交换数据",{"2":{"1568":1}}],["任务概述",{"0":{"1320":1}}],["任务及其涉及的数据",{"2":{"1318":1}}],["任务来进行预训练",{"2":{"1317":1}}],["任务转换为",{"2":{"1317":1}}],["任务都转换为",{"2":{"1317":1}}],["任务上取得了优异的性能",{"2":{"1315":1}}],["任务上都远远超过先前的最强基准",{"2":{"1312":1}}],["任务替换为句子排序预测",{"2":{"1315":1}}],["任务需求",{"2":{"568":1}}],["任务",{"0":{"1319":1},"1":{"1320":1,"1321":1,"1322":1,"1323":1,"1324":1,"1325":1,"1326":1},"2":{"508":1,"542":1,"711":1,"911":1,"1315":1,"1317":1,"1318":1,"1997":1}}],["任务模型使用特征模型把x转换为特征向量f",{"2":{"263":1}}],["任务模型包含四个部分",{"2":{"263":1}}],["任务模型",{"0":{"263":1}}],["任务相关",{"0":{"560":1},"2":{"122":1}}],["任务而定",{"2":{"20":1}}],[">read",{"2":{"1902":1}}],[">tellg",{"2":{"1902":1}}],[">tensor",{"2":{"1087":1}}],[">close",{"2":{"1902":1}}],[">is",{"2":{"1902":4}}],[">use",{"2":{"1891":4}}],[">show",{"2":{"2006":1}}],[">showinfo",{"2":{"1691":1}}],[">seekp",{"2":{"1902":1}}],[">seekg",{"2":{"1902":2}}],[">second++",{"2":{"1933":1}}],[">second",{"2":{"1725":4,"1807":1}}],[">speak",{"2":{"1866":2}}],[">first",{"2":{"1725":3}}],[">makesound",{"2":{"1691":1}}],[">draw",{"2":{"1688":4}}],[">getscore",{"2":{"1680":1}}],[">value",{"2":{"1638":2}}],[">nqhd",{"2":{"1217":1}}],[">nhqk",{"2":{"1217":1}}],[">>",{"2":{"1083":1,"1608":3,"1619":1,"1624":1,"1673":2,"1729":1,"1811":1,"1814":1,"1824":1,"1825":4,"1829":1,"1832":1,"1842":1,"1843":4,"1933":1,"2059":2}}],[">>>",{"2":{"557":8}}],[">=",{"2":{"592":2,"1236":1,"1237":1,"1238":1,"1240":1,"1246":1,"1619":2,"1677":1,"1684":1,"1825":4,"1843":4,"1874":1,"2153":1}}],[">",{"2":{"76":1,"201":2,"399":1,"428":1,"449":1,"557":1,"558":1,"572":1,"573":1,"590":2,"591":4,"592":3,"702":3,"723":1,"804":1,"976":15,"1083":5,"1085":76,"1086":27,"1087":731,"1098":3,"1099":1,"1147":1,"1150":1,"1177":1,"1178":2,"1179":1,"1181":1,"1182":2,"1183":1,"1184":1,"1214":46,"1227":29,"1254":1,"1298":2,"1328":1,"1329":2,"1330":8,"1331":1,"1345":1,"1350":1,"1398":3,"1481":15,"1485":3,"1486":1,"1487":1,"1488":1,"1489":2,"1510":1,"1611":1,"1630":1,"1645":1,"1683":2,"1698":1,"1704":2,"1705":5,"1709":1,"1715":1,"1728":1,"1820":1,"1838":1,"1883":2,"1905":1,"1912":5,"1922":2,"1924":1,"1925":2,"1933":1,"1993":1}}],["++count",{"2":{"1868":1}}],["++rit",{"2":{"1713":1}}],["++iter",{"2":{"1720":1,"1721":1,"1722":1}}],["++it",{"2":{"1713":1,"1714":1,"1718":1,"1720":2,"1721":2,"1722":2,"1724":2,"1725":3}}],["++i",{"2":{"1634":1,"1647":2,"1667":2,"1668":2,"1670":1,"1671":1,"1696":1,"1706":2,"1708":1,"1714":1,"1719":5,"1891":1}}],["++a",{"2":{"1630":2}}],["++",{"2":{"1630":1,"1635":2,"1712":1}}],["+2",{"2":{"1440":1}}],["+w",{"2":{"1388":1,"1393":2}}],["+p",{"2":{"1339":1,"1340":2}}],["+positional",{"2":{"698":4}}],["+r",{"2":{"1339":2}}],["+t",{"2":{"1324":4}}],["+b",{"2":{"1003":1,"1095":2,"1096":1,"1388":1,"1393":1}}],["+var",{"2":{"1002":4,"1003":6}}],["+⋯+var",{"2":{"1002":2}}],["+norm",{"2":{"1180":1}}],["+n",{"2":{"1000":2}}],["+e",{"2":{"1389":1,"1393":1}}],["+e​m",{"2":{"943":1,"961":1}}],["+e^",{"2":{"943":1,"961":1}}],["+em",{"2":{"943":1,"961":1}}],["+ϵ​​​​​x−e",{"2":{"640":1}}],["+ϵ∗γ+βy",{"2":{"640":1}}],["+1y​i​​+1​​",{"2":{"2023":1}}],["+1m",{"2":{"1532":1}}],["+1",{"2":{"582":1}}],["+|",{"2":{"571":2}}],["+=",{"2":{"385":8,"503":1,"590":2,"591":1,"723":1,"1215":2,"1295":3,"1328":4,"1329":1,"1331":6,"1594":1,"1620":1,"1621":1,"1623":1,"1630":1,"1633":2,"1635":1,"1677":1,"1710":1,"1712":1,"1713":3,"1803":1,"1874":1,"1883":1,"1914":2,"1933":1,"2153":2}}],["+ft−1",{"2":{"334":2}}],["+f2",{"2":{"334":2}}],["+f1",{"2":{"334":2}}],["+x",{"2":{"294":1,"300":1,"301":1,"304":1,"470":1,"1340":1}}],["+0",{"2":{"169":2,"582":1}}],["+表示llm能力的增强",{"2":{"139":1}}],["+",{"0":{"1085":2,"1319":1},"1":{"1320":1,"1321":1,"1322":1,"1323":1,"1324":1,"1325":1,"1326":1},"2":{"76":2,"77":1,"90":7,"97":2,"105":4,"106":1,"108":1,"109":3,"114":1,"119":1,"155":1,"167":6,"169":3,"170":3,"175":1,"185":1,"201":16,"300":1,"313":1,"314":1,"329":2,"330":4,"332":1,"334":8,"343":5,"344":4,"346":2,"394":2,"395":1,"399":1,"427":2,"446":1,"457":1,"503":2,"504":1,"510":2,"519":2,"520":2,"526":1,"530":4,"557":4,"571":3,"572":1,"590":1,"591":3,"592":2,"598":1,"640":2,"698":2,"700":1,"702":1,"713":1,"723":2,"762":1,"766":1,"768":1,"807":4,"808":4,"809":2,"810":2,"839":2,"844":1,"846":2,"856":1,"914":1,"945":1,"950":1,"965":1,"970":1,"971":1,"975":1,"987":1,"1000":3,"1002":2,"1003":6,"1075":4,"1078":1,"1082":1,"1087":2,"1093":2,"1098":3,"1099":1,"1102":2,"1177":1,"1180":1,"1189":1,"1190":2,"1191":3,"1192":3,"1193":4,"1202":6,"1211":5,"1216":2,"1217":3,"1218":8,"1243":1,"1244":2,"1254":2,"1280":1,"1284":2,"1299":1,"1329":3,"1330":6,"1331":2,"1335":1,"1340":1,"1345":2,"1350":1,"1361":2,"1388":2,"1389":4,"1393":3,"1394":1,"1395":14,"1398":8,"1436":1,"1440":3,"1474":1,"1520":1,"1541":1,"1547":2,"1550":1,"1551":2,"1557":2,"1561":3,"1583":1,"1594":2,"1596":2,"1607":2,"1608":1,"1624":2,"1630":2,"1632":3,"1633":2,"1635":2,"1646":1,"1651":1,"1657":2,"1680":2,"1687":4,"1699":1,"1705":10,"1706":1,"1707":4,"1712":11,"1713":4,"1719":3,"1751":1,"1752":1,"1763":1,"1788":4,"1789":3,"1792":3,"1825":2,"1843":2,"1874":1,"1887":3,"1902":1,"1905":3,"1906":3,"1916":2,"1924":1,"1999":2,"2006":1,"2017":1,"2018":2,"2019":1,"2021":2,"2027":1,"2093":1}}],["基类protected成员在派生类中变为private",{"2":{"1860":1}}],["基类protected成员在派生类中仍为protected",{"2":{"1852":1,"1856":1}}],["基类public成员在派生类中变为private",{"2":{"1860":1}}],["基类public成员在派生类中变为protected",{"2":{"1856":1}}],["基类public成员在派生类中仍为public",{"2":{"1852":1}}],["基类的实现方式可以作为派生类的细节不对外暴露",{"2":{"1861":1}}],["基类的private成员无法在派生类中直接被访问",{"2":{"1852":1,"1856":1,"1860":1}}],["基类的所有",{"2":{"1851":1,"1855":1,"1859":1}}],["基类的友元可以访问派生类对象中从基类继承来的成员",{"2":{"1786":1}}],["基类的",{"2":{"1655":1}}],["基类",{"2":{"1654":2,"1657":1,"1866":1,"1867":1,"1868":1,"1873":1}}],["基准上击败人类的模型",{"2":{"1315":1}}],["基准上超过了当时所有的最强模型",{"2":{"1315":1}}],["基准和翻译任务上都取得了最好的性能",{"2":{"1315":1}}],["基数树最初为空",{"2":{"986":1}}],["基础环境",{"0":{"2089":1}}],["基础知识回顾",{"0":{"2002":1},"1":{"2003":1,"2004":1,"2005":1,"2006":1,"2007":1,"2008":1}}],["基础笔记",{"0":{"2000":1}}],["基础设施开发",{"0":{"1950":1},"1":{"1951":1,"1952":1,"1953":1,"1954":1}}],["基础数据类型部分进行补充和完善",{"2":{"1606":1}}],["基础大纲",{"2":{"1502":1}}],["基础",{"0":{"1491":1},"2":{"1474":1}}],["基础语法",{"0":{"1436":1}}],["基础方案是论文",{"2":{"748":1}}],["基础方案",{"0":{"748":1},"2":{"741":1}}],["基础上",{"2":{"736":1}}],["基础函数",{"0":{"590":1}}],["基础概念",{"0":{"546":1},"1":{"547":1,"548":1,"549":1,"550":1,"551":1,"552":1,"553":1,"554":1,"555":1}}],["基础学习率",{"2":{"372":1}}],["基线",{"2":{"354":1,"355":1,"356":1}}],["基于现有类",{"2":{"1654":1}}],["基于vmware",{"2":{"1582":1}}],["基于openclip在laion",{"2":{"1363":1}}],["基于发射分数使用crf解码最优的标签路径",{"2":{"1320":1}}],["基于预训练模型进行微调会是一个更好的选择",{"2":{"1313":1}}],["基于上下文",{"2":{"1312":1}}],["基于上述信息",{"2":{"126":1}}],["基于句子的前",{"2":{"1312":1}}],["基于随机移动的低差异序列的quasi",{"2":{"1175":1}}],["基于低差异序列",{"2":{"1175":1}}],["基于时间间隔进行评估可能会使解释训练曲线变得更加困难",{"2":{"1164":1}}],["基于每个映射版本",{"2":{"926":1}}],["基于rw和hs嵌入的互补性",{"2":{"739":1}}],["基于关键词的检索往往关注字面匹配",{"2":{"696":1}}],["基于扩散的lcm是一种生成式潜变量模型",{"2":{"633":1}}],["基于扩散的lcm",{"0":{"633":1}}],["基于空白字节来划分patch",{"2":{"613":1}}],["基于熵的patch划分",{"2":{"613":1}}],["基于熵的",{"2":{"612":1}}],["基于token的学习方式对于学习语义来说效率也比较低",{"2":{"626":1}}],["基于token化的llm为每个token分配相同的计算量",{"2":{"612":1}}],["基于transformer的模型以非灰色显示",{"2":{"540":1}}],["基于第3步数据训练语言模型",{"2":{"599":1}}],["基于基础词表将语料拆分为最小单元",{"2":{"599":1}}],["基于基础词表将将语料中所有文本切成最小单元",{"2":{"576":1}}],["基于贪婪算法和确定的符号替换导致bpe不能提供带概率的多个分词结果",{"2":{"595":1}}],["基于导数的和基于损失函数参数拟合的估计方法",{"2":{"561":1}}],["基于统计的分词方式和基于深度学习的分词方式",{"2":{"549":1}}],["基于词典的分词方式",{"2":{"549":1}}],["基于前缀解码器的现有代表性llm包括glm130b和u",{"2":{"541":1}}],["基于该发现",{"2":{"497":1}}],["基于距离的分析根据加权向量与注意力输出的接近程度来估计加权向量的贡献",{"2":{"478":1}}],["基于贝叶斯概率理论和生物物理学原理",{"2":{"363":1}}],["基于base的variable",{"2":{"1110":1}}],["基于batchnorm的归一化无从谈起",{"2":{"316":1}}],["基于bahdanau",{"2":{"285":1}}],["基于少量样本的bn的效果会变得很差",{"2":{"316":1}}],["基于长期神经记忆模块",{"2":{"229":1}}],["基于分类器的适配",{"2":{"225":1}}],["基于prompt的适配",{"2":{"225":1}}],["基于相似度排序结果",{"2":{"164":1}}],["基于",{"2":{"145":1,"561":1,"696":1}}],["基于这个内积",{"2":{"692":1}}],["基于这些见解",{"2":{"692":1}}],["基于这些",{"2":{"620":1}}],["基于这些概念",{"2":{"137":1}}],["基于这一发现",{"2":{"346":1}}],["基于这一假设",{"2":{"136":1}}],["基于这两个模块",{"2":{"226":1}}],["基于这种新的理解和假设",{"2":{"144":1}}],["基于这种观察以及对现有研究的回顾",{"2":{"144":1}}],["基于一个事实三元组",{"2":{"130":1}}],["基于此",{"2":{"29":1,"44":1,"230":1,"492":1}}],["基本每看一两集都会有我的泪点",{"2":{"2056":1}}],["基本博客结构和装修完成",{"2":{"2042":1}}],["基本思想",{"0":{"2027":1}}],["基本思想是当改变模型的某个位置的参数",{"2":{"475":1}}],["基本实现思路",{"0":{"2015":1}}],["基本格式",{"2":{"1917":1}}],["基本语法4",{"2":{"1917":1}}],["基本结构",{"0":{"1965":1},"1":{"1966":1,"1967":1,"1968":1,"1969":1},"2":{"1674":1}}],["基本用法和常用操作",{"2":{"1714":1}}],["基本用法",{"2":{"1673":1}}],["基本用户命令存放目录",{"2":{"1506":1}}],["基本数据类型",{"2":{"1607":1}}],["基本环境配置",{"0":{"1581":1},"1":{"1582":1,"1583":1,"1584":1,"1585":1,"1586":1,"1587":1,"1588":1,"1589":1}}],["基本环境要求如下",{"2":{"1561":1}}],["基本原理",{"0":{"1571":1,"2119":1},"1":{"1572":1,"1573":1,"1574":1,"1575":1,"1576":1,"1577":1,"1578":1,"1579":1}}],["基本操作",{"0":{"1520":1},"2":{"1584":1}}],["基本查看",{"2":{"1515":1}}],["基本查询",{"2":{"1492":1}}],["基本命令详解",{"0":{"1970":1},"1":{"1971":1,"1972":1,"1973":1,"1974":1,"1975":1,"1976":1,"1977":1}}],["基本命令操作",{"0":{"1508":1},"1":{"1509":1,"1510":1}}],["基本命令使用",{"0":{"1503":1},"1":{"1504":1,"1505":1,"1506":1,"1507":1,"1508":1,"1509":1,"1510":1,"1511":1,"1512":1,"1513":1,"1514":1,"1515":1,"1516":1,"1517":1,"1518":1,"1519":1,"1520":1,"1521":1,"1522":1,"1523":1,"1524":1,"1525":1,"1526":1,"1527":1,"1528":1,"1529":1,"1530":1,"1531":1,"1532":1,"1533":1,"1534":1,"1535":1,"1536":1,"1537":1,"1538":1,"1539":1}}],["基本概念",{"0":{"1406":1,"1563":1},"1":{"1407":1,"1408":1,"1409":1,"1410":1,"1411":1,"1412":1,"1413":1}}],["基本的三角循环",{"2":{"1241":2}}],["基本超参数轴图",{"2":{"1156":1}}],["基本步骤和原则都是相似的",{"2":{"1138":1}}],["基本运算符",{"0":{"1085":1}}],["基本粒子",{"2":{"638":1}}],["基本流程如下",{"2":{"587":1}}],["基本上就是指令微调",{"2":{"542":1}}],["基本上超过了",{"2":{"119":1}}],["基本动力学特性",{"0":{"507":1}}],["基本模块不同",{"2":{"285":1}}],["基本和哈佛思路一致",{"2":{"76":1}}],["还剩下一些钱",{"2":{"2131":1}}],["还剩",{"2":{"2131":1,"2139":2}}],["还在使用",{"2":{"1891":1}}],["还打印排水量",{"2":{"1664":1}}],["还打印轮子数量",{"2":{"1664":1}}],["还提供了文件流",{"2":{"1818":1,"1836":1}}],["还提供了其他几种有用的有序容器",{"2":{"1798":1}}],["还提供了复合数据类型",{"2":{"1622":1}}],["还提高了计算效率",{"2":{"594":1}}],["还包括768x768版本的模型",{"2":{"1363":1}}],["还包含模型generator部分的前向传播逻辑",{"2":{"398":1}}],["还包含了其他单词的信息",{"2":{"270":1}}],["还展示了小样本学习",{"2":{"1316":1}}],["还将",{"2":{"1315":1}}],["还依赖于它所处的上下文",{"2":{"719":1}}],["还代表基于单词出现的上下文的有意义的关系",{"2":{"709":1}}],["还考虑了单个词之间的相似度",{"2":{"692":1}}],["还考虑了合并后带来的概率增益",{"2":{"598":1}}],["还希望做到相似性搜索",{"2":{"678":1}}],["还有哪些场景可以用",{"2":{"1645":1}}],["还有一种好处就是我们可以将数据降维至2d或3d以便于观察分布情况",{"2":{"1370":1}}],["还有一种结构是把输入信息x作为每个阶段的输入",{"2":{"882":1}}],["还有三种梯度模式可以从",{"2":{"1118":1}}],["还有",{"2":{"665":1,"1322":1}}],["还有buffer也搬迁到cuda",{"2":{"662":1}}],["还传入了参数",{"2":{"620":1}}],["还用于token和模型参数之间的交互",{"2":{"617":1}}],["还不能直接用于模型的计算",{"2":{"545":1}}],["还深刻影响着模型的性能与精度",{"2":{"545":1}}],["还关乎单词的顺序",{"2":{"431":1}}],["还会计算损失",{"2":{"399":1}}],["还会回看上下文中相关的其他句子或词语",{"2":{"167":1}}],["还负责计算损失",{"2":{"399":1}}],["还保证了数据的高质量和多样性",{"2":{"369":1}}],["还增强了模型对多语言数据的处理能力",{"2":{"369":1}}],["还可以包含函数",{"2":{"1728":1}}],["还可以使用",{"2":{"1673":1}}],["还可以用不同长度的元素序列标记",{"2":{"986":1}}],["还可以对每层的输出做加权后做变换得到context",{"2":{"888":1}}],["还可以对最后的隐状态做一个变换得到context",{"2":{"888":1}}],["还可以在一定程度上缓解梯度爆炸问题",{"2":{"393":1}}],["还可以加速模型的收敛",{"2":{"335":1}}],["还可以考虑更长距离的依赖关系",{"2":{"242":1}}],["还原默认设置",{"2":{"2094":1}}],["还原",{"2":{"313":2}}],["还要快",{"2":{"1228":1}}],["还要检查其他问题的清单",{"2":{"1146":1}}],["还要注意的是",{"2":{"765":1}}],["还要依据匹配程度做加权求和",{"2":{"265":1}}],["还要考虑其他词对这个词的影响",{"2":{"167":1,"259":1}}],["还请帮忙指出",{"2":{"339":1}}],["还请各位读者指出",{"2":{"235":1}}],["还请指出",{"2":{"235":1}}],["还省去了位置编码",{"2":{"232":1}}],["还引发隐私问题",{"2":{"230":1}}],["还基于",{"2":{"217":1}}],["还对q和k进行额外的归一化",{"2":{"355":1}}],["还对",{"2":{"316":1}}],["还对注意力头进行了编辑",{"2":{"144":1}}],["还对特征进行了非线性的映射",{"2":{"137":1}}],["还需要额外调整β2",{"2":{"1174":1}}],["还需要庞大的人力去维护",{"2":{"908":1}}],["还需要一个方式来学习到源文本信息",{"2":{"536":1}}],["还需要注意它们的映射参数λ",{"2":{"341":1}}],["还需要考虑具体",{"2":{"1318":1}}],["还需要考虑利用批次统计量更新全局统计量的方法",{"2":{"316":1}}],["还需要考虑注意力机制的作用",{"2":{"131":1}}],["还需要将对角线右上的也都盖住",{"2":{"74":1,"382":1}}],["还是最花费时间的最先安排",{"2":{"2118":1}}],["还是生活中的小确幸",{"2":{"2109":1}}],["还是制定计划",{"2":{"2108":1}}],["还是做决策",{"2":{"2107":1}}],["还是分布式",{"2":{"1565":1}}],["还是相对位置编码",{"2":{"766":1}}],["还是存在差距",{"2":{"736":1}}],["还是说dataloader准备拿一个batch",{"2":{"665":1}}],["还是放在了",{"2":{"344":1}}],["还是全连接层",{"2":{"344":1}}],["还是选择使用",{"2":{"335":1}}],["还是",{"2":{"246":1,"277":2,"516":2,"773":1,"1441":1}}],["还是单纯的模式识别",{"2":{"156":1}}],["还是单纯因为该问题在训练集中出现过而依据表层的统计模式匹配之后输出答案",{"2":{"121":1}}],["还是mlp",{"2":{"122":1}}],["还允许对其中的信息进行高效且有针对性的更新",{"2":{"121":1}}],["还使用了一个",{"2":{"99":1}}],["还分析了bert对词语之间依存关系的识别效果",{"2":{"20":1}}],["除和取余运算",{"2":{"1607":1}}],["除法和取模",{"2":{"1635":1}}],["除法",{"2":{"1436":1,"1607":1,"1997":1}}],["除此以外",{"2":{"1412":1}}],["除此之外",{"2":{"316":1,"318":1,"816":1,"885":1}}],["除非这些构造函数和析构函数是平凡的",{"2":{"1728":1}}],["除非有明确的需要",{"2":{"1728":1}}],["除非你有大量的语料",{"2":{"1313":1}}],["除非明确同步处理",{"2":{"1168":1}}],["除非所有目标超参数值都有相同的冗余超参数集",{"2":{"1144":1}}],["除非您的内存压力非常大",{"2":{"1123":1}}],["除非包装在",{"2":{"1117":1}}],["除少数情况外",{"2":{"911":1}}],["除以序列的长度",{"2":{"904":1}}],["除以√22",{"2":{"332":1}}],["除标准差",{"2":{"312":1,"314":1,"316":1,"322":1}}],["除了标准输入输出流",{"2":{"1818":1,"1836":1}}],["除了",{"2":{"1798":1}}],["除了打印交通工具的基本信息外",{"2":{"1664":2}}],["除了基本数据类型",{"2":{"1622":1}}],["除了512x512版本的模型",{"2":{"1363":1}}],["除了第一个元素没有前继元素",{"2":{"1322":1}}],["除了能生成令人印象深刻的真实篇章之外",{"2":{"1316":1}}],["除了尝试实现每组实验的原始科学目标之外",{"2":{"1146":1}}],["除了通过实验为每个新问题找到它之外",{"2":{"1133":1}}],["除了设置",{"2":{"1118":1}}],["除了讨论上述机制之外",{"2":{"1116":1}}],["除了注意力之外的所有模块",{"2":{"976":1}}],["除了每个编码器层中的两个子层之外",{"2":{"915":1}}],["除了拼接维度",{"2":{"825":1}}],["除了具象的物体之外",{"2":{"754":1}}],["除了具有独立组件的好处外",{"2":{"222":1}}],["除了架构差异外",{"2":{"721":1}}],["除了线性模型",{"2":{"692":1}}],["除了token化",{"2":{"612":1}}],["除了上面分析的优点之外",{"2":{"512":1}}],["除了上述主要模块之外",{"2":{"467":1}}],["除了自己之外",{"2":{"170":1}}],["除了在",{"2":{"144":1}}],["除了为了保证",{"2":{"138":1}}],["除了知识存储之外",{"2":{"132":1}}],["除了应用注意力掩码和限制可达令牌的数量外",{"2":{"93":1}}],["除了需要盖住pad部分",{"2":{"74":1,"382":1}}],["除而不是直接除",{"2":{"41":1}}],["^=",{"2":{"1630":1,"1635":1,"2061":1}}],["^3",{"2":{"1175":2}}],["^σbσ^b",{"2":{"343":1}}],["^μbμ^b",{"2":{"343":1}}],["^2",{"2":{"210":1,"1191":1,"1192":1,"1193":1}}],["^t",{"2":{"210":1}}],["^⊤",{"2":{"192":1}}],["^d",{"2":{"183":2}}],["^n|x",{"2":{"692":2}}],["^n",{"2":{"178":2,"692":1}}],["^q",{"2":{"161":3}}],["^k",{"2":{"161":3,"759":1}}],["^i",{"2":{"71":1}}],["^",{"2":{"71":2,"93":1,"126":2,"145":2,"161":7,"192":1,"402":3,"499":1,"571":2,"899":1,"903":1,"927":10,"941":4,"944":5,"957":2,"960":4,"1002":3,"1003":12,"1007":1,"1240":1,"1339":10,"1340":14,"1343":8,"1344":1,"1389":2,"1393":6,"1395":13,"1630":1,"1635":1,"2059":3,"2060":3,"2062":2}}],["^v和akijaijka",{"2":{"759":1}}],["^v",{"2":{"54":1,"161":3,"191":7}}],["被销毁",{"2":{"1891":1}}],["被继承的公共基类称为虚基类",{"2":{"1693":1}}],["被创建了",{"2":{"1674":1,"1675":1}}],["被当作",{"2":{"1667":1}}],["被覆盖的成员无法直接调用",{"2":{"1663":1}}],["被虚继承的类称为虚基类",{"2":{"1662":1}}],["被推断为",{"2":{"1615":4}}],["被mask",{"2":{"1330":2}}],["被提出之后",{"2":{"1312":1}}],["被load",{"2":{"1214":1}}],["被state",{"2":{"1214":1}}],["被调用",{"2":{"1208":1,"1214":1,"1649":1}}],["被拆分为两个节点",{"2":{"986":1}}],["被合并到树中作为一个单独的边",{"2":{"986":1}}],["被称为",{"2":{"2054":1}}],["被称为尾零或空终止符",{"2":{"1704":1}}],["被称为输出层神经元的误差信号",{"2":{"1443":1}}],["被称为条件语言模型",{"2":{"894":1}}],["被称为位置偏置",{"2":{"766":1}}],["被",{"2":{"714":1,"1083":1,"1227":2}}],["被动",{"2":{"712":1}}],["被用来把输入和输出的token",{"2":{"700":1}}],["被用来构建batch",{"2":{"383":1}}],["被定义为整体的不可分的",{"2":{"628":1}}],["被分配来建模每个单词",{"2":{"613":1}}],["被分为不同的head",{"2":{"1":1}}],["被传递给所有三个参数",{"2":{"525":1}}],["被加入或减去到",{"2":{"485":1}}],["被转换成嵌入后送入解码器",{"2":{"427":1}}],["被证明可以提升模型的最终性能",{"2":{"346":1}}],["被严重削弱了",{"2":{"332":1}}],["被广泛验证是有利于加快系统的学习速度的",{"2":{"298":1}}],["被关注的序列",{"2":{"265":1}}],["被设置为1",{"2":{"108":1}}],["被认为是神经网络机器翻译",{"2":{"281":1}}],["被认为是",{"2":{"108":1}}],["被屏蔽的元素被设置为负无穷大",{"2":{"70":1}}],["内置异常类型",{"2":{"1762":1}}],["内联静态成员变量",{"2":{"1923":1}}],["内联变量",{"0":{"1923":1},"2":{"1920":1,"1932":1}}],["内联的原理",{"2":{"1709":1}}],["内联函数可能使调试更加困难",{"2":{"1709":1}}],["内联函数的声明和定义应该放在同一个头文件中",{"2":{"1709":1}}],["内联函数的注意事项",{"2":{"1709":1}}],["内联函数的定义可以放在头文件中",{"2":{"1628":1}}],["内联函数是一种编译器优化技术",{"2":{"1709":1}}],["内联函数",{"0":{"1709":1},"2":{"1628":1,"1632":1,"1709":2}}],["内层覆盖外层",{"2":{"1663":1}}],["内层作用域和外层作用域名称相同",{"2":{"1663":1}}],["内外层循环和flashattention1",{"2":{"971":1}}],["内核",{"2":{"778":1}}],["内积结果有衰减趋势的出现",{"2":{"1344":1}}],["内积满足线性叠加性",{"2":{"1343":1}}],["内积上的",{"2":{"765":1}}],["内积相似度",{"2":{"175":3}}],["内聚度",{"2":{"598":1}}],["内",{"2":{"510":1,"763":1}}],["内侧",{"2":{"490":1}}],["内计算均值和方差",{"2":{"341":1}}],["内的token用于pooling",{"2":{"731":1}}],["内的均值和方法",{"2":{"338":1}}],["内的",{"2":{"323":2}}],["内的维度",{"2":{"101":1}}],["内部可以使用基类方法",{"2":{"1861":1}}],["内部元素根据键自动排序",{"2":{"1807":1}}],["内部会自动对元素进行排序",{"2":{"1806":1}}],["内部列出了枚举常量",{"2":{"1728":1}}],["内部定义了联合体的成员",{"2":{"1728":1}}],["内部定义了结构体的成员",{"2":{"1728":1}}],["内部定义的变量",{"2":{"1649":1}}],["内部使用",{"2":{"1214":1}}],["内部的空格",{"2":{"582":1}}],["内部求均值和标准差",{"2":{"337":1}}],["内部协变量偏移",{"2":{"309":1}}],["内部非线性层之后的数值",{"2":{"145":1}}],["内存占用较低",{"2":{"1801":1}}],["内存占用和序列长度呈线性关系",{"2":{"945":1,"965":1}}],["内存不足",{"2":{"1761":1}}],["内存泄漏是指程序在动态分配内存后",{"2":{"1671":1}}],["内存泄漏及其危害",{"0":{"1671":1}}],["内存泄漏的概念和危害",{"2":{"1647":1}}],["内存越界指的是程序试图访问不属于其分配范围的内存区域",{"2":{"1670":1}}],["内存越界及其危害",{"0":{"1670":1}}],["内存对齐",{"2":{"1653":1}}],["内存中的栈区和堆区",{"0":{"1648":1}}],["内存垃圾",{"2":{"1647":1}}],["内存就像一块可以自由分配和回收的土地",{"2":{"1647":1}}],["内存分配失败",{"2":{"1762":1}}],["内存分配和释放更加灵活",{"2":{"1648":1}}],["内存分配和释放就像叠盘子一样",{"2":{"1648":1}}],["内存分配",{"2":{"1639":1,"1714":2}}],["内存地址",{"2":{"1611":1}}],["内存地址通常以十六进制形式显示",{"2":{"1611":1}}],["内存地址的表示",{"2":{"1611":1}}],["内存地址的艺术",{"2":{"1611":1}}],["内存管理和类的基础",{"0":{"1666":1},"1":{"1667":1,"1668":1,"1669":1,"1670":1,"1671":1,"1672":1,"1673":1,"1674":1,"1675":1,"1676":1,"1677":1,"1678":1}}],["内存管理运算符",{"2":{"1630":1}}],["内存管理",{"2":{"1602":1}}],["内存的价格要高2~3个数量级",{"2":{"1477":1}}],["内存过于昂贵",{"2":{"1477":1}}],["内存断电后数据会丢失",{"2":{"1477":1}}],["内存优化等",{"2":{"1288":1}}],["内存效率",{"2":{"1027":1}}],["内存很快就会被kv",{"2":{"986":1}}],["内存共享",{"0":{"983":1}}],["内存浪费仅发生在序列的最后一个块中",{"2":{"982":1}}],["内存需求与序列长度𝑁成线性关系",{"2":{"964":1}}],["内存开销",{"2":{"945":1,"965":1}}],["内存循环遍历q",{"2":{"944":1}}],["内存消耗和数据传输",{"2":{"1288":1}}],["内存消耗限制了只能使用小批量的bn",{"2":{"338":1}}],["内存消耗大",{"2":{"279":1}}],["内存层中的哈希表整合了可学习向量",{"2":{"153":1}}],["内存层的设计与工作原理",{"2":{"153":1}}],["内存受限操作",{"2":{"17":1}}],["内容是token",{"2":{"399":1}}],["内容略少",{"2":{"362":1}}],["内容相关的大模型主成分微调",{"2":{"233":1}}],["内容进行信息计量",{"2":{"147":1}}],["内容依据实际情况而定",{"2":{"70":1}}],["内嵌的知识",{"2":{"139":1}}],["内在知识编辑",{"0":{"142":1},"1":{"143":1,"144":1,"145":1},"2":{"96":1,"141":1}}],["操作步骤",{"0":{"2065":1}}],["操作系统进程同步",{"2":{"2043":1}}],["操作系统开发",{"2":{"1941":1}}],["操作系统可能会阻止程序访问非法内存",{"2":{"1670":1}}],["操作系统维护着一个空闲内存列表",{"2":{"1648":1}}],["操作系统将其划分成了不同的区域",{"2":{"1648":1}}],["操作系统",{"0":{"1431":1},"2":{"2090":1}}],["操作之前",{"2":{"1345":1}}],["操作的流程是",{"2":{"1344":1}}],["操作和控制流等信息",{"2":{"1291":1}}],["操作后的",{"2":{"1227":1}}],["操作指南",{"2":{"1196":1}}],["操作tensor对象",{"2":{"1083":1}}],["操作中的行为",{"2":{"1083":1}}],["操作示例",{"2":{"986":1}}],["操作是在row",{"2":{"967":1}}],["操作吗",{"2":{"829":1}}],["操作对象",{"2":{"745":2}}],["操作结果张量的形状",{"2":{"520":1,"530":1}}],["操作也在单个样本内部进行",{"2":{"337":1}}],["操作就相当于",{"2":{"332":1}}],["操作就相当于除以√1+σ21+σ2",{"2":{"332":1}}],["操作负责将方差重新变为",{"2":{"332":1}}],["操作产生的",{"2":{"314":1}}],["操作会对矩阵相乘结果进行缩放",{"2":{"199":1}}],["操作如下",{"2":{"113":1}}],["操作",{"2":{"69":1,"83":1,"212":1,"284":1,"294":1,"316":1,"344":1,"520":1,"530":1,"604":1,"829":1,"970":1,"1003":1,"1086":1,"1098":1,"1624":1}}],["操作把投影输出",{"2":{"29":1}}],["先做截止时间是",{"2":{"2135":1}}],["先做人",{"2":{"2054":1}}],["先选择",{"2":{"2131":1}}],["先刷牙",{"2":{"2097":1}}],["先进先出",{"2":{"1723":1}}],["先编译所有成员变量",{"2":{"1653":1}}],["先执行构造函数初始化列表",{"2":{"1653":1}}],["先返回当前值",{"2":{"1630":1}}],["先自增",{"2":{"1630":1}}],["先完成",{"2":{"1596":1}}],["先上手",{"2":{"1596":1}}],["先前的实验表明最好的优化器和当前的目标超参数无关",{"2":{"1143":1}}],["先前的工作还提出利用环拓扑来计算自注意力",{"2":{"975":1}}],["先前token的键",{"2":{"542":1}}],["先用",{"2":{"777":1}}],["先映射到一个低维空间",{"2":{"698":1}}],["先将嵌入投影回原始空间",{"2":{"692":1}}],["先将隐藏层张量的数据类型提升为torch",{"2":{"346":1}}],["先行概念",{"2":{"632":1}}],["先初始化一个大词表",{"2":{"601":1}}],["先使用基于掩码语言建模目标训练",{"2":{"542":1}}],["先保存输入hidden",{"2":{"346":1}}],["先计算均值和方差不同",{"2":{"346":1}}],["先计算注意力分数",{"2":{"67":1}}],["先解决每组内部的问题",{"2":{"216":1}}],["先独立计算块内部的词之间的注意力分数",{"2":{"216":1}}],["先预测当前词是哪一类",{"2":{"185":1}}],["先预测词属于哪一类",{"2":{"185":1}}],["先排序",{"2":{"90":1}}],["先对第二和第三维进行转置",{"2":{"36":1}}],["\\tprintcuboidinfo",{"2":{"1792":1}}],["\\treturn",{"2":{"1792":2}}],["\\tcuboid",{"2":{"1792":4}}],["\\tcounter",{"2":{"557":1}}],["\\tdouble",{"2":{"1792":1}}],["\\tstd",{"2":{"1791":1,"1792":1}}],["\\tfriend",{"2":{"1791":1,"1792":2}}],["\\t执行公式中的softmax",{"2":{"199":1}}],["\\t\\t\\t\\t\\t",{"2":{"1533":1}}],["\\t\\tdropout",{"2":{"523":1}}],["\\t\\t",{"2":{"65":1,"113":1}}],["\\t",{"2":{"65":1,"84":1,"428":2,"522":1,"557":1,"1691":1}}],["创建并写入文件",{"2":{"1930":1}}],["创建子目录",{"2":{"1930":1}}],["创建目录",{"2":{"1930":1}}],["创建目标语言掩码",{"2":{"380":1}}],["创建目标语言的掩码",{"2":{"74":1}}],["创建filemanager对象",{"2":{"1902":1}}],["创建两个文件",{"2":{"1825":1,"1843":1}}],["创建两个具有相同参数的sublayerconnection实例",{"2":{"523":1}}],["创建包含",{"2":{"1713":1}}],["创建了存储",{"2":{"1700":1}}],["创建了一个",{"2":{"1315":1}}],["创建几个派生类",{"2":{"1690":1}}],["创建基类",{"2":{"1690":1}}],["创建动态数据结构",{"2":{"1647":1}}],["创建和访问结构体变量",{"2":{"1728":1}}],["创建和启动线程",{"2":{"1566":1}}],["创建和删除目录",{"2":{"1509":1}}],["创建消息",{"2":{"1563":2}}],["创建空文件",{"2":{"1510":1}}],["创建文件",{"2":{"1510":1}}],["创建实体类",{"2":{"1481":1}}],["创建优化器和学习率调度器",{"2":{"1243":1}}],["创建掩码",{"2":{"1216":1}}],["创建输入数据",{"2":{"1216":1}}],["创建模型",{"2":{"1216":1,"1218":1}}],["创建模型实例",{"2":{"1211":1}}],["创建研究通常涉及几个方面",{"2":{"1144":1}}],["创建一组研究",{"0":{"1144":1}}],["创建一系列研究以比较目标超参数的不同值",{"2":{"1142":1}}],["创建一个可执行文件",{"2":{"1997":1}}],["创建一个类",{"2":{"1902":1}}],["创建一个vector并使用lambda打印每个元素",{"2":{"1883":1}}],["创建一个存储",{"2":{"1797":1}}],["创建一个存储整数的",{"2":{"1714":1}}],["创建一个空字符串",{"2":{"1713":1}}],["创建一个",{"2":{"1690":1,"1873":2,"1874":2}}],["创建一个包含4位的bitset",{"2":{"2062":1}}],["创建一个包含",{"2":{"1623":1}}],["创建一个名为",{"2":{"1509":1,"1678":1,"1966":1}}],["创建一个优化器对象",{"0":{"1221":1}}],["创建一个需要梯度的张量",{"2":{"1094":1}}],["创建一个现有",{"2":{"1087":1}}],["创建一个示例张量",{"2":{"832":1}}],["创建一个nn",{"2":{"701":1}}],["创建一个layernorm层",{"2":{"522":1}}],["创建一个与x有相同形状的张量",{"2":{"399":1}}],["创建一个掩码矩阵",{"2":{"63":1}}],["创建者的第几个输出",{"2":{"1082":1}}],["创建",{"0":{"1069":1},"2":{"1891":1,"1911":1,"1930":1,"1999":1}}],["创建pytorch",{"0":{"1068":1},"1":{"1069":1,"1070":1,"1071":1,"1072":1}}],["创建新的假数据相当简单",{"2":{"1015":1}}],["创建的项目",{"2":{"2070":1}}],["创建的",{"2":{"708":1}}],["创建信息密度相对均匀的上下文化字节分组",{"2":{"612":1}}],["创建三个sublayerconnection类实例",{"2":{"533":1}}],["创建大小为",{"2":{"201":1}}],["创建源语言的掩码",{"2":{"66":1,"380":1}}],["创新点",{"0":{"92":1},"2":{"49":1}}],["加工厂",{"2":{"1729":1}}],["加噪过程是按照noisy",{"2":{"1350":1}}],["加扰的",{"2":{"1176":1}}],["加快编译速度",{"2":{"1628":1}}],["加快训练速度就等于改善训练效果",{"2":{"1154":1}}],["加快收敛速度",{"2":{"411":1,"807":1}}],["加了一项随机扰动",{"2":{"1044":1}}],["加进子词表",{"2":{"582":1}}],["加进去",{"2":{"580":1}}],["加深llm在范畴对象和态射中遍历的深度",{"2":{"480":1}}],["加深网络只是为了加深而加深吗",{"2":{"301":1}}],["加权平均得到的",{"2":{"1350":1}}],["加权平均过程",{"2":{"923":1}}],["加权",{"2":{"731":1}}],["加权积累与关联生成自己的语言",{"2":{"460":1}}],["加权求和可能会掩盖某些嵌入的特定优势",{"2":{"739":1}}],["加权求和可以分为加权和求和两方面来看",{"2":{"276":1}}],["加权求和集成",{"2":{"739":1}}],["加权求和以一种动态",{"2":{"276":1}}],["加权求和",{"0":{"169":1,"270":1,"276":1},"2":{"157":1}}],["加载训练中的状态",{"0":{"1267":1}}],["加载存储的优化器状态",{"2":{"1227":1}}],["加载完",{"2":{"1227":1}}],["加载保存的模型状态",{"2":{"1227":1}}],["加载状态时的后向钩子函数",{"2":{"1208":1}}],["加载状态的pre",{"2":{"1208":1}}],["加载到sram",{"2":{"944":1}}],["加载到",{"2":{"944":1}}],["加载batch",{"0":{"384":1}}],["加载数据集",{"2":{"375":1}}],["加载数据",{"0":{"375":1},"1":{"376":1,"377":1,"378":1,"379":1,"380":1,"381":1,"382":1,"383":1,"384":1,"385":1,"386":1}}],["加载词表",{"0":{"374":1}}],["加载分词器",{"0":{"373":1},"2":{"371":1}}],["加载模型到",{"2":{"1298":1}}],["加载模型并运行",{"2":{"1298":1}}],["加载模型静态图",{"0":{"1270":1}}],["加载模型的状态",{"0":{"1259":1}}],["加载模型",{"0":{"372":1,"1263":1},"2":{"371":1}}],["加载功能模块",{"0":{"371":1},"1":{"372":1,"373":1,"374":1}}],["加载总结",{"2":{"95":1,"387":1}}],["加速您的模型",{"2":{"1121":1}}],["加速参数更新",{"2":{"1031":1}}],["加速3x",{"2":{"945":1,"965":1}}],["加速15",{"2":{"945":1,"965":1}}],["加速效果",{"2":{"945":1,"965":1}}],["加速训练",{"2":{"807":1}}],["加速原理",{"0":{"798":1}}],["加速带来的运行效率的提升",{"2":{"415":1}}],["加速收敛",{"2":{"310":1,"898":1,"1032":1}}],["加速模型的收敛",{"2":{"310":1}}],["加速模型的训练过程",{"2":{"187":1}}],["加大学习难度",{"2":{"187":1}}],["加法模块",{"2":{"1916":1}}],["加法和减法",{"2":{"1635":1}}],["加法",{"2":{"1436":1,"1607":1,"1997":1}}],["加法式注意机制略微但始终优于乘法式注意力机制",{"2":{"175":1}}],["加法注意力可能会略优于点乘注意力",{"2":{"175":1}}],["加法方案优点是",{"2":{"175":1}}],["加上常见的friend和love",{"2":{"567":1}}],["加上位置编码的原因是由于transformer模型没有循环或卷积操作",{"2":{"518":1}}],["加上位置编码后的结果通常仍可以称为",{"2":{"460":1}}],["加上了残差结构",{"2":{"349":1}}],["加上第",{"2":{"315":1}}],["加上第2个样本第1个通道",{"2":{"315":1}}],["加上训练提取的结构与关联信息",{"2":{"116":1}}],["加上它们之后",{"2":{"63":1}}],["加入c++基础笔记c++基础",{"2":{"2049":1}}],["加入缩放和平移变量的原因是",{"2":{"807":1}}],["加入缩放和平移变量",{"2":{"807":1}}],["加入词典",{"2":{"576":1}}],["加入掩码的原因是要隐藏未来信息",{"2":{"528":1}}],["加入前一层的attention",{"2":{"349":1}}],["加入参数",{"2":{"313":1}}],["加入这一步的原因是因为",{"2":{"313":1}}],["加入这个掩码",{"2":{"70":1}}],["加入了不同的线性变换相当于对",{"2":{"172":1}}],["加入mask之后的注意力计算的具体步骤如下",{"2":{"63":1}}],["加粗",{"2":{"3":1}}],["俗话说",{"2":{"58":1}}],["我可不想让你走开我的算法生活课",{"2":{"2140":1}}],["我可以给自己基本定位为",{"2":{"2054":1}}],["我都乐于分享",{"2":{"2109":1}}],["我尝试创建",{"2":{"2070":1}}],["我使用createuser",{"2":{"2070":1}}],["我依旧找到了我宝贵的东西",{"2":{"2056":1}}],["我会在其中看到我自己的影子",{"2":{"2056":1}}],["我希望如此",{"2":{"2054":1}}],["我才能够慢慢理解吧",{"2":{"2054":1}}],["我才慢慢觉知到其中的奥秘",{"2":{"2054":1}}],["我自己暂时的确还没参悟出孔子这里的思想",{"2":{"2054":1}}],["我自然明白老板用意",{"2":{"2051":1}}],["我先说说目前的大概理解",{"2":{"2054":1}}],["我还不能够完全理解清楚",{"2":{"2054":1}}],["我这里不再详细赘述",{"2":{"2155":1}}],["我这里更不能去解读",{"2":{"2054":1}}],["我这里的这两点不是为了反驳教学上的理解",{"2":{"2054":1}}],["我这个tensor",{"2":{"1106":1}}],["我个人而言",{"2":{"2054":1}}],["我个人的路径",{"2":{"1598":1}}],["我懂得",{"2":{"2054":1}}],["我三四岁就开始上学了呢",{"2":{"2054":1}}],["我以前看到这句话的时候心里想",{"2":{"2054":1}}],["我并没有带太多情绪的地回复到",{"2":{"2054":1}}],["我想说",{"0":{"2117":1}}],["我想大家想到的并不是什么算法",{"2":{"2112":1}}],["我想也并不是不生气",{"2":{"2054":1}}],["我想表达的是教材解释成",{"2":{"2054":1}}],["我想起一件事",{"2":{"2054":1}}],["我想问的也就是在一个batch或者一个seq的访存情况",{"2":{"656":1}}],["我认为一定要结合自己的日常真实生活",{"2":{"2054":1}}],["我领悟这段话用了很久的时间",{"2":{"2054":1}}],["我觉得这是一种很高的境界",{"2":{"2054":1}}],["我觉得没必要",{"2":{"2054":1}}],["我觉得已经用言语精炼的表达了",{"2":{"2054":1}}],["我觉得想理解孔子及其弟子想传达的思想",{"2":{"2052":1}}],["我觉得主要就是",{"2":{"1565":1}}],["我仅仅是根据个人认知和生活习得整理的",{"2":{"2052":1}}],["我说还不如我们自己弄",{"2":{"2051":1}}],["我也不是什么君子",{"2":{"2054":1}}],["我也不是很想说什么了",{"2":{"2051":1}}],["我也无法真正书写出我的感悟",{"2":{"2052":1}}],["我也发现了自己很多似是而非的错误理解",{"2":{"235":1}}],["我要回去找自己的枝枝蔓蔓了",{"2":{"2056":1}}],["我要承担风险",{"2":{"2051":1}}],["我要去后海玩",{"2":{"714":1}}],["我的建议",{"2":{"2146":1}}],["我的感悟",{"2":{"2042":1,"2043":1}}],["我的ai宏愿是启动更多像机器翻译项目那样的惊人应用项目",{"2":{"284":1}}],["我是否也是这样",{"2":{"2054":1}}],["我是",{"2":{"1674":1,"1675":1}}],["我是大黄同学呀的博客",{"2":{"429":1}}],["我将你提供的",{"2":{"1606":1}}],["我能做什么准备",{"2":{"1598":1}}],["我应该使用哪种学习率衰减方案作为默认值",{"0":{"1172":1}}],["我画了一个形象生动的图来示意smt和nmt的区别",{"2":{"909":1}}],["我特别喜欢吃苹果",{"2":{"714":1}}],["我喜欢不焦虑的人",{"2":{"2056":1}}],["我喜欢苹果",{"2":{"536":2}}],["我喜欢吃苹果",{"2":{"391":1}}],["我吃了一个",{"2":{"516":1}}],["我吃了一个苹果翻译成英语",{"2":{"537":1}}],["我吃了一个苹果",{"2":{"170":2,"267":1,"277":1,"405":5,"407":7,"427":5,"445":2,"453":2,"516":1,"537":1,"547":1}}],["我在参考文献中进行增补",{"2":{"235":1}}],["我在网上找了下",{"2":{"235":1}}],["我",{"2":{"170":5,"245":2,"267":5,"381":1,"536":1,"547":1,"709":1,"714":2}}],["我爱你",{"2":{"161":1,"245":1,"428":1,"457":1,"528":4,"529":1}}],["我爱中国",{"2":{"58":1,"676":1}}],["我爱",{"2":{"58":1}}],["我们每次都选最大的数字放在高位",{"2":{"2126":1}}],["我们每次降低这个距离",{"2":{"1377":1}}],["我们解决问题的时候会觉得这是本该如此的",{"2":{"2120":1}}],["我们简单理解一下这个东西",{"2":{"2118":1}}],["我们绘不自觉地优先选择价格最低或折扣最大的商品",{"2":{"2112":1}}],["我们绘制了在",{"2":{"1150":1}}],["我们渐渐步入到新的纪元",{"2":{"2107":1}}],["我们人生一旦领悟到要学",{"2":{"2054":1}}],["我们提问如果更换新的怎样",{"2":{"2051":1}}],["我们提出对blockwise",{"2":{"975":1}}],["我们提出了动态tanh",{"2":{"360":1}}],["我们提出的几种改进思路",{"2":{"272":1}}],["我们自己买来修",{"2":{"2051":1}}],["我们自然会找到越来越好的配置",{"2":{"1139":1}}],["我们如何获取导致异常的文件名",{"2":{"1763":1}}],["我们经常需要想办法提高效率",{"2":{"2106":1}}],["我们经常需要表示一组固定的",{"2":{"1728":1}}],["我们经过公式变换",{"2":{"908":1}}],["我们分别实例化了",{"2":{"1700":1}}],["我们分别使用字符串和整数作为",{"2":{"1701":1}}],["我们分别使用",{"2":{"1698":1}}],["我们初步了解了类的概念",{"2":{"1678":1}}],["我们初步展开",{"2":{"1339":1}}],["我们了解了数组传参的特性",{"2":{"1678":1}}],["我们求",{"2":{"1651":1}}],["我们学习的书本知识或许只是学习的很小很小的一部分",{"2":{"2054":1}}],["我们学习了指针安全使用的原则",{"2":{"1678":1}}],["我们学习了",{"2":{"1610":1,"1728":1}}],["我们学习速度会渐渐加快",{"2":{"402":1}}],["我们尽量将parameter参数和resulttype都写上",{"2":{"1488":1}}],["我们称这种现象为过拟合",{"2":{"1374":1}}],["我们称之为异常",{"2":{"1761":1}}],["我们称之为",{"2":{"1175":1}}],["我们称之为上线",{"2":{"1139":1}}],["我们称之为word",{"2":{"518":1}}],["我们称之为和sequence",{"2":{"382":1}}],["我们称之为padding",{"2":{"382":1}}],["我们相当于将预训练模型已经获得的知识",{"2":{"1313":1}}],["我们相信",{"2":{"691":1}}],["我们安装好了",{"2":{"1276":1}}],["我们具体的实现过程",{"2":{"1206":1}}],["我们具体分析下这三种注意力",{"2":{"441":1}}],["我们具体分析下",{"2":{"274":1}}],["我们拿不到input的梯度",{"2":{"1205":1}}],["我们欢迎听到来自您的反馈",{"2":{"1196":1}}],["我们复用了最初由naman",{"2":{"1194":1}}],["我们推荐其他人使用",{"2":{"1185":1}}],["我们决定为该领域的一个混乱来源做出贡献",{"2":{"1185":1}}],["我们认为这将",{"2":{"1158":1}}],["我们建议学习最新的稳定版本",{"2":{"1603":1}}],["我们建议进行两轮调整",{"2":{"1157":1}}],["我们建议坚持使用成熟",{"2":{"1130":1}}],["我们愿意等待的时间",{"2":{"1154":1}}],["我们甚至可以将验证集折叠到训练集中",{"2":{"1153":1}}],["我们实际上需要在每个设备上对批次进行二次抽样",{"2":{"1168":1}}],["我们实验的主要目标只需要考虑每次试验的验证误差",{"2":{"1149":1}}],["我们实现了一个最近最少使用",{"2":{"985":1}}],["我们喜欢在被我们称之为基本超参数轴图的图表绘制已完成的试验",{"2":{"1147":1}}],["我们想要通过样本x来估计关于z的分布",{"2":{"1377":1}}],["我们想要获得数据经过压缩后满足什么规律",{"2":{"1372":1}}],["我们想要评估实验为该目标提供的证据",{"2":{"1146":1}}],["我们想要比较大量目标超参数的值",{"2":{"1144":1}}],["我们想要计算注意力输出",{"2":{"941":1,"960":1}}],["我们更愿意使用",{"2":{"1185":1}}],["我们更加偏好使用quasi",{"2":{"1144":1}}],["我们更偏向使用准随机算法",{"2":{"1144":1}}],["我们更希望要稠密的向量",{"2":{"682":1}}],["我们没有选择顶不住的权利",{"2":{"2056":1}}],["我们没有优化器超参数值的先验倾向",{"2":{"1143":1}}],["我们没能充分针对每个目标超参数调优冗余超参数的风险就越高",{"2":{"1143":1}}],["我们根据实验目的确定目标超参数",{"2":{"1143":1}}],["我们可能采取了一种不同的说法",{"2":{"1185":1}}],["我们可能无法获得它们理论上能够提供的优势",{"2":{"1175":1}}],["我们可能无法理清各自的影响",{"2":{"1141":1}}],["我们可能能够回答很多问题",{"2":{"1157":1}}],["我们可能能够减少训练步数",{"2":{"1149":1}}],["我们可能可以在增加训练时间和重新调整学习率衰减策略中受益",{"2":{"1155":1}}],["我们可能需要使用更新鲜的估计",{"2":{"1152":1}}],["我们可能想要确定导致最佳验证误差的权重衰减值",{"2":{"1150":1}}],["我们可能会觉得有些搞笑",{"2":{"2054":1}}],["我们可能会运行少量的长时间",{"2":{"1157":1}}],["我们可能会错误地得出结论",{"2":{"1156":1}}],["我们可能会说",{"2":{"1155":1}}],["我们可能会看到训练损失会一直在缓慢减小",{"2":{"1155":1}}],["我们可能会在特定搜索空间运行相同的实验",{"2":{"1152":1}}],["我们可能会观察到",{"2":{"1152":1}}],["我们可能会受益于增加训练步数或更改学习率计划",{"2":{"1149":1}}],["我们可能会找到更优秀的点",{"2":{"1147":1}}],["我们可能会得出不正确的结论",{"2":{"1146":1}}],["我们可能对不同目标超参数值进行了不公平的比较",{"2":{"1145":1}}],["我们可能因为没有搜索某些冗余超参数空间",{"2":{"1145":1}}],["我们可能出于各种原因将其设为固定超参数",{"2":{"1143":1}}],["我们可能固定其中一些参数",{"2":{"1143":1}}],["我们可能通过过去的实验发现最优激活函数和模型深度无关",{"2":{"1143":1}}],["我们可以称这种思维方式为",{"2":{"2103":1}}],["我们可以创建一个名为",{"2":{"1728":1}}],["我们可以创建一个研究",{"2":{"1144":1}}],["我们可以重载",{"2":{"1712":1}}],["我们可以重用它们的共同祖先",{"2":{"986":1}}],["我们可以间接地调用函数",{"2":{"1706":1}}],["我们可以定义一个通用的算法或数据结构",{"2":{"1698":1}}],["我们可以防止外部代码直接修改对象的状态",{"2":{"1677":1}}],["我们可以让同一个函数执行不同的操作",{"2":{"1645":1}}],["我们可以让transformer模型执行任何复杂的计算任务",{"2":{"504":1}}],["我们可以考虑直接使用map实现",{"2":{"1485":1}}],["我们可以选择一个在大规模英文语料上预训练好的模型",{"2":{"1313":1}}],["我们可以想做多少轮就做多少轮",{"2":{"1157":1}}],["我们可以更有条理",{"2":{"2108":1}}],["我们可以更容易地定位到出错的函数",{"2":{"1729":1}}],["我们可以更快地找到最佳的模型和优化器超参数",{"2":{"1157":1}}],["我们可以更精确地评估它们在语义上的相似度",{"2":{"692":1}}],["我们可以负担得起训练的时间",{"2":{"1154":1}}],["我们可以只在工作流发生重大变化后重新对试验方差进行估计",{"2":{"1152":1}}],["我们可以只在序列开始进行输入计算",{"2":{"882":1}}],["我们可以也添加许多其他有用的潜在图表和可视化",{"2":{"1151":1}}],["我们可以进行这种转换",{"2":{"1143":1}}],["我们可以集中提升验证集效果",{"2":{"1140":1}}],["我们可以估计每步时间",{"2":{"1132":1}}],["我们可以自动识别和利用键值缓存重用机会",{"2":{"985":1}}],["我们可以像操作系统的虚拟内存一样以更加灵活的方式管理键和值",{"2":{"982":1}}],["我们可以跳过对该块的计算",{"2":{"970":1}}],["我们可以一次处理一个块计算",{"2":{"944":1,"963":1}}],["我们可以比较放心地猜测",{"2":{"713":1}}],["我们可以固定embedding",{"2":{"706":1}}],["我们可以直接将它作为参数训练出来",{"2":{"1340":1}}],["我们可以直接使用相应的结构和权重",{"2":{"1009":1}}],["我们可以直接用这个全连接层的参数作为特征传给transformer后续的模块",{"2":{"694":1}}],["我们可以直接简单地使用空格加一些标点符号来分词",{"2":{"579":1}}],["我们可以建立距离",{"2":{"692":1}}],["我们可以根据不同向量之间的关系",{"2":{"692":1}}],["我们可以根据问题类型优化",{"2":{"595":1}}],["我们可以利用这些数字来做些事情",{"2":{"679":1}}],["我们可以表达为一组数",{"2":{"679":1}}],["我们可以表示和理解更多的特征和概念",{"2":{"137":1}}],["我们可以延伸出很多应用",{"2":{"623":1}}],["我们可以采用系统的方法来找出训练任务中存在的稳定性问题",{"2":{"1179":1}}],["我们可以采用如下的方式来进行编码",{"2":{"587":1}}],["我们可以采用提示词工程方法",{"2":{"542":1}}],["我们可以对原始的词集合进行细粒度分词",{"2":{"582":1}}],["我们可以对矩阵采用分块",{"2":{"180":1}}],["我们可以清楚地看到模型在做出预测时",{"2":{"512":1}}],["我们可以从最佳试验的训练曲线中学到什么",{"2":{"1146":1}}],["我们可以从这三个指标分别探讨",{"2":{"511":1}}],["我们可以从mlp的输出中分解出一个任务感知的分量",{"2":{"122":1}}],["我们可以观察到token收敛到共识平衡点",{"2":{"507":1}}],["我们可以认为embedding是llm自己的语言系统",{"2":{"460":1}}],["我们可以使用访问修饰符",{"2":{"1677":1}}],["我们可以使用",{"2":{"1179":1}}],["我们可以使用这个实现",{"2":{"1176":1}}],["我们可以使用这些超参数来构建搜索空间",{"2":{"1153":1}}],["我们可以使用任何无梯度优化算法",{"2":{"1144":1}}],["我们可以使用算法自动搜索整个配置空间来最大化性能",{"2":{"1139":1}}],["我们可以使用与推理时相同的方法",{"2":{"405":1}}],["我们可以使用pytorch快速获得答案",{"2":{"119":1}}],["我们可以看到数学家和物理学家都喜欢咖啡",{"2":{"690":1}}],["我们可以看到每个概念都代表了句子的一个想法",{"2":{"628":1}}],["我们可以看到所有token开始都并保持在一个半球中",{"2":{"507":1}}],["我们可以看到10个token在由随机生成的正定对称矩阵定义的椭球体上的运动",{"2":{"507":1}}],["我们可以看到",{"2":{"291":1,"620":1}}],["我们可以看到自注意力机制",{"2":{"273":1}}],["我们可以看出来",{"2":{"277":1}}],["我们可以知道是在哪个阶段出现问题",{"2":{"406":1}}],["我们可以知道",{"2":{"262":1}}],["我们可以知道它们的相关程度有多高",{"2":{"169":1}}],["我们可以将",{"2":{"1228":1}}],["我们可以将目标超参数与冗余超参数放在相同的搜索空间中",{"2":{"1144":1}}],["我们可以将目标转变为",{"2":{"908":1}}],["我们可以将资源消耗分解为以下几个部分",{"2":{"1134":1}}],["我们可以将拼接后的向量",{"2":{"943":1,"961":1}}],["我们可以将原本复杂的问题简化为更一般性的形式",{"2":{"505":1}}],["我们可以将其中的黑色评估位置视为神经元",{"2":{"496":1}}],["我们可以将输出层",{"2":{"494":1}}],["我们可以将整个句子",{"2":{"408":1}}],["我们可以将不确定性视为注意力的",{"2":{"194":1}}],["我们可以将上述结构表示如下",{"2":{"99":1}}],["我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制",{"2":{"1464":1}}],["我们可以通过多种方式来评估模型的性能",{"2":{"1163":1}}],["我们可以通过对基本超参数轴图的x轴值进行分桶",{"2":{"1150":1}}],["我们可以通过对数值取log",{"2":{"183":1}}],["我们可以通过手动指定一组超参数配置来创建研究",{"2":{"1144":1}}],["我们可以通过增加计算资源来应对这种风险",{"2":{"1143":1}}],["我们可以通过反向传播来调整单词的向量来做到这一点",{"2":{"709":1}}],["我们可以通过例子来解答",{"2":{"516":1}}],["我们可以通过下图来和代码互相印证",{"2":{"199":1}}],["我们可以通过一种精炼策略去更准确地定位事实知识",{"2":{"135":1}}],["我们可以用另一个指针来存储一个指针变量的地址",{"2":{"1611":1}}],["我们可以用现有的子词来表示它",{"2":{"594":1}}],["我们可以用可学习的参数来描述从词嵌入到query",{"2":{"172":1}}],["我们可以用两个向量表示",{"2":{"80":1}}],["我们可以得到",{"2":{"163":1}}],["我们可以统一表示这些方法为",{"2":{"141":1}}],["我们可以找到共同生成有效",{"2":{"137":1}}],["我们可以有效地提取出这些方向",{"2":{"137":1}}],["我们可以查看这些特征来检查模型在处理给定上下文时内部发生的情况",{"2":{"137":1}}],["我们可以把它分解成几个小任务",{"2":{"2104":1}}],["我们可以把前l−1层看作表示",{"2":{"1464":1}}],["我们可以把可能导致这种不一致结果的最重要的变化来源分为以下几大类",{"2":{"1152":1}}],["我们可以把数学家和物理学家都投射成向量",{"2":{"690":1}}],["我们可以把多层加工理解为工厂的流水线",{"2":{"437":1}}],["我们可以把全量输入和真值标签一次性直接投入到解码器中来完成并行训练",{"2":{"416":1}}],["我们可以把源序列认为是key",{"2":{"267":1}}],["我们可以把q",{"2":{"180":1}}],["我们可以把自注意力机制分为三个阶段",{"2":{"158":1}}],["我们可以把这个高维的语言空间",{"2":{"116":1}}],["我们可以把六种掩码矩阵分成两类",{"2":{"84":1}}],["我们可以把mha的多个attention计算视为多个独立的小模型",{"2":{"13":1}}],["我们可以举一个例子",{"2":{"80":1}}],["我们可以在项目开始时一次性做出决定",{"2":{"1128":1}}],["我们可以在反向传播过程中从sram中的q",{"2":{"946":1,"966":1}}],["我们可以在输入和输出张量元素之间建立一一对应关系",{"2":{"359":1}}],["我们可以在",{"2":{"34":1,"325":1}}],["我们大部分时间都花在了",{"2":{"1140":1}}],["我们大体有两个选择方案",{"2":{"744":1,"757":1}}],["我们才完全清楚在深度学习从业者的工作流程中可以找到多少有趣及被忽视的研究问题",{"2":{"1127":1}}],["我们鼓励发现我们的建议存在问题的读者提出替代建议以及令人信服的证据",{"2":{"1127":1}}],["我们是否应该在我们能接受的情况下一直训练",{"2":{"1157":1}}],["我们是否从搜索空间中采样了足够多的点",{"2":{"1146":1}}],["我们是一个由五名研究人员和工程师组成的团队",{"2":{"1127":1}}],["我们是允许decoder看到目标序列输入的全部信息的",{"2":{"72":1}}],["我们找不到任何系统性的资料来解释如何使用深度学习获得良好的结果",{"2":{"1127":1}}],["我们按照以下规则定义基本操作的梯度计算顺序",{"2":{"1115":1}}],["我们按块计算注意力",{"2":{"943":1,"961":1}}],["我们拥有的数据量是很有限的",{"2":{"1015":1}}],["我们迁移了这些学来的特征",{"2":{"1009":1}}],["我们记为",{"2":{"1003":1}}],["我们淘汰",{"2":{"986":1}}],["我们让每层输出方差相等",{"2":{"1003":1}}],["我们让缓存的token和当前正在运行的请求共享同一个memory",{"2":{"986":1}}],["我们让模型回顾之前保存的所有隐状态",{"2":{"267":1}}],["我们引入了一个简单的",{"2":{"986":1}}],["我们引入了",{"2":{"985":1}}],["我们引入了pagedattention",{"2":{"981":1}}],["我们确定llm服务的性能受到内存的限制",{"2":{"981":1}}],["我们很高兴地介绍vllm",{"2":{"980":1}}],["我们很自然会想到使用向量",{"2":{"679":1}}],["我们逐块计算原始transformer",{"2":{"975":1}}],["我们逐一进行分析",{"2":{"258":1}}],["我们利用内部循环key",{"2":{"975":1}}],["我们调整前向传递算法",{"2":{"973":1}}],["我们重新设计了flashattention",{"2":{"973":1}}],["我们重新输入迄今为止生成的整个输出序列",{"2":{"427":1}}],["我们定义了一种专门针对warp的软件流水线方案",{"2":{"973":1}}],["我们定义sm",{"2":{"210":1}}],["我们并没有以精确或数学定义良好的方式使用短语",{"2":{"1155":1}}],["我们并行执行attention函数",{"2":{"926":1}}],["我们并不能确定此时的模型是否已经获得了输出正确结果所需要的所有信息",{"2":{"245":1}}],["我们这里也暂时不做深入的研究",{"2":{"908":1}}],["我们所知的每一个事实都与参考系中的一个位置相对应",{"2":{"754":1}}],["我们所了解的更多是实践的成果和经验性的积累",{"2":{"474":1}}],["我们本节从各个角度出发",{"2":{"743":1}}],["我们必须要让这些个椭圆尽可能的推叠在一起",{"2":{"1374":1}}],["我们必须意识到",{"2":{"1149":1}}],["我们必须分别调整学习率",{"2":{"1143":1}}],["我们必须确保更改是有据可循的",{"2":{"1139":1}}],["我们必须确定起点",{"2":{"1137":1}}],["我们必须将它们转换为数字表示",{"2":{"708":1}}],["我们必须考虑单词的上下文才能更好的把单词的语义识别出来",{"2":{"167":1,"259":1}}],["我们此处只给出input",{"2":{"699":1}}],["我们从sharegpt数据集中对请求的输入",{"2":{"980":1}}],["我们从每个当前输入对应的所有可能输出",{"2":{"902":1}}],["我们从几个方面来看看相对位置的意义",{"2":{"753":1}}],["我们从elmo的论文题目",{"2":{"717":1}}],["我们从数值的角度来看",{"2":{"691":1}}],["我们从最初的10个字母成功得到了6个子词",{"2":{"584":1}}],["我们只简单地输出了错误信息",{"2":{"1761":1}}],["我们只是把最较大概率的部分框出来",{"2":{"1373":1}}],["我们只是知道它们在某些潜在的语义维度上是相似的",{"2":{"690":1}}],["我们只能从其他要求中想办法节约资源",{"2":{"1145":1}}],["我们只需对",{"2":{"2120":1}}],["我们只需要选择一个值并将其用于所有试验",{"2":{"1155":1}}],["我们只需要把",{"2":{"974":1}}],["我们只需要将因果掩码应用到1个块",{"2":{"970":1}}],["我们只需要通过求",{"2":{"908":1}}],["我们只需将路由权重和解码器嵌入直接连接起来",{"2":{"739":1}}],["我们只知道前i",{"2":{"413":1}}],["我们仔细分析下",{"2":{"684":1}}],["我们总是查看至少是最好的几项试验的训练曲线",{"2":{"1149":1}}],["我们总希望模型可以处理更多的词",{"2":{"681":1}}],["我们总结下",{"2":{"538":1}}],["我们总结训练和推理在流程上的区别如下表",{"2":{"427":1}}],["我们总结cnn和rnn这两个方案的主要问题如下",{"2":{"256":1}}],["我们总结一下在transformer模型中",{"2":{"120":1}}],["我们总结其联系如下",{"2":{"84":1}}],["我们给这四个字分配如下索引",{"2":{"679":1}}],["我们给每本书做了编号和信息摘要",{"2":{"169":1}}],["我们不用区分得太清楚",{"2":{"1340":1}}],["我们不再需要对每一个任务都从头开始训练模型",{"2":{"1312":1}}],["我们不再去统计一个单词在一个大文档里面出现的次数",{"2":{"713":1}}],["我们不应该仅仅因为两个条件超参数具有相同的名称就认为它们是相同的",{"2":{"1143":1}}],["我们不知道最好的深度学习秘诀",{"2":{"1127":1}}],["我们不鼓励使用它们",{"2":{"1123":1}}],["我们不预先分配一个固定大小的内存池作为缓存",{"2":{"986":1}}],["我们不能指望找到它",{"2":{"1127":1}}],["我们不能淘汰当前正在运行的批处理中使用的节点",{"2":{"986":1}}],["我们不能改变一个非叶子节点的",{"2":{"661":1,"1104":1}}],["我们不需要为计算复制键头和值头",{"2":{"971":1}}],["我们不需要应用因果掩码",{"2":{"970":1}}],["我们不仅仅希望只做到语义搜索",{"2":{"678":1}}],["我们不仅可以关注如何让模型理解任务",{"2":{"504":1}}],["我们缩小点积",{"2":{"647":1,"924":1}}],["我们怀疑",{"2":{"647":1,"924":1}}],["我们基本上是通过在每个pattention块的键和值矩阵中添加更多的参数token行来扩展现有的键值参数集",{"2":{"620":1}}],["我们依次遍历排好序的词典",{"2":{"587":1}}],["我们最关心的是学习方差",{"2":{"1152":1}}],["我们最终获得了数据压缩的分布规律",{"2":{"1372":1}}],["我们最终保存的是这个新的model",{"2":{"1269":1}}],["我们最终得到了正确的结果",{"2":{"942":1,"959":1}}],["我们最终工程上使用的就是上式",{"2":{"770":1}}],["我们最终构建的模块是",{"2":{"82":1}}],["我们最初的token列表为",{"2":{"585":1}}],["我们继续重复以上步骤",{"2":{"584":1}}],["我们继续细化编码流程",{"2":{"519":1}}],["我们选择字母来构成基础词表",{"2":{"580":1}}],["我们进入此函数来继续深挖",{"2":{"558":1}}],["我们一句一句来看",{"2":{"2054":1}}],["我们一般使用delete标签进行删除操作",{"2":{"1488":1}}],["我们一般使用update标签进行更新操作",{"2":{"1487":1}}],["我们一般使用insert标签进行插入操作",{"2":{"1486":1}}],["我们一般使用动态规划",{"2":{"908":1}}],["我们一般会遇到一个瓶颈期",{"2":{"402":1}}],["我们一一介绍",{"2":{"551":1}}],["我们得到了一个原始词集合",{"2":{"577":1}}],["我们得到三个矩阵连乘qk⊤vqk⊤vqk^⊤v",{"2":{"180":1}}],["我们得出与本篇相关的以下观察结果",{"2":{"540":1}}],["我们假定您具备机器学习和深度学习概念的基本知识",{"2":{"1126":1}}],["我们假定有完整的输入序列",{"2":{"525":1}}],["我们假设从业者已经确定了让模型变得不稳定的学习率",{"2":{"1183":1}}],["我们假设机器学习问题是监督学习或类似的东西",{"2":{"1126":1}}],["我们假设要把",{"2":{"407":1}}],["我们假设源序列和目标序列为同一个序列",{"2":{"265":1}}],["我们假设矩阵x为任务模型的输入",{"2":{"263":1}}],["我们假设",{"2":{"80":1}}],["我们假设是10个单词",{"2":{"23":1}}],["我们假设是512",{"2":{"23":1}}],["我们假设有8个注意力头",{"2":{"12":1}}],["我们马上会进行分析",{"2":{"525":1}}],["我们已经步入",{"0":{"2140":1}}],["我们已经掌握了",{"2":{"1644":1}}],["我们已经了解了",{"2":{"1607":1}}],["我们已经描述了如何从第一轮",{"2":{"1159":1}}],["我们已经看到它大于试验方差的情况以及它小得多的情况",{"2":{"1152":1}}],["我们已经将深度学习应用到从语音识别到天文学的方方面面",{"2":{"1127":1}}],["我们已经发现",{"2":{"861":1}}],["我们已经找到了可以用于表达词义的数字化形式",{"2":{"682":1}}],["我们已经学习了一种映射",{"2":{"473":1}}],["我们已经知道ffn",{"2":{"125":1}}],["我们指的是使用已经训练好的模型来对输入文本进行处理",{"2":{"425":1}}],["我们指定输入矩阵x的第",{"2":{"172":1}}],["我们应用了两种已经建立的技术",{"2":{"942":1,"959":1}}],["我们应用离散余弦变换",{"2":{"637":1}}],["我们应该尽量使用快速转换时间的试验来获得尽可能多的问题理解",{"2":{"1157":1}}],["我们应该检查研究中的最佳学习率是否在搜索空间的边界",{"2":{"1156":1}}],["我们应该期望能够训练到非常低的训练误差",{"2":{"1154":1}}],["我们应该看到较低的训练损失",{"2":{"1154":1}}],["我们应该有一个精确的搜索空间",{"2":{"1153":1}}],["我们应该只采用产生的改进超过它们增加的复杂性的更改",{"2":{"1152":1}}],["我们应该至少在目标超参数的每个设置相对应的最佳试验中检查是否有过拟合问题",{"2":{"1149":1}}],["我们应该尝试调整搜索空间以避免对这些点进行采样",{"2":{"1146":1}}],["我们应该进行另一项具有扩展搜索空间的研究",{"2":{"1146":1}}],["我们应该问自己以下额外的问题",{"2":{"1146":1}}],["我们应该训练足够长的时间以准确估计训练吞吐量",{"2":{"1132":1}}],["我们应该知道对于这种解码的过程",{"2":{"908":1}}],["我们应该更倾向于只使用解码器的模型结构",{"2":{"542":1}}],["我们应该在每一个时间步t都会产生一个隐向量hthth",{"2":{"267":1}}],["我们应采用并行",{"2":{"405":1}}],["我们现在知道具备语义性的稠密多维向量的重要性",{"2":{"687":1}}],["我们现在取平滑因子ϵ",{"2":{"399":1}}],["我们现在对每个头都有单独的z",{"2":{"35":1}}],["我们期望在短运行中找到的哪些超参数值会转移到更长的训练运行中",{"2":{"1158":1}}],["我们期望向量是可以被训练的",{"2":{"686":1}}],["我们期望解码器的输出尽可能接近真实标签",{"2":{"391":1}}],["我们期望捕捉更多的模式",{"2":{"12":1}}],["我们举例如下",{"2":{"381":1}}],["我们还要感谢",{"2":{"1194":1}}],["我们还应该考虑检查测试集上的性能",{"2":{"1153":1}}],["我们还希望借此机会从每组实验中提取其他有用的见解",{"2":{"1146":1}}],["我们还将比较标准注意力和我们的方法",{"2":{"941":1,"960":1}}],["我们还修改解码器堆栈中的self",{"2":{"915":1}}],["我们还可以将单词对应的向量在平面上绘制出来",{"2":{"685":1}}],["我们还需要一个序列去记录拼接文本的长度",{"2":{"377":1}}],["我们还是用把中文",{"2":{"453":1}}],["我们还是搬出之前的图来进行辅助解析",{"2":{"274":1}}],["我们还是以之前两个句子为例进行解释",{"2":{"167":1}}],["我们测量了归一化层的输入和输出",{"2":{"359":1}}],["我们采样了一个小批量的样本",{"2":{"359":1}}],["我们在购物的时候",{"2":{"2112":1}}],["我们在生活中做出很多决定时",{"2":{"2105":1}}],["我们在做长期计划时",{"2":{"2102":1}}],["我们在深度学习中调优的学习率和大部分其他被叫做",{"2":{"1185":1}}],["我们在这一节会对此进行进一步探讨",{"2":{"1175":1}}],["我们在这里还是包括了它",{"2":{"1122":1}}],["我们在训练期间定期进行评估",{"2":{"1164":1}}],["我们在定期评估时的目标是在训练期间获得可靠信号",{"2":{"1163":1}}],["我们在使用相同的超参数但不同的随机种子的训练运行之间看到的差异",{"2":{"1152":1}}],["我们在其中绘制验证目标值与其中一个超参数",{"2":{"1147":1}}],["我们在每一轮调整中都使用自动搜索算法",{"2":{"1139":1}}],["我们在每个子层再采用残差连接",{"2":{"915":1}}],["我们在实践中使用的许多函数并不具备这个性质",{"2":{"1115":1}}],["我们在反向传播中评估这个图",{"2":{"1113":1}}],["我们在两种设置下进行评估",{"2":{"980":1}}],["我们在第2",{"2":{"973":1}}],["我们在附录d",{"2":{"956":1}}],["我们在下述算法中描述了这一过程",{"2":{"942":1,"959":1}}],["我们在下图中绘制了这些映射关系",{"2":{"359":1}}],["我们在算法0中描述了标准的注意力实现",{"2":{"941":1,"960":1}}],["我们在初始输出中添加了起始符",{"2":{"408":1}}],["我们在后续会逐步细化",{"2":{"318":1}}],["我们在直觉上是希望关注语义上相关的单词",{"2":{"180":1}}],["我们即无法凭借一个字来确定这些字的含义",{"2":{"318":1}}],["我们后续把单个层称为编码器层或解码器层",{"2":{"436":1}}],["我们后续会分析",{"2":{"309":1}}],["我们后续有专门篇幅介绍",{"2":{"205":1,"206":1}}],["我们无法确定英文应该翻译成",{"2":{"277":1}}],["我们无法判断模型是真正理解它所处理的概念",{"2":{"121":1}}],["我们目的就是要获取特征向量列表中哪个特征向量包含对于",{"2":{"263":1}}],["我们知道有一种距离可以量化两种分布的差异kullback",{"2":{"1377":1}}],["我们知道llm推理主要有两个阶段",{"2":{"420":1}}],["我们知道深度通常比宽度更重要",{"2":{"334":1}}],["我们知道每一层的输出是其值向量分布的聚合",{"2":{"306":1}}],["我们知道",{"2":{"246":1,"689":1}}],["我们命名为z",{"2":{"199":1}}],["我们梳理下计算过程中的重点三步如下",{"2":{"271":1}}],["我们梳理下计算过程如下",{"2":{"173":1}}],["我们梳理自注意力机制的计算过程如下图所示",{"2":{"195":1}}],["我们也可能引发错误",{"2":{"1115":1}}],["我们也可以看到",{"2":{"519":1}}],["我们也可以在增加维度的同时缩放矩阵以保持方差一致",{"2":{"193":1}}],["我们也对查询执行低秩压缩",{"2":{"957":1}}],["我们也希望看到可能有不同建议的替代指南和方式",{"2":{"1127":1}}],["我们也希望用数学方法来比较向量的语义相似性和差异性",{"2":{"685":1}}],["我们也希望可以看到transformer是怎么没的",{"2":{"280":1}}],["我们也看到了在编码的过程中",{"2":{"584":1}}],["我们也介绍下instance",{"2":{"337":1}}],["我们也期望未来会有更多创新的模型和方法来让transformer在强化学习领域发挥更大的作用",{"2":{"291":1}}],["我们也不希望这些位置参与后期的反向传播过程",{"2":{"54":1}}],["我们细化原始论文的解释如下",{"2":{"187":1}}],["我们要尽量减小kl散度",{"2":{"1377":1}}],["我们要感谢will",{"2":{"1194":1}}],["我们要感谢max",{"2":{"1194":1}}],["我们要寻找w的分布使得输出y与输入z的方差保持一致",{"2":{"1000":1}}],["我们要找出最佳的翻译是什么",{"2":{"908":1}}],["我们要看看嵌入",{"2":{"676":1}}],["我们要使用",{"2":{"580":1}}],["我们要先通过印坯和修坯来确定器物形状",{"2":{"437":1}}],["我们要把输入的所有token向右移一个位置",{"2":{"407":1}}],["我们要保留句子中词与词之间的可比较性",{"2":{"318":1}}],["我们要知道transformer是怎么来的",{"2":{"280":1}}],["我们要提取的值或者说应该从各位置获取的实际内容",{"2":{"172":1}}],["我们要对解码器的输入再做一下说明",{"2":{"528":1}}],["我们要对所有的句子按照预先设定的最长长度做填充或者裁剪",{"2":{"53":1}}],["我们要对输入序列进行对齐",{"2":{"53":1,"376":1,"933":1}}],["我们计算相关性就相当于注意力机制中的",{"2":{"169":1}}],["我们看到了博客文章和社交媒体上的建议片段",{"2":{"1127":1}}],["我们看到输出值迅速向0靠拢",{"2":{"994":1}}],["我们看到decoder的每一步产生隐状态",{"2":{"899":1}}],["我们看到数学家和物理学家都可以跑步",{"2":{"690":1}}],["我们看看一种分类方式",{"2":{"766":1}}],["我们看看embedding的优点",{"2":{"696":1}}],["我们看看如何从文本晋级到embedding的演化思路",{"2":{"676":1}}],["我们看看transformer对此做了哪些改进从而完成并行",{"2":{"414":1}}],["我们看看模型是如何利用teacher",{"2":{"407":1}}],["我们看看论文",{"2":{"145":1}}],["我们看书就相当于获取其书中的详细信息",{"2":{"169":1}}],["我们希望在完全切换之前给它们更多的时间来适应",{"2":{"1228":1}}],["我们希望在独热编码基础上进行改进",{"2":{"682":1}}],["我们希望测量出其对于模型由何种影响的参数",{"2":{"1143":1}}],["我们希望无限期地改进模型",{"2":{"1139":1}}],["我们希望这项工作成为一份活的文件",{"2":{"1127":1}}],["我们希望本文档能鼓励其他人也来帮助系统化该领域的实验细节",{"2":{"1127":1}}],["我们希望",{"2":{"974":1}}],["我们希望尽可能多地花费时间在矩阵乘法flop上",{"2":{"968":1}}],["我们希望gqa在较大的模型中能够达到一个特别好的权衡",{"2":{"937":1,"953":1}}],["我们希望向量可以表达单词之间的语义相似性",{"2":{"685":1}}],["我们希望可以通过训练进行调整",{"2":{"682":1}}],["我们希望可以做到同时向前预测和向后回顾",{"2":{"256":1}}],["我们希望单词只能它以及它之前的的单词",{"2":{"528":1}}],["我们希望单词只能看到它以及它之前的的单词",{"2":{"525":1}}],["我们希望得到与预期的目标序列",{"2":{"398":1}}],["我们希望熵不变",{"2":{"194":1}}],["我们希望神经网络的输出可以反映每个类别的概率",{"2":{"180":1}}],["我们希望有不一样的计算相似度的办法以及更多的参数",{"2":{"172":1}}],["我们希望了解郁达夫",{"2":{"169":1}}],["我们希望选择一些向量值",{"2":{"145":1}}],["我们将ta理解成",{"2":{"2054":1}}],["我们将继续深入探讨函数的更多高级特性",{"2":{"1729":1}}],["我们将继续深入探索函数的奥秘",{"2":{"1644":1}}],["我们将继续探索更多",{"2":{"1709":1}}],["我们将继续执行此合并步骤",{"2":{"581":1}}],["我们将深入探索",{"2":{"1610":1}}],["我们将学习更多关于如何使用这些数据类型进行更复杂的操作",{"2":{"1607":1}}],["我们将原来",{"2":{"1373":1}}],["我们将此称为",{"2":{"1166":1}}],["我们将保存一些单个示例的预测",{"2":{"1164":1}}],["我们将对目标超参数的每个配置进行单独研究",{"2":{"1144":1}}],["我们将一些冗余超参数转作为固定超参数",{"2":{"1143":1}}],["我们将所有其他超参数视为冗余超参数",{"2":{"1143":1}}],["我们将正则化后的目标函数记为",{"2":{"1014":1}}],["我们将vllm的吞吐量与huggingface",{"2":{"980":1}}],["我们将预填集群中的每",{"2":{"977":1}}],["我们将与softmax相关的比较低吞吐量的非gemm操作",{"2":{"973":1}}],["我们将输入",{"2":{"944":1,"963":1}}],["我们将输入门的输出与细胞状态进行逐元素相加",{"2":{"867":1}}],["我们将展示标准的注意力实现在序列长度",{"2":{"941":1,"960":1}}],["我们将整个过程表达为矩阵形式",{"2":{"923":1}}],["我们将",{"2":{"868":1}}],["我们将经过修改后的细胞状态传递到",{"2":{"868":1}}],["我们将前一个隐藏状态和当前输入传递到",{"2":{"866":1,"868":1}}],["我们将这种规律用概率的形式表示",{"2":{"1372":1}}],["我们将这个安排添加到现有安排上",{"2":{"1183":1}}],["我们将这个转换过程叫做索引化",{"2":{"679":1}}],["我们将这一对字符合并为新的子词",{"2":{"583":1}}],["我们将加入新的重新初始化的",{"2":{"623":1}}],["我们将其合并为一个新的子词",{"2":{"582":1}}],["我们将其描述为x的跨度",{"2":{"485":1}}],["我们将其中第",{"2":{"313":1}}],["我们将矩阵描述为δ的跨度",{"2":{"485":1}}],["我们将它们视为dyt层的一部分",{"2":{"360":1}}],["我们将它们水平投影到x=0上",{"2":{"320":1}}],["我们将它们球形投影到圆x2+y2=1x2+y2=1x^2+y^2=1上",{"2":{"320":1}}],["我们将它们垂直投影到y=0",{"2":{"320":1}}],["我们将它们旋转45°",{"2":{"320":1}}],["我们将第一个向量的第一个值与第二个向量的第一列相乘",{"2":{"189":1}}],["我们将没有做softmax归一化之前的结果称为注意力分数",{"2":{"178":1}}],["我们将在下一篇进行详细解释",{"2":{"173":1}}],["我们将实际获得什么",{"2":{"164":1}}],["我们将选择代表主语的最后一个token的输入作为查找键",{"2":{"145":1}}],["我们正在寻找什么",{"2":{"164":1}}],["我们暂时不用关心细节",{"2":{"161":1}}],["我们暂时先忘记teacher",{"2":{"57":1}}],["我们能完成的实验就会越少",{"2":{"1157":1}}],["我们能通过更多的迭代次数将token列表缩小更多",{"2":{"585":1}}],["我们能通过提炼出不同",{"2":{"135":1}}],["我们能从中看到现实的本质吗",{"2":{"233":1}}],["我们能够更全面地理解信息在模型中的流动",{"2":{"148":1}}],["我们读取",{"2":{"145":1}}],["我们通过数学统计学的应用可以来做人工感知方面的决定问题",{"2":{"1456":1}}],["我们通过显示在主机环中",{"2":{"975":1}}],["我们通过将所有主机概念化为形成一个环结构来利用这一属性",{"2":{"975":1}}],["我们通过对该组内所有原始头进行平均汇总来构建每个组的键头和值头",{"2":{"937":1,"953":1}}],["我们通过收集激活来计算",{"2":{"145":1}}],["我们通常将",{"2":{"1464":1}}],["我们通常还会采用迁移学习",{"2":{"1312":1}}],["我们通常希望根据任何训练点上达到的验证误差来找到最佳试验",{"2":{"1175":1}}],["我们通常希望优先调整靠近输出层的参数",{"2":{"333":1}}],["我们通常会认为以某种方式将超过",{"2":{"1184":1}}],["我们通常会在比较目标超参数的不同值之前使用额外的正则化技术重新运行实验和",{"2":{"1149":1}}],["我们通常会采样我们可以负担得起的代价",{"2":{"1148":1}}],["我们通常会发现在一组给定的实验能够朝着最初的目标取得很大进展之前需要纠正的问题",{"2":{"1146":1}}],["我们通常可以认为每步的时间近似恒定",{"2":{"1133":1}}],["我们通常不希望模型将注意力放在这些无关紧要的填充的词上",{"2":{"54":1}}],["我们仍能不断发现新的特征",{"2":{"137":1}}],["我们发现参数更新是基于两部分组成",{"2":{"1036":1}}],["我们发现这比选择单个键和值头或从头开始随机初始化新的键和值头效果更好",{"2":{"938":1,"954":1}}],["我们发现起始位置的信息似乎包含了很多高频的token",{"2":{"437":1}}],["我们发现输入和输出之间的关系大多是线性的",{"2":{"359":1}}],["我们发现",{"2":{"135":1,"981":1,"995":1,"1167":1}}],["我们对模型的理解就会越深入",{"2":{"1157":1}}],["我们对模型结构做进一步抽象",{"2":{"125":1}}],["我们对目标超参数的不同设置选取最佳试验效果的试验",{"2":{"1149":1}}],["我们对以下问题感兴趣",{"2":{"1149":1}}],["我们对每个子层再采用一个残差连接",{"2":{"914":1}}],["我们对dct系数进行量化",{"2":{"637":1}}],["我们对代码中提到的sublayerconnection做下简要说明",{"2":{"523":1}}],["我们对上图流程分析如下",{"2":{"519":1}}],["我们对网络的隐藏层神经元的连续动态进行参数化",{"2":{"494":1}}],["我们对transformer的主要设计动机分析如下",{"2":{"434":1}}],["我们对它们各自的列向量进行多次点乘",{"2":{"189":1}}],["我们对公式作进一步的解读",{"2":{"176":1}}],["我们对这些",{"2":{"135":1}}],["我们输入",{"2":{"135":1}}],["我们先从",{"2":{"2125":1}}],["我们先去了小镇的的一家店",{"2":{"2051":1}}],["我们先分析后面三个参数",{"2":{"449":1}}],["我们先脱离代码来构思下",{"2":{"449":1}}],["我们先回想翻译场景",{"2":{"267":1}}],["我们先用文本翻译为例来做初步介绍",{"2":{"265":1}}],["我们先开始从seq2seq介绍",{"2":{"235":1}}],["我们先看看权威机构对embedding的定义",{"2":{"688":1}}],["我们先看看如何使用sublayerconnection类",{"2":{"344":1}}],["我们先看看如何用rnn实现编码器",{"2":{"249":1}}],["我们先看看序列转换面对的主要技术挑战",{"2":{"244":1}}],["我们先看看",{"2":{"135":1}}],["我们先来仔细分析一下为何需要掩码",{"2":{"51":1}}],["我们讨论的依然是某个特定的键值对",{"2":{"129":1}}],["我们有三个主要的实现类别",{"2":{"1228":1}}],["我们有以下经验法则",{"2":{"1143":1}}],["我们有了更好的方法",{"2":{"907":1}}],["我们有了更为先进复杂的机器翻译技术",{"2":{"907":1}}],["我们有输出门",{"2":{"868":1}}],["我们有输入门",{"2":{"866":1}}],["我们有必要去了解物理学的思想如何影响人们对神经网络乃至自我的认知",{"2":{"506":1}}],["我们有如下几步可以选择",{"2":{"256":1}}],["我们有",{"2":{"125":1}}],["我们首先来对比一下",{"2":{"1602":1}}],["我们首先计算这些词在语料库中的出现频率",{"2":{"578":1}}],["我们首先从seq2seq模型维度来看看",{"2":{"414":1}}],["我们首先给出llama的架构",{"2":{"510":1}}],["我们首先给出训练代码的精简版",{"2":{"364":1}}],["我们首先给出将transformer",{"2":{"295":1}}],["我们首先用一个图来展示下注意力发展历史",{"2":{"280":1}}],["我们首先确定关键点",{"2":{"122":1}}],["我们首先看看embedding和独热编码的对比",{"2":{"694":1}}],["我们首先看看哈佛代码",{"2":{"472":1}}],["我们首先看看总体思路",{"2":{"267":1}}],["我们首先看看几个概念",{"2":{"137":1}}],["我们首先看看一些典型的思考和研究",{"2":{"123":1}}],["我们首先看看知识提取的步骤",{"2":{"122":1}}],["我们首先看看两种掩码在逻辑上应该用于哪个模块的哪种注意力",{"2":{"78":1}}],["我们都在运用",{"2":{"2108":1}}],["我们都有过找钥匙",{"2":{"2100":1}}],["我们都可以表示为二维情形的拼接",{"2":{"1343":1}}],["我们都可以将它们降到o",{"2":{"684":1}}],["我们都应该尝试找到一个尽可能接近我们任务的预训练模型",{"2":{"1313":1}}],["我们都不会从头训练模型",{"2":{"1313":1}}],["我们都系统地确保我们的选择仍然是正确的",{"2":{"1159":1}}],["我们都会总结该研究是否将冗余超参数调整得足够好",{"2":{"1145":1}}],["我们都会去对所有的可能输出",{"2":{"902":1}}],["我们都需要确保它均匀搜索目标超参数",{"2":{"1144":1}}],["我们都需要分别进行线性变换",{"2":{"9":1}}],["我们都知道cnn和rnn都是具备参数共享功能的",{"2":{"119":1}}],["我们使用",{"2":{"1728":1}}],["我们使用redix",{"2":{"986":1}}],["我们使用与原始transformer相同的模型架构",{"2":{"975":1}}],["我们使用块量化和不一致处理技术来减轻由于转换为fp8精度而导致的精度损失",{"2":{"973":1}}],["我们使用cuda实现了flashattention",{"2":{"940":1,"962":1}}],["我们使用如下的score函数来定义序列得分",{"2":{"903":1}}],["我们使用交叉熵作为损失函数",{"2":{"899":1}}],["我们使用上面生成的子词表",{"2":{"587":1}}],["我们使用一个例子来进行比对",{"2":{"339":1}}],["我们使用实例来进行算法解读",{"2":{"313":1}}],["我们使用llamamlp的代码来看看",{"2":{"110":1}}],["我们使用多头注意力就是同时关注字体和颜色等多方面信息",{"2":{"5":1}}],["我们直接来看预测过程中的",{"2":{"83":1}}],["我们接着深入到解码器中看看其参数",{"2":{"82":1}}],["我们接下来仔细分析",{"2":{"691":1}}],["我们接下来用minbpe来看看具体实现",{"2":{"589":1}}],["我们接下来先使用哈佛代码来看看如何构建",{"2":{"556":1}}],["我们接下来分析两种decoder",{"2":{"541":1}}],["我们接下来结合哈佛源码进行分析和学习",{"2":{"447":1}}],["我们接下来结合源语句和目标语句来一一介绍",{"2":{"382":1}}],["我们接下来看几个细节",{"2":{"301":1}}],["我们接下来看看使用llm生成embedding",{"2":{"728":1}}],["我们接下来看看论文作者们都做了哪些努力",{"2":{"724":1}}],["我们接下来看看嵌入模型以及embedding在演化进程的几个典型案例",{"2":{"711":1}}],["我们接下来看看相似性",{"2":{"691":1}}],["我们接下来看看词嵌入的两个关键点",{"2":{"688":1}}],["我们接下来看看和token相关的一些有特色或者较新的论文",{"2":{"609":1}}],["我们接下来看看交叉注意力的业务逻辑",{"2":{"536":1}}],["我们接下来看看哈佛源码中编码器的实现",{"2":{"521":1}}],["我们接下来看看其推导思路",{"2":{"498":1}}],["我们接下来看看如何利用反向传播中的梯度更新",{"2":{"485":1}}],["我们接下来看看如何调用",{"2":{"37":1}}],["我们接下来看看在nlp领域中",{"2":{"457":1}}],["我们接下来看看训练时候的并行机制",{"2":{"413":1}}],["我们接下来看看数据集",{"2":{"365":1}}],["我们接下来看看batchnorm的公式",{"2":{"313":1}}],["我们接下来看看几个经典的使用注意力机制对encoder",{"2":{"280":1}}],["我们接下来看看cnn方案和rnn方案如何应对这两个技术挑战",{"2":{"246":1}}],["我们接下来看看对ffn的优化与演进方案",{"2":{"149":1}}],["我们接下来看看这个领域内的几篇重要论文",{"2":{"124":1}}],["我们接下来看看带layernorm的掩码注意力会有什么性质",{"2":{"94":1}}],["我们接下来看看padding",{"2":{"60":1}}],["我们接下来通过几个问题来引导",{"2":{"294":1}}],["我们接下来从深度学习最常见的batchnorm入手开始介绍",{"2":{"311":1}}],["我们接下来从不同角度对加权求和与cnn",{"2":{"276":1}}],["我们接下来从几个角度来学习下模型如何在",{"2":{"121":1}}],["我们接下来就分析下",{"2":{"535":1}}],["我们接下来就看看训练中的各个要点具体如何实施",{"2":{"390":1}}],["我们接下来就进行分析如何加载batch",{"2":{"375":1}}],["我们接下来就从batchnorm入手进行分析",{"2":{"312":1}}],["我们接下来就以自注意力机制为例",{"2":{"272":1}}],["我们接下来就继续学习",{"2":{"262":1}}],["我们接下来对几个关键概念再进行分析",{"2":{"267":1}}],["我们接下来对上面过程中的一些重点进行详细梳理",{"2":{"173":1}}],["我们接下来介绍几个利用大模型生成embedding的方案",{"2":{"733":1}}],["我们接下来介绍几个特色案例",{"2":{"208":1}}],["我们接下来介绍几种定位模型行为的不同方法",{"2":{"476":1}}],["我们接下来介绍的注意力机制就可以在一定程度上解决上述问题",{"2":{"256":1}}],["我们接下来逐一分析",{"2":{"188":1}}],["我们接下来具体看看提取特征和加权求和这两个操作",{"2":{"167":1}}],["我们接下来以第一个编码器层为例",{"2":{"161":1}}],["我们接下来一步一步进行分析",{"2":{"160":1}}],["我们接下来一一进行分析",{"2":{"115":1}}],["我们接下来回头看看学习知识的过程中",{"2":{"146":1}}],["我们接下来详细分析下这些论点",{"2":{"126":1}}],["我们接下来再看看masked",{"2":{"71":1}}],["我们允许",{"2":{"78":1}}],["我们再回忆下一般的带绝对位置编码的attention",{"2":{"1339":1}}],["我们再次统计更新后的字符对频率",{"2":{"583":1}}],["我们再从代码逻辑进行梳理",{"2":{"538":1}}],["我们再从模型架构角度给出交互数据流图",{"2":{"85":1}}],["我们再把resnet和highway",{"2":{"301":1}}],["我们再仔细分析上下文或者说窗口机制",{"2":{"714":1}}],["我们再仔细看看",{"2":{"256":1}}],["我们再仔细梳理下数据流程",{"2":{"77":1}}],["我们再给出自己的定义",{"2":{"688":1}}],["我们再给出编码器",{"2":{"249":1}}],["我们再给出一个表格",{"2":{"77":1}}],["我们再用工业界的代码来进行学习",{"2":{"201":1}}],["我们再用三个权重矩阵来细化注意力公式如下",{"2":{"172":1}}],["我们再来结合模型结构图来简述推理阶段的计算流程",{"2":{"445":1}}],["我们再来探讨要做什么样的归一化",{"2":{"321":1}}],["我们再来对编码器和解码器中的注意力机制进行回顾和温习",{"2":{"200":1}}],["我们再来看看测试代码",{"2":{"315":1}}],["我们再来看看苏剑林大神对公式的精彩解读",{"2":{"176":1}}],["我们再来看看一个问题",{"2":{"116":1}}],["我们再来看看transformer的代码",{"2":{"76":1}}],["我们再来看ffn的几个作用",{"2":{"115":1}}],["我们再看看词表大小",{"2":{"576":1}}],["我们再看看解码过程中的历次推理",{"2":{"407":1}}],["我们再看看self",{"2":{"344":1}}],["我们再看看使用权重矩阵的优势所在",{"2":{"172":1}}],["我们再看看decoder的forward函数",{"2":{"83":1}}],["我们打印输出看看",{"2":{"74":1}}],["我们用转移分数表示一个标签向另一个标签转移的分数",{"2":{"1324":1}}],["我们用tiny",{"2":{"1302":1}}],["我们用tensorboard就行",{"2":{"1275":1}}],["我们用通俗的语言来看看一个良好的embedding方法应该具备何种特性",{"2":{"689":1}}],["我们用整个文本库中出现的词汇构建词典",{"2":{"679":1}}],["我们用淘宝搜索来类比",{"2":{"463":1}}],["我们用下图来把迭代流程做一下梳理",{"2":{"585":1}}],["我们用下图来进行分析",{"2":{"398":1}}],["我们用下面表格来看看上面两个问题",{"2":{"405":1}}],["我们用实例来进行演示",{"2":{"399":1}}],["我们用实例进行说明",{"2":{"122":1}}],["我们用一些简单的",{"2":{"2121":1}}],["我们用一个完整的图展示训练的总体数据流程如下",{"2":{"386":1}}],["我们用一个句子",{"2":{"263":1}}],["我们用一张大图把整个过程表示出来",{"2":{"16":1}}],["我们用每一个query对所有key都做一个对齐",{"2":{"165":1}}],["我们用个通俗例子来分析下",{"2":{"163":1}}],["我们用第一个解码器层来解释其操作序列如下",{"2":{"71":1}}],["我们以机器翻译为例",{"2":{"890":1}}],["我们以机器翻译任务为例",{"2":{"536":1}}],["我们以当前单词为中心",{"2":{"713":1}}],["我们以前面生成的词典对象为例",{"2":{"679":1}}],["我们以causal",{"2":{"475":1}}],["我们以encoder为例来说明",{"2":{"449":1}}],["我们以文本摘要为例",{"2":{"252":1}}],["我们以下面句子为例来进行分析",{"2":{"246":1}}],["我们以下图为基础来思考计算强度",{"2":{"17":1}}],["我们以",{"2":{"170":1,"267":1}}],["我们以目标句子为例",{"2":{"65":1}}],["我们把这些程序的片段称作临界区或临界段",{"2":{"1412":1}}],["我们把这些模块和代码中一一对应起来看",{"2":{"449":1}}],["我们把总的使用成本",{"2":{"1134":1}}],["我们把解码器的输入和输出都一起纳入",{"2":{"530":1}}],["我们把x",{"2":{"344":1}}],["我们把pre",{"2":{"334":1}}],["我们把padding的代码一起加入进来",{"2":{"64":1}}],["我们把注意力机制引入到seq2seq领域来详细看看它的计算流程",{"2":{"266":1}}],["我们把注意力机制看作是一种模糊寻址",{"2":{"164":1}}],["我们把上述问题点梳理如下",{"2":{"188":1}}],["我们把时间归一化成和为1的概率值",{"2":{"169":1}}],["我们把多个这样的特征融合操作并联起来",{"2":{"7":1}}],["我们的日常生活中到处都有算法的影子",{"2":{"2096":1}}],["我们的系统架构中",{"2":{"1478":1}}],["我们的系统中",{"2":{"1478":1}}],["我们的优化目标为",{"2":{"1386":1}}],["我们的优先事项将从学习更多优化经验转向产生一个最佳配置来启动或以其他方式使用",{"2":{"1153":1}}],["我们的一些优化器甚至具有更快的fused实现方式",{"2":{"1228":1}}],["我们的许多算法都有各种不同的实现方式",{"2":{"1228":1}}],["我们的偏好是linear",{"2":{"1172":1}}],["我们的训练时间越长",{"2":{"1157":1}}],["我们的主要目标是确保我们训练的时间足够长",{"2":{"1155":1}}],["我们的主要目标是通过有效地在多个主机之间分配长序列",{"2":{"975":1}}],["我们的探索工作应该已经揭示了最重要的要调整的超参数",{"2":{"1153":1}}],["我们的增量调优策略需要重复以下四个步骤",{"2":{"1139":1}}],["我们的最终目标是找到一种训练配置来最大化我们模型的性能",{"2":{"1139":1}}],["我们的指导原则是找到一个简单",{"2":{"1137":1}}],["我们的在此文档中一些建议也将需要更新以考虑新的结果和改进的工作流程",{"2":{"1127":1}}],["我们的重点是超参数调优的过程",{"2":{"1126":1}}],["我们的重计算由于减少了hbm访问次数而加速了反向传播过程",{"2":{"946":1,"966":1}}],["我们的方法在完成生成请求后不会丢弃kv",{"2":{"985":1}}],["我们的工作不同之处在于利用块并行transformer大幅降低内存成本",{"2":{"975":1}}],["我们的目标之一是不在反向传播过程中存储",{"2":{"946":1,"966":1}}],["我们的目标是找到最少的",{"2":{"1183":1}}],["我们的目标是更深入地理解问题",{"2":{"1140":1}}],["我们的目标是在固定截止日期",{"2":{"1139":1}}],["我们的目标是减少",{"2":{"942":1,"959":1}}],["我们的目标是计算注意力输出",{"2":{"942":1,"959":1}}],["我们的目标是创建一个单一的上下文向量作为注意力模型的输出",{"2":{"16":1}}],["我们的算法比标准注意力运行得更快",{"2":{"940":1,"962":1}}],["我们的实验表明",{"2":{"542":1}}],["我们的人脑中存在类似自然语言处理中常用的词向量",{"2":{"490":1}}],["我们的问题已经简化为降低包含初始注意力分数的乘积矩阵的方差",{"2":{"193":1}}],["我们的具体处理如下",{"2":{"145":1}}],["我们的解码输出应该只能依赖于",{"2":{"59":1,"934":1}}],["我们回忆下注意力计算公式如下",{"2":{"57":1}}],["我们来看下李航老师在",{"2":{"1322":1}}],["我们来看看解码过程中的张量形状变化",{"2":{"530":1}}],["我们来看看编码过程中的张量形状变化",{"2":{"520":1}}],["我们来看看为何研究人员做出这个选择",{"2":{"326":1}}],["我们来看看为什么要对齐",{"2":{"245":1}}],["我们来看看论文的推导",{"2":{"300":1}}],["我们来分析下这几个改进的必要性",{"2":{"682":1}}],["我们来分析哈佛代码",{"2":{"64":1}}],["我们来具体看看",{"2":{"581":1}}],["我们来具体分析下",{"2":{"54":1}}],["我们来概述下一段输入文本在转化为embedding之前都经历了什么",{"2":{"545":1}}],["我们来到了发生在掌握阶段的内在知识编辑",{"2":{"142":1}}],["我们来反推或者猜测一下transformer作者的设计思路大致为",{"2":{"11":1}}],["我们会优先完成那些耗时短",{"2":{"2106":1}}],["我们会优先选择最先进的贝叶斯优化方法",{"2":{"1144":1}}],["我们会把大目标分成多个小目标",{"2":{"2102":1}}],["我们会慢慢领悟其中的道理的",{"2":{"2054":1}}],["我们会一步步地理解它",{"2":{"1611":1}}],["我们会默认选择",{"2":{"1228":1}}],["我们会尝试默认选择当前设备上通常最快的实现方式",{"2":{"1228":1}}],["我们会使用github提供的pull",{"2":{"1198":1}}],["我们会定期进行大大小小的修改",{"2":{"1196":1}}],["我们会根据之前实验的结果来动态调整采样策略这导致我们不能随意的更换目标",{"2":{"1175":1}}],["我们会考虑以下因素",{"2":{"1165":1}}],["我们会自动为所有试验生成训练曲线",{"2":{"1151":1}}],["我们会为我们在实验中变化的所有超参数自动生成基本超参数轴图",{"2":{"1151":1}}],["我们会为每个batch",{"2":{"34":1}}],["我们会设计一个",{"2":{"1144":1}}],["我们会将",{"2":{"1143":1}}],["我们会将所有非目标超参数保留为冗余超参数",{"2":{"1143":1}}],["我们会有一个较大的幅度越过山谷",{"2":{"1036":1}}],["我们会发现这就是经常对",{"2":{"713":1}}],["我们会发现翻译结果中的语序和原来句子的语序并不相同",{"2":{"245":1}}],["我们会不断构建新词",{"2":{"580":1}}],["我们会生成嵌入并添加位置编码来传给那些解码器",{"2":{"515":1}}],["我们会逐步深入transformer的各个组成模块",{"2":{"473":1}}],["我们会对这几个问题做出回答",{"2":{"294":1}}],["我们会在后续章节对teacher",{"2":{"528":1}}],["我们会在后续文章中进行分析",{"2":{"523":1}}],["我们会在后续文章中对moe进行学习",{"2":{"150":1}}],["我们会在接下来详细论述注意力模型",{"2":{"263":1}}],["我们会分别花费2小时在",{"2":{"169":1}}],["我们会截取左边的内容",{"2":{"53":1}}],["我们需要优先把最大的数字放在高位",{"2":{"2124":1}}],["我们需要同时计算出",{"2":{"1441":1}}],["我们需要比较得精细一些",{"2":{"1340":1}}],["我们需要保存哪些内容以在之前基础上继续训练",{"2":{"1265":1}}],["我们需要更多的研究",{"2":{"1158":1}}],["我们需要更加重视模型的泛化能力",{"2":{"181":1}}],["我们需要理解导致我们结果中不同的变化的来源",{"2":{"1152":1}}],["我们需要分配有限的预算",{"2":{"1145":1}}],["我们需要设定一个候选集的大小beam",{"2":{"902":1}}],["我们需要等到",{"2":{"897":1}}],["我们需要在参考系中激活适当的位置",{"2":{"754":1}}],["我们需要从共享的知识和经验中汲取",{"2":{"715":1}}],["我们需要多方面特征才能完整的表示出来其特点",{"2":{"683":1}}],["我们需要拓展为用多维向量来表示语义信息",{"2":{"683":1}}],["我们需要",{"2":{"682":1}}],["我们需要进入第二次迭代",{"2":{"582":1}}],["我们需要对",{"2":{"1183":1}}],["我们需要对隐式复制的不同头部之间的梯度dk和dv进行求和",{"2":{"971":1}}],["我们需要对已完成的n个序列做一个抉择",{"2":{"904":1}}],["我们需要对标注数据进行多任务微调",{"2":{"542":1}}],["我们需要对词表大小的向量进行softmax",{"2":{"184":1}}],["我们需要用并行手段来保证在一次训练中输出一个序列中所有的单词的预测结果",{"2":{"406":1}}],["我们需要做的是在每个句子中分析其所在句子中上下文",{"2":{"318":1}}],["我们需要找一个方法让各个特征有相似的尺度",{"2":{"309":1}}],["我们需要找到一个适合的机制将这种运算进行落地",{"2":{"265":1}}],["我们需要找到一种解决方案",{"2":{"193":1}}],["我们需要关注源序列的所有词和目标序列中当前词的相关性大小",{"2":{"267":1}}],["我们需要模型能够将过去历史的抽象编码到其参数中",{"2":{"230":1}}],["我们需要把本地kv复制为2份",{"2":{"201":1}}],["我们需要先看过所有项才能准确地定义某一个项",{"2":{"168":1}}],["我们需要过滤掉这些",{"2":{"135":1}}],["我们需要探寻语言模型中概念形成",{"2":{"121":1}}],["我们需要产生一个mask",{"2":{"70":1}}],["我们需要针对整个输入序列进行注意力计算",{"2":{"57":1}}],["我们需要将其转换为输入的形状以方便后续的计算",{"2":{"36":1}}],["我们需要一种可以处理非结构化数据的方法",{"2":{"682":1}}],["我们需要一种可以允许模型在不同的子空间中进行信息选择的机制",{"2":{"4":1}}],["我们需要一个压缩",{"2":{"10":1}}],["我们就得到了一个较为标准的数据压缩形态",{"2":{"1374":1}}],["我们就得到了每个头计算结果组成的4维张量",{"2":{"36":1}}],["我们就说它是等变",{"2":{"772":1}}],["我们就去构造一个",{"2":{"713":1}}],["我们就希望具体看看商品内容",{"2":{"463":1}}],["我们就以行为为单位求均值和方差",{"2":{"319":1}}],["我们就应用上图标号1的方程",{"2":{"145":1}}],["我们就来做进一步的分析",{"2":{"121":1}}],["我们就可以进行反向传播",{"2":{"1442":1}}],["我们就可以增加训练时间并继续调整",{"2":{"1157":1}}],["我们就可以更有效地使用我们的资源来调整最有可能在生产环境中表现良好的模型",{"2":{"1157":1}}],["我们就可以继续评估实验为我们最初的目标提供的证据",{"2":{"1146":1}}],["我们就可以很容易的构建起tranformer了",{"2":{"449":1}}],["我们就可以复用代码",{"2":{"344":1}}],["我们就可以直接插入任何事实",{"2":{"145":1}}],["我们就可以一次性计算整个decoder输出序列的损失",{"2":{"70":1}}],["我们就可以让多组关注不同的上下文的",{"2":{"9":1}}],["完善下面的这个类",{"2":{"1696":1}}],["完美",{"2":{"1156":1}}],["完美拟合训练集",{"2":{"1155":1}}],["完美缩放适用于batch",{"2":{"1133":1}}],["完形填空问题等",{"2":{"880":1}}],["完形填空",{"2":{"670":1}}],["完整的",{"2":{"1619":1}}],["完整的类名或者别名",{"2":{"1485":1}}],["完整的transformer被表示如下图所示",{"2":{"499":1}}],["完整的目标序列",{"2":{"58":1}}],["完整流程包括多个阶段",{"2":{"431":1}}],["完成后",{"2":{"1645":1}}],["完成进程间的数据传输",{"2":{"1589":1}}],["完成持久化工作的代码块",{"2":{"1478":1}}],["完成前向传播后",{"2":{"1113":1}}],["完成第一步改造后",{"2":{"734":1}}],["完成",{"2":{"702":1}}],["完成彼此的联系",{"2":{"515":1}}],["完成输入和输出序列之间的对齐",{"2":{"444":1}}],["完成所有任务",{"2":{"222":1}}],["完成了",{"2":{"122":1}}],["完全可以跳过代码部分",{"2":{"2146":1}}],["完全两种不同的感受",{"2":{"2051":1}}],["完全封闭继承",{"0":{"1868":1}}],["完全展开得到",{"2":{"1340":1}}],["完全拟合",{"2":{"1155":2}}],["完全重新计算时约为30",{"2":{"976":1}}],["完全连接的层",{"2":{"912":1}}],["完全避免在嵌入空间中工作",{"2":{"692":1}}],["完全基于注意力的架构设计",{"2":{"617":1}}],["完全是",{"2":{"419":1}}],["完全是因为计算的需要",{"2":{"34":1}}],["完全没有相关性",{"2":{"176":1,"692":1}}],["完全忘记了这些知识",{"2":{"143":1}}],["假变真",{"2":{"1619":1}}],["假",{"2":{"1607":1}}],["假如随机取数据的时候",{"2":{"1374":1}}],["假如输入序列长度为n",{"2":{"883":1}}],["假如词表中有",{"2":{"681":1}}],["假如decoder当前的输入为",{"2":{"409":1}}],["假如要生成两个句子",{"2":{"378":1}}],["假如本地q数为2",{"2":{"201":1}}],["假如我们的wight",{"2":{"1000":1}}],["假如我们的训练任务得到了如下文本对",{"2":{"528":1}}],["假如我们要让模型将英语",{"2":{"245":1}}],["假如我们一共要花费11小时在了解郁达夫上",{"2":{"169":1}}],["假如我们在淘宝上进搜索",{"2":{"164":1,"463":1}}],["假如我们进行英译中",{"2":{"161":1}}],["假定现在词嵌入向量的维度是两维",{"2":{"1342":1}}],["假定",{"2":{"1342":1}}],["假定一个桌子",{"2":{"683":1}}],["假定要生产一件瓷器",{"2":{"437":1}}],["假定模型维度为d",{"2":{"161":1}}],["假定我们就是要进行并行计算",{"2":{"57":1}}],["假设任务的截止时间分别是",{"2":{"2135":1}}],["假设你有",{"2":{"2137":1}}],["假设你有多种交通方式可以选择去不同的地点",{"2":{"2129":1}}],["假设你有三件事情要做",{"2":{"2099":1}}],["假设你正在训练一个深度学习模型",{"2":{"1175":1}}],["假设深度学习要处理的信息是",{"2":{"1466":1}}],["假设一共有5个图片",{"2":{"1373":1}}],["假设一个神经元",{"2":{"1000":1}}],["假设当前位置的标签是",{"2":{"1324":1}}],["假设当前有个key=query的查询",{"2":{"164":1}}],["假设回顾性",{"2":{"1155":1}}],["假设随机变量",{"2":{"1004":1}}],["假设进行conv",{"2":{"1003":1}}],["假设是方块块",{"2":{"970":1}}],["假设beam",{"2":{"902":1}}],["假设b是batch",{"2":{"185":1}}],["假设选用了3000个常用汉字",{"2":{"709":1}}],["假设词汇量是v",{"2":{"899":1}}],["假设词汇表大小为",{"2":{"700":1}}],["假设词表大小是10000",{"2":{"455":1}}],["假设词表包含6个单词",{"2":{"398":1}}],["假设模型的输入是句子",{"2":{"698":1}}],["假设模型每一层都使用的是同一个词表矩阵",{"2":{"128":1}}],["假设高维空间是512维",{"2":{"674":1}}],["假设已经训练好了一个",{"2":{"623":1}}],["假设这个词典的大小为|v||v||v|",{"2":{"681":1}}],["假设这个5个样本都是人",{"2":{"313":1}}],["假设这些词出现的频率如下",{"2":{"578":1}}],["假设现在需要将",{"2":{"537":1}}],["假设embedding",{"2":{"460":1}}],["假设维度是512",{"2":{"455":1}}],["假设序列长度为8k",{"2":{"976":1}}],["假设序列的最大长度为4",{"2":{"316":1}}],["假设序号为",{"2":{"453":1}}],["假设我们要开发一个计算器程序",{"2":{"1916":1}}],["假设我们要将",{"2":{"428":1}}],["假设我们现在有个标签序列",{"2":{"1324":1}}],["假设我们想知道",{"2":{"1143":1}}],["假设我们所知的一切都存储在参考系中",{"2":{"754":1}}],["假设我们正在做机器翻译",{"2":{"704":1}}],["假设我们有一个语料库",{"2":{"577":1}}],["假设我们进行机器翻译工作",{"2":{"445":1}}],["假设我们的输入数量级很大",{"2":{"192":1}}],["假设此例中target",{"2":{"399":1}}],["假设target",{"2":{"399":1}}],["假设tgt是",{"2":{"380":1}}],["假设有两个batch",{"2":{"398":1}}],["假设计算之后得到out是",{"2":{"381":1}}],["假设原始目标语言句子是",{"2":{"381":1}}],["假设网络同时使用batch",{"2":{"361":1}}],["假设单个方格的长度是1",{"2":{"341":1}}],["假设它们各自的方差是",{"2":{"332":1}}],["假设feature",{"2":{"315":1}}],["假设ffn层是一个key",{"2":{"125":1}}],["假设svd将权重矩阵分解为五个组件",{"2":{"224":1}}],["假设qiqiq",{"2":{"189":1}}],["假设",{"2":{"184":1,"332":1,"399":1,"948":1,"978":1,"1345":1,"1611":1,"1671":1}}],["假设函数f表示一个深度网络",{"2":{"134":1}}],["假设输入的prompt是",{"2":{"122":1}}],["假设某个句子内容是",{"2":{"66":1,"74":1,"382":1}}],["假设某一行向量是",{"2":{"54":1}}],["假设为8",{"2":{"23":1}}],["直线光栅化",{"2":{"2046":1}}],["直线是所有图形中的基础",{"2":{"2013":1}}],["直觉上来讲效果会好一些",{"2":{"721":1}}],["直觉上这是有道理的",{"2":{"542":1}}],["直觉的解决方案是",{"2":{"55":1}}],["直至主机n",{"2":{"975":1}}],["直至结束",{"2":{"902":1}}],["直至输出end就结束",{"2":{"894":1}}],["直至n个encoderlayer都处理完毕",{"2":{"519":1}}],["直至得到整个句子的语义表示",{"2":{"287":1}}],["直路",{"2":{"332":1}}],["直通",{"2":{"304":1}}],["直观来看",{"2":{"990":1}}],["直观上看",{"2":{"694":1}}],["直观的理解就是每个",{"2":{"623":1}}],["直观解释注意力机制",{"2":{"513":1}}],["直观介绍",{"2":{"292":1}}],["直观地说",{"2":{"164":1,"756":1}}],["直到找到为止",{"2":{"2100":1}}],["直到大三",{"2":{"2054":1}}],["直到文件末尾或遇到",{"2":{"1632":1}}],["直到用户输入",{"2":{"1620":1}}],["直到上世纪八十年代中期",{"2":{"1443":1}}],["直到训练周期数达到预定义的里程碑",{"2":{"1237":1,"1238":1}}],["直到会议",{"2":{"1173":1}}],["直到性能提升似乎停滞为止",{"2":{"1173":1}}],["直到其中一个实验超过可用内存",{"2":{"1132":1}}],["直到这些祖先成为叶子节点并被淘汰",{"2":{"986":1}}],["直到输出停止符为止",{"2":{"886":1}}],["直到输出所有结果",{"2":{"515":1}}],["直到最佳观察点不再靠近边界",{"2":{"1147":1}}],["直到最近",{"2":{"729":1}}],["直到最后一层",{"2":{"126":1}}],["直到词表大小减少到设定值",{"2":{"602":1}}],["直到词汇量达到目标大小",{"2":{"595":1}}],["直到满足限定条件",{"2":{"601":1}}],["直到达到指定范围再clip",{"2":{"1340":1}}],["直到达到浮点极限",{"2":{"1155":1}}],["直到达到预设的词表规模或者满足迭代条件",{"2":{"584":1}}],["直到达到预设的合并次数或词汇表大小",{"2":{"576":1}}],["直到达到预先设置的token数限制或迭代限制",{"2":{"581":1}}],["直到达到我们预先设置的token数限制或迭代次数限制",{"2":{"581":1}}],["直到达到我们理想的词表规模",{"2":{"580":1}}],["直到生成最后的输出",{"2":{"1464":1}}],["直到生成",{"2":{"515":1}}],["直到llm输出结束流",{"2":{"431":1}}],["直到decoder输出",{"2":{"428":1}}],["直到遇到句末标记",{"2":{"427":1}}],["直到解码器产生一个",{"2":{"241":1}}],["直到新输出一个结束符号或者句子长度达到预设的最大阈值",{"2":{"239":1}}],["直接说",{"2":{"2051":1}}],["直接明了",{"2":{"2051":1}}],["直接法这里不再赘述",{"2":{"2016":1}}],["直接改变元素内容或在新位置生成改变后的内容",{"2":{"1758":1}}],["直接打印字符数组或字符指针",{"2":{"1704":1}}],["直接指向内存地址的指针",{"2":{"1695":1}}],["直接访问age",{"2":{"1662":1}}],["直接或间接地调用了自身",{"2":{"1646":1}}],["直接开始下一次循环的迭代",{"2":{"1631":1}}],["直接操作实参",{"2":{"1612":1}}],["直接操作高层级显式语义表示信息",{"2":{"627":1}}],["直接使用模板参数size",{"2":{"2063":1}}],["直接使用顺序文章",{"2":{"2049":1}}],["直接使用插件生成侧边栏",{"2":{"2043":1}}],["直接使用引用名",{"2":{"1612":1}}],["直接使用预训练好的词向量且对词向量不做改变",{"2":{"706":1}}],["直接传递参数即可",{"2":{"1485":1}}],["直接取",{"2":{"1485":1}}],["直接在返回值处构造对象",{"2":{"1931":1}}],["直接在需要使用函数的地方创建函数",{"2":{"1882":1}}],["直接在方法中传递参数",{"2":{"1485":1}}],["直接在attention计算时增加跳连连接并不会增加指数级的运算量",{"2":{"349":1}}],["直接计算内部神经元的误差信号是不可能的",{"2":{"1443":1}}],["直接理解同步",{"2":{"1408":1}}],["直接整理成文档内容",{"2":{"1404":1}}],["直接相连的两个邻居",{"2":{"1322":1}}],["直接生成特殊的tensor",{"0":{"1070":1}}],["直接",{"2":{"1011":1,"1887":1}}],["直接用索引来实现",{"2":{"832":1}}],["直接赋予了每个channel实际的类别意义",{"2":{"816":1}}],["直接剔除了全连接层中黑箱的特征",{"2":{"816":1}}],["直接对自注意力矩阵操作",{"2":{"766":1}}],["直接对这些参数使用较大的学习率可能不会导致模型的改进",{"2":{"333":1}}],["直接把偏置加qjqjq",{"2":{"766":1}}],["直接给",{"2":{"765":1}}],["直接截断在区间",{"2":{"763":1}}],["直接刻画相对位置信息",{"2":{"762":1}}],["直接考虑每个元素对应的两个token间的相对位置关系",{"2":{"757":1}}],["直接从嵌入矩阵中取出相应的嵌入向量",{"2":{"702":1}}],["直接针对余弦相似度训练模型",{"2":{"692":1}}],["直接输入完整字符会导致信息丢失或复杂性增加",{"2":{"547":1}}],["直接逻辑归因",{"2":{"479":1}}],["直接经过后续的处理",{"2":{"304":1}}],["直接跳过一个或多个层",{"2":{"300":1}}],["直接跳转原博客查看",{"2":{"80":1}}],["直接建立联系",{"2":{"274":1}}],["直接的线性归一化虽然可以满足第一个条件",{"2":{"180":1}}],["直接参与一些网络结构的计算",{"2":{"172":1}}],["直接将函数名作为参数传递即可",{"2":{"1645":1}}],["直接将模型加载到device上",{"2":{"1263":1}}],["直接将",{"2":{"128":1}}],["直接拼接成一个长向量",{"2":{"10":1}}],["句向量",{"2":{"718":1}}],["句首字符等特殊字符",{"2":{"548":1}}],["句首字符的特殊字符等",{"2":{"363":1}}],["句子翻译",{"2":{"850":1}}],["句子间隔和结束用",{"2":{"721":1}}],["句子或段落在不同语境下可能的含义",{"2":{"707":1,"715":1}}],["句子才是实现语言独立性的恰当的单元",{"2":{"628":1}}],["句子嵌入空间",{"2":{"627":1}}],["句子与句子之间的关系",{"2":{"566":1}}],["句子结束",{"2":{"557":1}}],["句子开始",{"2":{"557":1}}],["句子长度是7",{"2":{"450":1}}],["句子中每个字都会带上其它字的信息",{"2":{"445":1}}],["句子中的多个词特征构成句子特征",{"2":{"316":1}}],["句子中的每个词又被表达为一个定长向量",{"2":{"316":1}}],["句子中的其他词",{"2":{"167":1}}],["句子后面加上",{"2":{"384":2,"558":2}}],["句子",{"2":{"257":1,"510":1,"671":1}}],["句子最长的长度是32",{"2":{"379":1}}],["句子最长长度",{"2":{"66":1,"382":1}}],["句子最大填充长度",{"2":{"375":1}}],["句子最大长度",{"2":{"65":1,"372":1,"384":1,"558":1}}],["句子对的列表",{"2":{"65":1,"384":2,"558":2}}],["句子的长度是不同的",{"2":{"53":1}}],["句法",{"2":{"13":1}}],["看出",{"2":{"1651":1}}],["看宏观",{"2":{"1597":1}}],["看起来token粒度过小对于性能的影响已经不那么大了",{"2":{"626":1}}],["看看成才",{"2":{"2056":1}}],["看看许三多",{"2":{"2056":1}}],["看看第一句",{"2":{"2054":1}}],["看看计算对象转换的发展历程",{"2":{"677":1}}],["看看transformer中各个注意力模块的q",{"2":{"535":1}}],["看看其为何是一个好的结构",{"2":{"474":1}}],["看看其如何通过自己的优点来解决之前提到的问题",{"2":{"272":1}}],["看看架构图上的哪些模块可以作为积木",{"2":{"449":1}}],["看来collate",{"2":{"375":1}}],["看完也许能进一步了解batch",{"2":{"361":1}}],["看成求",{"2":{"340":1}}],["看不到其它样本",{"2":{"316":1}}],["看到别人做违反道德和法律的事",{"2":{"2054":1}}],["看到这里会发现",{"2":{"1342":1}}],["看到",{"2":{"307":1}}],["看见它们的树荫",{"2":{"246":1}}],["看过源码之后",{"2":{"200":1}}],["看图学",{"2":{"156":1,"233":1,"292":1}}],["看图学大模型",{"2":{"156":1,"280":1,"292":1}}],["看作一种",{"2":{"498":1}}],["看作是一个通用的可微计算机",{"2":{"294":1}}],["看作value",{"2":{"145":1}}],["看作key",{"2":{"145":1}}],["看",{"2":{"50":1}}],["避免错误",{"0":{"2105":1}}],["避免不必要的字符串拷贝",{"2":{"1929":1,"1933":1}}],["避免不必要的拷贝",{"2":{"1886":1}}],["避免破坏类的封装性",{"2":{"1793":1}}],["避免捕获不应该捕获的异常",{"2":{"1764":1}}],["避免忽略错误",{"2":{"1761":1}}],["避免忽略靠前的输入",{"2":{"272":1}}],["避免代码冗余",{"2":{"1729":1}}],["避免出现未定义的行为",{"2":{"1728":1}}],["避免将枚举类型的值与普通的整型值进行直接运算",{"2":{"1728":1}}],["避免使用难以理解的",{"2":{"1728":1}}],["避免手动内存管理",{"2":{"1715":1}}],["避免返回指向局部变量的指针",{"2":{"1706":1}}],["避免数据错误",{"2":{"1728":1}}],["避免数据冗余和二义性",{"2":{"1693":1}}],["避免数据丢失",{"2":{"1684":1}}],["避免函数重载冲突",{"2":{"1687":1}}],["避免悬挂指针",{"2":{"1672":1}}],["避免访问无效内存",{"2":{"1672":1}}],["避免内存越界的关键",{"2":{"1670":1}}],["避免内存泄漏的关键",{"2":{"1671":1}}],["避免内存泄漏",{"2":{"1666":1,"1695":1}}],["避免过深的递归调用",{"2":{"1646":1}}],["避免无限递归",{"2":{"1646":1}}],["避免无意义的计算",{"2":{"533":1}}],["避免重复编写相同的代码",{"2":{"1729":2}}],["避免重复释放",{"2":{"1672":1}}],["避免重复包含",{"2":{"1628":1}}],["避免重复代码",{"2":{"1628":1}}],["避免声明大量的独立变量",{"2":{"1623":1}}],["避免拷贝",{"2":{"1612":1}}],["避免野指针",{"2":{"1611":1}}],["避免冲突",{"2":{"1407":1}}],["避免一些耗时的检查",{"2":{"1227":1}}],["避免在头文件中定义变量或函数",{"2":{"1916":1}}],["避免在多轮评估中对验证集过度适应",{"2":{"1165":1}}],["避免在hbm和sram中移动p矩阵",{"2":{"180":1}}],["避免仅因历史原因而表现良好的不必要更改",{"2":{"1140":1}}],["避免梯度在传播过程中出现过大或过小的变化",{"2":{"838":1}}],["避免梯度消失或爆炸的问题",{"2":{"294":1}}],["避免分布外位置",{"2":{"759":1}}],["避免直接融合带来的复杂性",{"2":{"739":1}}],["避免像rnn那样更容易注意到靠后的内容",{"2":{"256":1}}],["避免引入过多概念",{"2":{"237":1}}],["避免了传统union可能导致的类型错误",{"2":{"1926":1}}],["避免了传统全连接层中高昂的矩阵运算",{"2":{"153":1}}],["避免了手动",{"2":{"1911":1}}],["避免了缓冲区溢出等安全问题",{"2":{"1715":1}}],["避免了",{"2":{"1713":1}}],["避免了内存泄漏和缓冲区溢出的风险",{"2":{"1713":1}}],["避免了数据共享带来的问题",{"2":{"1694":1}}],["避免了访问空指针",{"2":{"1672":1}}],["避免了静态分配内存可能造成的浪费或不足",{"2":{"1647":1}}],["避免了几乎所有的",{"2":{"1476":1}}],["避免了重复实现相同的功能",{"2":{"1849":1}}],["避免了重复的图构建和优化过程",{"2":{"1292":1}}],["避免了重复计算整个注意力矩阵",{"2":{"210":1}}],["避免了对每个block用",{"2":{"970":1}}],["避免了多次读写",{"2":{"970":1}}],["避免了模型结构的复杂化",{"2":{"542":1}}],["避免了训练与推理之间的行为差异",{"2":{"525":1}}],["避免了某些领域数据过多而导致的偏差",{"2":{"368":1}}],["避免了梯度消失和梯度爆炸的问题",{"2":{"331":1}}],["避免了因果语言建模中的累积求和操作",{"2":{"216":1}}],["避免填充符号对计算产生干扰",{"2":{"50":1}}],["避免偏差",{"0":{"52":1},"1":{"53":1,"54":1,"55":1},"2":{"49":1}}],["机械可解释性的几个主要流派如下",{"2":{"475":1}}],["机械可解释性",{"0":{"475":1},"1":{"476":1,"477":1,"478":1,"479":1,"480":1},"2":{"475":1}}],["机器语言是由二进制指令组成的",{"2":{"1604":1}}],["机器语言",{"2":{"1603":1}}],["机器的",{"2":{"1594":1}}],["机器的处理量将会很大",{"2":{"1370":1}}],["机器",{"2":{"1594":2}}],["机器如何认识文本",{"2":{"638":1}}],["机器翻译的思路十分简单",{"2":{"907":1}}],["机器翻译的发展历程",{"0":{"907":1}}],["机器翻译的输出和输入都不是等长的",{"2":{"241":1}}],["机器翻译",{"2":{"906":1}}],["机器翻译等任务",{"2":{"894":1,"906":1}}],["机器翻译会从概率角度对语言建模",{"2":{"238":1}}],["机器翻译其实就是文本生成",{"2":{"238":1}}],["机器阅读理解之推理网络",{"2":{"156":1}}],["机器学习开发的最终目标是最大化模型的效用",{"2":{"1138":1}}],["机器学习中的一个核心问题是设计不仅在训练数据上表现好",{"2":{"1011":1}}],["机器学习中的高维数据可以看作是向量空间中的点",{"2":{"689":1}}],["机器学习模型",{"2":{"885":1}}],["机器学习模型不使用非结构化数据",{"2":{"708":1}}],["机器学习将一切都表示为向量",{"2":{"696":1}}],["机器学习角度",{"0":{"481":1},"1":{"482":1,"483":1,"484":1,"485":1}}],["机器学习",{"2":{"474":1}}],["机器学习与大模型",{"2":{"233":2}}],["机器学习社区",{"2":{"156":1}}],["机器学习领域中",{"2":{"50":1}}],["机器之心",{"2":{"156":2,"233":4,"638":1,"740":1}}],["机制不足",{"0":{"1109":1}}],["机制使得系统可以更好地处理长上下文输入",{"2":{"977":1}}],["机制来扩展单个请求的处理能力",{"2":{"977":1}}],["机制来说",{"2":{"38":1}}],["机制和",{"2":{"740":1}}],["机制完成的",{"2":{"700":1}}],["机制拓展到",{"2":{"624":1}}],["机制可以灵活地构造任意的网络结构",{"2":{"620":1}}],["机制可以用在知识更新过程中",{"2":{"148":1}}],["机制相同",{"2":{"513":1}}],["机制的双阶段过程",{"2":{"485":1}}],["机制的区别",{"2":{"292":1}}],["机制把",{"2":{"420":1}}],["机制由bengio团队2015年在论文",{"2":{"257":1}}],["机制",{"0":{"891":1,"892":1},"1":{"892":1},"2":{"41":1,"211":1,"420":1,"1210":1}}],["机制分割在",{"2":{"12":1,"33":1}}],["策略执行系统开发",{"2":{"1946":1}}],["策略全解",{"0":{"1232":1}}],["策略",{"0":{"90":1},"2":{"49":1,"90":1,"595":1,"986":1,"2118":1}}],["训练出了类似",{"2":{"1316":1}}],["训练编码器",{"2":{"1315":1}}],["训练这些",{"2":{"1312":1}}],["训练周期数",{"2":{"1242":1}}],["训练完成",{"2":{"1218":1}}],["训练循环",{"2":{"1218":1,"1239":1}}],["训练脚本",{"2":{"1218":1}}],["训练策略",{"2":{"1202":1}}],["训练步骤的数量可能也需要进行调整",{"2":{"1186":1}}],["训练步数",{"2":{"1137":1}}],["训练中期突然出现的梯度尖峰",{"2":{"1184":1}}],["训练中期突然出现的不稳定",{"2":{"1179":1}}],["训练中的保存和加载",{"0":{"1265":1},"1":{"1266":1,"1267":1},"2":{"668":1}}],["训练早期中存在的不稳定",{"2":{"1179":1}}],["训练次数试验次数小于10",{"2":{"1174":1}}],["训练所需的配置和运行命令",{"2":{"1167":1}}],["训练误差或某些替代评估指标来找到最佳试验",{"2":{"1175":1}}],["训练误差会无限地改善",{"2":{"1157":1}}],["训练误差在训练过程中增加",{"2":{"1149":1}}],["训练集",{"2":{"1155":1}}],["训练集所需的step数",{"2":{"1155":1}}],["训练集和验证集的性能在最后的训练步骤之前很久就饱和了吗",{"2":{"1149":1}}],["训练受限于我们愿意等待的时间",{"2":{"1154":1}}],["训练损失或比平均值差很多标准差的训练误差的点",{"2":{"1153":1}}],["训练程序方差",{"2":{"1152":1}}],["训练结束时试验是否仍能改进",{"2":{"1149":1}}],["训练或验证误差是否存在较高的步与步之间的方差",{"2":{"1149":1}}],["训练更长的时间并没有多大帮助",{"2":{"1154":1}}],["训练更多的步数可以提高性能并使超参数调整更容易",{"2":{"1137":1}}],["训练更大的模型大大加快了速度",{"2":{"621":1}}],["训练吞吐量也应该加倍",{"2":{"1132":1}}],["训练吞吐量",{"2":{"1132":2}}],["训练好了之后",{"2":{"898":1}}],["训练起来非常艰难",{"2":{"895":1}}],["训练速度稍快一些",{"2":{"874":1}}],["训练速度变慢",{"2":{"576":1}}],["训练式",{"0":{"749":1},"2":{"741":1}}],["训练将向量推离不共享其上下文的随机单词的向量",{"2":{"709":1}}],["训练流程图",{"0":{"1350":1}}],["训练流程的最佳学习率是多少",{"2":{"1143":1}}],["训练流程",{"0":{"709":1}}],["训练流程是之所以是一步操作",{"2":{"412":1}}],["训练大概念模型需要基于句子嵌入空间的解码器和编码器来训练一个新的嵌入空间",{"2":{"629":1}}],["训练tokenizer与训练模型不同",{"2":{"576":1}}],["训练友好",{"2":{"563":1}}],["训练不足",{"2":{"562":2}}],["训练成本低",{"2":{"542":1}}],["训练数据集",{"2":{"1215":1}}],["训练数据",{"2":{"1152":1}}],["训练数据的shuffles",{"2":{"1152":1}}],["训练数据和词表大小",{"2":{"561":1}}],["训练数据效率高",{"2":{"542":1}}],["训练数据由两部分组成",{"2":{"391":1}}],["训练一个深度transformer模型",{"2":{"508":1}}],["训练和预测的时间效率将大为提高",{"2":{"1370":1}}],["训练和预测的执行过程存在不同",{"2":{"426":1}}],["训练和推理时dropout",{"2":{"1017":1}}],["训练和推理时这个算子表现有何不同",{"2":{"835":1}}],["训练和推理的最大不同之处在于每个时间步的输入区别",{"2":{"81":1}}],["训练和分词都和某种语言特性无关",{"2":{"563":1}}],["训练方式",{"0":{"422":1}}],["训练方法难以同时实现这两个特性",{"2":{"222":1}}],["训练过程也可以更快收敛",{"2":{"896":1}}],["训练过程中断",{"2":{"1265":1}}],["训练过程中的摆动",{"2":{"508":1}}],["训练过程中每个时间步的输入是全部目标序列",{"2":{"81":1,"525":1}}],["训练过程就是简单地在上述推理过程的基础之上加上对每次推理预测的新元素的监督即可",{"2":{"427":1}}],["训练过程会从teacher",{"2":{"411":1}}],["训练效率大幅提升",{"2":{"411":1}}],["训练效果下降",{"2":{"296":1}}],["训练代码会先把目标句子扩展为",{"2":{"408":1}}],["训练得到的模型也趋向于出现多样性的数据",{"2":{"399":1}}],["训练会依据分类结果来计算损失",{"2":{"397":1}}],["训练会变得极不稳定",{"2":{"239":1}}],["训练使用",{"0":{"385":1}}],["训练时噪音的严重程度是和时间步有关的",{"2":{"1350":1}}],["训练时流程",{"0":{"895":1},"1":{"896":1,"897":1}}],["训练时候显存消耗的主要项",{"2":{"666":1}}],["训练时候有目标语言句子",{"2":{"380":1}}],["训练时",{"2":{"472":1}}],["训练时间更长可能会略微减少训练误差",{"2":{"1154":1}}],["训练时间总是",{"2":{"1154":1}}],["训练时间的大幅加速可能会在过程的早期非常有利",{"2":{"1134":1}}],["训练时间",{"2":{"1133":1}}],["训练时间变长",{"2":{"400":1}}],["训练时间都会丰富大模型在某一问题域的信息量",{"2":{"363":1}}],["训练时间过长",{"2":{"254":1}}],["训练阶段具体如下图所示",{"2":{"718":1}}],["训练阶段时时用真值拼接成下一次输入",{"2":{"426":1}}],["训练阶段",{"2":{"380":1}}],["训练目标则是预测出被遮盖掉的文本",{"2":{"1317":1}}],["训练目标",{"2":{"367":1}}],["训练语料",{"2":{"367":1}}],["训练稳定性的标准化方法",{"2":{"347":1}}],["训练深度网络的困难部分是由于模型的不可识别性导致的奇异性",{"2":{"305":1}}],["训练长期记忆的关键思路是将训练视为在线学习问题",{"2":{"230":1}}],["训练超参数等都无关",{"2":{"147":1}}],["训练整个模型需要大量的计算资源",{"2":{"142":1}}],["训练的效果也不是太好",{"2":{"576":1}}],["训练的稳定性得到大大增强",{"2":{"411":1}}],["训练的任务分类器",{"2":{"225":1}}],["训练的",{"2":{"95":1}}],["训练是等价的",{"2":{"89":1}}],["训练",{"0":{"82":1,"388":1,"390":1,"528":1,"705":1,"718":1,"726":1},"1":{"389":1,"390":1,"391":2,"392":2,"393":2,"394":2,"395":2,"396":2,"397":2,"398":2,"399":2,"400":2,"401":2,"402":2,"403":2,"404":2,"405":2,"406":2,"407":2,"408":2,"409":2,"410":2,"411":2,"412":2,"413":2,"414":2,"415":2,"416":2,"417":2,"418":2,"419":2,"420":2,"421":2,"422":2,"423":2,"424":2,"425":1,"426":1,"427":1,"428":1,"429":1,"706":1,"707":1,"708":1,"709":1},"2":{"49":1,"426":1,"427":1,"861":1}}],["交易系统架构师",{"2":{"1948":1}}],["交通信号灯的三种状态",{"2":{"1728":1}}],["交通工具",{"2":{"1664":1}}],["交互的内部循环中出现了挑战",{"2":{"975":1}}],["交互的灵活性",{"2":{"624":1}}],["交互",{"0":{"615":1},"2":{"689":1}}],["交叉熵损失",{"2":{"429":1}}],["交叉熵损失函数能够量化模型输出的概率分布与真实标签之间的差异",{"2":{"180":1}}],["交叉熵",{"0":{"398":1}}],["交叉自注意力中用到的掩码矩阵",{"2":{"84":1}}],["交叉注意力的计算会使用tgt",{"2":{"538":1}}],["交叉注意力的输入来源有两处",{"2":{"525":1}}],["交叉注意力机制使模型能够在生成输出序列的每一步都考虑到输入序列的全部信息",{"2":{"538":1}}],["交叉注意力机制使用src",{"2":{"82":1}}],["交叉注意力机制可以让解码器和编码器进行交互",{"2":{"536":1}}],["交叉注意力计算的是每个源序列单词与每个目标序列单词之间的相互作用或者相似度",{"2":{"536":1}}],["交叉注意力深入",{"0":{"534":1},"1":{"535":1,"536":1,"537":1,"538":1}}],["交叉注意力模块",{"2":{"533":1}}],["交叉注意力是编码器和解码器的第二个不同之处",{"2":{"525":1}}],["交叉注意力是序列到序列模式",{"2":{"444":1}}],["交叉注意力和ffn",{"2":{"525":1}}],["交叉注意力层则负责基于编码器的所有输出隐向量来进一步建模每个解码器的输出向量",{"2":{"515":1}}],["交叉注意力层位于解码器中",{"2":{"444":1}}],["交叉注意力层",{"0":{"444":1},"2":{"444":1}}],["交叉注意力主要用于处理两个不同序列之间的关系",{"2":{"439":1}}],["交叉注意力中的q来自解码器",{"2":{"84":1}}],["交叉注意力",{"0":{"72":1},"2":{"49":1,"525":1,"535":2}}],["交换后",{"2":{"1650":3}}],["交换前",{"2":{"1650":4}}],["交换数据",{"2":{"1568":1}}],["交换kv还可以利用mqa",{"2":{"976":1}}],["交换tensor的两个轴并返回",{"2":{"821":1}}],["交换的目的是方便后续矩阵乘法和不同头部的注意力计算",{"2":{"36":1}}],["交换",{"2":{"36":2}}],["需注意的是可训练的weight",{"2":{"808":1}}],["需求",{"0":{"51":1,"677":1},"1":{"52":1,"53":1,"54":1,"55":1,"56":1,"57":1,"58":1,"59":1},"2":{"49":1,"1485":1,"1486":1,"1487":1,"1488":1,"1657":1}}],["需要提前创建好doccano服务才行",{"2":{"2066":1}}],["需要提前建立一个全局通信组",{"2":{"977":1}}],["需要为",{"2":{"1906":1}}],["需要为此构建一个n×n",{"2":{"210":1}}],["需要考虑菱形继承引发的问题",{"2":{"1869":1}}],["需要我们自定义数据类型来更好地描述和管理",{"2":{"1728":1}}],["需要程序员手动管理",{"2":{"1714":1}}],["需要包含这个头文件来处理异常",{"2":{"1713":1}}],["需要包含此头文件",{"2":{"1673":1}}],["需要再次使用",{"2":{"1701":1}}],["需要自己去领悟",{"2":{"2054":1}}],["需要自己在该方法内做转化",{"2":{"1083":1}}],["需要自定义拷贝构造函数来实现深拷贝",{"2":{"1694":1}}],["需要强调的是",{"2":{"1678":1}}],["需要显式指定返回类型或使用",{"2":{"1905":1}}],["需要显式地传递数组的大小信息",{"2":{"1667":1}}],["需要显式管理通信和数据分布",{"2":{"1579":1}}],["需要避免",{"2":{"1660":1}}],["需要复制",{"2":{"1650":1}}],["需要将一些数据传递给函数进行处理",{"2":{"1650":1}}],["需要将长度不足的句子用无意义的特殊字符",{"2":{"376":1}}],["需要动态分配内存",{"2":{"1647":1}}],["需要确保每栋房屋都被妥善处理",{"2":{"1647":1}}],["需要额外的作用域",{"2":{"1922":1}}],["需要额外传递数组的大小信息",{"2":{"1634":1}}],["需要额外显存",{"2":{"396":2}}],["需要先转换为具体的指针类型",{"2":{"1633":1}}],["需要先将其强制转换为具体的指针类型才能访问其指向的值",{"2":{"1611":1}}],["需要先将代码编译成机器语言才能执行",{"2":{"1604":1}}],["需要使用",{"2":{"1621":1,"1647":1}}],["需要修改实参",{"2":{"1612":1,"1650":2}}],["需要通过地址来找到值",{"2":{"1612":1}}],["需要一个",{"2":{"1605":1}}],["需要手动使用",{"2":{"1714":1}}],["需要手动释放内存",{"2":{"1695":1}}],["需要手动释放",{"2":{"1668":1}}],["需要手动管理内存",{"2":{"1602":1,"1695":1}}],["需要手动配置和调整",{"2":{"985":1}}],["需要传递参数类型",{"2":{"1485":1}}],["需要持久化来缓存到外存",{"2":{"1477":1}}],["需要事先定义一个net的实例",{"2":{"1267":1}}],["需要有",{"2":{"1206":1}}],["需要多少多少钱",{"2":{"2051":1}}],["需要多少次试验才能通过quasi",{"0":{"1177":1}}],["需要多态支持",{"2":{"1629":1}}],["需要多次重新评估函数",{"2":{"1223":1}}],["需要多方考虑",{"2":{"3":1}}],["需要付出大量的努力和猜测",{"2":{"1127":1}}],["需要计算梯度的",{"2":{"1106":1}}],["需要知道上一层的输出个数吗",{"2":{"1003":1}}],["需要知识整合和感悟",{"2":{"402":1}}],["需要高斯分布如下",{"2":{"1000":1}}],["需要每层两次昂贵的基于rdma的全局归约操作",{"2":{"977":1}}],["需要与同一序列中所有标记的键值",{"2":{"976":1}}],["需要反复往shared",{"2":{"967":1}}],["需要反复读写",{"2":{"967":1}}],["需要投入极大的成本",{"2":{"861":1}}],["需要根据具体任务",{"2":{"838":1}}],["需要根据具体任务进行调整",{"2":{"392":1}}],["需要满足什么规则",{"2":{"827":1}}],["需要从输入之后的模块入手",{"2":{"757":1}}],["需要更多的维度",{"2":{"746":1,"766":1}}],["需要设置超参数α来控制rw的贡献",{"2":{"739":1}}],["需要双向上下文信息的任务",{"2":{"720":1}}],["需要极高的维度才能够囊括所有词义的不同维度",{"2":{"712":1}}],["需要能够以理解每个单词",{"2":{"707":1}}],["需要放大",{"2":{"701":1}}],["需要维护数据之间的深层语义关系和相似性",{"2":{"676":1}}],["需要维护一个类似字典的模块或者功能",{"2":{"549":1}}],["需要",{"2":{"676":1}}],["需要原来net的签名",{"2":{"668":1}}],["需要综合考虑",{"2":{"568":1}}],["需要综合考虑模型参数",{"2":{"561":1}}],["需要把文本转换为模型可以理解的表征",{"2":{"545":1}}],["需要把该向量分类到词表中的正确token",{"2":{"473":1}}],["需要结合掩码并且整体右移",{"2":{"528":1}}],["需要结合掩码机制",{"2":{"525":1}}],["需要怎么和其它组件相配合",{"2":{"524":1}}],["需要依据",{"2":{"516":1}}],["需要保证",{"2":{"402":1}}],["需要生成下一个单词",{"2":{"378":1}}],["需要仔细初始化等",{"2":{"331":1}}],["需要仔细研究其内部结构",{"2":{"221":1}}],["需要第一次通过输入来计算分母",{"2":{"327":1}}],["需要特殊的支持或芯片外计算",{"2":{"327":1}}],["需要热身",{"0":{"333":1},"2":{"293":1}}],["需要编码器的上下文包括更多的有效信息",{"2":{"284":1}}],["需要存储大量的中间结果",{"2":{"279":1}}],["需要赋予更多的权重",{"2":{"277":1}}],["需要注意内存管理",{"2":{"1706":2}}],["需要注意的重要一点是",{"2":{"1113":1}}],["需要注意的是这些向量可能是整个网络的输入",{"2":{"920":1}}],["需要注意的是",{"2":{"25":1,"838":1,"898":1,"1025":1}}],["需要注意两个细节",{"2":{"381":1}}],["需要注意",{"2":{"267":1}}],["需要解决几个挑战",{"2":{"222":1}}],["需要进行累加求和操作导致无法矩阵运算的情况",{"2":{"216":1}}],["需要增加维度来捕捉更复杂的模式",{"2":{"193":1}}],["需要分配更多的注意力来重点看",{"2":{"169":1}}],["需要在每个可能出错的地方检查返回值",{"2":{"1761":1}}],["需要在构造函数初始化列表中显式调用虚基类的构造函数",{"2":{"1662":1}}],["需要在输入嵌入层中加入位置编码",{"2":{"518":1}}],["需要在参数空间进行数百甚至上千次编辑",{"2":{"143":1}}],["需要在相关背景下接触新知识",{"2":{"141":1}}],["需要在其它层捕捉",{"2":{"20":1}}],["需要探寻",{"2":{"121":1}}],["需要对数据进行归一化处理",{"2":{"692":1}}],["需要对数据进行排序",{"2":{"90":1}}],["需要对输入进行三次传递",{"2":{"327":1}}],["需要对注意力分数进行掩码操作",{"2":{"50":1}}],["需要和编码器中的key",{"2":{"84":1}}],["需要让模型适应新的双向注意力机制",{"2":{"734":1}}],["需要让隐状态和时序无关",{"2":{"256":1}}],["需要让前面的token不能观察到后面token的信息",{"2":{"81":1}}],["需要让其有一定的头的基数",{"2":{"13":1}}],["需要借助掩码把后面单词的信息隐藏掉",{"2":{"81":1}}],["写出更现代",{"2":{"1932":1}}],["写出更高效",{"2":{"1913":1}}],["写出到",{"2":{"944":1}}],["写文件",{"2":{"1820":1,"1838":1}}],["写成",{"2":{"1729":1}}],["写成矩阵形式参考dot",{"2":{"929":1}}],["写成矩阵形式",{"2":{"923":1}}],["写成矩阵的形式",{"0":{"923":1}}],["写一段代码",{"2":{"1611":1}}],["写权限",{"2":{"1512":1}}],["写入学生成绩和等级",{"2":{"1825":1,"1843":1}}],["写入一些数据到文件",{"2":{"1821":1,"1839":1}}],["写入文件",{"2":{"1820":1,"1838":1,"1902":1,"1930":1}}],["写入数据",{"2":{"1815":1,"1833":1}}],["写入内容",{"2":{"1510":1}}],["写入当前位置的残差流",{"2":{"45":1}}],["写时复制机制",{"2":{"983":1}}],["写代码",{"2":{"627":1}}],["写的深入且具体",{"2":{"48":1}}],["转义字符用于在字符串中表示一些特殊的字符",{"2":{"1616":1}}],["转到cuda",{"2":{"1214":1}}],["转移所有权",{"2":{"1911":1}}],["转移概率就越大",{"2":{"1324":1}}],["转移的概率会比较大",{"2":{"1324":1}}],["转移分数",{"0":{"1324":1},"2":{"1328":1}}],["转移",{"2":{"1158":1}}],["转为复数域",{"2":{"1345":1}}],["转为char",{"2":{"1087":1}}],["转为bfloat16",{"2":{"1087":1}}],["转为压缩稀疏行布局",{"2":{"1086":1}}],["转为压缩稀疏列布局",{"2":{"1086":1}}],["转为块稀疏行布局",{"2":{"1086":1}}],["转为块稀疏列布局",{"2":{"1086":1}}],["转为sparse",{"2":{"1086":1}}],["转为",{"2":{"1086":2}}],["转为整型",{"2":{"1085":1}}],["转化",{"0":{"1076":1}}],["转化为更容易理解的形式",{"2":{"770":1}}],["转而专注于那些可能与",{"2":{"691":1}}],["转而采用更高级别的",{"2":{"627":1}}],["转置卷积",{"0":{"779":1}}],["转置",{"2":{"658":1}}],["转变为一种可能保持权重概率性质的新形式",{"2":{"209":1}}],["转载请标注来源",{"2":{"48":1}}],["转换功能",{"2":{"2062":1}}],["转换构造函数",{"2":{"1685":1}}],["转换后的检查点接着使用相同的预训练方法进行预训练",{"2":{"938":1,"954":1}}],["转换到高维稠密的向量空间",{"2":{"676":1}}],["转换成一个个独立的最小语义单元",{"2":{"545":1}}],["转换",{"0":{"473":1},"2":{"516":1}}],["转换为无符号长整数",{"2":{"2062":1}}],["转换为字符串",{"2":{"2062":1}}],["转换为大写",{"2":{"1914":1}}],["转换为双向注意力并让模型适应新结构",{"2":{"732":1}}],["转换为总和为",{"2":{"473":1}}],["转换为对下一个词的预测",{"2":{"471":1}}],["转换为tgt为",{"2":{"381":1}}],["转换为归一化",{"2":{"357":1}}],["转换为有效的概率分布",{"2":{"180":1}}],["转换为",{"2":{"36":1,"1629":2,"1933":1}}],["转换和融合的手段",{"2":{"10":1}}],["请合理设计构造函数初始化类成员",{"2":{"1874":1}}],["请设计一个简单的类层次结构来表示三种不同类型的账户",{"2":{"1873":1}}],["请重新输入一个整数",{"2":{"1814":1,"1832":1}}],["请重新运行程序",{"2":{"1608":1}}],["请你选择一个容器类型作为数据结构来设计一",{"2":{"1808":1}}],["请你完成以下操作",{"2":{"1759":1}}],["请你使用指针的算术运算对数组中的元素进行操作",{"2":{"1634":1}}],["请自行选择内部数据结构设计一",{"2":{"1726":1}}],["请根据",{"2":{"1710":1}}],["请实现一个简单的",{"2":{"1689":1}}],["请求内存时",{"2":{"1648":1}}],["请输入另一行文本",{"2":{"1813":1,"1831":1}}],["请输入一行文本",{"2":{"1813":1,"1831":1}}],["请输入一个整数",{"2":{"1814":1,"1832":1}}],["请输入一个年份",{"2":{"1729":5}}],["请输入一个字符串",{"2":{"1624":1}}],["请输入一个字符",{"2":{"1619":1,"1813":1,"1831":1}}],["请输入您的年龄",{"2":{"1811":1,"1829":1}}],["请输入你的年龄",{"2":{"1673":1}}],["请输入你的姓名",{"2":{"1673":1}}],["请输入摄氏温度",{"2":{"1608":1}}],["请输入华氏温度",{"2":{"1608":3}}],["请选择转换类型",{"2":{"1608":2}}],["请分析上图中参数的类别",{"2":{"1441":1}}],["请将",{"2":{"1242":1}}],["请查看",{"2":{"1198":1}}],["请查看之前的章节",{"2":{"1184":1}}],["请前往",{"2":{"1197":1}}],["请不要在未通过问题跟踪系统与作者协调的情况下提交pull",{"2":{"1196":1}}],["请关注我们的仓库",{"2":{"1196":1}}],["请在",{"2":{"1196":1,"1873":1}}],["请给我们",{"2":{"1196":1}}],["请尽量确保其可重现性",{"2":{"1173":1}}],["请暂停训练",{"2":{"1173":1}}],["请偏向于选择延长训练时间",{"2":{"1155":1}}],["请修改实验并重新运行",{"2":{"1146":1}}],["请修复瓶颈或使用较小的batch",{"2":{"1132":1}}],["请尝试找到一篇尽可能接近手头问题的相关论文",{"2":{"1129":1}}],["请确保您满足以下假设",{"2":{"1128":1}}],["请检查您是否在退出推断模式后的自动求导记录的计算中使用了在推断模式下创建的张量",{"2":{"1121":1}}],["请启用推断模式",{"2":{"1121":1}}],["请使用模最小的超梯度",{"2":{"1115":1}}],["请使用模最小的子梯度",{"2":{"1115":1}}],["请使用它",{"2":{"1115":1}}],["请参阅具有未知约束的贝叶斯优化是处理此问题的绝佳方法",{"2":{"1153":1}}],["请参阅batchnorm的实现细节",{"2":{"1136":1}}],["请参阅推断模式",{"2":{"1121":1}}],["请参阅",{"2":{"1114":1,"1121":1,"1168":1,"1214":1}}],["请记住",{"2":{"868":1}}],["请看原文章",{"2":{"672":1}}],["请联系删除",{"2":{"47":1,"95":1,"156":1,"233":1,"361":1,"387":1,"429":1,"513":1,"543":1,"638":1,"740":1,"768":1}}],["请注意效果",{"2":{"1133":1}}],["请注意",{"2":{"20":1,"343":1,"690":1,"1115":1,"1130":1,"1174":1,"1177":1,"1214":1,"1228":1,"1235":1,"1236":1,"1237":1,"1238":1,"1242":1}}],["jetbrains",{"2":{"1605":3}}],["jeff",{"2":{"754":1}}],["jmp",{"2":{"1603":1}}],["jmlr",{"2":{"429":1}}],["jsp",{"2":{"1493":1}}],["json",{"2":{"1284":1,"1989":3,"1990":3}}],["jdbc就是一种持久化机制",{"2":{"1477":1}}],["jdbc",{"2":{"1474":1,"1476":1,"1481":3}}],["jdk",{"0":{"1435":1},"2":{"1435":1,"1474":1}}],["jre",{"0":{"1435":1},"2":{"1435":2}}],["jri",{"2":{"759":1}}],["jvm",{"0":{"1435":1},"2":{"1435":2,"1938":1}}],["jv=pv",{"2":{"1339":1}}],["jv",{"2":{"1339":1}}],["jvr",{"2":{"1339":2}}],["jvr^k",{"2":{"759":1}}],["jvj=∑jai",{"2":{"1339":1}}],["j​v​​=p​v​​",{"2":{"1339":1}}],["j​v​​",{"2":{"1339":3}}],["j​​",{"2":{"1339":2,"1340":2}}],["j​​v​j​​=∑​j​​a​i",{"2":{"1339":1}}],["j​​=softmax",{"2":{"1339":1}}],["j​k​​=p​k​​",{"2":{"1339":1}}],["j​k​​",{"2":{"1339":3}}],["jpg",{"2":{"1253":2}}],["j~",{"2":{"1014":1}}],["j+1",{"2":{"1000":2}}],["j无关的标量",{"2":{"766":1}}],["j|",{"2":{"765":1}}],["j|i",{"2":{"176":1}}],["j^t",{"2":{"765":2}}],["j换成了相对位置编码ri−jri−jr",{"2":{"760":1}}],["jk=pk",{"2":{"1339":1}}],["jkr",{"2":{"1339":1}}],["jk",{"2":{"759":1,"1339":2}}],["j是有约束条件的",{"2":{"759":1}}],["j加在k和v上面",{"2":{"759":1}}],["j进行建模",{"2":{"745":1}}],["jit",{"0":{"1291":1,"1292":1,"1296":1,"1297":1},"2":{"558":1,"1269":1,"1270":1,"1291":1,"1292":2,"1296":2,"1297":2}}],["jia",{"2":{"361":1}}],["jx",{"2":{"478":2,"1340":1}}],["justin",{"2":{"1124":1,"1195":1}}],["just",{"2":{"591":3,"592":1,"1254":1}}],["junit",{"2":{"1474":1,"1481":1}}],["junrong",{"2":{"387":1}}],["junge",{"2":{"370":1,"557":1}}],["junlong",{"2":{"361":1}}],["juliuszh",{"2":{"361":1}}],["j𝑎𝑖",{"2":{"194":1}}],["jar",{"2":{"1481":1}}],["java后端",{"2":{"2001":1}}],["javamap",{"2":{"1485":1}}],["javauser",{"2":{"1485":1}}],["javaimport",{"2":{"1481":2}}],["java对象",{"2":{"1476":1}}],["javapublic",{"2":{"1436":1,"1481":2,"1485":1}}],["java",{"0":{"1436":1,"1490":1,"1491":1,"1493":1},"1":{"1491":1,"1492":1,"1493":1,"1494":1,"1495":1,"1496":1,"1497":1,"1498":1,"1499":1,"1500":1,"1501":1},"2":{"1433":3,"1435":9,"1474":1,"1476":2,"1481":4,"1482":1,"1485":1,"1486":2,"1487":2,"1488":2,"1490":1,"1491":1,"1501":2,"1938":1}}],["javase",{"0":{"1433":1,"1434":1},"1":{"1434":1,"1435":2,"1436":2},"2":{"1433":1,"1436":1}}],["javascript",{"2":{"47":1}}],["jamieson",{"2":{"1175":1}}],["jax",{"2":{"1161":1}}],["japanese",{"2":{"597":1,"638":1}}],["jawahar",{"2":{"513":1}}],["jakob",{"2":{"434":1}}],["jauvin",{"2":{"429":1}}],["jay",{"2":{"429":1}}],["ja=jb",{"2":{"305":1}}],["ja",{"2":{"194":1}}],["jai",{"2":{"194":1}}],["j的内积上",{"2":{"766":1}}],["j的绝对值差将位置i和位置j的相对位置信息嵌入到attention计算过程中",{"2":{"765":1}}],["j的夹角大小",{"2":{"176":1}}],["j的交互",{"2":{"176":1}}],["j∥来完成",{"2":{"176":1}}],["j∥完成的",{"2":{"176":1}}],["j∥",{"2":{"176":1}}],["j∥是其它位置的的张量模长",{"2":{"176":1}}],["j∥cos",{"2":{"176":1}}],["j=softmax",{"2":{"1339":1}}],["j=−log",{"2":{"899":2}}],["j=∥q",{"2":{"176":1}}],["j=1",{"2":{"54":1,"71":2,"178":1,"191":7,"613":1}}],["j之间的关系得到",{"2":{"172":1}}],["john",{"2":{"1805":2}}],["johannes",{"2":{"638":1}}],["jordan",{"2":{"638":1}}],["join",{"2":{"592":1,"1250":1,"1408":1,"1566":1,"1895":1}}],["jointly",{"2":{"5":1,"257":1,"284":1,"292":2}}],["journal",{"2":{"429":1}}],["joy",{"2":{"95":1}}],["j",{"2":{"54":1,"71":12,"172":1,"176":4,"178":1,"183":2,"191":7,"194":1,"201":2,"210":1,"478":1,"613":1,"614":5,"745":2,"759":4,"760":1,"762":4,"766":3,"768":1,"847":2,"899":1,"944":14,"970":3,"971":3,"974":2,"986":1,"1000":2,"1014":1,"1023":1,"1124":1,"1195":1,"1308":2,"1316":2,"1329":4,"1330":2,"1339":30,"1340":15,"1438":3,"1547":1,"1557":1,"1607":3,"1630":4,"2149":1}}],["编程更加现代化",{"2":{"1932":1}}],["编程中",{"2":{"1645":1}}],["编程的基石",{"2":{"1703":1}}],["编程的基础",{"2":{"1601":1,"1826":1,"1844":1}}],["编程的关键基础",{"2":{"1678":1}}],["编程的第一步",{"0":{"1606":1}}],["编程时忽略",{"2":{"1590":1}}],["编程复杂度较高",{"2":{"1579":1}}],["编写部分文章",{"2":{"2043":1}}],["编写学而篇的",{"2":{"2043":1}}],["编写更可靠",{"2":{"1765":1}}],["编写可复用代码",{"2":{"1729":1}}],["编写结构清晰",{"2":{"1727":1}}],["编写健壮的",{"2":{"1666":1}}],["编写测试代码验证类的功能和继承关系",{"2":{"1657":1}}],["编写测试类",{"2":{"1481":1}}],["编写程序",{"2":{"1623":1}}],["编写一个c++程序",{"2":{"1933":1}}],["编写一个使用auto和decltype的程序",{"2":{"1900":1}}],["编写一个友元函数来输出该立方体对象的属性信息",{"2":{"1791":1}}],["编写一个函数",{"2":{"1650":1,"1678":1,"1729":1,"1914":1}}],["编写一个会导致栈溢出的程序",{"2":{"1648":1}}],["编写一个程序",{"2":{"1647":1,"1649":1,"1716":1,"1766":1}}],["编写一个递归函数来计算一个整数数组的所有元素的和",{"2":{"1646":1}}],["编写一个",{"2":{"1608":1,"1651":1}}],["编写对应的配置文件sql",{"2":{"1487":1,"1488":1}}],["编写接口方法",{"2":{"1487":1,"1488":1}}],["编写sql语句的时候",{"2":{"1485":1}}],["编写mapper",{"2":{"1481":1}}],["编写mapper接口类",{"2":{"1481":1}}],["编写mybatis工具类",{"2":{"1481":1}}],["编写mybatis核心配置文件",{"2":{"1481":1}}],["编写代码",{"2":{"1480":1}}],["编码维度为768",{"2":{"1337":1}}],["编码理解为一种在mlp权重中查询知识的过程",{"2":{"700":1}}],["编码无法捕捉类别之间的关系",{"2":{"694":1}}],["编码序列时",{"2":{"608":1}}],["编码序列",{"2":{"588":1}}],["编码过程的代码如下",{"2":{"572":1}}],["编码一致性",{"2":{"552":1}}],["编码后的结果",{"2":{"537":1}}],["编码后的隐向量",{"2":{"405":5,"407":5,"427":5}}],["编码结果记作memory",{"2":{"450":1}}],["编码函数",{"2":{"450":1,"592":1}}],["编码矩阵首先进入mha",{"2":{"445":1}}],["编码矩阵用",{"2":{"445":1}}],["编码",{"0":{"572":1,"587":1},"2":{"431":1,"460":1,"676":2,"1317":1}}],["编码历史上下文信息和预测下一个词的分布",{"2":{"288":1}}],["编码为",{"2":{"145":1}}],["编码器表示是并行计算的",{"2":{"937":1,"953":1}}],["编码器由n",{"2":{"914":1}}],["编码器映射一个用符号表示的输入序列",{"2":{"912":1}}],["编码器负责处理上下文信息",{"2":{"635":1}}],["编码器将输入转换为一个隐藏状态向量",{"2":{"885":1}}],["编码器将整个源语言句子作为输入",{"2":{"540":1}}],["编码器将源文本转换成连续的上下文向量",{"2":{"281":1}}],["编码器有关输入序列的信息",{"2":{"536":1}}],["编码器会将自己输出的隐向量编码矩阵c传递给解码器",{"2":{"532":1}}],["编码器会逐步调用编码器层的逻辑",{"2":{"529":1}}],["编码器会对输入句子",{"2":{"241":1}}],["编码器接受",{"2":{"528":1}}],["编码器接收输入序列的word",{"2":{"515":1}}],["编码器堆栈中最后一个编码器的输出",{"2":{"526":2}}],["编码器产生的隐状态和解码器之前预测的输出结果",{"2":{"524":1}}],["编码器内部数据转换时候的张量形状变化如下表所示",{"2":{"520":1}}],["编码器栈第一个encoderlayer的输入是单词的embedding加上位置编码",{"2":{"518":1}}],["编码器栈内部通过自注意力机制完成了对源序列的特征的提取",{"2":{"515":1}}],["编码器模块由一系列相同层构成",{"2":{"517":1}}],["编码器模块的输入",{"2":{"426":1}}],["编码器是售货员",{"2":{"524":1}}],["编码器是由多个相同的encoderlayer",{"2":{"517":1}}],["编码器是为每一个待预测词都生成一个上下文向量",{"2":{"516":1}}],["编码器是并行处理",{"2":{"515":1}}],["编码器层进行运算",{"2":{"522":1}}],["编码器层内部",{"2":{"520":4}}],["编码器层",{"2":{"517":1,"520":1}}],["编码器层把inputs编码成一个中间隐状态",{"2":{"453":1}}],["编码器层和解码器层可以作为两个单独的大积木块",{"2":{"449":1}}],["编码器输出的隐向量本质是聚合了输入序列信息的一个数据库",{"2":{"536":1}}],["编码器输出的最后一个时刻的隐状态就是编码了整个句子语义的语义上下文",{"2":{"241":1}}],["编码器输出被用做v和k",{"2":{"535":1}}],["编码器输出",{"2":{"450":1}}],["编码器对象",{"2":{"450":1}}],["编码器进行编码",{"2":{"445":1}}],["编码器并非只传递最后一步的隐状态",{"2":{"444":1}}],["编码器中的每个位置都可以关注编码器前一层输出的所有位置",{"2":{"442":1}}],["编码器中的掩码只是padding",{"2":{"80":1}}],["编码器处理",{"2":{"427":1}}],["编码器可以并行的关键是在自注意力机制中",{"2":{"418":1}}],["编码器可以同时从左到右和从右到左读取输入序列",{"2":{"278":1}}],["编码器解码器都可以并行",{"2":{"417":1}}],["编码器通过并行操作可以一次性计算出来",{"2":{"416":1}}],["编码器生成的编码隐向量",{"2":{"416":1}}],["编码器生成的上下文向量不再作为解码器每一步的输入",{"2":{"284":1}}],["编码器天然支持并行",{"2":{"415":1}}],["编码器每个时间步都考虑全部编码序列",{"2":{"284":1}}],["编码器每个时刻的输入是上一个时刻的隐状态和输入的新单词",{"2":{"241":1}}],["编码器用cnn实现",{"2":{"281":1}}],["编码器都会输出一个隐状态向量",{"2":{"267":1}}],["编码器依次执行",{"2":{"267":1}}],["编码器不只是传递最后一个隐藏状态",{"2":{"267":1}}],["编码器读入输入的token",{"2":{"249":1}}],["编码器需要把",{"2":{"249":1}}],["编码器和解码器堆栈",{"0":{"913":1},"1":{"914":1,"915":1}}],["编码器和解码器堆叠数",{"2":{"448":1}}],["编码器和解码器都使用self",{"2":{"912":1}}],["编码器和解码器都有自己的输入和输出",{"2":{"436":1}}],["编码器和解码器之间的隐状态",{"2":{"518":1}}],["编码器和解码器通常是由rnn或其变体",{"2":{"249":1}}],["编码器和解码器的作用分别如下",{"2":{"241":1}}],["编码器和解码器的运行方式不同",{"2":{"57":1}}],["编码器把输入句子的所有语义信息压缩成一个固定长度的中间语义向量",{"2":{"241":1}}],["编码器还有一个ffn",{"2":{"97":1}}],["编码器的目标就是对这个低阶语义向量序列进行特征提取",{"2":{"516":1}}],["编码器的输出就是对原始输入在更高维的向量空间中的表示",{"2":{"526":1}}],["编码器的输出就是对原始输入的高阶抽象表达",{"2":{"518":1}}],["编码器的输出和掩码多头注意力的输出",{"2":{"525":1}}],["编码器的输出结果",{"2":{"524":1}}],["编码器的输出",{"2":{"453":1,"526":2,"528":1,"532":1,"533":1}}],["编码器的输出会作为解码器输入的一部分",{"2":{"436":1}}],["编码器的输入被转换",{"2":{"517":1}}],["编码器的输入是待推理的句子序列x",{"2":{"520":1}}],["编码器的输入是word",{"2":{"516":1}}],["编码器的输入是词嵌入",{"2":{"25":1}}],["编码器的输入对应图上的inputs",{"2":{"453":1}}],["编码器的编码序列",{"2":{"165":1}}],["编码器的参数是src",{"2":{"82":1}}],["编码器self",{"2":{"77":1}}],["编码器",{"0":{"38":1,"241":1,"282":1,"415":1,"514":1,"516":1,"914":1},"1":{"515":1,"516":1,"517":2,"518":2,"519":2,"520":2,"521":2,"522":2,"523":2,"524":1,"525":1,"526":1,"527":1,"528":1,"529":1,"530":1,"531":1,"532":1,"533":1,"534":1,"535":1,"536":1,"537":1,"538":1,"539":1,"540":1,"541":1,"542":1,"543":1},"2":{"0":1,"244":1,"436":1,"520":1,"535":1,"536":1,"540":1,"886":1}}],["编辑器基础",{"0":{"1517":1},"1":{"1518":1,"1519":1,"1520":1}}],["编辑则是首先定位到知识存储在大模型中的位置",{"2":{"142":1}}],["编辑",{"2":{"142":1,"2094":1}}],["编译项目",{"2":{"1996":1}}],["编译规则",{"2":{"1917":1}}],["编译选项",{"2":{"1917":1}}],["编译所有目标",{"2":{"1917":1}}],["编译和链接",{"2":{"1916":1}}],["编译效率",{"2":{"1916":1}}],["编译时计算",{"2":{"1924":1}}],["编译时会产生警告",{"2":{"1909":1}}],["编译时确定",{"2":{"1714":1}}],["编译期整数序列等特性简化了常见任务的实现",{"2":{"1913":1}}],["编译期整数序列",{"0":{"1912":1},"2":{"1904":1}}],["编译并运行代码",{"2":{"1729":1}}],["编译错误",{"2":{"1660":1,"1674":1,"1677":1}}],["编译过程详解",{"2":{"1604":1}}],["编译原理",{"0":{"1604":1}}],["编译型语言",{"2":{"1602":1,"1604":1}}],["编译器最终决定是否内联",{"2":{"1709":1}}],["编译器可能会将",{"2":{"1709":2}}],["编译器可以选择忽略",{"2":{"1709":1}}],["编译器根据参数匹配调用相应的重载版本",{"2":{"1707":1}}],["编译器根据函数调用时的参数类型",{"2":{"1699":1}}],["编译器在遇到被",{"2":{"1909":1}}],["编译器在遇到使用模板的代码时",{"2":{"1698":1}}],["编译器在寻找可调用函数时",{"2":{"1698":1}}],["编译器默认生成的拷贝构造函数进行浅拷贝",{"2":{"1694":1}}],["编译器无法区分调用哪个函数",{"2":{"1687":1}}],["编译器无法确定子类对象调用的是哪个父类的成员",{"2":{"1660":1}}],["编译器和标准库的实现",{"2":{"1678":1}}],["编译器和运行工具是",{"2":{"1589":1}}],["编译器自动分配和释放",{"2":{"1648":1}}],["编译器自动推断大小为",{"2":{"1714":1}}],["编译器自动推断",{"2":{"1615":1}}],["编译器推断大小为",{"2":{"1634":1}}],["编译器会考虑函数的复杂度和大小来决定是否内联",{"2":{"1709":1}}],["编译器会根据函数调用时提供的参数类型和数量",{"2":{"1707":1}}],["编译器会根据传入的参数类型自动进行实例化",{"2":{"1701":1}}],["编译器会根据调用",{"2":{"1699":1}}],["编译器会首先在当前源文件所在目录下搜索",{"2":{"1628":1}}],["编译器会在系统头文件目录下搜索",{"2":{"1628":1}}],["编译器会自动添加",{"2":{"1715":1}}],["编译器会自动在",{"2":{"1704":1}}],["编译器会自动在末尾添加",{"2":{"1624":1}}],["编译器会自动生成两个不同版本的",{"2":{"1698":1}}],["编译器会自动生成一个默认构造函数",{"2":{"1675":1}}],["编译器会自动识别",{"2":{"1638":1}}],["编译器会自动进行隐式类型转换",{"2":{"1629":1}}],["编译器会自动推断数组大小为",{"2":{"1623":1}}],["编译器需要根据初始值来推断类型",{"2":{"1615":1}}],["编译器通常会将其优化为直接访问",{"2":{"1612":1}}],["编译器才能工作",{"2":{"1605":1}}],["编译器",{"2":{"1589":1,"1916":1,"1917":1}}],["编译器包装器",{"2":{"1589":1}}],["编译优化",{"2":{"1288":1}}],["编译成step",{"2":{"267":1}}],["编译",{"0":{"1587":1},"2":{"47":1,"233":2,"513":1,"1604":1,"1916":1}}],["邱震宇",{"2":{"47":1}}],["捆绑销售",{"2":{"45":1}}],["理所应当的算法",{"2":{"2117":1}}],["理论与实践的结合",{"2":{"2010":1}}],["理论和实践都反复证明",{"2":{"1465":1}}],["理论推导vae",{"0":{"1376":1},"1":{"1377":1}}],["理论基础",{"0":{"1048":1}}],["理论",{"2":{"638":1}}],["理论上任意的batch",{"2":{"1131":1}}],["理论上可以保持其全秩状态",{"2":{"542":1}}],["理论上可以用来模拟任何其他图灵完备的系统",{"2":{"504":1}}],["理论上只要梯度的绝对值大于随机误差",{"2":{"333":1}}],["理论上网络存在一个最优层数",{"2":{"296":1}}],["理论上",{"2":{"136":1,"248":1,"296":1,"863":1}}],["理论上我们只需使用等同于输入维度的基数量",{"2":{"116":1}}],["理想的截断阈值要刚好高于",{"2":{"1184":1}}],["理想的训练的step数也会发生变化",{"2":{"1155":1}}],["理想的训练方式应该是并行计算",{"2":{"390":1}}],["理想的参数初始化",{"0":{"997":1},"1":{"998":1,"999":1}}],["理想的网络参数初始化使模型训练事半功倍",{"2":{"990":1}}],["理想的部署将预填节点组织成两组",{"2":{"977":1}}],["理想的text",{"2":{"710":1}}],["理想的tokenizer应该具备如下基本特性",{"2":{"563":1}}],["理想的词汇表应该在保证模型性能和效率的同时",{"2":{"560":1}}],["理想情况下",{"2":{"34":1,"136":1,"1130":1,"1164":1}}],["理解算法本身或许会对当下的我们有一定的启发",{"2":{"2107":1}}],["理解基本的位运算符及其用法",{"2":{"2057":1}}],["理解完这三句之后",{"2":{"2054":1}}],["理解这里的",{"2":{"2054":1}}],["理解gpu如何进行图形加速",{"2":{"2009":1}}],["理解内置异常类型和自定义异常类型",{"2":{"1765":1}}],["理解函数的核心概念",{"2":{"1727":1}}],["理解运算符重载的概念和意义",{"2":{"1711":1}}],["理解运算符的优先级和结合性对于正确理解表达式的计算顺序至关重要",{"2":{"1630":1}}],["理解和掌握图形学技术可以打开许多职业发展的机会",{"2":{"2010":1}}],["理解和运用",{"2":{"1871":1}}],["理解和熟练运用它们对于编写高效",{"2":{"1709":1}}],["理解和建模",{"2":{"541":1}}],["理解指针数组和数组指针的区别",{"2":{"1705":1}}],["理解指针需要时间和实践",{"2":{"1611":1}}],["理解其内存布局和访问方式至关重要",{"2":{"1705":1}}],["理解其在对象销毁过程中的重要性",{"2":{"1666":1}}],["理解其在对象创建过程中的重要性",{"2":{"1666":1}}],["理解其在内存操作中的意义",{"2":{"1627":1}}],["理解它们对于编写高效",{"2":{"1703":1}}],["理解公有成员和私有成员的区别和作用",{"2":{"1666":1}}],["理解面向对象编程的基本思想",{"2":{"1666":1}}],["理解动态内存分配的原理和应用场景",{"2":{"1666":1}}],["理解动态图和静态图",{"0":{"1287":1}}],["理解不同继承方式对成员访问权限的影响",{"2":{"1652":1}}],["理解好今天的内容",{"2":{"1644":1}}],["理解宏定义的原理和使用场景",{"2":{"1627":1}}],["理解头文件的作用",{"2":{"1627":1}}],["理解变量和常量的概念至关重要",{"2":{"1613":1}}],["理解二级指针就足够了",{"2":{"1611":1}}],["理解了它们",{"2":{"1610":1}}],["理解了为什么要做归一化之后",{"2":{"321":1}}],["理解编译过程对于学习",{"2":{"1604":1}}],["理解差异",{"0":{"1602":1}}],["理解transformer的位置编码",{"2":{"768":1}}],["理解nlp最重要的编码方式",{"2":{"638":1}}],["理解上下文或任务",{"2":{"542":1}}],["理解llama",{"2":{"513":1}}],["理解语言模型的内部运作意味着定位前向传播中的哪些元素",{"2":{"476":1}}],["理解",{"2":{"158":1,"274":1,"541":1,"1607":1,"1611":1,"1614":3,"1652":1,"1678":1}}],["理解为llm的脑",{"2":{"116":1}}],["理解attention",{"2":{"47":1,"292":1}}],["理应可以分别指定并按需自由组合",{"2":{"45":1}}],["些",{"2":{"45":1}}],["或hlsl",{"2":{"2009":1}}],["或使用",{"2":{"1926":1}}],["或使用polyak平均法",{"2":{"1149":1}}],["或总长度",{"2":{"1914":1}}],["或换行符或文件结束符停止",{"2":{"1813":2,"1831":2}}],["或从程序",{"2":{"1810":1,"1828":1}}],["或空着",{"2":{"1607":1}}],["或安装",{"2":{"1605":1}}],["或无后缀文件",{"2":{"1604":1}}],["或响应",{"2":{"1563":1}}],["或注解来配置和映射原生信息",{"2":{"1476":1}}],["或类神经网络",{"2":{"1456":1}}],["或表征学习",{"2":{"1455":1}}],["或线程",{"2":{"1414":1}}],["或是让你在忙碌的生活中找到些许放松的时光",{"2":{"2109":1}}],["或是资源",{"2":{"1411":1}}],["或是带噪声的梯度",{"2":{"1028":1}}],["或前nbset大",{"2":{"1330":1}}],["或通用性进行了优化",{"2":{"1228":1}}],["或你的雇主",{"2":{"1197":1}}],["或额外的正则化技术",{"2":{"1186":1}}],["或中期训练中出现的不稳定性",{"2":{"1184":1}}],["或异常高的损失值的试验",{"2":{"1182":1}}],["或恒定学习率",{"2":{"1173":1}}],["或用于",{"2":{"1161":1}}],["或甚至验证误差",{"2":{"1157":1}}],["或在某些时候",{"2":{"1154":1}}],["或在交互式环境中直接输入",{"2":{"1083":1}}],["或全部",{"2":{"1150":1}}],["或采取其他一些纠正措施",{"2":{"1146":1}}],["或抽样更多试验",{"2":{"1146":1}}],["或研究组",{"2":{"1146":1}}],["或一系列研究",{"2":{"1144":1}}],["或一个固定超参数",{"2":{"1143":1}}],["或烘烤不必要的并发症",{"2":{"1137":1}}],["或计算节点间的同步",{"2":{"1132":1}}],["或至少接近恒定的",{"2":{"1132":1}}],["或至少接近加倍",{"2":{"1132":1}}],["或具有固定",{"2":{"1130":1}}],["或看到出错",{"2":{"1127":1}}],["或等效地使用",{"2":{"1122":1}}],["或当输入为",{"2":{"1115":1}}],["或副本",{"2":{"1083":1}}],["或最大化",{"2":{"1021":1,"1023":1}}],["或内存加载",{"2":{"973":1}}],["或微调",{"2":{"938":1,"954":1}}],["或更大",{"2":{"1714":1}}],["或更新版本是主流",{"2":{"1603":1}}],["或更小的恒定学习率",{"2":{"1173":1}}],["或更好地调整现有的正则化参数",{"2":{"1149":1}}],["或更本源的",{"2":{"974":1}}],["或更常用的lstm",{"2":{"885":1}}],["或更相关或更相似",{"2":{"277":1}}],["或称膨胀卷积",{"2":{"778":1}}],["或称为线性变换",{"2":{"172":1}}],["或称为transformer组件的隐藏状态",{"2":{"144":1}}],["或其他非自适应搜索算法",{"2":{"1175":1}}],["或其他变化",{"2":{"1152":1}}],["或其他类别索引",{"2":{"702":1}}],["或其近似的",{"2":{"354":1}}],["或向量",{"2":{"692":1}}],["或句子",{"2":{"691":1}}],["或考虑",{"2":{"683":1}}],["或发生",{"2":{"683":1}}],["或第5步的结果不再变化",{"2":{"602":1}}],["或下一个最高频的字节对的频率为1",{"2":{"576":1}}],["或经典的编码器",{"2":{"539":1}}],["或残差块来形成一个隐层状态以建立复杂的转换",{"2":{"494":1}}],["或达到句子长度",{"2":{"428":1}}],["或推理",{"2":{"392":1}}],["或增长",{"2":{"255":1}}],["或有用",{"2":{"230":1}}],["或",{"0":{"1441":1},"2":{"224":1,"249":1,"310":1,"379":1,"381":1,"460":1,"709":1,"1115":1,"1143":1,"1144":1,"1149":1,"1163":1,"1225":1,"1227":1,"1317":2,"1441":1,"1460":1,"1464":1,"1526":1,"1527":1,"1544":1,"1549":1,"1551":2,"1553":1,"1557":2,"1589":4,"1604":1,"1605":1,"1607":3,"1608":1,"1619":2,"1628":2,"1630":1,"1631":2,"1632":1,"1633":1,"1647":2,"1667":1,"1668":1,"1669":2,"1698":1,"1714":1,"1728":1,"1817":1,"1835":1,"1910":1,"1996":1}}],["或许",{"2":{"2054":1}}],["或许以后也会另有看法",{"2":{"2054":1}}],["或许你会哭",{"2":{"2054":1}}],["或许有些人这辈子都不会感悟到",{"2":{"2054":1}}],["或许有同学会疑问啥是系统资源",{"2":{"1411":1}}],["或许现在我是这种理解",{"2":{"2054":1}}],["或许与先前的各位大师有所矛盾",{"2":{"2052":1}}],["或许我看的视角有点片面",{"2":{"2051":1}}],["或许是中间层维度需要较大的一种解释",{"2":{"126":1}}],["或许多高效transformer模型中部署的稀疏注意力模式",{"2":{"93":1}}],["或键值对数量",{"2":{"116":1}}],["或投影什么属性",{"2":{"45":1}}],["或叫投影变换回路",{"2":{"45":1}}],["或叫查找选择回路",{"2":{"45":1}}],["或者其他编辑器",{"2":{"2070":1}}],["或者在遇到定界符",{"2":{"1813":1,"1831":1}}],["或者在收敛到最小值前在某个局部的极小值收敛了",{"2":{"991":1}}],["或者是抛出异常类型的基类",{"2":{"1762":1}}],["或者使用不带返回值的",{"2":{"1729":1}}],["或者创建一个函数来打印一条欢迎消息",{"2":{"1729":1}}],["或者当左操作数不是类的对象时",{"2":{"1712":1}}],["或者当作argmax的一种平滑近似",{"2":{"180":1}}],["或者存储在一组函数列表中",{"2":{"1706":1}}],["或者存储在磁盘文件中",{"2":{"1477":1}}],["或者指向在函数外部定义的变量的地址",{"2":{"1706":1}}],["或者阻止虚函数被重写",{"2":{"1656":1}}],["或者局部变量占用过多内存时",{"2":{"1648":1}}],["或者能被",{"2":{"1619":1,"1729":1}}],["或者重复执行特定的代码段",{"2":{"1618":1}}],["或者多层感知机",{"2":{"1457":1}}],["或者握手使得达成协议或者使得操作序列有序",{"2":{"1408":1}}],["或者一个分布",{"2":{"1371":1}}],["或者直接安装",{"2":{"1309":1}}],["或者直接用矩阵形式表达",{"2":{"161":1}}],["或者可选择返回一个新的",{"2":{"1227":1}}],["或者可选择返回一个新的state",{"2":{"1227":1}}],["或者可以说",{"2":{"119":1}}],["或者训练数据没有被适当地打乱",{"2":{"1164":1}}],["或者因为违反某些隐式约束而根本无法运行",{"2":{"1146":1}}],["或者我们也可以将激活函数作为一个冗余超参数和深度一起调优",{"2":{"1143":1}}],["或者我们接受实验得到的最优深度的仅在某个激活函数上有效",{"2":{"1143":1}}],["或者我们不希望它们",{"2":{"1143":1}}],["或者改进模型的性能",{"2":{"938":1,"954":1}}],["或者已经积累了n条已完成的句子",{"2":{"904":1}}],["或者下一时刻的输出要依赖于上一时刻的输出",{"2":{"851":1}}],["或者下一个最高频的字符对出现频率为",{"2":{"584":1}}],["或者为空",{"2":{"825":1}}],["或者更普遍的由于训练数据生成过程而产生的方差",{"2":{"1152":1}}],["或者更确切的说",{"2":{"754":1}}],["或者更彻底些",{"2":{"206":1}}],["或者字",{"2":{"676":1}}],["或者字典",{"2":{"164":1}}],["或者xxx",{"2":{"659":1}}],["或者point",{"2":{"642":1}}],["或者等效的语音片段",{"2":{"628":1}}],["或者等价地",{"2":{"84":1}}],["或者成语",{"2":{"566":1}}],["或者按语法规则来分词",{"2":{"547":1}}],["或者按语法规则分词",{"2":{"363":1}}],["或者sequence之间",{"2":{"535":1}}],["或者简化来看",{"2":{"517":1}}],["或者从另一个角度来理解",{"2":{"444":1}}],["或者把若干层分为一组",{"2":{"402":1}}],["或者采用平衡采样策略",{"2":{"368":1}}],["或者有",{"2":{"344":1}}],["或者",{"2":{"330":1,"344":2,"1132":1,"1143":1,"1144":1,"1145":1,"1155":1,"1196":1,"1561":1,"1608":1,"1612":1,"1914":1,"2054":1,"2140":1}}],["或者学到了错误的信息",{"2":{"304":1}}],["或者叫编码",{"2":{"460":1,"676":1}}],["或者叫",{"2":{"287":1}}],["或者让各个词直接建立联系",{"2":{"256":1,"272":1}}],["或者增加新影响力因子来增加信息含量",{"2":{"272":1}}],["或者增加新影响力因子",{"2":{"256":1}}],["或者将",{"2":{"245":1}}],["或者缓解",{"2":{"243":1}}],["或者类似moe",{"2":{"225":1}}],["或者也可以把模型想象成在翻阅一本巨大的书",{"2":{"216":1}}],["或者动态决定",{"2":{"204":1}}],["或者共享权重矩阵",{"2":{"172":1}}],["或者引入一个新的训练目标",{"2":{"142":1}}],["或者通过检索来校验llm的回答",{"2":{"141":1}}],["或者语义模式",{"2":{"127":1}}],["或者知识记忆和存储起来了",{"2":{"126":1}}],["或者换句话说",{"2":{"122":1}}],["或者上三角",{"2":{"50":1}}],["或者这么理解",{"2":{"9":1}}],["或者说密码",{"2":{"1372":1}}],["或者说对于人类的预判来说",{"2":{"689":1}}],["或者说对绝对位置不敏感",{"2":{"274":1}}],["或者说编码器和解码器",{"2":{"453":1}}],["或者说某个单词依据自己特征来回答其它单词的提问",{"2":{"265":1}}],["或者说注意力的熵变大",{"2":{"181":1}}],["或者说key可以体现value上的语义信息",{"2":{"164":1}}],["或者说是基于两个token之间的相对距离来表示位置关系",{"2":{"745":1}}],["或者说是编码器把源语言的完整句子一次性编码输出给解码器",{"2":{"453":1}}],["或者说是像rnn一样循环执行的",{"2":{"453":1}}],["或者说是对标签做平滑处理",{"2":{"399":1}}],["或者说是目标单词和输入中某单词对齐的可能性大小",{"2":{"268":1}}],["或者说是是",{"2":{"258":1}}],["或者说是遗忘问题",{"2":{"244":1}}],["或者说是限制注意力",{"2":{"204":1}}],["或者说是一个模糊的",{"2":{"164":1}}],["或者说是多义的",{"2":{"118":1}}],["或者说是表达人类概念的途径或者方法",{"2":{"4":1}}],["或者说方差",{"2":{"20":1}}],["或者说",{"2":{"4":1,"10":1,"29":1,"117":1,"121":1,"126":2,"164":1,"165":1,"172":2,"194":1,"221":1,"263":1,"265":1,"314":1,"397":1,"407":1,"409":1,"499":1,"510":1,"515":3,"525":1,"535":1,"536":1,"567":1,"629":1,"681":1,"687":1,"694":1,"700":1,"714":1,"739":1,"755":1,"759":1,"762":1}}],["或者像同时有多个视角在看同一个东西",{"2":{"1":1}}],["中心点画线法",{"0":{"2021":1},"1":{"2022":1,"2023":1,"2024":1,"2025":1}}],["中心化从数学上是一个线性变换",{"2":{"320":1}}],["中心化",{"2":{"320":1}}],["中望软件",{"2":{"1959":1}}],["中信证券",{"2":{"1949":1}}],["中科曙光",{"2":{"1944":1}}],["中会变为",{"2":{"1874":1}}],["中将包含学生的成绩和对应的等级",{"2":{"1825":1,"1843":1}}],["中输入学生成绩数据",{"2":{"1825":1,"1843":1}}],["中用于存储两个关联数据的容器",{"2":{"1805":1}}],["中自定义数据类型",{"2":{"1729":1}}],["中插入元素",{"2":{"1724":1}}],["中所有值为",{"2":{"1720":1}}],["中所有可能路径的",{"2":{"1330":1}}],["中关于字符串常量",{"2":{"1709":1}}],["中几个至关重要的概念",{"2":{"1703":1}}],["中指针",{"2":{"1678":1}}],["中主要的访问修饰符有",{"2":{"1677":1}}],["中主流的优化器是",{"2":{"333":1}}],["中强大但容易出错的特性",{"2":{"1672":1}}],["中类的大小计算方式",{"2":{"1652":1}}],["中内存泄漏检测工具",{"2":{"1648":1}}],["中非常重要的内存管理概念",{"2":{"1644":1}}],["中非0元素的索引",{"2":{"1087":1}}],["中函数的基本用法",{"2":{"1644":1}}],["中赋值操作的多种形式和类型转换",{"2":{"1627":1}}],["中数组的下标从",{"2":{"1623":1}}],["中数据的连续性",{"0":{"658":1,"1080":1}}],["中另一个重要的概念",{"2":{"1612":1}}],["中最强大的特性之一",{"2":{"1611":1}}],["中最长的样本的长度",{"2":{"87":1}}],["中一个非常重要且强大的概念",{"2":{"1610":1}}],["中使用最广泛的容器之二",{"2":{"1797":1}}],["中使用",{"2":{"1566":1}}],["中使用的一种显式编码",{"2":{"1336":1}}],["中断屏蔽方法",{"0":{"1421":1}}],["中并提升",{"2":{"1341":1}}],["中采用的就是这种编码方式",{"2":{"1337":1}}],["中加入位置向量",{"2":{"1335":1}}],["中加入噪声",{"2":{"399":1}}],["中都展现出了巨大的潜力",{"2":{"1318":1}}],["中衰减每个参数组的学习率",{"2":{"1240":1}}],["中对应的torch",{"2":{"1200":1}}],["中更改单个残差块",{"2":{"1178":1}}],["中同步设备和主机之间的数据时",{"2":{"1161":1}}],["中哪个优化器的验证错误率更低",{"2":{"1143":1}}],["中尝试推断模式",{"2":{"1121":1}}],["中选择最大值",{"2":{"2125":3}}],["中选择最佳优化器",{"2":{"1144":1}}],["中选择",{"2":{"1118":1}}],["中局部禁用梯度计算",{"2":{"1116":1}}],["中保存算子的梯度函数",{"2":{"1089":1}}],["中维度",{"2":{"1087":1}}],["中元素的相反数视图",{"2":{"1086":1}}],["中每个group",{"2":{"1226":1}}],["中每个",{"2":{"1086":1}}],["中indices",{"2":{"1086":1}}],["中大家都了解过",{"2":{"1079":1}}],["中源码定义位置",{"2":{"1073":1}}],["中后期",{"2":{"1045":1}}],["中实现",{"0":{"1037":1},"1":{"1038":1,"1039":1}}],["中需要特别注意的地方",{"2":{"1623":1}}],["中需要",{"2":{"934":1}}],["中常用的参数传递方式有两种",{"2":{"1729":1}}],["中常用的流程控制语句",{"2":{"1618":1}}],["中常用的操作",{"2":{"813":1}}],["中常见的数据增强手段",{"2":{"1015":2}}],["中常见任务",{"0":{"906":1}}],["中必须包含原始序列中的所有信息",{"2":{"891":1}}],["中逐元素进行的操作",{"2":{"829":1}}],["中kv",{"2":{"746":1}}],["中训练得到一个embedding层",{"2":{"706":1}}],["中摘取的代码",{"2":{"700":1}}],["中就提到",{"2":{"698":1}}],["中就提到了这点",{"2":{"326":1}}],["中止",{"2":{"679":1}}],["中只要有一个",{"2":{"661":1,"1104":1}}],["中存储grad的地方",{"2":{"661":1,"1104":1}}],["中途会取出cache里的k",{"2":{"656":1}}],["中译英",{"2":{"545":1}}],["中查询memory中各个位置与q有关的信息",{"2":{"537":1}}],["中不同单词之间的相关性",{"2":{"535":1}}],["中构造",{"2":{"523":1}}],["中找到每个token对应的token",{"2":{"455":1}}],["中变换矩阵的维度",{"2":{"448":1}}],["中词数这个维度",{"2":{"428":1}}],["中词的个数",{"2":{"423":1}}],["中两个全连接层中间也有",{"2":{"394":1}}],["中间含有两个字",{"2":{"2056":1}}],["中间件架构师",{"2":{"1953":1}}],["中间件开发",{"2":{"1951":1}}],["中间activation的梯度最好释放",{"2":{"1106":1}}],["中间计算过程",{"2":{"1099":1}}],["中间计算过程使用torch",{"2":{"346":1}}],["中间tensor的梯度控制",{"2":{"1099":1}}],["中间数量的组导致插值模型",{"2":{"937":1,"953":1}}],["中间激活",{"2":{"768":1}}],["中间",{"2":{"757":1}}],["中间有gelu激活",{"2":{"735":1}}],["中间有一个激活函数",{"2":{"466":1}}],["中间的tensor",{"2":{"1106":1}}],["中间的",{"2":{"661":1,"1104":1}}],["中间夹有一个relu激活函数",{"2":{"517":1}}],["中间层节点为字向量维数的全连接层",{"2":{"694":1}}],["中间层提取语法特征",{"2":{"437":1}}],["中间层学习句法特征",{"2":{"437":1}}],["中间层维度对应memory",{"2":{"126":1}}],["中间层比率",{"0":{"100":1},"2":{"96":1}}],["中可能出现的异常值问题",{"2":{"346":1}}],["中可能会影响模型收敛",{"2":{"88":1}}],["中多层叠加的结果更多是增加宽度而不是深度",{"2":{"334":1}}],["中相当于对",{"2":{"323":1}}],["中受",{"2":{"322":1}}],["中文可能对应多个字节",{"2":{"592":1}}],["中文可以使用jieba分词或者直接按照字来进行分词",{"2":{"579":1}}],["中文因为没有空格分割",{"2":{"553":1}}],["中文",{"2":{"277":1,"516":1,"528":1}}],["中有三种常用模式",{"2":{"1541":1}}],["中有一段话就深刻的印证了资源分配这个角度",{"2":{"260":1}}],["中有具体论述",{"2":{"103":1}}],["中发布了transformer",{"2":{"235":1}}],["中发现",{"2":{"115":1}}],["中检索相应",{"2":{"230":1}}],["中庞大",{"2":{"221":1}}],["中固有的指数函数的尖锐性",{"2":{"213":1}}],["中森",{"2":{"156":1}}],["中计算成本高昂的全连接层",{"2":{"151":1}}],["中隐藏层的维数",{"2":{"141":1}}],["中得分很低",{"2":{"140":1}}],["中容易存在过时或错误的信息",{"2":{"140":1}}],["中已存储的知识",{"2":{"140":1}}],["中提出的bge是比较有名的开源embedding",{"2":{"724":1}}],["中提出了一种名为",{"2":{"1312":1}}],["中提出了一种可以根据不同任务动态调整模型权重的机器学习系统",{"2":{"218":1}}],["中提出了",{"2":{"1312":1}}],["中提出了基于核的线性注意力机制",{"2":{"210":1}}],["中提出",{"2":{"108":1,"237":1,"257":1,"393":1}}],["中讨论了部分",{"2":{"89":1}}],["中标注",{"2":{"84":1}}],["中因果自注意力掩码",{"2":{"84":1}}],["中包含的xn+1xn+1x",{"2":{"58":1}}],["中国的首都是北京",{"2":{"118":1}}],["中国的首都是哪座城市",{"2":{"118":1}}],["中国",{"2":{"58":1}}],["中",{"2":{"44":1,"78":1,"82":1,"83":1,"87":1,"90":1,"103":1,"105":1,"118":1,"136":1,"153":1,"156":1,"204":1,"211":1,"222":1,"245":2,"273":1,"325":1,"329":1,"330":1,"334":1,"343":1,"352":2,"355":1,"382":1,"399":1,"429":1,"460":1,"485":1,"510":1,"535":1,"538":1,"542":2,"709":1,"808":1,"844":1,"889":1,"940":1,"941":1,"942":1,"959":1,"960":1,"962":1,"985":1,"986":9,"1059":1,"1087":2,"1117":2,"1118":1,"1143":1,"1155":1,"1226":1,"1364":1,"1624":1,"1647":1,"1649":1,"1650":1,"1667":1,"1668":1,"1704":2,"1728":2,"1729":1,"1810":1,"1828":1,"1873":1,"1874":1,"1997":1}}],["中的功能实现计算器的功能",{"2":{"1997":1}}],["中的初始化语句和条件之间使用分号",{"2":{"1922":1}}],["中的元素",{"2":{"1912":1}}],["中的元素是",{"2":{"1724":1}}],["中的键值对集合",{"2":{"1807":1}}],["中的数据类型",{"2":{"1607":1}}],["中的数据尽量接近",{"2":{"90":1}}],["中的rope",{"0":{"1345":1}}],["中的相对位置编码",{"0":{"1340":1}}],["中的dataset",{"0":{"1253":1}}],["中的后置钩子函数",{"2":{"1226":1}}],["中的前置钩子函数",{"2":{"1226":1}}],["中的版本号实现特定于类的向后兼容加载",{"2":{"1214":1}}],["中的容器",{"0":{"1207":1}}],["中的步幅会导致训练不稳定",{"2":{"1178":1}}],["中的所有超参数都一样重要",{"2":{"1174":1}}],["中的所有的线性投影层",{"2":{"621":1}}],["中的实现也依赖于无梯度模式",{"2":{"1120":1}}],["中的每个切片的最小值",{"2":{"1087":2}}],["中的每一位置进行解码时",{"2":{"537":1}}],["中的问题采样更多答案",{"2":{"986":1}}],["中的节点",{"2":{"986":3}}],["中的sequence长度",{"2":{"808":1}}],["中的scaled是缩放的意思",{"2":{"186":1}}],["中的一种可行甚至更好的方法",{"2":{"756":1}}],["中的路由权重可以为解码器嵌入提供补充信息",{"2":{"739":1}}],["中的路径",{"2":{"161":1}}],["中的点",{"2":{"691":1}}],["中的tokenizer",{"2":{"638":1}}],["中的tanh",{"2":{"334":1}}],["中的训练数据",{"2":{"595":1}}],["中的prompt机制具有图灵完备性",{"2":{"504":1}}],["中的许多已知神经现象",{"2":{"490":1}}],["中的更新方程",{"2":{"354":1}}],["中的embedding",{"2":{"326":1}}],["中的γγ",{"2":{"313":1}}],["中的层代表非残差函数",{"2":{"301":1}}],["中的间接目标识别电路",{"2":{"156":1}}],["中的不准确信息",{"2":{"140":1}}],["中的不同细分逻辑子空间中",{"2":{"33":1}}],["中的不同细分逻辑子空间中来运作了",{"2":{"12":1}}],["中的知识神经元",{"2":{"156":1}}],["中的知识",{"2":{"136":1}}],["中的知识存储模式",{"2":{"123":1}}],["中的计算复杂度尽量接近",{"2":{"90":1}}],["中的长度不均衡",{"2":{"90":1}}],["中的序列长度接近",{"2":{"90":1}}],["中的自注意力掩码",{"2":{"84":1}}],["中的",{"0":{"891":1,"892":1,"900":1,"1074":1,"1251":1,"1254":1},"1":{"892":1,"901":1,"902":1,"903":1,"904":1,"1075":1,"1076":1},"2":{"78":1,"204":1,"210":1,"231":1,"257":1,"304":1,"326":2,"522":1,"714":1,"717":1,"1206":1,"1226":1,"1339":1,"1364":1,"1728":2}}],["中的专家",{"2":{"42":1}}],["中的语义逻辑子空间",{"2":{"12":1}}],["默认目标",{"2":{"1917":1}}],["默认精度",{"2":{"1817":1,"1835":1}}],["默认",{"2":{"1817":2,"1820":2,"1835":2,"1838":2}}],["默认值为",{"2":{"1728":7}}],["默认继承方式是",{"2":{"1728":2}}],["默认访问权限",{"2":{"1728":1}}],["默认打印次数为",{"2":{"1708":1}}],["默认宽度为",{"2":{"1708":1}}],["默认参数值以及内联函数等重要概念",{"2":{"1709":1}}],["默认参数值在函数声明中指定",{"2":{"1708":1}}],["默认参数必须从右向左定义",{"2":{"1708":1}}],["默认参数必须从参数列表的右侧开始定义",{"2":{"1708":1}}],["默认参数在函数声明时指定",{"2":{"1708":1}}],["默认参数的规则",{"2":{"1708":1}}],["默认拷贝构造函数",{"2":{"1694":2}}],["默认构造函数",{"2":{"1675":2,"1713":1,"1788":1}}],["默认设置是尝试使用",{"2":{"1183":1}}],["默认设置为0",{"2":{"42":1}}],["默认模式是在没有启用其他模式",{"2":{"1119":1}}],["默认模式",{"0":{"1119":1},"2":{"1118":1,"1119":1}}],["默认情况下会生成",{"2":{"1184":1}}],["默认情况下",{"2":{"1117":1,"1728":1}}],["默认的context",{"2":{"976":1}}],["默认为",{"2":{"702":1,"1086":1,"1117":1}}],["默认为1",{"2":{"385":1}}],["默认使用",{"2":{"621":1}}],["默认是vgg类似的网络",{"2":{"1001":1}}],["默认是1e",{"2":{"343":1}}],["默认是none",{"2":{"36":1}}],["默认是0",{"2":{"23":1}}],["总计",{"2":{"2139":3}}],["总花费是",{"2":{"2131":1}}],["总强过记住一个人的坏处",{"2":{"2056":1}}],["总和",{"2":{"1594":1}}],["总共用了",{"2":{"2139":1}}],["总共47个章节",{"2":{"1404":1}}],["总共有",{"2":{"340":1}}],["总误差为",{"2":{"1389":1}}],["总得知道那些个密码符号表示的含义是什么才可以吧",{"2":{"1372":1}}],["总路径得分计算",{"0":{"1329":1}}],["总消耗量",{"2":{"1134":1}}],["总步数可以通过",{"2":{"1242":1}}],["总步数",{"2":{"1133":1,"1134":1}}],["总是被覆盖为",{"2":{"1119":1}}],["总是得到相同的结果",{"2":{"576":1}}],["总是表示相应的关系事实",{"2":{"135":1}}],["总之",{"2":{"545":1,"629":1,"758":1}}],["总的时间复杂度为",{"2":{"2155":1}}],["总的",{"2":{"1329":1}}],["总的训练周期数",{"2":{"1243":1}}],["总的计算flop高于传统attention",{"2":{"945":1,"965":1}}],["总的transformer模型",{"2":{"449":1}}],["总的来说",{"2":{"77":1,"172":1,"413":1,"560":1,"1012":1}}],["总体架构",{"0":{"433":1,"501":1,"629":1},"1":{"434":1,"435":1,"436":1,"437":1,"438":1,"439":1,"440":1,"441":1,"442":1,"443":1,"444":1,"445":1,"446":1}}],["总体代码如下所示",{"2":{"424":1}}],["总体代码",{"0":{"424":1}}],["总体趋势是学习率先增加再减少",{"2":{"402":1}}],["总体流程",{"0":{"364":1}}],["总体流程分为",{"2":{"16":1}}],["总体算法包括",{"2":{"313":1}}],["总体效果也不好",{"2":{"296":1}}],["总体分作一类",{"2":{"185":1}}],["总体而言",{"2":{"141":1}}],["总而言之",{"2":{"140":1,"267":1}}],["总结回顾",{"0":{"1901":1}}],["总结与注意事项",{"0":{"1793":1},"2":{"1768":1}}],["总结与习题",{"0":{"1757":1},"1":{"1758":1},"2":{"1732":1}}],["总结",{"0":{"510":1,"624":1,"848":1,"869":1,"1334":1,"1758":1,"1765":1,"1826":1,"1844":1,"1913":1,"1932":1,"2120":1},"1":{"511":1,"512":1,"1335":1,"1336":1,"1337":1,"1338":1,"1339":1,"1340":1,"1341":1,"1342":1,"1343":1,"1344":1,"1345":1,"1933":1},"2":{"838":1,"1129":1,"1130":1,"1131":1,"1139":1,"1140":1,"1141":1,"1142":1,"1146":1,"1149":1,"1152":1,"1153":1,"1161":1,"1162":1,"1166":1,"1167":1,"1168":1,"1169":1,"1178":1,"1485":1,"1607":1,"1677":1,"1709":1,"1715":1,"1729":1,"1904":1,"1918":1,"1920":1}}],["总结来说",{"2":{"309":1,"796":1,"1344":1}}],["总结下",{"2":{"72":1,"326":1}}],["总结一下",{"2":{"59":1,"402":1}}],["总训练损失是任务特定损失和负载平衡损失的加权和",{"2":{"42":1}}],["总训练目标",{"2":{"42":1}}],["应尽量避免使用",{"2":{"1631":1}}],["应谨慎使用",{"2":{"1629":1}}],["应调用",{"2":{"1241":1,"1242":1}}],["应当从训练集中随机抽取样本",{"2":{"1149":1}}],["应该考虑拿孩子遍历还是饼干",{"2":{"2157":1}}],["应该尽量使用类的公有接口来实现功能",{"2":{"1793":1}}],["应该在析构函数内部处理",{"2":{"1764":1}}],["应该在提高代码可读性和维护性之间找到平衡",{"2":{"1615":1}}],["应该使用",{"2":{"1669":2,"1712":1}}],["应该使用与计算梯度不同的",{"2":{"1136":1}}],["应该谨慎使用全局变量",{"2":{"1649":1}}],["应该有一个相对独立的逻辑层面",{"2":{"1478":1}}],["应该对应什么样的标签",{"2":{"1323":1}}],["应该以几个不同的数量级进行扫描",{"2":{"1183":1}}],["应该是控制模型中哪些部分参与梯度计算的主要方法",{"2":{"1117":1}}],["应该是从这个工作开始",{"2":{"760":1}}],["应该如何对注意力进行分配",{"2":{"537":1}}],["应该如何配合gpu进行优化呢",{"2":{"185":1}}],["应该承担更多的对",{"2":{"277":1}}],["应该翻译成",{"2":{"259":1}}],["应该与",{"2":{"245":1}}],["应该综合利用各方面的信息",{"2":{"3":1}}],["应用示例与实践",{"0":{"2060":1}}],["应用最广",{"2":{"2026":1}}],["应用广泛",{"2":{"2010":1}}],["应用领域",{"2":{"1602":1}}],["应用所需",{"2":{"1435":1}}],["应用旋转位置编码",{"2":{"1345":1}}],["应用旋转操作",{"2":{"1345":1}}],["应用到前面公式",{"2":{"1343":1}}],["应用用户指定的函数",{"2":{"1214":1}}],["应用于不希望更新的参数",{"2":{"1117":1}}],["应用于位置嵌入和词嵌入的加法运算带来了两种异构信息资源之间的混合相关性",{"2":{"764":1}}],["应用dropout之后",{"2":{"1017":1}}],["应用场景",{"0":{"879":1},"1":{"880":1,"881":1,"882":1},"2":{"1611":1,"1612":1,"1620":1,"1866":1,"1867":1,"1868":1,"1869":1}}],["应用案例",{"0":{"860":1}}],["应用层归一化等归一化技术能有效提升相似度计算的准确性",{"2":{"692":1}}],["应用和实现上有显著的区别",{"2":{"676":1}}],["应用一个",{"2":{"515":1}}],["应用各种归一化技术被认为是有益的",{"2":{"351":1}}],["应用标准化后",{"2":{"343":1}}],["应用了残差连接和层归一化",{"2":{"533":1}}],["应用了逐元素",{"2":{"213":1}}],["应用了负载平衡损失",{"2":{"42":1}}],["应用词嵌入之后的输入h",{"2":{"201":1}}],["应用函数",{"2":{"122":1}}],["应用softmax函数在点积上以获取权重",{"2":{"158":1}}],["应用softmax函数",{"2":{"63":1}}],["两栖交通工具",{"2":{"1664":1}}],["两项attention可以删掉",{"2":{"1340":1}}],["两数据差异比较",{"2":{"1087":1}}],["两者地址相同",{"2":{"1704":1}}],["两者都不能被修改",{"2":{"1614":1}}],["两者是很相似的",{"2":{"1469":1}}],["两者一定要相等嘛",{"2":{"698":1}}],["两者各有好坏",{"2":{"659":1}}],["两者的模型结构如下图",{"2":{"714":1}}],["两者的结合是现代深度学习架构成功的重要因素之一",{"2":{"294":1}}],["两者的核心差异在于",{"2":{"107":1}}],["两种点积机制",{"2":{"647":1,"924":1}}],["两种方式各有优劣",{"2":{"523":1}}],["两种方式的公式化如下",{"2":{"329":1}}],["两种归一化方法的差异主要体现在二者因面对任务的不同而引起的作用对象差异",{"2":{"321":1}}],["两种",{"2":{"292":1}}],["两种机制",{"2":{"285":1}}],["两部分",{"2":{"216":1}}],["两层之间使用rele激活函数",{"2":{"109":1}}],["两个整数相除",{"2":{"1997":1}}],["两个整数相乘",{"2":{"1997":1}}],["两个整数相减",{"2":{"1997":1}}],["两个整数相加",{"2":{"1997":1}}],["两个对象共享相同的内存空间",{"2":{"1694":1}}],["两个相同类型的指针相减",{"2":{"1633":1}}],["两个相似的概念在向量空间中会彼此靠近",{"2":{"691":1}}],["两个",{"2":{"1614":1}}],["两个或两个以上的进程",{"2":{"1409":1}}],["两个可取函数y",{"2":{"1377":1}}],["两个边界之间的距离可以按每次迭代或每个循环来进行缩放",{"2":{"1241":1}}],["两个属性",{"2":{"1111":1}}],["两个tensor",{"2":{"1087":1}}],["两个角度认识",{"0":{"1078":1}}],["两个w对应wk",{"2":{"760":1}}],["两个随机生成的向量近似正交",{"2":{"709":1}}],["两个随机变量相加",{"2":{"314":1}}],["两个值",{"2":{"698":1}}],["两个向量越相似",{"2":{"692":1}}],["两个任务",{"2":{"671":1}}],["两个子词在语言模型上就拥有越强的关联性",{"2":{"598":1}}],["两个子词",{"2":{"567":1}}],["两个词典都是vocab对象",{"2":{"557":1}}],["两个词之间的紧密程度仅取决于它们的真实相关性",{"2":{"274":1}}],["两个单词",{"2":{"537":1}}],["两个注意力计算的mask参数不同",{"2":{"533":1}}],["两个标号1对应的实例彼此之间相互独立",{"2":{"449":1}}],["两个部分",{"2":{"402":1}}],["两个部分通过一个隐状态",{"2":{"241":1}}],["两个光标",{"2":{"284":1}}],["两个输入",{"2":{"264":1}}],["两个方案都难以确定词之间的依赖关系",{"2":{"256":1}}],["两个位置之间距离越远",{"2":{"246":1}}],["两个位置之间路径越短",{"2":{"246":1}}],["两个矩阵",{"2":{"1398":1}}],["两个矩阵相乘",{"2":{"180":1}}],["两个矩阵可能形状不匹配",{"2":{"172":1}}],["两个线性层的特点如下",{"2":{"113":1}}],["两个低秩矩阵w1",{"2":{"46":1}}],["两个头关注的地方不一样",{"2":{"18":1}}],["两阶段路由",{"2":{"42":1}}],["指南",{"0":{"2035":1}}],["指令包含在其他文件中",{"2":{"1916":1}}],["指令末尾不需要加分号",{"2":{"1632":1}}],["指令",{"2":{"1632":1}}],["指表达式结束后就不再存在的临时对象或字面量值",{"2":{"1629":1}}],["指表达式结束后依然存在的持久对象",{"2":{"1629":1}}],["指const",{"2":{"1614":1}}],["指针函数返回指针",{"2":{"1706":1}}],["指针函数的返回值是一个指针类型",{"2":{"1706":1}}],["指针函数",{"2":{"1706":2}}],["指针在函数中扮演着重要的角色",{"2":{"1706":1}}],["指针在声明时应初始化为一个有效的地址或空指针",{"2":{"1672":1}}],["指针数组",{"2":{"1705":2}}],["指针以及函数",{"2":{"1703":1}}],["指针大小",{"2":{"1680":1}}],["指针传递后",{"2":{"1650":1}}],["指针传递函数内部",{"2":{"1650":2}}],["指针传递",{"2":{"1650":4}}],["指针传递和引用传递",{"2":{"1650":1}}],["指针会导致未定义的行为",{"2":{"1647":1}}],["指针访问年份",{"2":{"1642":1}}],["指针访问成员变量",{"2":{"1638":1}}],["指针区分成员变量和参数",{"2":{"1638":1}}],["指针像数组一样访问元素",{"2":{"1634":1}}],["指针算术运算在遍历数组元素时非常有用",{"2":{"1633":1}}],["指针运算通常用于数组",{"2":{"1633":1}}],["指针运算不能超出数组的边界或访问未分配的内存",{"2":{"1633":1}}],["指针运算的有效性",{"2":{"1633":1}}],["指针与函数",{"0":{"1703":1},"1":{"1704":1,"1705":1,"1706":1,"1707":1,"1708":1,"1709":1,"1710":1}}],["指针与整数的加减运算",{"2":{"1633":1}}],["指针与引用的区别",{"0":{"2005":1}}],["指针与引用",{"0":{"1610":1}}],["指针所指向的数据类型",{"2":{"1633":1}}],["指针加",{"2":{"1633":1}}],["指针可以像数组一样使用下标运算符访问元素",{"2":{"1634":1}}],["指针可以进行加减运算",{"2":{"1633":1}}],["指针可以指向任何类型的数据",{"2":{"1633":1}}],["指针可以指向",{"2":{"1611":1}}],["指针成员选择",{"2":{"1630":1}}],["指针本身是常量",{"2":{"1614":1}}],["指针本身是一个常量",{"2":{"1614":1}}],["指针本身也是变量",{"2":{"1611":1}}],["指针常量",{"2":{"1614":1}}],["指针指向的内存地址中的值是常量",{"2":{"1614":1}}],["指针不知道它指向的数据类型",{"2":{"1611":1}}],["指针也可以指向",{"2":{"1611":1}}],["指针为空",{"2":{"1611":2}}],["指针有效",{"2":{"1611":1}}],["指针变量名",{"2":{"1614":2}}],["指针变量",{"2":{"1611":1,"1647":4}}],["指针变量本身也需要内存来存储地址",{"2":{"1611":1}}],["指针的安全使用原则",{"0":{"1672":1}}],["指针的类型为",{"2":{"1638":1}}],["指针的类型是指向当前对象的指针",{"2":{"1638":1}}],["指针的类型",{"2":{"1638":1}}],["指针的比较运算",{"2":{"1633":1}}],["指针的减法运算",{"2":{"1633":1}}],["指针的算术运算",{"0":{"1633":1}}],["指针的初始化和空指针",{"2":{"1611":1}}],["指针的大小会影响对象的大小",{"2":{"1678":1}}],["指针的大小通常是",{"2":{"1611":2}}],["指针的大小",{"2":{"1611":1}}],["指针的基本操作",{"2":{"1611":1}}],["指针的概念",{"2":{"1611":1}}],["指针就像一把双刃剑",{"2":{"1611":1}}],["指针是指向常量的指针",{"2":{"1640":1}}],["指针是一个隐含的参数",{"2":{"1638":1}}],["指针是什么",{"2":{"1616":1}}],["指针是不同的概念",{"2":{"1611":1}}],["指针是驾驭内存的灵活缰绳",{"2":{"1611":1}}],["指针是",{"2":{"1611":1,"1672":1}}],["指针",{"0":{"1611":1,"1638":1,"1666":1,"1667":1},"1":{"1667":1,"1668":1,"1669":1,"1670":1,"1671":1,"1672":1,"1673":1,"1674":1,"1675":1,"1676":1,"1677":1,"1678":1},"2":{"1611":1,"1612":1,"1614":2,"1633":3,"1638":8,"1639":1,"1647":3,"1650":1,"1695":1,"1704":1,"1712":1}}],["指针和引用的概念",{"2":{"1650":1}}],["指针和引用的区别",{"2":{"1612":1}}],["指针和引用之间的区别是什么",{"2":{"1616":1}}],["指针和引用",{"2":{"1610":1}}],["指示是否应该将键分配给其在state",{"2":{"1214":1}}],["指示有多少个正在运行的请求正在使用它",{"2":{"986":1}}],["指模型在生产环境中运行的时间",{"2":{"1157":1}}],["指模型如何修改存储的某个知识",{"2":{"121":1}}],["指模型如何回忆基本知识",{"2":{"121":1}}],["指模型如何存储知识",{"2":{"121":1}}],["指哪些位置呢",{"2":{"933":1}}],["指具有大小",{"2":{"680":1}}],["指引下进行预测",{"2":{"411":1}}],["指导来进行训练",{"2":{"406":1}}],["指导未来的眼球运动和决策",{"2":{"260":1}}],["指的主要的是",{"2":{"785":1}}],["指的就是真值",{"2":{"406":1}}],["指的是什么",{"2":{"1221":1}}],["指的是在特定的语言任务中",{"2":{"714":1}}],["指的是数值表示形式本身",{"2":{"676":1}}],["指的是norm所处的位置",{"2":{"329":1}}],["指的是归一化过程能够适应输入数据的缩放",{"2":{"320":1}}],["指代的具体意思在最初定义时是可以随机互换的",{"2":{"709":1}}],["指代的就是",{"2":{"246":1}}],["指代牛奶",{"2":{"261":1}}],["指代是同一个对象",{"2":{"246":1}}],["指定安装规则",{"2":{"1987":1}}],["指定编译选项",{"2":{"1984":1}}],["指定包含目录",{"2":{"1976":1}}],["指定",{"2":{"1971":1}}],["指定要编译的源文件",{"2":{"1966":1}}],["指定文件名和打开模式",{"2":{"1820":1,"1838":1}}],["指定函数执行完成后返回给调用者的值的类型",{"2":{"1729":1}}],["指定枚举常量的值",{"2":{"1728":1}}],["指定环境变量",{"2":{"1309":1}}],["指定应该进行优化的张量",{"2":{"1225":1}}],["指定一个索引",{"2":{"702":1}}],["指定一部分头为始终保持激活的共享头",{"2":{"42":1}}],["指定了权重矩阵中每个组件的期望强度",{"2":{"224":1}}],["指数位",{"2":{"1075":2}}],["指数加权",{"2":{"1059":1}}],["指数函数",{"2":{"178":1}}],["指数函数的使用放大了分值之间的差异",{"2":{"180":1}}],["指数函数的使用",{"2":{"178":1}}],["指数速率在ϵϵ",{"2":{"93":1}}],["指数速率",{"2":{"93":1}}],["指被训练过的夹角只是一个有限的集合",{"2":{"176":1}}],["指对其中填入的不同数值",{"2":{"147":1}}],["指出如果没有skip",{"2":{"446":1}}],["指出",{"2":{"147":1,"181":1,"305":1,"437":1,"487":1,"1175":1}}],["指向字符串字面量",{"2":{"1929":1}}],["指向原来的内存",{"2":{"1911":1}}],["指向函数的指针",{"2":{"1706":1}}],["指向第二个元素",{"2":{"1719":3}}],["指向第二行",{"2":{"1705":1}}],["指向第一行",{"2":{"1705":1}}],["指向包含",{"2":{"1705":1}}],["指向该对象",{"2":{"1695":1}}],["指向独立的内存空间",{"2":{"1694":1}}],["指向实参的内存地址",{"2":{"1650":1}}],["指向的地址",{"2":{"1704":1}}],["指向的内存已被释放",{"2":{"1694":1}}],["指向的内存地址中存储的值",{"2":{"1611":1}}],["指向的值",{"2":{"1647":1,"1648":1}}],["指向前面的内存地址",{"2":{"1633":1}}],["指向常量的指针常量",{"2":{"1614":1}}],["指向二级指针的指针",{"2":{"1611":1}}],["指向",{"2":{"1611":2,"1614":3,"1633":4,"1634":1,"1683":1,"1704":1,"1705":1,"1715":1,"1929":1}}],["指向指针的指针",{"2":{"1611":2}}],["指向句子中生僻词的头",{"2":{"20":1}}],["指向具有特定语法关系的token的句法头",{"2":{"20":1}}],["无奈时候的自己",{"2":{"2056":1}}],["无人工场",{"2":{"1936":1}}],["无缓冲",{"2":{"1811":1,"1829":1}}],["无重复元素",{"2":{"1806":1}}],["无序容器",{"2":{"1795":1}}],["无纯虚函数",{"2":{"1693":1}}],["无效的选择",{"2":{"1608":1}}],["无符号类型",{"2":{"1607":1}}],["无参数",{"2":{"1824":1,"1842":1}}],["无参",{"2":{"1481":1}}],["无分支",{"2":{"1442":1}}],["无监督学习",{"2":{"1312":1}}],["无振幅缩放",{"2":{"1241":1}}],["无意间的同步屏障干扰数据管道预读取",{"2":{"1161":1}}],["无梯度模式可能非常有用",{"2":{"1120":1}}],["无梯度模式",{"0":{"1120":1},"2":{"1118":1,"1119":1}}],["无缝整合",{"2":{"1086":1}}],["无数的资源积累",{"2":{"908":1}}],["无损压缩",{"2":{"563":1}}],["无迭代",{"2":{"427":1}}],["无",{"2":{"427":1}}],["无关",{"2":{"338":1,"504":1,"1775":1}}],["无影寺",{"2":{"233":1}}],["无需参数",{"2":{"2063":1}}],["无需",{"2":{"1925":1}}],["无需复制",{"2":{"1650":1}}],["无需重复编写声明",{"2":{"1628":1}}],["无需手动分配内存",{"2":{"1797":1}}],["无需手动分配和释放内存",{"2":{"1713":1}}],["无需手动处理字符串的长度和结尾的空字符",{"2":{"1624":1}}],["无需手动调整梯度",{"2":{"1044":1}}],["无需连续",{"0":{"982":1}}],["无需进行任何更改",{"2":{"976":1}}],["无需进行大量的重新训练",{"2":{"204":1}}],["无需再变化",{"2":{"898":1}}],["无需tokenizer的架构来了",{"2":{"638":1}}],["无需归一化层的transformer架构",{"2":{"361":1}}],["无需担心数据的原始分布被破坏",{"2":{"313":1}}],["无需额外资源消耗",{"2":{"233":1}}],["无需对文本进行窗口切分",{"2":{"217":1}}],["无需增加参数数量",{"2":{"42":1}}],["无论你从哪个方向来看",{"2":{"2115":1}}],["无论你使用哪种操作系统",{"2":{"1605":1}}],["无论数据来源是文件",{"2":{"1810":1,"1828":1}}],["无论搜索算法如何",{"2":{"1144":1}}],["无论它们在序列中的位置如何",{"2":{"762":1}}],["无论它们在句子中的绝对位置如何",{"2":{"762":1}}],["无论在哪种上下文中",{"2":{"715":1}}],["无论使用哪种架构",{"2":{"715":1}}],["无论位置远近",{"2":{"709":1}}],["无论是否满足都要处理下一个饼干",{"2":{"2153":1}}],["无论是否发生oom",{"2":{"976":1}}],["无论是关于工作",{"2":{"2109":1}}],["无论是做选择",{"2":{"2108":1}}],["无论是选择日常任务的顺序",{"2":{"2107":1}}],["无论是函数还是变量",{"2":{"1654":1}}],["无论是绝对位置编码",{"2":{"766":1}}],["无论是在query和key的dot",{"2":{"757":1}}],["无论是训练和推理",{"2":{"426":1}}],["无论是batchnorm还是layernorm",{"2":{"312":1}}],["无论是模型最后一层的注意力头",{"2":{"122":1}}],["无论这两个词距离多远",{"2":{"249":1}}],["无论存储位置如何",{"2":{"136":1}}],["无论堆叠多少层",{"2":{"117":1}}],["无法创建普通用户",{"2":{"2064":1}}],["无法创建文件",{"2":{"1820":2,"1838":2}}],["无法通过cache访问storage的public函数",{"2":{"1867":1}}],["无法访问counter的public成员",{"2":{"1868":1}}],["无法访问继承自",{"2":{"1857":1}}],["无法访问基类的",{"2":{"1853":1,"1857":1,"1861":1}}],["无法访问子类私有成员",{"2":{"1784":1}}],["无法打开文件",{"2":{"1820":1,"1838":1}}],["无法继续",{"2":{"1814":1,"1832":1}}],["无法进行隐式类型转换",{"2":{"1685":1}}],["无法被继承",{"2":{"1656":1}}],["无法从外部访问",{"2":{"1655":1}}],["无法在合理的时间内计算完整离线评估集的指标",{"2":{"1165":1}}],["无法在由自动求导记录的计算中使用在推断模式下创建的张量",{"2":{"1121":1}}],["无法在token维度进行统计",{"2":{"326":1}}],["无法重叠通信延迟",{"2":{"976":1}}],["无法处理的不稳定影响",{"2":{"1180":1}}],["无法处理很长的输入序列",{"2":{"861":1}}],["无法处理一些非线性的特征",{"2":{"117":1}}],["无法将对",{"2":{"765":1}}],["无法对任意位置进行建模",{"2":{"749":1}}],["无法对单词的重要程度进行区分",{"2":{"252":1}}],["无法做到彻底的惩罚",{"2":{"746":1}}],["无法计算相对距离",{"2":{"745":1}}],["无法捕获的单词顺序会导致我们很难理解一句话的含义",{"2":{"744":1}}],["无法直接走官方售后",{"2":{"2051":1}}],["无法直接调用或访问",{"2":{"1874":1}}],["无法直接访问私有成员变量",{"2":{"1674":1,"1677":1}}],["无法直接访问",{"2":{"1655":1}}],["无法直接捕获每个词在序列中的位置信息",{"2":{"742":1}}],["无法直接用来推理",{"2":{"473":1}}],["无法像编码器",{"2":{"729":1}}],["无法用单一维度表达的",{"2":{"683":1}}],["无法调整或者训练",{"2":{"681":1}}],["无法承载更多信息",{"2":{"681":1}}],["无法描述语义",{"2":{"679":1}}],["无法选择",{"2":{"320":2}}],["无法确定贡献度",{"2":{"245":1}}],["无法充分利用多层表示的优势",{"2":{"117":1}}],["无限循环",{"2":{"1621":1}}],["无限接近0",{"2":{"71":1}}],["无限不相似",{"2":{"70":1}}],["无疑是通过多次投票降低这种概率",{"2":{"13":1}}],["用最小的饼干满足最小胃口的小孩",{"2":{"2151":1}}],["用强胶粘一下",{"2":{"2051":1}}],["用字符串",{"2":{"1824":1,"1842":1}}],["用字节级别表示文本并使用",{"2":{"606":1}}],["用友元函数实现立方体相加",{"0":{"1792":1},"2":{"1768":1}}],["用友元函数访问它的私有成员",{"0":{"1791":1},"2":{"1768":1}}],["用逗号分隔",{"2":{"1699":1,"1728":1}}],["用助记符代替二进制代码",{"2":{"1603":1}}],["用二进制代码表示指令",{"2":{"1603":1}}],["用哪查哪",{"2":{"1584":1}}],["用途",{"2":{"1569":6,"1698":1,"2072":1}}],["用一个特定值替代范围内的所有元素",{"2":{"1744":1}}],["用一个函数去近似另一个函数",{"2":{"1377":1}}],["用一对花括号",{"2":{"1729":1}}],["用一堆非线性网络去仿真恒等映射是很困难的",{"2":{"301":1}}],["用另一个分布",{"2":{"1377":1}}],["用另一种方式整合多头注意力",{"2":{"47":1}}],["用概率的语言描述就是",{"2":{"1377":1}}],["用oor",{"2":{"1330":1}}],["用原score代替",{"2":{"1330":1}}],["用我们的图像读取工具来读取图片",{"2":{"1250":1}}],["用register",{"2":{"1208":1}}],["用rnn实现编码器和解码器",{"2":{"283":1}}],["用起来不方便",{"2":{"1109":1}}],["用作条件表达式的条件",{"2":{"1085":1}}],["用np",{"2":{"1083":1}}],["用netron打开查看",{"2":{"930":1}}],["用torch",{"0":{"1069":1}}],["用余下神经元所构成网络来继续训练",{"2":{"1017":1}}],["用很小的计算代价实现了降维",{"2":{"816":1}}],["用在后面测试时用",{"2":{"807":1}}],["用卷积提取特征",{"2":{"771":1}}],["用过在注意力计算时候再把相对位置信息加上",{"2":{"757":1}}],["用这部分表征作为思维链会提升表征的质量",{"2":{"731":1}}],["用这个全连接层的参数作为字",{"2":{"694":1}}],["用这个query和每个key通过内积的方式来计算出相似度或者相关性",{"2":{"168":1}}],["用户可以选择进行哪种转换",{"2":{"1608":1}}],["用户可以选择性地传递一个可映射对象给state",{"2":{"1214":1}}],["用户名>",{"2":{"2069":1}}],["用户名",{"2":{"1594":1}}],["用户不需要记住",{"2":{"1589":1}}],["用户不应依赖此特性",{"2":{"1114":1}}],["用户管理",{"0":{"1530":1}}],["用户和组管理",{"0":{"1529":1},"1":{"1530":1}}],["用户程序及数据目录",{"2":{"1506":1}}],["用户",{"2":{"1506":1}}],["用户应该实现此函数",{"2":{"1214":1}}],["用户消息",{"2":{"986":1}}],["用户行为等等",{"2":{"696":1}}],["用户输入自然语言句子",{"2":{"445":1}}],["用离散token表示",{"2":{"637":1}}],["用语言模型来估计每个子词在语料上的概率",{"2":{"602":1}}],["用词作为切分粒度",{"2":{"565":1}}],["用上面计算结果来顺序运行两个函数",{"2":{"523":1}}],["用数学的方法设计出了一个更好的网络",{"2":{"498":1}}],["用数学公式展示如下",{"2":{"394":1}}],["用lie",{"2":{"498":1}}],["用高维概率向量编织起来事物之间的各种复杂关系",{"2":{"472":1}}],["用真心去感受",{"2":{"2054":1}}],["用真实目标序列和掩码作为输入来指导解码器的生成过程",{"2":{"410":1}}],["用真值纠正",{"2":{"407":2}}],["用反向传播算法来略微调整所有模型的权重",{"2":{"398":1}}],["用x∈bx∈b",{"2":{"343":1}}],["用前n",{"2":{"288":1}}],["用predict来编码下一个词的分布",{"2":{"288":1}}],["用value来编码上下文信息",{"2":{"288":1}}],["用value来编码下一个词的分布和上下文信息",{"2":{"288":1}}],["用key来计算注意力向量",{"2":{"288":2}}],["用范伟的话说",{"2":{"280":1}}],["用输入的query",{"2":{"276":1}}],["用q",{"2":{"265":1}}],["用query最后一个维度的大小来给d",{"2":{"199":1}}],["用query与数据库内所有keys进行计算相似度",{"2":{"164":1}}],["用下图总结了注意力模型的通用结构",{"2":{"263":1}}],["用线性",{"2":{"233":1}}],["用来去更多的地点",{"2":{"2130":1}}],["用来存储学生的姓名",{"2":{"1728":1}}],["用来控制移动平均的长度范围",{"2":{"1048":1}}],["用来计算实值向量之间的距离",{"2":{"692":1}}],["用来描述某个计算系统的计算能力",{"2":{"504":1}}],["用来定义token与整数之间的映射关系",{"2":{"363":1,"550":1}}],["用来定义输入序列的哪些部分被允许关注",{"2":{"84":1}}],["用来平衡残差分支和主干路",{"2":{"347":1}}],["用来确定任务模型需要关注x中的哪些向量",{"2":{"263":1}}],["用来直接将整个文本拼接成一个连续的序列",{"2":{"217":1}}],["用注意力权重p",{"2":{"199":1}}],["用",{"0":{"1202":1},"2":{"184":2,"200":1,"614":1,"933":1,"1083":1,"1132":1,"2139":3}}],["用两个元素之间的关系",{"2":{"174":1}}],["用权重和v向量相乘",{"2":{"170":1}}],["用通俗例子来讲解",{"2":{"164":1}}],["用以遍历模块的buffers",{"2":{"1214":1}}],["用以保证在使用超网络更新内在相关知识的同时",{"2":{"142":1}}],["用以存储一个特定模式的权重会分布的存于不同层",{"2":{"129":1}}],["用以近似输入数据的高维映射",{"2":{"116":1}}],["用mha+ffn来解决不能用已有运算符来表达的计算和变化",{"2":{"120":1}}],["用mask让这些无效区域不参与运算",{"2":{"55":1}}],["用于描述曲面",{"2":{"2009":1}}],["用于描述深度学习模型的计算过程",{"2":{"1287":1}}],["用于编写自定义的顶点和片段着色器",{"2":{"2009":1}}],["用于模拟随机事件和逼近复杂光照模型",{"2":{"2009":1}}],["用于模拟物体之间的相互作用",{"2":{"2009":1}}],["用于模拟真实的光照环境",{"2":{"2009":1}}],["用于模型初始化",{"2":{"1169":1}}],["用于更方便地管理外部依赖",{"2":{"1990":1}}],["用于更新参数",{"2":{"1223":1}}],["用于管理外部项目的构建",{"2":{"1989":1}}],["用于管理动态分配的内存",{"2":{"1695":1}}],["用于统计单词总数",{"2":{"1933":1}}],["用于文件操作",{"2":{"1933":1}}],["用于文本选择和批量操作",{"2":{"1551":1}}],["用于文本选择操作",{"2":{"1541":1}}],["用于文本编辑",{"2":{"1519":1}}],["用于检查头文件是否存在",{"2":{"1931":1}}],["用于标记不推荐使用的函数",{"2":{"1909":1}}],["用于标记字符串的结束位置",{"2":{"1704":1}}],["用于展示右值引用",{"2":{"1887":1}}],["用于访问或操作与流关联的底层缓冲区",{"2":{"1824":1,"1842":1}}],["用于向字符串写入数据",{"2":{"1823":1,"1841":1}}],["用于向文件写入数据",{"2":{"1819":1,"1837":1}}],["用于输出流",{"2":{"1821":2,"1839":2}}],["用于输入流",{"2":{"1821":2,"1839":2}}],["用于输入和编辑文本",{"2":{"1541":1}}],["用于从字符串读取数据",{"2":{"1823":1,"1841":1}}],["用于从文件读取数据",{"2":{"1819":1,"1837":1}}],["用于从函数中返回一个值给调用者",{"2":{"1729":1}}],["用于从函数中返回",{"2":{"1631":1}}],["用于读写文件",{"2":{"1818":1,"1836":1}}],["用于扩展类的访问权限",{"2":{"1769":1}}],["用于抛出一个异常对象",{"2":{"1762":1}}],["用于捕获并处理特定类型的异常",{"2":{"1762":1}}],["用于包含可能抛出异常的代码",{"2":{"1762":1}}],["用于给定范围填充连续递增的值",{"2":{"1747":1}}],["用于与用户进行交互",{"2":{"1729":1}}],["用于与query进行比较",{"2":{"162":1}}],["用于判断一个年份是否为闰年",{"2":{"1729":1}}],["用于判断某个值是否存在于",{"2":{"1083":1}}],["用于替换函数定义中的形式参数",{"2":{"1729":1}}],["用于操作结构体内部的数据",{"2":{"1728":1}}],["用于操作私有成员",{"2":{"1677":1}}],["用于自定义对象的输入和输出格式",{"2":{"1712":1}}],["用于自动补全",{"2":{"894":1,"906":1}}],["用于比较对象",{"2":{"1712":1}}],["用于比较两个表达式的值",{"2":{"1630":1}}],["用于减少函数调用的开销",{"2":{"1709":1}}],["用于隐藏类的内部实现细节",{"2":{"1677":1}}],["用于高效地初始化成员变量",{"2":{"1674":1}}],["用于释放通过",{"2":{"1669":2}}],["用于释放",{"2":{"1668":1}}],["用于释放由",{"2":{"1647":2}}],["用于重新分配内存",{"2":{"1668":1}}],["用于动态分配并初始化为零",{"2":{"1668":1}}],["用于动态分配内存",{"2":{"1668":1}}],["用于分配数组的内存",{"2":{"1668":1}}],["用于分配单个对象的内存",{"2":{"1668":1}}],["用于分布式并行计算",{"2":{"1569":1}}],["用于打印交通工具的型号名称和最大速度",{"2":{"1664":1}}],["用于初始化对象的成员变量",{"2":{"1674":1}}],["用于初始化成员变量",{"2":{"1641":1}}],["用于初始化嵌入矩阵的权重",{"2":{"702":1}}],["用于跳过当前循环体中",{"2":{"1631":1}}],["用于立即退出",{"2":{"1631":1}}],["用于终止",{"2":{"1631":1}}],["用于整数",{"2":{"1630":1}}],["用于移除或添加",{"2":{"1629":1}}],["用于安全的向下转型",{"2":{"1629":1}}],["用于良性且可预测的类型转换",{"2":{"1629":1}}],["用于组合多个条件表达式",{"2":{"1619":1}}],["用于构建条件表达式",{"2":{"1619":1}}],["用于避免某些编译器将",{"2":{"1616":1}}],["用于设置输出精度",{"2":{"1608":1}}],["用于区分不同类型的消息",{"2":{"1576":1}}],["用于点对点通信",{"2":{"1575":1}}],["用于多个进程之间的数据传输和聚合",{"2":{"1573":1}}],["用于gpu加速的并行计算",{"2":{"1569":1}}],["用于学习和小规模并行计算",{"2":{"1569":1}}],["用于执行清理工作",{"2":{"1676":1}}],["用于执行清理操作",{"2":{"1674":1}}],["用于执行命令操作",{"2":{"1541":1}}],["用于执行评估的",{"2":{"1164":1}}],["用于导航和命令操作",{"2":{"1519":1}}],["用于对范围内的元素进行累加",{"2":{"1746":1}}],["用于对函数进行估计或近似",{"2":{"1456":1}}],["用于对loss进行正则化",{"2":{"385":1}}],["用于传播误差的权重系数",{"2":{"1443":1}}],["用于存储键和值的对应关系",{"2":{"1807":1}}],["用于存储小狗的名字和年龄",{"2":{"1674":1}}],["用于存储一串字符",{"2":{"1624":1}}],["用于存储逻辑值",{"2":{"1607":1}}],["用于存储带有小数部分的数值",{"2":{"1607":1}}],["用于存储单个字符",{"2":{"1607":1}}],["用于存储整数",{"2":{"1607":1}}],["用于存储结果的字典",{"2":{"1566":1}}],["用于存储数据",{"2":{"1436":1}}],["用于存储提示和生成结果的kv",{"2":{"985":1}}],["用于表示反斜杠自身",{"2":{"1616":1}}],["用于表示模型的计算图及其相关操作",{"2":{"1290":1}}],["用于表示和转换pytorch模型",{"2":{"1290":2}}],["用于优化模型的训练过程",{"2":{"1229":1}}],["用于插入",{"2":{"1227":1}}],["用于pytorch2",{"2":{"1227":1}}],["用于load",{"2":{"1226":2}}],["用于step",{"2":{"1226":1}}],["用于创建对象的副本",{"2":{"1675":1}}],["用于创建模块的副本",{"2":{"1214":1}}],["用于创建子类时使用",{"2":{"1086":1}}],["用于遍历",{"2":{"1933":1}}],["用于遍历网络中的所有模块",{"2":{"1214":2}}],["用于遍历直接子模块",{"2":{"1214":2}}],["用于遍历模块的参数",{"2":{"1214":1}}],["用于生成模块的各种名称和成员的辅助方法",{"2":{"1214":1}}],["用于生成一条平滑曲线",{"2":{"155":1}}],["用于在像素网格中高效地绘制直线和圆等基本图形",{"2":{"2009":1}}],["用于在循环内部更简洁地处理插入",{"2":{"1933":1}}],["用于在程序中唯一地识别该函数",{"2":{"1729":1}}],["用于在堆区动态地分配一块数组的内存空间",{"2":{"1647":1}}],["用于在堆区",{"2":{"1647":1}}],["用于在预编译阶段进行文本替换",{"2":{"1632":1}}],["用于在双引号包围的字符串字面量中表示双引号自身",{"2":{"1616":1}}],["用于在单引号包围的字符常量中表示单引号自身",{"2":{"1616":1}}],["用于在注册非完整反向传播钩子时发出警告",{"2":{"1214":1}}],["用于在神经元之间引入非线性关系",{"2":{"102":1}}],["用于数据混洗和预处理",{"2":{"1169":1}}],["用于定期评估的数据集应该足够小",{"2":{"1165":1}}],["用于定义自定义的分发逻辑",{"2":{"1083":1}}],["用于",{"2":{"1161":1,"1226":3,"1616":1,"1814":1,"1832":1}}],["用于确定模型的大小和其他细节",{"2":{"1129":1}}],["用于计算复数张量的共轭张量",{"2":{"1086":1}}],["用于计算损失",{"2":{"385":1}}],["用于通过索引或切片操作访问",{"2":{"1085":1}}],["用于通过在训练期间随机将一部分模型权重设置为零来防止过拟合",{"2":{"469":1}}],["用于指示流的状态",{"2":{"1814":1,"1832":1}}],["用于指示",{"2":{"1083":1}}],["用于原地修改",{"2":{"1083":1}}],["用于将张量的内存共享给其他进程",{"2":{"1214":1}}],["用于将",{"2":{"1083":1}}],["用于将一个稠密",{"2":{"1083":1}}],["用于将一个连续的一维",{"2":{"1083":1}}],["用于将输入字节编码为patch表示",{"2":{"614":1}}],["用于返回一个描述异常的",{"2":{"1763":1}}],["用于返回模块的额外表示形式",{"2":{"1214":1}}],["用于返回",{"2":{"1083":1}}],["用于使",{"2":{"1083":1}}],["用于实现泛型编程",{"2":{"1632":1}}],["用于实现移动语义",{"2":{"1629":1}}],["用于实现右侧矩阵乘法运算符",{"2":{"1083":1}}],["用于实现整数除法运算",{"2":{"1083":1}}],["用于实现就地真除法运算符",{"2":{"1083":1}}],["用于最小化",{"2":{"1023":1}}],["用于加速机器学习工作负载中最重要的操作",{"2":{"973":1}}],["用于决定要忘记多少过去的信息",{"2":{"874":1}}],["用于图像上",{"2":{"808":1}}],["用于控制路由权重的贡献",{"2":{"739":1}}],["用于裁剪范数的类型",{"2":{"702":1}}],["用于填充",{"2":{"702":1}}],["用于任意形状的输入到输出的线性变换",{"2":{"702":1}}],["用于逼近数据分布q",{"2":{"633":1}}],["用于解决",{"2":{"1695":1}}],["用于解决llms作为嵌入模型的问题",{"2":{"737":1}}],["用于解决日语和韩语的语音问题",{"2":{"597":1}}],["用于解码下一个patch的字节",{"2":{"614":1}}],["用于获取数据类型或变量所占用的内存大小",{"2":{"1607":1}}],["用于获取",{"2":{"1083":1}}],["用于获取输出句子",{"2":{"535":1}}],["用于获取输入序列中不同单词之间的相关性",{"2":{"517":1}}],["用于获得自适应的有效构建块",{"2":{"224":1}}],["用于保存",{"2":{"1519":1}}],["用于保存预测结果",{"2":{"428":1}}],["用于保存一些训练状态",{"2":{"385":1}}],["用于调整adam的学习率",{"2":{"385":1}}],["用于调整模型的",{"2":{"187":1}}],["用于提升深层",{"2":{"347":1}}],["用于信息传输通路只有一条",{"2":{"255":1}}],["用于处理需要多层迭代的情况",{"2":{"1631":1}}],["用于处理更复杂的条件判断",{"2":{"1631":1}}],["用于处理连续的",{"2":{"204":1}}],["用于处理不同块之间的标记",{"2":{"204":1}}],["用于推断llms的上下文窗口",{"2":{"204":1}}],["用于添加随机性",{"2":{"198":1}}],["用于遮掩某些位置",{"2":{"198":1}}],["用于忽略填充部分",{"2":{"74":2}}],["用了recompute",{"2":{"945":1,"965":1}}],["用了",{"2":{"83":1}}],["用加权求和取代了多头注意力的标准求和",{"2":{"42":1}}],["用程序化的语言来说",{"2":{"3":1}}],["带的团队能差吗",{"2":{"2051":1}}],["带他去了另一家",{"2":{"2051":1}}],["带着室友离开了",{"2":{"2051":1}}],["带室友修电脑",{"0":{"2051":1},"2":{"2046":1,"2050":1}}],["带入数据",{"2":{"1394":1}}],["带头进行的",{"2":{"1316":1}}],["带",{"2":{"1244":1}}],["带scatter通信的",{"2":{"1087":1}}],["带nesterov",{"2":{"1049":1}}],["带了lr",{"2":{"1046":1}}],["带动量的随机梯度下降可以平滑参数更新的路径",{"2":{"1032":1}}],["带动量的随机梯度下降可以加速模型的收敛速度",{"2":{"1032":1}}],["带动量的随机梯度下降可以加速参数更新的速度",{"2":{"1031":1}}],["带动量的随机梯度下降能够在搜索空间中跳出局部最小值并继续寻找更好的解",{"2":{"1032":1}}],["带动量的随机梯度下降利用一个动量变量来积累梯度的历史信息",{"2":{"1031":1}}],["带单个参数会略微增加网络的大小",{"2":{"838":1}}],["带来了许多令人兴奋的新特性",{"2":{"1932":1}}],["带来了过多的资源消耗",{"2":{"624":1}}],["带来的梯度问题",{"2":{"621":1}}],["带来更好的性能",{"2":{"41":1}}],["带位置信息的词嵌入",{"2":{"460":1}}],["带宽",{"2":{"260":1}}],["带有参数的构造函数",{"2":{"1675":1}}],["带有目的性的关注度",{"2":{"163":1}}],["带有强连通图掩码的transformer是sequence",{"2":{"93":1}}],["带有layernorm的自注意力动态可以同时拥有从1到满秩的任意秩的平衡点",{"2":{"92":1}}],["带有子语义逻辑的",{"2":{"28":1}}],["带layernorm的掩码注意力",{"0":{"94":1},"2":{"49":1}}],["作业内容",{"0":{"1872":1},"1":{"1873":1,"1874":1}}],["作业目标",{"0":{"1871":1}}],["作业要求",{"2":{"1825":1,"1843":1}}],["作业提示",{"2":{"1766":1}}],["作业描述",{"0":{"1689":1},"1":{"1690":1,"1691":1}}],["作业是写一个文本文件",{"2":{"1616":1}}],["作业",{"0":{"1608":1,"1642":1,"1651":1,"1679":1,"1696":1,"1702":1,"1710":1,"1716":1,"1726":1,"1759":1,"1790":1,"1791":1,"1792":1,"1808":1,"1825":1,"1843":1,"1902":1,"1914":1,"1918":1,"1998":1},"1":{"1680":1,"1791":1,"1792":1},"2":{"1634":1,"1729":1,"1768":1,"2063":1}}],["作用域与变量生命周期",{"0":{"2003":1}}],["作用域",{"2":{"1649":1}}],["作用域解析运算符",{"2":{"1630":1,"1712":1}}],["作用同编码器的ffn",{"2":{"525":1}}],["作用同第一个layer",{"2":{"517":1}}],["作用同第一个残差连接",{"2":{"517":1}}],["作用方向",{"0":{"323":1},"2":{"293":1}}],["作用对象",{"0":{"322":1},"2":{"293":1}}],["作用",{"0":{"314":1,"320":1,"441":1,"1032":1},"1":{"442":1,"443":1,"444":1},"2":{"293":2,"807":1,"813":1,"814":1,"816":1,"1544":1,"1545":1,"1547":1,"1548":1,"1549":1,"1550":1,"1551":1,"1613":1,"1645":1,"1693":1}}],["作用是把src中的盖住",{"2":{"380":1}}],["作用是对目标语言做sequence",{"2":{"82":1}}],["作用是对目标语言做",{"2":{"82":1}}],["作用是增加了一个维度",{"2":{"66":1}}],["作三次线性投影",{"2":{"172":1}}],["作为分隔符",{"2":{"1910":1}}],["作为private成员访问",{"2":{"1861":1}}],["作为protected成员访问",{"2":{"1857":1}}],["作为函数参数和返回值",{"2":{"1728":1}}],["作为函数返回值",{"2":{"1612":1}}],["作为函数返回类型",{"2":{"1611":1}}],["作为成员函数",{"2":{"1712":1}}],["作为结束标志",{"2":{"1624":1}}],["作为参数类型",{"2":{"1906":1}}],["作为参数被调用",{"2":{"1227":2}}],["作为参数训练出来",{"2":{"762":1}}],["作为起点",{"2":{"1157":1}}],["作为计算需要",{"2":{"1117":1}}],["作为梯度",{"2":{"1115":1}}],["作为其中的一部分",{"2":{"973":1}}],["作为其跨度集合",{"2":{"485":1}}],["作为激活函数",{"2":{"840":1}}],["作为全连接层的替代操作",{"2":{"816":1}}],["作为目标函数训练lm",{"2":{"721":1}}],["作为目标函数",{"2":{"721":1}}],["作为副词",{"2":{"683":1}}],["作为形容词",{"2":{"683":1}}],["作为名词",{"2":{"683":1}}],["作为模型参数",{"2":{"621":1}}],["作为该子词的得分",{"2":{"602":1}}],["作为该架构的代表性语言模型",{"2":{"541":1}}],["作为解码器的组成单元",{"2":{"533":1}}],["作为编码器的组成单元",{"2":{"523":1}}],["作为query",{"2":{"517":1}}],["作为outputs",{"2":{"453":1}}],["作为inputs传给encoder层",{"2":{"453":1}}],["作为最有可能出现的下一个单词",{"2":{"431":1}}],["作为之后decoder的输入",{"2":{"428":1}}],["作为正则化技术来防止过拟合",{"2":{"399":1}}],["作为输入",{"2":{"528":1}}],["作为输入序列来生成输出序列的下一个",{"2":{"515":1}}],["作为输入传递给解码器的只有输入序列",{"2":{"389":1}}],["作为输入参数",{"2":{"343":1,"344":1}}],["作为归一化层的替代方案",{"2":{"360":1}}],["作为隐状态",{"2":{"273":1}}],["作为隐式函数",{"2":{"122":1}}],["作为下游任务模型的输入",{"2":{"718":1}}],["作为下一次的输入",{"2":{"407":1,"453":1,"529":1}}],["作为下一次推理的增加输入",{"2":{"406":1}}],["作为下一轮的输入",{"2":{"239":1}}],["作为下当前时刻ttt的输入之一",{"2":{"241":1}}],["作为一个目标超参数",{"2":{"1143":1}}],["作为一个抽象符号",{"2":{"545":1}}],["作为一个理想的知识库",{"2":{"140":1}}],["作为一种更具可行性的解决方案使用核化特征映射替换",{"2":{"212":1}}],["作为庞大的知识库",{"2":{"140":1}}],["作为这类方法的代表性技术路线",{"2":{"138":1}}],["作为",{"2":{"78":1,"200":1,"621":1,"889":1,"1611":1}}],["作为key和values",{"2":{"735":1}}],["作为key和value",{"2":{"39":1}}],["作弊",{"2":{"58":1,"525":1,"528":1}}],["作者没有具体的解释",{"2":{"1336":1}}],["作者是transformer的原班人马",{"2":{"759":1}}],["作者们利用",{"2":{"739":1}}],["作者们称这种方法为",{"2":{"739":1}}],["作者们提出了一种称为",{"2":{"739":1}}],["作者们发现",{"2":{"739":1}}],["作者训练了一个小字节级自回归语言模型",{"2":{"613":1}}],["作者之所以这样设计",{"2":{"525":1}}],["作者依据自己的思考对该比喻做进一步调节",{"2":{"524":1}}],["作者选择了三个模型进行分析",{"2":{"359":1}}],["作者选择用参数样条函数替代参数+激活函数的组合",{"2":{"155":1}}],["作者引入了一个可训练的缩放参数来调整",{"2":{"352":1}}],["作者借此将transformer的深度扩充到了1000层",{"2":{"347":1}}],["作者使用cnn来做编码器和解码器",{"2":{"290":1}}],["作者使用注意力机制打破了传统rnn方案中依赖固定长度上下文信息的限制",{"2":{"284":1}}],["作者又继续进行改进",{"2":{"288":1}}],["作者对原始模型的输出做进一步划分为三部分",{"2":{"288":1}}],["作者构建的注意力机制把编码器所有的隐状态都利用起来",{"2":{"284":1}}],["作者认为目前注意力机制的隐向量承担了太多功能",{"2":{"288":1}}],["作者认为我们在顺序地阅读每个单词的时候",{"2":{"287":1}}],["作者认为不合理",{"2":{"284":1}}],["作者认为该固定长度的上下文信息限制了模型性能",{"2":{"284":1}}],["作者认为在翻译过程中",{"2":{"284":1}}],["作者认为kans是样条",{"2":{"155":1}}],["作者在论文中使用注意力机制解决了如何建模",{"2":{"284":1}}],["作者在q",{"2":{"41":1}}],["作者给出的解释是rl对数据集的要求更低",{"2":{"224":1}}],["作者设计了可学习的幂次来捕捉每个维度的不同重要性",{"2":{"213":1}}],["作者采用通道级可学习的幂函数进行重新缩放",{"2":{"213":1}}],["作者提供了数学理论基础",{"2":{"213":1}}],["作者提出对矩阵wuwuw",{"2":{"356":1}}],["作者提出对wkwkw",{"2":{"355":1}}],["作者提出在训练算法的每一步之后",{"2":{"352":1}}],["作者提出将构成网络矩阵嵌入维度的所有向量归一化",{"2":{"352":1}}],["作者提出了一种极性感知线性注意力",{"2":{"211":1}}],["作者提出了一种名为mohsa",{"2":{"41":1}}],["作者提出了",{"2":{"89":1}}],["作者提出信息交换在视觉",{"2":{"41":1}}],["作者沿着通道维度拆分",{"2":{"213":1}}],["作者将其改写为",{"2":{"354":1}}],["作者将上一层学习到的结果直接与新加入层的学习结果相加",{"2":{"304":1}}],["作者将原始的kolmogorov",{"2":{"155":1}}],["作者将",{"2":{"145":1}}],["作者假设这个位置的中间层mlp存储了事实之间的关联信息",{"2":{"145":1}}],["作者通过实验发现在subject的最后一个token中",{"2":{"145":1}}],["作者通过知识三元组",{"2":{"145":1}}],["作者通过构建非平凡的反例",{"2":{"92":1}}],["作者发现了一个关键的优化机会",{"2":{"985":1}}],["作者发现每个键向量至少对应一种人类可解读的模式",{"2":{"127":1}}],["作者发现尽管纯掩码注意力仍然会指数级地崩溃到一个秩为1的子空间",{"2":{"91":1}}],["作者强调了使用kolmogorov",{"2":{"155":1}}],["作者强调",{"2":{"101":1}}],["作者展示了一个普适性的结果",{"2":{"94":1}}],["作者展示了反例",{"2":{"94":1}}],["作者讨论了以下几个有趣的含义",{"2":{"93":1}}],["作者首先提出了奇异值微调",{"2":{"224":1}}],["作者首先分析不带layernorm的情况",{"2":{"93":1}}],["作者首先展示了对于某些类别的值矩阵",{"2":{"91":1}}],["作者的结果反驳了之前关于layernorm在自注意力秩崩溃中不起作用的假设",{"2":{"91":1}}],["作者证明了在适当选择值矩阵的情况下",{"2":{"91":1}}],["作者分析了不同的",{"2":{"90":1}}],["作者总结了不同",{"2":{"90":1}}],["作者",{"2":{"47":1,"233":2}}],["之",{"2":{"2043":1}}],["之外",{"2":{"1622":1}}],["之一",{"2":{"1236":1}}],["之类的工具对输入管道预读取数据",{"2":{"1161":1}}],["之类的问题",{"2":{"121":1}}],["之上的",{"2":{"230":1}}],["之后以并发方式来加以解决",{"2":{"1564":1}}],["之后详述",{"2":{"1320":1}}],["之后将会被调用",{"2":{"1227":1}}],["之后调用该函数",{"2":{"1223":1}}],["之后可以切换到更通用的优化器",{"2":{"1130":1}}],["之后再构建自定义模型也是可行的",{"2":{"1129":1}}],["之后再反正切",{"2":{"1087":1}}],["之后再进行卷积来实现",{"2":{"779":1}}],["之后更是蔓延到了vision",{"2":{"911":1}}],["之后每一步根据评估准则不断丢弃词表中的子词",{"2":{"601":1}}],["之后构成word",{"2":{"515":1}}],["之后要进行梯度下降",{"2":{"343":1}}],["之后引入两个可训练参数α",{"2":{"319":1}}],["之后",{"2":{"204":1,"516":1,"577":1,"909":1,"986":1,"1127":1,"1227":1,"1630":1,"2054":1}}],["之后的函数调用不会再次初始化",{"2":{"1649":1}}],["之后的语句",{"2":{"1631":1}}],["之后的一项研究表明",{"2":{"1315":1}}],["之后的字节要困难得多",{"2":{"613":1}}],["之后的单词",{"2":{"532":1}}],["之后的部分尚未发生",{"2":{"59":1}}],["之后的输出",{"2":{"59":1,"934":1}}],["之后的信息给隐藏起来",{"2":{"59":1,"69":1,"934":1}}],["之所以要独立出一个",{"2":{"1478":1}}],["之所以要这么处理",{"2":{"34":1}}],["之所以没有被关注到",{"2":{"935":1,"951":1}}],["之所以不对",{"2":{"396":1}}],["之所以能更高效",{"2":{"346":1}}],["之所以效果没有ln好",{"2":{"326":1}}],["之所以这样可以获取",{"2":{"199":1}}],["之前或之后",{"2":{"1662":1}}],["之前版本异同",{"0":{"1363":1}}],["之前将会被调用",{"2":{"1227":1}}],["之前最大限度地改进模型",{"2":{"1139":1}}],["之前普遍认为仅解码器模型不能用于嵌入提取",{"2":{"729":1}}],["之前随着推理硬件性能的提升和各种优化方式的出现",{"2":{"626":1}}],["之前时间步的激活状态会作为输入参与到当前数据的处理中",{"2":{"249":1}}],["之前时间步的记忆会作为输入参与到当前数据的处理中",{"2":{"248":1}}],["之前写得一些草稿也没有时间整理",{"2":{"235":1}}],["之前",{"2":{"193":1,"1227":1,"1921":1,"1922":1,"1923":1,"1925":1,"1931":1}}],["之前篇幅中介绍过qkv",{"2":{"162":1}}],["之前已经有使用kolmogorov",{"2":{"155":1}}],["之前的值都不再有效",{"2":{"1728":1}}],["之前的研究通过采用更高效的自注意力机制或无注意力架构来缓解这些问题",{"2":{"612":1}}],["之前的预处理步骤",{"2":{"553":1}}],["之前的注意力机制都关注不同序列之间的注意力",{"2":{"287":1}}],["之前的线性注意力方法",{"2":{"213":1}}],["之前的token而变化",{"2":{"145":1}}],["之前的部分",{"2":{"82":1}}],["之前由于计算成本较高",{"2":{"106":1}}],["之间数据交互的机制",{"2":{"1810":1,"1828":1}}],["之间无密码运行",{"2":{"1594":1}}],["之间",{"2":{"865":1,"866":1}}],["之间且总和为1的概率分布",{"2":{"847":1}}],["之间形状有何关系",{"2":{"773":1}}],["之间也存在连线",{"2":{"1324":1}}],["之间也无关系",{"2":{"709":1}}],["之间也有dropout",{"2":{"394":1}}],["之间距离的最标准方法",{"2":{"692":1}}],["之间具有相关性",{"2":{"642":1}}],["之间通过注意力机制来进行交互",{"2":{"624":1}}],["之间存在着密切的关系",{"2":{"496":1}}],["之间路由信息",{"2":{"461":1}}],["之间做标准化就更加合理",{"2":{"326":1}}],["之间高效协作",{"2":{"217":1}}],["之间依赖关系的理解",{"2":{"209":1}}],["之间共享的神经元",{"2":{"135":1}}],["之间共享参数",{"2":{"99":1}}],["之间的所有奇数",{"2":{"1621":1}}],["之间的互斥访问",{"2":{"1414":1}}],["之间的内积得到",{"2":{"1344":1}}],["之间的内积操作可以被一个函数",{"2":{"1342":1}}],["之间的区别",{"2":{"1133":1}}],["之间的一种梯度下降优化算法",{"2":{"1027":1}}],["之间的范围",{"2":{"866":1}}],["之间的几何向量关系大致与",{"2":{"713":1}}],["之间的差异",{"2":{"398":1}}],["之间的部分差异",{"2":{"346":1}}],["之间的残差",{"2":{"301":1}}],["之间的交互",{"2":{"621":1}}],["之间的交互关系进行建模",{"2":{"246":1}}],["之间的交叉污染",{"2":{"89":1}}],["之间的桥梁",{"2":{"164":1}}],["之间的相互作用",{"2":{"148":1,"485":1}}],["之间的关系相同",{"2":{"713":1}}],["之间的关系",{"2":{"131":1}}],["之间的边就可以判断出某个边对于知识是否为关键边",{"2":{"130":1}}],["之间的有效信息交换",{"2":{"41":1}}],["之和",{"2":{"46":1}}],["人生处处是算法",{"2":{"2111":1}}],["人生也就到中后期了",{"2":{"1598":1}}],["人",{"2":{"2054":1}}],["人不知",{"2":{"2054":2}}],["人不知而不愠",{"2":{"2053":2,"2054":1}}],["人的本性",{"0":{"2111":1},"1":{"2112":1,"2113":1,"2114":1,"2115":1,"2116":1,"2117":1,"2118":1,"2119":1,"2120":1},"2":{"2043":1}}],["人的思维明显是分层的",{"2":{"626":1}}],["人性",{"2":{"1598":1}}],["人际关系",{"2":{"1598":1}}],["人工神经网络能够类似人一样具有简单的决定能力和简单的判断能力",{"2":{"1456":1}}],["人工神经网络",{"2":{"1456":1}}],["人工智能",{"2":{"1602":1}}],["人工智能是什么",{"0":{"1453":1}}],["人工智能与算法学习",{"2":{"513":1}}],["人为输入或随机输入数据",{"2":{"1371":1}}],["人物",{"2":{"712":1}}],["人脸识别等都有应用",{"2":{"696":1}}],["人是可以理解每个语言单词",{"2":{"689":1}}],["人是可以理解每个语言单词的意义的",{"2":{"545":1}}],["人力",{"2":{"498":1}}],["人也许可以在脑中构思整个句子",{"2":{"405":1}}],["人脑在受伤后能够重新连接自身神经回路",{"2":{"220":1}}],["人们还无法保证内存永不掉电",{"2":{"1477":1}}],["人们就提出了动态向量",{"2":{"715":1}}],["人们提出了交叉注意力来完成这个功能",{"2":{"536":1}}],["人们提出了稀疏计算",{"2":{"204":1}}],["人们使用了teacher",{"2":{"525":1}}],["人们可以将大语言模型的示例泛化归结为两体自旋模型",{"2":{"506":1}}],["人们可以获取非平衡神经动力学的稳态全貌以及隐藏的动力学相变",{"2":{"506":1}}],["人们用的都是strang",{"2":{"498":1}}],["人们引入了掩码",{"2":{"443":1}}],["人们不会一次处理整个场景",{"2":{"260":1}}],["人们也做出了很多精彩论断",{"2":{"258":1}}],["人们也对多头注意力进行了一些改进",{"2":{"40":1}}],["人们一般会堆叠更多的卷积",{"2":{"247":1}}],["人们往往先把所有输入编码到一个隐状态",{"2":{"245":1}}],["人们很容易想到马尔可夫假设",{"2":{"242":1}}],["人们对softmax也有很多改进",{"2":{"182":1}}],["人们对这些前馈神经网络类型的偏好也发生了变化",{"2":{"98":1}}],["人们采用了teacher",{"2":{"57":1}}],["人类大脑神经元大约只有",{"2":{"840":1}}],["人类的概念之所以如此复杂且有效",{"2":{"689":1}}],["人类的概念是一个及其复杂的系统",{"2":{"4":1}}],["人类将具象物理的世界抽象到主观认识中",{"2":{"689":1}}],["人类先进的预见能力必须依赖于同样先进的表征能力",{"2":{"689":1}}],["人类通常采用自上而下的流程",{"2":{"627":1}}],["人类学习能力是一个螺旋式的渐进过程",{"2":{"402":1}}],["人类有选择地将注意力集中在视觉空间的某些部分上",{"2":{"260":1}}],["人类感知的一个重要特性是",{"2":{"260":1}}],["人类在感知",{"2":{"257":1}}],["人类在读一篇文章时",{"2":{"167":1}}],["人类可以很容易的同时看到",{"2":{"246":1}}],["人类可以知道哪些词提供了上下文",{"2":{"167":1}}],["人类往往能够记住背离预期",{"2":{"230":1}}],["人类注意力机制本身就是天然可以同时处理多个方面的信息的",{"2":{"3":1}}],["里特定的定义",{"2":{"1769":1}}],["里边用到了一种更简单的相对位置编码",{"2":{"1340":1}}],["里面的人生道理是我们平常很难得到的",{"2":{"2056":1}}],["里面的道理了",{"2":{"2054":1}}],["里面还有一个叫",{"2":{"1275":1}}],["里面涉及到大量的特征工程",{"2":{"908":1}}],["里面存储了模型词汇表中每个",{"2":{"700":1}}],["里面是分好的词",{"2":{"557":1}}],["里面说明了玩具每个组件的用法",{"2":{"524":1}}],["里面包含了训练和使用训练好的模型进行推理",{"2":{"424":1}}],["里面分别是29000行德文和英文",{"2":{"370":1}}],["里面有很多书",{"2":{"169":1}}],["里面可能会包含多个不同的",{"2":{"89":1}}],["里包含多个样本",{"2":{"89":1}}],["里",{"2":{"37":1,"89":1,"344":2}}],["形参",{"2":{"1729":1}}],["形参是实参的一个别名",{"2":{"1650":1}}],["形参是一个指针变量",{"2":{"1650":1}}],["形容词性",{"2":{"712":1}}],["形容词",{"2":{"567":1}}],["形式参数成为实际参数的别名",{"2":{"1729":2}}],["形式参数",{"2":{"1729":1}}],["形式统一解决",{"2":{"1317":1}}],["形式2",{"2":{"889":1}}],["形式1",{"2":{"889":1}}],["形式上和前面公式",{"2":{"1344":1}}],["形式上与正弦位置编码有联系",{"2":{"759":1}}],["形式上如下",{"2":{"748":1}}],["形式上来看",{"2":{"747":1}}],["形式上",{"2":{"612":1,"761":1}}],["形式如下图标号4",{"2":{"209":1}}],["形式如下图标号3",{"2":{"209":1}}],["形式如下图标号2",{"2":{"209":1}}],["形式如下图标号1",{"2":{"209":1}}],["形式和理解",{"2":{"156":1}}],["形式为",{"2":{"145":1}}],["形状或其他条件来选择适当的实现",{"2":{"1083":1}}],["形状n",{"2":{"944":1}}],["形状为n",{"2":{"944":1}}],["形状为",{"2":{"84":6,"199":1,"399":2,"522":1,"944":1}}],["形状是torch",{"2":{"380":2}}],["形状是",{"2":{"36":1,"66":1,"198":2,"451":1,"704":1}}],["形而上学transformer",{"2":{"47":1}}],["形成继承链",{"2":{"1654":1}}],["形成",{"2":{"778":1}}],["形成了概念",{"2":{"689":1}}],["形成了一种很自然的契合",{"2":{"498":1}}],["形成一个有意义的新的数据类型",{"2":{"1728":1}}],["形成一个独立的单元",{"2":{"1674":1}}],["形成一个时间离散的动态系统",{"2":{"492":1}}],["形成一个完整的软件产品",{"2":{"5":1}}],["形成的矩阵可以同时从两个视角进行解释",{"2":{"485":1}}],["形成其关系图",{"2":{"472":1}}],["形成推理结果句子",{"2":{"431":1}}],["形成最终的mask",{"2":{"201":1}}],["形成参数",{"2":{"122":1}}],["形成上三角阵",{"2":{"74":1}}],["形成多个长度一样的句子",{"2":{"53":1}}],["形成若干独立的query",{"2":{"33":1}}],["形成对比赛的完整理解",{"2":{"5":1}}],["则补充了",{"2":{"1911":1}}],["则允许在捕获列表中创建新的变量",{"2":{"1907":1}}],["则通过泛型",{"2":{"1906":1}}],["则通过累加和乘积的形式",{"2":{"510":1}}],["则清空文件内容",{"2":{"1820":2,"1838":2}}],["则抛出一个自定义异常",{"2":{"1766":2}}],["则执行这里",{"2":{"1762":1}}],["则执行该",{"2":{"1762":1}}],["则执行循环体",{"2":{"1621":1}}],["则执行循环体中的代码",{"2":{"1620":1}}],["则调用时也需要使用空括号",{"2":{"1729":1}}],["则参数列表可以为空",{"2":{"1729":1}}],["则参数列表可以写为",{"2":{"1607":1}}],["则更进一步",{"2":{"1905":1}}],["则更常用于表示具有复杂行为和需要封装性的抽象数据类型",{"2":{"1728":1}}],["则更新",{"2":{"141":1}}],["则输出",{"2":{"1695":1}}],["则动态绑定不会发生",{"2":{"1688":1}}],["则无法进行隐式类型转换",{"2":{"1685":1}}],["则跳出循环",{"2":{"1620":1,"1621":1}}],["则跳过此步骤",{"2":{"1227":1}}],["则表示",{"2":{"1607":1}}],["则表示它们方向完全相反",{"2":{"692":1}}],["则表示它们是正交的",{"2":{"692":1}}],["则进行相加",{"2":{"1443":1}}],["则结果介于0和1之间",{"2":{"1373":1}}],["则说明",{"2":{"1373":1}}],["则说明对英语进行分词",{"2":{"557":1}}],["则说明对德语进行分词",{"2":{"557":1}}],["则称该问题是线性可分的",{"2":{"1462":1}}],["则称",{"2":{"1322":1}}],["则预测第一个模型的输出中哪些词语是被遮盖的",{"2":{"1315":1}}],["则state",{"2":{"1214":1}}],["则softmax重写为",{"2":{"191":1}}],["则不会自动归类",{"2":{"1210":1}}],["则划归于",{"2":{"1210":1}}],["则两个batch",{"2":{"1186":1}}],["则添加它们",{"2":{"1180":1}}],["则很有用",{"2":{"1179":1}}],["则保持第一轮中衰减大小的固定值",{"2":{"1159":1}}],["则max",{"2":{"1155":1}}],["则metaformer将分别成为一个transformer或类似mlp的模型",{"2":{"446":1}}],["则可能需要增加max",{"2":{"1183":1}}],["则可能需要扩展搜索空间边界",{"2":{"1147":1}}],["则可以换成伪随机均匀搜索",{"2":{"1176":1}}],["则可以很好地使用",{"2":{"1173":1}}],["则可以重新缩放",{"2":{"211":1,"213":1}}],["则搜索可能不够广泛",{"2":{"1146":1}}],["则任意选择一个",{"2":{"1115":1}}],["则该层的权重",{"2":{"1003":1}}],["则该节点可以被淘汰",{"2":{"986":1}}],["则n的大小表示一个输出值是由多少个输入值计算出来的",{"2":{"1003":1}}],["则n个单词组合数可能性太多",{"2":{"242":1}}],["则根据gorot",{"2":{"1000":1}}],["则loss",{"2":{"991":1}}],["则loss是这60个预测结果计算损失的和",{"2":{"398":1}}],["则loss是这6个预测结果计算损失的和",{"2":{"398":1}}],["则被归于",{"2":{"1210":1}}],["则被称为翻译模型",{"2":{"908":1}}],["则被分为两个token",{"2":{"456":1}}],["则这一步的损失就是它的负导数",{"2":{"899":1}}],["则这两个向量线性无关",{"2":{"176":1,"692":1}}],["则只有以start",{"2":{"828":1}}],["则内核元素之间没有插入空格",{"2":{"778":1}}],["则内存占用会大幅度降低",{"2":{"180":1}}],["则区间",{"2":{"765":1}}],["则采用了裁剪的机制",{"2":{"759":1}}],["则效果更好",{"2":{"731":1}}],["则整个字向量就是一个3000",{"2":{"709":1}}],["则embedding的输入输出如下",{"2":{"704":1}}],["则有两个缺点",{"2":{"698":1}}],["则此方法不但要能够表达出各种属性",{"2":{"689":1}}],["则特征维度的数目需要和字典中的token数量相同",{"2":{"684":1}}],["则特殊单词会加入到词典最前面specials",{"2":{"557":1}}],["则嵌入层会生成一个",{"2":{"674":1}}],["则打破了原有人们区别看待",{"2":{"624":1}}],["则过于冗长",{"2":{"607":1}}],["则使用当前子词替换w中的子字符串",{"2":{"587":1}}],["则使用pad",{"2":{"65":1}}],["则需要将特殊单词排除",{"2":{"557":1}}],["则每次应该是",{"2":{"520":1,"530":1}}],["则为新的batch",{"2":{"1135":1}}],["则为",{"2":{"511":1}}],["则令牌将收敛到共识均衡点",{"2":{"507":1}}],["则深度学习模型的层可以被视为范畴中的对象",{"2":{"505":1}}],["则从负责记忆的海马体",{"2":{"490":1}}],["则提高了低深度transformer在内在串行问题上的表达能力",{"2":{"480":1}}],["则矩阵维度是",{"2":{"460":1}}],["则描述了一种通用架构",{"2":{"446":1}}],["则发现",{"2":{"437":1}}],["则第一次的`ys`为",{"2":{"428":1}}],["则设为1",{"2":{"402":1}}],["则模型可能处于正则化不太可能有太大的帮助",{"2":{"1158":1}}],["则模型可能遇到了稳定性问题",{"2":{"1147":1}}],["则模型可能在训练开始就快速学习",{"2":{"401":1}}],["则模型在某些情况下是使用的",{"2":{"378":1}}],["则target",{"2":{"399":1}}],["则true",{"2":{"399":2}}],["则tgt",{"2":{"398":1}}],["则平滑之后的标签是",{"2":{"399":1}}],["则`tgt",{"2":{"385":1}}],["则填充",{"2":{"384":2}}],["则是截取左边的内容",{"2":{"376":1,"933":1}}],["则构建字典",{"2":{"374":1}}],["则切片是",{"2":{"341":1}}],["则指出",{"2":{"320":1}}],["则多余层数带来的效果并不超过该最优层数下的模型效果",{"2":{"296":1}}],["则最后便会得到一个深度的卷积结果",{"2":{"776":1}}],["则最后一维是",{"2":{"199":1}}],["则最后一维都是词向量的维度",{"2":{"199":1}}],["则最终编码器会生成一个隐向量",{"2":{"267":1}}],["则如下定义线性注意力",{"2":{"232":1}}],["则对语序区分能力较弱",{"2":{"542":1}}],["则对p",{"2":{"199":1}}],["则对应",{"2":{"89":1}}],["则把上下文压缩到模型的权重中",{"2":{"273":1}}],["则把注意力分数中对应元素设置为",{"2":{"199":1}}],["则把多个zizi",{"2":{"71":1}}],["则屏蔽不想要的元素",{"2":{"199":1}}],["则形状为",{"2":{"199":4}}],["则形状是",{"2":{"198":2}}],["则q可以认为是解码器输出的隐向量",{"2":{"268":1}}],["则qiktiqikitq",{"2":{"189":1}}],["则qqq和klklk",{"2":{"189":1}}],["则计算出的向量会有所改变",{"2":{"715":1}}],["则计算复杂度会大幅度降低",{"2":{"180":1}}],["则计算的相似度结果是个对称矩阵",{"2":{"172":1}}],["则将指针都向后移动",{"2":{"2152":1}}],["则将其返回类型声明为",{"2":{"1607":1}}],["则将其标记为不可微分",{"2":{"1115":1}}],["则将其初始化成一个服从均匀分布的矩阵",{"2":{"449":1}}],["则将两子词直接拼接",{"2":{"588":1}}],["则将剩余字符串替换为特殊符号",{"2":{"587":1}}],["则将剩余的子词替换为特殊token",{"2":{"584":1}}],["则将掩码矩阵中值为true的位置对应的得分矩阵元素置为负无穷",{"2":{"173":1}}],["则将语言模型完成事实回忆任务的过程分为两个阶段",{"2":{"122":1}}],["则dk=dmodel",{"2":{"173":1}}],["则在系统头文件目录下搜索",{"2":{"1628":1}}],["则在隐空间中一共有5个二维正态分布",{"2":{"1373":1}}],["则在调整目标中运行更多点或不那么雄心勃勃",{"2":{"1146":1}}],["则在拿到y之后",{"2":{"168":1}}],["则在掩码矩阵的对应位置放置一个非常大的负数",{"2":{"63":1}}],["则直接告诉我们此处的",{"2":{"167":1,"259":1}}],["则x相较x",{"2":{"134":1}}],["则记忆网络的结构为memorynet",{"2":{"125":1}}],["则意味着模型容量变小",{"2":{"118":1}}],["则其结果和使用x自身进行点乘的结果相同",{"2":{"172":1}}],["则其假设空间受限",{"2":{"117":1}}],["则其对应的掩码是",{"2":{"66":1,"74":1}}],["则相邻位置之间就具有依赖性了就不能叫做position",{"2":{"101":1}}],["则会使用默认值",{"2":{"1708":1}}],["则会降低学习率",{"2":{"1245":1}}],["则会视为普通属性",{"2":{"1210":1}}],["则会导致梯度消失",{"2":{"991":1}}],["则会导致梯度爆炸",{"2":{"991":1}}],["则会导致batch",{"2":{"338":1}}],["则会输出一个v维度的向量",{"2":{"899":1}}],["则会进行accumulate",{"2":{"659":1}}],["则会用某些特殊词填充多余的位置",{"2":{"453":1}}],["则会调用train",{"2":{"372":1}}],["则会出现长时信息丢失",{"2":{"246":1}}],["则会有显著的损失",{"2":{"147":1}}],["则会造成峰值内存过高",{"2":{"100":1}}],["则会给最终结果带来误差",{"2":{"54":1}}],["则返回值类型应声明为",{"2":{"1729":1}}],["则返回输入本身",{"2":{"99":1}}],["则返回",{"2":{"99":1,"1630":1}}],["则一个",{"2":{"89":1}}],["则就应该被遮蔽",{"2":{"72":1}}],["则截断",{"2":{"65":1,"384":2}}],["则",{"2":{"36":1,"89":1,"173":1,"191":1,"246":1,"332":1,"974":1,"1003":1,"1004":1,"1345":1,"1398":1,"1630":2}}],["利用counter的功能",{"2":{"1868":1}}],["利用模板",{"2":{"1698":1}}],["利用当前位置处先前的梯度值先做一个参数更新",{"2":{"1034":1}}],["利用",{"2":{"977":1,"1140":1,"1625":2,"1868":1}}],["利用最新的oss和cudnn快闪注意力内核",{"2":{"976":1}}],["利用数据移动和张量核心的异步执行",{"2":{"973":1}}],["利用低秩kv联合压缩来消除推理时kvcache的瓶颈",{"2":{"956":1}}],["利用encoder所有隐藏层状态",{"2":{"892":1}}],["利用mask后的文本和encoder生成的句向量对文本进行重建",{"2":{"727":1}}],["利用scale",{"2":{"542":1}}],["利用梯度信息在",{"2":{"485":1}}],["利用词汇投影",{"2":{"485":1}}],["利用层次结构来捕获词之间远程的依赖关系",{"2":{"290":1}}],["利用通过hidden",{"2":{"287":1}}],["利用前面所有信息",{"2":{"248":1}}],["利用p",{"2":{"200":3}}],["利用上下文学习或改变llm的表示空间来实现这种保留",{"2":{"141":1}}],["利用知识以及知识的动态演化规律依然是未解之谜",{"2":{"121":1}}],["利用一个激活函数",{"2":{"98":1}}],["利用的是原始输入的倒数第一和第二维",{"2":{"36":1}}],["利用wowow^o",{"2":{"10":1}}],["`+`",{"2":{"1788":1,"1789":2}}],["`const",{"2":{"1614":1}}],["`contiguous",{"2":{"36":1}}],["`pwd`",{"2":{"1481":2}}],["`pad`",{"2":{"834":1}}],["`id`",{"2":{"1481":3}}],["`indices`",{"2":{"558":1}}],["`user`",{"2":{"1481":3}}],["`mybatis`",{"2":{"1481":2}}],["`max",{"2":{"557":1}}],["`dataloader`",{"2":{"1254":1}}],["`disallowed",{"2":{"572":1}}],["`name`",{"2":{"1481":2}}],["`nbest`",{"2":{"1330":1}}],["`np",{"2":{"1254":1}}],["`numpy",{"2":{"1083":1}}],["`randomcrop`",{"2":{"1254":1}}],["```markdown",{"2":{"1708":1}}],["```",{"2":{"1614":1}}],["``torchvision",{"2":{"1254":1}}],["``totensor``",{"2":{"1254":1}}],["``",{"2":{"1254":4}}],["``randomcrop``",{"2":{"1254":2}}],["``rescale``",{"2":{"1254":2}}],["`b`",{"2":{"1093":1}}],["`a`",{"2":{"1093":1}}],["`allowed",{"2":{"572":1}}],["`torch",{"2":{"1254":1}}],["`tokens`",{"2":{"558":1}}],["`tf",{"2":{"700":1}}],["`lookup",{"2":{"558":1}}],["`vocab`",{"2":{"557":1}}],["`方法将变量放到一块连续的物理内存中",{"2":{"36":1}}],["`x",{"2":{"36":1}}],["`",{"2":{"36":3,"557":3,"700":1,"1635":5,"1789":4}}],["变分推断和变分自编码器的最终目标是相同的",{"2":{"1377":1}}],["变分",{"2":{"1377":1}}],["变分自编码器的变分就来源于此",{"2":{"1377":1}}],["变体",{"2":{"1130":1,"1315":1}}],["变体来回答以上问题",{"2":{"231":1}}],["变换为另一个向量空间",{"2":{"838":1}}],["变得非常稀疏",{"2":{"681":1}}],["变化的就是",{"2":{"428":1}}],["变化",{"2":{"326":1}}],["变化为",{"2":{"36":1}}],["变压器",{"2":{"259":1}}],["变大",{"2":{"188":1}}],["变形",{"2":{"170":2}}],["变形金刚的吸引力肯定比盐要高",{"2":{"163":1}}],["变量3",{"2":{"1917":1}}],["变量等",{"2":{"1909":1,"1916":2}}],["变量模板等特性使得泛型编程更加灵活强大",{"2":{"1913":1}}],["变量模板",{"0":{"1908":1},"2":{"1904":1}}],["变量中",{"2":{"1729":1}}],["变量类型",{"2":{"1649":1}}],["变量根据其声明的位置和",{"2":{"1649":1}}],["变量名",{"2":{"1629":1}}],["变量名称",{"2":{"77":1}}],["变量与常量",{"0":{"1613":1}}],["变量的值在程序运行过程中可以被修改和读取",{"2":{"1613":1}}],["变量的非常量引用",{"2":{"1612":1}}],["变量的别名",{"2":{"1612":2}}],["变量就像住在这房子里的人",{"2":{"1611":1}}],["变量声明和初始化",{"2":{"1607":1}}],["变量",{"2":{"1436":1,"1491":1,"1613":1}}],["变量说明",{"0":{"79":1},"1":{"80":1},"2":{"49":1}}],["变为非叶子节点",{"2":{"1099":1}}],["变为标准卷积",{"2":{"778":1}}],["变为了公式",{"2":{"349":1}}],["变为",{"2":{"36":1,"199":1,"698":1,"1335":1,"1630":4}}],["变成pad",{"2":{"66":1}}],["变成和输入形状相同",{"2":{"36":1}}],["变成",{"2":{"35":1,"1339":1}}],["变成了在8维空间里去分别衡量任意两个字的相关度的变化",{"2":{"33":1}}],["变成了多组",{"2":{"9":1}}],["到另一端",{"2":{"1810":1,"1828":1}}],["到达终止条件",{"2":{"1646":1}}],["到现在为止",{"2":{"1375":1}}],["到生成式任务",{"2":{"1318":1}}],["到了新的领域",{"2":{"1313":1}}],["到我们正在应对的特定问题中",{"2":{"1009":1}}],["到一个连续的表示",{"2":{"912":1}}],["到tf",{"2":{"711":1}}],["到连续向量空间的转换",{"2":{"700":1}}],["到向量空间的映射",{"2":{"698":2}}],["到多种用途中",{"2":{"689":1}}],["到底哪个大",{"2":{"595":1}}],["到底是怎么回事",{"2":{"156":1}}],["到这个高维的语言空间中",{"2":{"460":1,"676":1}}],["到解码器",{"2":{"273":1}}],["到后面",{"2":{"255":1}}],["到2024年",{"2":{"103":1}}],["到所有其它时刻的第三维也是",{"2":{"198":1}}],["到所有其它时刻",{"2":{"79":1}}],["到",{"0":{"877":1,"976":1,"977":1,"1364":1,"1600":1},"1":{"878":1,"879":1,"880":1,"881":1,"882":1,"883":1,"1601":1,"1602":1,"1603":1,"1604":1,"1605":1,"1606":1,"1607":1,"1608":1},"2":{"36":1,"536":1,"595":1,"627":1,"726":1,"926":1,"944":1,"1214":1,"1330":1,"1594":2,"1607":1,"1620":2,"1621":3,"1623":1,"1684":1,"1685":3,"2054":1}}],["ubuntu等",{"2":{"2089":1}}],["ubuntu版本",{"2":{"1584":2}}],["uber",{"2":{"2082":1}}],["ulong",{"2":{"2062":2}}],["ulmfit",{"2":{"1312":1}}],["ulm会倾向于保留那些以较高频率出现在很多句子的分词结果中的子词",{"2":{"602":1}}],["ulm算法考虑了句子的不同分词可能",{"2":{"602":1}}],["ulm",{"2":{"601":1,"638":6}}],["ulm出自论文",{"2":{"601":1}}],["ua",{"2":{"1957":1}}],["u>",{"2":{"1701":1}}],["u+x",{"2":{"1513":1}}],["urls",{"2":{"1566":2}}],["url",{"2":{"1195":1,"1306":1,"1307":2,"1308":5,"1481":1,"1566":12}}],["uv",{"2":{"957":1,"1087":1}}],["uk",{"2":{"957":1}}],["ukuku",{"2":{"764":1}}],["uququ",{"2":{"764":1}}],["utilize",{"2":{"2082":1}}],["utility>",{"2":{"1805":2,"1887":1,"1912":1,"1925":1}}],["util",{"2":{"1481":1}}],["utils",{"2":{"1215":3,"1241":1,"1242":1,"1251":1,"1253":1,"1278":2,"1283":3}}],["utiwtquitwqtu",{"2":{"760":1}}],["utf8",{"2":{"1440":1}}],["utf",{"2":{"557":1,"591":4,"592":3,"607":2,"1481":2}}],["u∈rdu∈rdu∈r^d",{"2":{"760":1}}],["ujuju",{"2":{"760":1}}],["uc伯克利等提出具身智能",{"2":{"638":1}}],["u",{"2":{"545":1,"564":1,"1000":1,"1007":3,"1243":1,"1520":1,"1547":1,"1550":1,"1701":1}}],["u和svsvs",{"2":{"356":1}}],["u和wvwvw",{"2":{"356":2}}],["unreal",{"2":{"1936":1}}],["unwinding",{"2":{"1762":2}}],["unclip",{"2":{"1363":1}}],["unet",{"0":{"1358":1},"2":{"1363":1}}],["unexpected",{"2":{"1214":1,"1254":1}}],["unequenze",{"0":{"823":1}}],["unequal",{"2":{"801":2,"802":1}}],["unfold",{"2":{"1087":1}}],["unflatten",{"2":{"1083":1,"1087":2}}],["unbind",{"2":{"1087":2}}],["unbiased",{"2":{"1087":6}}],["untyped",{"2":{"1087":1}}],["untypedstorage",{"2":{"1087":3}}],["untie",{"2":{"764":1}}],["untrained",{"2":{"428":1,"529":1}}],["un",{"2":{"564":1}}],["unveiled",{"2":{"429":1}}],["unsetmultipleprimaryemails",{"2":{"2070":1}}],["unsigned",{"2":{"1607":7,"2059":3,"2062":2}}],["unstable",{"2":{"1183":4}}],["unsafe",{"2":{"1087":3}}],["unsupervised",{"2":{"734":2}}],["unsure",{"2":{"399":1}}],["unsqueeze",{"2":{"36":1,"66":4,"74":2,"79":1,"83":1,"380":3,"382":1,"399":5,"840":1,"1087":2,"1216":1,"1329":4,"1330":13,"1350":2}}],["undone",{"2":{"740":1}}],["undef",{"2":{"1632":1}}],["undefinedtim",{"2":{"628":2}}],["underlying",{"2":{"1083":1}}],["understand",{"2":{"688":1}}],["understanding",{"2":{"156":1,"361":1,"449":1,"498":1,"513":2,"747":1,"1315":2}}],["under",{"2":{"562":2}}],["und",{"2":{"370":2}}],["unix",{"2":{"1985":1,"1999":1}}],["unidirectional",{"2":{"1781":1}}],["universities",{"2":{"2083":1}}],["universe",{"2":{"1713":2}}],["universal",{"2":{"296":1,"1312":1}}],["unimplemented",{"2":{"1214":1}}],["unique",{"0":{"1911":1},"2":{"1083":2,"1671":1,"1695":8,"1806":4,"1890":1,"1904":1,"1907":2,"1911":9,"1913":2}}],["unified",{"2":{"768":1,"795":1,"1340":1}}],["uniform",{"2":{"449":1,"1087":1}}],["union",{"2":{"572":2,"1085":13,"1086":5,"1087":260,"1214":14,"1227":1,"1727":1,"1728":2}}],["unigram",{"2":{"567":1,"601":4,"604":1,"638":1}}],["unigram等等",{"2":{"564":1}}],["unigram或sentencepiece等",{"2":{"554":1}}],["unicode标准化等",{"2":{"545":1}}],["unilm",{"0":{"601":1},"1":{"602":1,"603":1,"604":1},"2":{"503":1,"601":1,"638":1}}],["unitriangular",{"2":{"1087":1}}],["units中提出的一种按子词粒度分词方法",{"2":{"574":1}}],["units",{"0":{"843":1},"2":{"104":1,"105":1,"106":1,"156":3,"638":3,"840":1}}],["unit",{"0":{"844":1,"872":1},"1":{"873":1,"874":1,"875":1},"2":{"103":3,"106":1,"107":2,"108":1,"313":1,"795":2,"840":1,"842":3,"1214":1,"1461":1}}],["unlabeled",{"2":{"726":1}}],["unlimited",{"2":{"233":1}}],["unlearning",{"2":{"123":1,"139":1}}],["unknown",{"2":{"1675":1,"1762":1}}],["unk>`",{"2":{"557":2}}],["unk>",{"2":{"557":6,"558":1,"679":1}}],["unk",{"2":{"143":1}}],["uint8",{"2":{"74":1,"382":1}}],["upgrade",{"2":{"1537":1}}],["upper",{"2":{"1087":4}}],["uptraining",{"0":{"938":1,"954":1},"2":{"938":3,"954":3}}],["upsample",{"2":{"802":2}}],["updates",{"2":{"2086":1}}],["updateradius",{"2":{"1779":2}}],["updated",{"2":{"1774":2,"1779":2,"1902":1}}],["updatedimensions",{"2":{"1774":3}}],["update>",{"2":{"1487":1}}],["updateuser",{"2":{"1487":3}}],["update",{"0":{"1487":1},"2":{"557":1,"590":1,"874":2,"1083":1,"1205":2,"1215":2,"1487":2,"1537":2,"1584":2,"2076":1,"2086":1}}],["up学习率策略",{"2":{"333":1}}],["up咚咚呛",{"2":{"233":1}}],["up",{"2":{"38":1,"110":2,"335":1,"344":1,"523":1,"542":1,"592":1,"700":1,"1215":1,"1284":1,"1308":1,"1408":1}}],["usr",{"2":{"1506":1}}],["usage",{"0":{"1308":1}}],["us",{"2":{"1254":1}}],["uszkoreit",{"2":{"434":1}}],["useunicode=true",{"2":{"1481":1}}],["useless",{"2":{"1304":1}}],["useful",{"2":{"1304":1,"1308":1,"2073":1}}],["userpermission",{"2":{"2060":3}}],["usermod",{"2":{"1530":1}}],["usermodbashsudo",{"2":{"1530":1}}],["usermapper",{"2":{"1481":6,"1485":3,"1486":2,"1487":2,"1488":2}}],["useradd",{"2":{"1530":1}}],["useraddbashsudo",{"2":{"1530":1}}],["user>",{"2":{"1481":3,"1485":1}}],["username",{"2":{"1481":1,"1485":5,"2069":2,"2070":2}}],["user",{"2":{"1201":1,"1284":1,"1481":7,"1485":13,"1486":8,"1487":8,"1488":1,"1509":1,"1513":2,"1530":1,"1583":1,"1594":3,"2021":1,"2087":1}}],["users",{"2":{"431":1,"455":1,"460":1,"698":1,"1086":1,"1481":3,"2021":1,"2087":1}}],["usessl=true",{"2":{"1481":1}}],["uses",{"2":{"387":1,"1235":1,"1236":1,"1237":1,"1238":1,"1240":1,"1246":1,"1247":1,"1254":1,"2075":1}}],["use",{"0":{"1304":1,"2078":1,"2084":1},"1":{"2085":1,"2086":1},"2":{"110":1,"160":1,"428":1,"503":2,"591":1,"700":5,"1083":1,"1086":1,"1100":1,"1215":5,"1254":1,"1303":1,"1305":1,"1308":5,"1481":1,"1891":1,"1909":2,"2073":1}}],["used",{"2":{"8":1,"167":1,"259":1,"395":1,"557":1,"558":1,"591":1,"592":2,"985":1,"1284":1,"1308":1,"1330":1,"1891":1,"2077":1}}],["using",{"2":{"36":1,"90":1,"235":1,"237":1,"282":1,"292":1,"423":1,"429":1,"571":1,"591":1,"764":1,"1208":1,"1242":1,"1254":1,"1299":1,"1302":1,"1304":1,"1308":1,"1606":1,"1607":1,"1608":1,"1619":1,"1620":1,"1621":2,"1623":1,"1624":1,"1625":2,"1683":4,"1684":2,"1685":2,"1687":2,"1688":2,"1691":1,"1713":6,"1714":2,"1715":2,"1824":1,"1825":1,"1842":1,"1843":1,"1999":1,"2003":1,"2004":1,"2005":1,"2006":5,"2007":1,"2008":1,"2059":1}}],["会考虑薪资",{"2":{"2105":1}}],["会有不一样的收获",{"2":{"2056":1}}],["会有越多的信息集中在最大的奇异值上",{"2":{"204":1}}],["会抛出overflow",{"2":{"2062":1}}],["会抛出异常",{"2":{"1928":1}}],["会抛出",{"2":{"1926":1}}],["会抛出一个",{"2":{"1647":1}}],["会自动分配更大的内存空间",{"2":{"1713":1}}],["会自动调用对象的",{"2":{"1227":1}}],["会尝试将函数体的代码直接插入到调用处",{"2":{"1709":1}}],["会decay",{"2":{"1705":1}}],["会输出整个字符串直到",{"2":{"1704":1}}],["会被",{"2":{"1704":1}}],["会优先寻找完全匹配的函数",{"2":{"1698":1}}],["会根据实际传入的类型参数生成具体的代码",{"2":{"1698":1}}],["会根据需要在二者间切换",{"2":{"34":1}}],["会退化为指针",{"2":{"1634":1}}],["会分配不同的计算节点或",{"2":{"1589":1}}],["会分别乘以这三个矩阵",{"2":{"161":1}}],["会引起sql注入",{"2":{"1489":1}}],["会引起问题",{"2":{"1180":1}}],["会检测成员的",{"2":{"1210":1}}],["会出现部分",{"2":{"1164":1}}],["会通过一个projection层映射到对应的词",{"2":{"899":1}}],["会通过调用parameters",{"2":{"664":1}}],["会遇到很多序列型输入数据的情况或问题",{"2":{"878":1}}],["会让周围的嵌入把自己的信息传入给",{"2":{"709":1}}],["会参与梯度下降计算",{"2":{"702":1}}],["会放入",{"2":{"668":1}}],["会直接截断还是截断再拼接到下一个batch",{"2":{"665":1}}],["会占用显存",{"2":{"659":1}}],["会置零",{"2":{"659":1}}],["会重新拷贝数据",{"2":{"658":1}}],["会得到friend",{"2":{"567":1}}],["会得到friendly",{"2":{"567":1}}],["会穷举所有出现的字符",{"2":{"564":1}}],["会向解码器输入整个目标序列",{"2":{"528":1}}],["会一层层往上传",{"2":{"519":1}}],["会比较句子中的所有单词以生成预测",{"2":{"488":1}}],["会做两件事",{"2":{"456":1}}],["会加到上次的输入后面",{"2":{"453":1}}],["会用到所有词的信息",{"2":{"415":1}}],["会作为解码器的输入",{"2":{"407":1}}],["会给target",{"2":{"399":1}}],["会将标了×",{"2":{"1017":1}}],["会将前一个时间步的输出序列和历史预测单词一齐送到下一个时间步的解码器",{"2":{"427":1}}],["会将目标句子删掉最后一个token",{"2":{"381":1}}],["会将encoder的输出memory",{"2":{"39":1}}],["会使代码难以维护和理解",{"2":{"1776":1}}],["会使分母过于敏感",{"2":{"1045":1}}],["会使得里面的新的tensor",{"2":{"661":1,"1104":1}}],["会使得统计量不置信",{"2":{"316":1}}],["会使用相同的变换矩阵来计算",{"2":{"419":1}}],["会使用来进行预测下一个单词",{"2":{"378":1}}],["会使用",{"2":{"378":1}}],["会把词表作为参数传入collate",{"2":{"375":1}}],["会调用对象的",{"2":{"1083":1}}],["会调用把从数据集之中获取的数据构建成一个batch",{"2":{"385":1}}],["会调用",{"2":{"344":1}}],["会更有利于大家理解",{"2":{"323":1}}],["会更高效",{"2":{"154":1}}],["会大大降低学习速度",{"2":{"309":1}}],["会大幅变小",{"2":{"185":1}}],["会针对编码器输出的所有语义向量",{"2":{"267":1}}],["会学习相同符号值和相反符号值之间的互补关系",{"2":{"213":1}}],["会接近于0",{"2":{"191":1}}],["会对",{"2":{"170":1}}],["会发生梯度消失问题",{"2":{"840":1}}],["会发生copy",{"2":{"658":1}}],["会发生以下两个主要阶段的变化",{"2":{"148":1,"485":1}}],["会发现大概率上",{"2":{"222":1}}],["会发现有六种掩码矩阵",{"2":{"84":1}}],["会识别出",{"2":{"145":1}}],["会在后续章节进行分析位置编码的实现",{"2":{"699":1}}],["会在句首加入一个特殊",{"2":{"379":1}}],["会在源句和目标句之间反复移动",{"2":{"284":1}}],["会在残差流中脱颖而出",{"2":{"122":1}}],["会在第二个维度插入head数量",{"2":{"36":1}}],["会融入很多与主语相关的属性",{"2":{"122":1}}],["会归零或者接近于0",{"2":{"62":1}}],["会看到xnxnx",{"2":{"58":1}}],["会导致重复定义错误",{"2":{"1916":1}}],["会导致网络收敛缓慢或者收敛到局部极小值",{"2":{"991":1}}],["会导致我们训练出来的模型在翻译结果的语言通畅性方面很差",{"2":{"908":1}}],["会导致那些很短的句子更容易被选出",{"2":{"904":1}}],["会导致词特征的维度过高",{"2":{"681":1}}],["会导致序列长度较长",{"2":{"612":1}}],["会导致模型在后续训练中偏离正轨",{"2":{"407":1}}],["会导致模型参数变少",{"2":{"100":1}}],["会导致隐向量保存上下文的能力在本质上是有限的",{"2":{"252":1}}],["会导致明显的",{"2":{"89":1}}],["会导致训练速度过慢",{"2":{"57":1}}],["会导致注意力分数会出现偏差",{"2":{"54":1}}],["本段整理日志",{"2":{"2053":1}}],["本次课程我们详细讲解了",{"2":{"1826":1,"1844":1}}],["本次预测结束",{"2":{"239":1}}],["本部分将介绍三种强大的自定义数据类型",{"2":{"1728":1}}],["本部分将介绍数组和字符串",{"2":{"1622":1}}],["本部分将介绍",{"2":{"1618":1}}],["本是apache的一个开源项目ibatis",{"2":{"1476":1}}],["本项目遵循",{"2":{"1199":1}}],["本方案的具体推导过程如下",{"2":{"759":1,"760":1,"761":1,"762":1,"763":1,"764":1,"765":1}}],["本方案基于这样的假设",{"2":{"204":1}}],["本句子可以总结成什么内容",{"2":{"736":1}}],["本阶段的subword的粒度是字符",{"2":{"579":1}}],["本阶段输出张量的形状是",{"2":{"460":1}}],["本单词更应该关注在哪些单词上",{"2":{"519":1}}],["本章依然用机器翻译来分析说明",{"2":{"515":1}}],["本章我们来分析哈佛代码的数据处理部分",{"2":{"363":1}}],["本小节我们来看看wordpiece和unigram",{"2":{"596":1}}],["本小节先概要介绍多头自注意力机制和ffn层",{"2":{"461":1}}],["本小节下面行文和注释中的一些术语解释如下",{"2":{"198":1}}],["本身并不拥有数据",{"2":{"1929":1}}],["本身并没有变",{"2":{"170":1}}],["本身就很小且简单",{"2":{"1479":1}}],["本身也是一个variable",{"2":{"1110":1}}],["本身在超球面上执行多步优化",{"2":{"352":1}}],["本层的输入",{"2":{"344":1}}],["本类实现的功能是layer",{"2":{"344":1}}],["本",{"2":{"340":1}}],["本例来自网络",{"2":{"339":1}}],["本节课我们学习了",{"2":{"1729":1}}],["本节课我们学习了运算符重载",{"2":{"1715":1}}],["本节课我们将深入探讨",{"2":{"1703":1}}],["本节课我们将深入学习如何通过流程控制语句来决定程序的执行路径",{"2":{"1617":1}}],["本节课我们深入学习了",{"2":{"1678":1,"1709":1}}],["本节课总结",{"2":{"1678":1}}],["本节的其余部分将更详细地讲解增量调优策略",{"2":{"1139":1}}],["本节只是从普遍意义或者说是在典型问题上进行阐释",{"2":{"243":1}}],["本节我们将介绍一些背景知识和概念",{"2":{"236":1}}],["本系列主要以下面几项为基础",{"2":{"432":1}}],["本系列有些内容是个人梳理和思考的结果",{"2":{"235":1}}],["本系列是对论文",{"2":{"235":1}}],["本系列试图从零开始解析transformer",{"2":{"235":1}}],["本系列内容主要来源于柳浩老师的博客",{"2":{"48":1}}],["本地kv数为1",{"2":{"201":1}}],["本函数计算缩放点积注意力",{"2":{"199":1}}],["本函数是论文中图2",{"2":{"36":1}}],["本来残差的意思是给前面的层搞一条",{"2":{"332":1}}],["本来你去买盐",{"2":{"163":1}}],["本来是独立的两件事",{"2":{"45":1}}],["本篇前置内容",{"2":{"1426":1}}],["本篇内容课前定位",{"2":{"1405":1}}],["本篇为了更好的说明",{"2":{"445":1,"454":1}}],["本篇以文本翻译功能为例来进行说明",{"2":{"432":1}}],["本篇依然以文本翻译为例进行学习",{"2":{"389":1}}],["本篇会介绍如何做好单词到数字的映射",{"2":{"545":1}}],["本篇会涉及到词表和分词器",{"2":{"363":1}}],["本篇会进行详细分析",{"2":{"162":1}}],["本篇我们主要介绍标准ffn",{"2":{"98":1}}],["本质性",{"2":{"2058":1}}],["本质",{"2":{"1612":1}}],["本质上是被引用变量的另一个名字",{"2":{"1612":1}}],["本质上",{"2":{"1183":1}}],["本质上来说",{"2":{"714":1}}],["本质上来讲",{"2":{"404":1}}],["本质上就是",{"2":{"714":1}}],["本质上看",{"2":{"676":1}}],["本质上也就是memory",{"2":{"537":1}}],["本质上都是在拟合一个复杂的非线性复合函数",{"2":{"495":1}}],["本质上都是并行计算",{"2":{"34":1}}],["本质上捕捉权重",{"2":{"209":1}}],["本质是一个具有全局语义整合功能的数据库",{"2":{"164":1}}],["本质是一个跟需要掩盖的目标张量大小一致的",{"2":{"50":1}}],["本文档源于我们自己训练神经网络",{"2":{"1127":1}}],["本文档中描述的技术也可能适用于其他类型的问题",{"2":{"1126":1}}],["本文档适用于对最大化深度学习的性能感兴趣的工程师和研究人员",{"2":{"1126":1}}],["本文将概述自动微分",{"2":{"1112":1}}],["本文归类时是按照其出发点来归为ape",{"2":{"764":1}}],["本文就让我们来欣赏一下研究人员为了更好地表达位置信息所构建出来的",{"2":{"746":1}}],["本文首次对字节级模型进行了flops控制的扩展研究",{"2":{"611":1}}],["本文为系列第一篇",{"2":{"235":1}}],["本文选取几个典型的方面进行分析",{"2":{"203":1}}],["本文采用了可训练的",{"2":{"154":1}}],["本文转自",{"2":{"47":1,"95":1,"156":1,"233":1,"292":1,"361":1,"387":1,"429":1,"513":1,"543":1,"638":1,"740":1,"768":1}}],["本文研究团队在mha中引入compose操作",{"2":{"46":1}}],["经理类",{"2":{"1657":1}}],["经济和环境成本都要低得多",{"2":{"1313":1}}],["经常与前面的三种方法混淆",{"2":{"1116":1}}],["经常被视为支撑知识导向任务的虚拟知识库",{"2":{"121":1}}],["经验上已经发现",{"2":{"1042":1}}],["经典激活函数",{"0":{"1460":1}}],["经典的相对位置编码",{"0":{"1339":1}}],["经典的绝对位置编码有三种",{"2":{"747":1}}],["经典结构可表示如下",{"2":{"887":1}}],["经典rnn的适用范围比较小",{"2":{"879":1}}],["经典rnn的计算图如下",{"0":{"855":1}}],["经典rnn",{"0":{"861":1}}],["经典式",{"0":{"759":1},"2":{"741":1}}],["经典transformer可能越来越难以捕捉到关键信息",{"2":{"500":1}}],["经过前几节课的学习",{"2":{"1644":1}}],["经过它激活得到的数据为非",{"2":{"839":1}}],["经过它的处理后就可返回结果",{"2":{"533":1}}],["经过word",{"2":{"714":1}}],["经过wq",{"2":{"198":1}}],["经过嵌入之后的数据是具有语义关系的",{"2":{"676":1}}],["经过上述操作之后",{"2":{"545":1}}],["经过大量训练后",{"2":{"542":1}}],["经过tokenizer处理成token",{"2":{"528":1}}],["经过多层计算之后",{"2":{"518":1}}],["经过一系列计算",{"2":{"510":1}}],["经过一系列transformer模块的精心处理",{"2":{"471":1}}],["经过embedding层将输入token序列变为一个三维张量",{"2":{"510":1}}],["经过词典处理过的",{"2":{"451":1}}],["经过batchnorm的加工",{"2":{"313":1}}],["经过自注意力机制处理之后",{"2":{"261":1}}],["经过",{"2":{"204":1,"394":1,"651":1,"933":1}}],["经过这样修改",{"2":{"194":1}}],["经过早期的多层mlp处理之后",{"2":{"122":1}}],["经过第一个线性层",{"2":{"113":1}}],["经过线性变换",{"2":{"71":1}}],["经过softmax操作",{"2":{"71":1}}],["经过softmax函数后",{"2":{"63":1}}],["经过三次线性变化",{"2":{"71":1}}],["经过input",{"2":{"71":1}}],["经过全连接层融合后的最后一维仍然是",{"2":{"35":1}}],["经由线性层输出的",{"2":{"28":1}}],["将它放在千位",{"2":{"2125":1}}],["将它们应用到我们正在面对的问题上",{"2":{"1009":1}}],["将它们的内存使用量降低了高达55",{"2":{"983":1}}],["将它们加载到sram",{"2":{"940":1,"962":1}}],["将它们连接",{"2":{"926":1}}],["将它们根据熵动态分组为patch",{"2":{"610":1}}],["将它们",{"2":{"595":1}}],["将它们合并为一个整体",{"2":{"576":1}}],["将它们整合起来是一个挑战",{"2":{"154":1}}],["将库文件链接到可执行文件",{"2":{"1975":1}}],["将容器内所有元素翻倍",{"2":{"1914":1}}],["将整数转换为字符串",{"2":{"1824":1,"1842":1}}],["将各种类型的数据插入到字符串流中",{"2":{"1824":1,"1842":1}}],["将各进程的数据聚合为一个单一结果",{"2":{"1575":1}}],["将各进程的数据收集到一个进程中",{"2":{"1575":1}}],["将填充字符插入到符号和数字之间",{"2":{"1817":1,"1835":1}}],["将友元声明集中在类的开头或结尾",{"2":{"1793":1}}],["将是",{"2":{"1778":1}}],["将列表中的每个数奇数加",{"2":{"1759":1}}],["将指定范围内的所有等于某个值的元素替换为新值",{"2":{"1743":1}}],["将指针初始化为",{"2":{"1611":1}}],["将代码复制到",{"2":{"1729":1}}],["将代码中所有出现的",{"2":{"1632":1}}],["将代码中的",{"2":{"1604":1}}],["将常用的功能封装成函数",{"2":{"1729":1}}],["将电路分成不同的模块可以更快地找到问题所在",{"2":{"1729":1}}],["将复杂的程序逻辑分解成多个小型",{"2":{"1729":1}}],["将继承基类的实现",{"2":{"1693":1}}],["将派生类强制实现该函数",{"2":{"1693":1}}],["将实参的别名传递给形参",{"2":{"1650":1}}],["将实参的内存地址传递给形参",{"2":{"1650":1}}],["将实参的值复制一份传递给形参",{"2":{"1650":1}}],["将程序划分为多个模块",{"2":{"1628":1}}],["将接口",{"2":{"1628":1}}],["将接口和",{"2":{"1476":1}}],["将函数和变量的声明放在头文件中",{"2":{"1628":1}}],["将之前你理解的关键的概念加入总结",{"2":{"1616":1}}],["将光标移动到下一个制表位",{"2":{"1616":1}}],["将光标移动到下一行开头",{"2":{"1616":1}}],["将计算器的功能实现放在一个单独的库",{"2":{"1997":1}}],["将计算得到的摄氏温度值输出到控制台",{"2":{"1608":1}}],["将计算资源集中在场景的各个部分可以节省",{"2":{"260":1}}],["将字符串转换为小写并移除标点符号",{"2":{"1933":1}}],["将字符串转换为整数",{"2":{"1824":1,"1842":1}}],["将字符串",{"2":{"1729":1}}],["将字符串或字符",{"2":{"1713":1}}],["将字符串和变量",{"2":{"1607":1}}],["将字节分组为patch",{"2":{"612":1}}],["将两个操作数相乘",{"2":{"1607":1}}],["将两个操作数相加",{"2":{"1607":1}}],["将两个信号之间的差来消除输入的共模噪声",{"2":{"502":1}}],["将多个目标文件和库文件链接成可执行文件",{"2":{"1916":1}}],["将多个目标文件和所需的库文件",{"2":{"1604":1}}],["将多头检查点转换为多查询检查点",{"2":{"938":1,"954":1}}],["将多头注意力学到的东西进行一波混合操作",{"2":{"116":1}}],["将多头输出的多个z通过全连接合并为一个输出z",{"2":{"35":1}}],["将汇编语言代码翻译成机器语言",{"2":{"1604":1}}],["将预处理后的",{"2":{"1604":1}}],["将预测出的第二个单词",{"2":{"239":1}}],["将预测出的第一个单词",{"2":{"239":1}}],["将公钥复制到",{"2":{"1594":1}}],["将状态码和内容存储到结果字典中",{"2":{"1566":1}}],["将消息发送给另一个进程或节点",{"2":{"1563":1}}],["将文件指针定位到文件末尾",{"2":{"1820":1,"1838":1}}],["将文件权限改为所有者可读写执行",{"2":{"1513":1}}],["将文本集合分成不同的群组",{"2":{"906":1}}],["将文本分为不同的类别或标签",{"2":{"906":1}}],["将文本中词汇的数字表示转变为高维的向量表示",{"2":{"697":1}}],["将文本拆分为token",{"2":{"554":1}}],["将usermapper",{"2":{"1484":1}}],["将业务逻辑和数据访问逻辑分离",{"2":{"1479":1}}],["将水果做成罐头的方法也是",{"2":{"1477":1}}],["将鲜肉冷藏",{"2":{"1477":1}}],["将猫分离出来",{"2":{"1462":1}}],["将与神经元的阀值进行比较",{"2":{"1459":1}}],["将神经元抽象为数学概念上的的简单模型",{"2":{"1459":1}}],["将神经元激活模式",{"2":{"137":1}}],["将前馈网络写成矩阵形式",{"0":{"1397":1}}],["将图片进行某种编码",{"2":{"1373":1}}],["将2维推广到任意维度",{"2":{"1343":1}}],["将可处理的文本长度从大多数模型的",{"2":{"1317":1}}],["将基类函数声明为虚函数",{"2":{"1688":1}}],["将基于海量数据获得的统计理解能力应用于我们的任务",{"2":{"1313":1}}],["将基线",{"2":{"357":1}}],["将动态操作转换为优化的静态内核",{"2":{"1288":1}}],["将动量加入rmsprop",{"2":{"1059":1}}],["将学习率调度器的列表链接在一起",{"2":{"1246":1}}],["将学习率从",{"2":{"401":1}}],["将初始振幅按",{"2":{"1241":1}}],["将初始学习率设置为",{"2":{"1235":1,"1236":1,"1237":1,"1238":1,"1239":1,"1240":1}}],["将初始学习率设置为lr",{"2":{"1233":1,"1234":1}}],["将大问题划分成若干小问题",{"2":{"2119":1}}],["将大型程序分解成多个小型",{"2":{"1729":1}}],["将大块的计算融合成一个内核",{"2":{"1228":1}}],["将大小为",{"2":{"201":1}}],["将使用参数self和state",{"2":{"1227":1}}],["将使用参数self调用该钩子",{"2":{"1227":1}}],["将param",{"2":{"1226":1}}],["将参数",{"2":{"1214":1}}],["将参数和缓冲区从state",{"2":{"1214":1}}],["将参数和缓冲区转成tensor",{"2":{"1214":1}}],["将参数和缓冲区转成指定的数据类型",{"2":{"1214":1}}],["将参数和缓冲区移动到指定的设备",{"2":{"1214":2}}],["将执行",{"2":{"1214":1}}],["将创建并返回一个ordereddict",{"2":{"1214":1}}],["将模板中的类型参数替换成具体的类型",{"2":{"1698":1}}],["将模块的状态保存到包含模块状态但不包含其子模块状态的destination字典中",{"2":{"1214":1}}],["将模型转换为script模式",{"2":{"1296":1}}],["将模型参数也视为一种",{"2":{"624":1}}],["将模型输出的",{"2":{"471":1}}],["将来版本中",{"2":{"1214":1}}],["将数字翻倍",{"2":{"1914":1}}],["将数组传递给函数",{"2":{"1634":1}}],["将数组",{"2":{"1623":1}}],["将数值从0提升到",{"2":{"1183":1}}],["将数据插入到输出流中",{"2":{"1816":1,"1834":1}}],["将数据打包成有意义的整体",{"2":{"1728":1}}],["将数据发送到输出流",{"2":{"1673":1}}],["将数据和目标转换为tensor类型",{"2":{"1295":1}}],["将数据拉回到正态分布",{"2":{"807":1}}],["将数据拉回标准正态分布",{"2":{"310":1}}],["将数据转换为数值向量",{"2":{"676":1}}],["将总批大小与用于计算批归一化统计数据的样本数量分离对于批次大小的比较特别有用",{"2":{"1168":1}}],["将超参数分为三类",{"2":{"1142":1}}],["将深化理解置于短期收益之上可以帮助我们",{"2":{"1140":1}}],["将不会改变",{"2":{"1134":1}}],["将不同长度的文本拼接成连续的长序列",{"2":{"217":1}}],["将不同的数据片段分发给各个进程",{"2":{"1575":1}}],["将不同的",{"2":{"89":1}}],["将取决于每步的消耗如何变化",{"2":{"1134":1}}],["将batch",{"2":{"1133":2}}],["将bpe的思想从字符级别扩展到子节级别",{"2":{"605":1}}],["将优化器超参数视为冗余参数是有意义的",{"2":{"1130":1}}],["将tensor",{"2":{"1085":1}}],["将transformer看作一定深度的复杂电路",{"2":{"480":1}}],["将train",{"2":{"375":1}}],["将true",{"2":{"399":2}}],["将底层存储移动到共享内存",{"2":{"1083":1}}],["将张量从创建它的计算图中分离",{"2":{"1083":1}}],["将张量分割成多个块",{"2":{"831":1}}],["将被调用",{"2":{"1083":1}}],["将此部分梯度然后跟之前累积下来的梯度值矢量相加",{"2":{"1034":1}}],["将ℓ分割为",{"2":{"944":1}}],["将output的矩阵",{"2":{"944":1}}],["将output和权重wo进行线性计算",{"2":{"201":1}}],["将键值缓存",{"2":{"937":1,"953":1}}],["将查询头分成g个组",{"2":{"937":1,"953":1}}],["将一个范围的元素复制到另一个范围",{"2":{"1742":1}}],["将一个超出范围的值赋给较小的数据类型",{"2":{"1684":1}}],["将一个tensor",{"2":{"1083":1}}],["将一个tm任务转化成tm+lm两种任务",{"2":{"908":1}}],["将一条消息发送给所有进程",{"2":{"1575":1}}],["将一种语言的文本翻译成另一种语言的文本",{"2":{"906":1}}],["将context",{"2":{"889":1}}],["将值压缩在",{"2":{"866":1}}],["将错误率降低到了当时的最低水平",{"2":{"840":1}}],["将第一个操作数除以第二个操作数",{"2":{"1607":1}}],["将第一个的12和第二个的56视为一个整体",{"2":{"315":1}}],["将第二个操作数从第一个操作数中减去",{"2":{"1607":1}}],["将第4项扔掉",{"2":{"763":1}}],["将二三项中的位置编码换成相对位置编码",{"2":{"763":1}}],["将相关的各种信息捆绑在一起",{"2":{"1728":1}}],["将相关信息传递到整个序列链的末端",{"2":{"863":1}}],["将相对位置直接映射成可学习的标量",{"2":{"762":1}}],["将相似的嵌入映射到相同的内存位置",{"2":{"153":1}}],["将llm转换为文本编码器",{"2":{"740":1}}],["将logit",{"2":{"485":1}}],["将句子逐时间步输入到rnn中",{"2":{"860":1}}],["将句子用",{"2":{"736":1}}],["将句子用一个",{"2":{"736":1}}],["将句子拼接起来",{"2":{"83":1}}],["将表征映射到较低维度表征",{"2":{"733":1}}],["将单词的位置信息编码成特征向量",{"2":{"722":1}}],["将训练出的表征作为feature用于下游任务",{"2":{"718":1}}],["将词嵌入维度保存为类属性",{"2":{"701":1}}],["将词典中的所有子词按照长度由大到小进行排序",{"2":{"587":1}}],["将庞大的词嵌入矩阵转换成两个小的矩阵",{"2":{"698":1}}],["将里面的",{"2":{"662":1}}],["将softmax函数推向具有极小梯度的区域",{"2":{"647":1,"924":1}}],["将source",{"2":{"450":1}}],["将信号转换为频域",{"2":{"637":1}}],["将did",{"2":{"2023":1}}],["将dct和字节对编码",{"2":{"637":1}}],["将decoder的输出送给generator进行预测",{"2":{"428":1}}],["将连续的嵌入向量转换为离散的码本",{"2":{"636":1}}],["将连续的文本拆分",{"2":{"545":1}}],["将子词按照score大小进行排序",{"2":{"602":1}}],["将增加模型的健壮性",{"2":{"595":1}}],["将word拆分为子串",{"2":{"564":1}}],["将所有代码放在一个文件中会变得难以维护和理解",{"2":{"1916":1}}],["将所有代码都写在一个",{"2":{"1729":1}}],["将所有",{"2":{"1317":1}}],["将所有参数和缓冲区转换为",{"2":{"1214":1}}],["将所有模型参数和缓冲区移动到",{"2":{"1214":3}}],["将所有的pspsp",{"2":{"760":1}}],["将所有的粗略的知识神经元集聚合在一起",{"2":{"135":1}}],["将所有数字替换为一个占位符或特定的标记",{"2":{"552":1}}],["将ffn计算结果传递给第一个sublayerconnection实例",{"2":{"523":1}}],["将自注意力结果传递给第一个sublayerconnection实例",{"2":{"523":1}}],["将其抽象成类和成员",{"2":{"1657":1}}],["将其转化为计算机能够理解的机器语言",{"2":{"1604":1}}],["将其转换为新的查询和键",{"2":{"209":1}}],["将其构建成为一个新的token",{"2":{"592":1}}],["将其对应回一种更好的网络结构",{"2":{"498":1}}],["将其在数学上解释为一种数值常微分方程",{"2":{"498":1}}],["将input",{"2":{"450":2}}],["将module",{"2":{"1214":2}}],["将module里的parameters传给optim",{"2":{"664":1}}],["将m",{"2":{"944":1}}],["将memory表示成m方便之后使用",{"2":{"533":1}}],["将memory",{"2":{"450":1}}],["将mask后的weight与v矩阵相乘",{"2":{"71":1}}],["将上面案例中的接口userdao改名为usermapper",{"2":{"1484":1}}],["将上式化简为",{"2":{"1003":1}}],["将上下文向量作为输入",{"2":{"635":1}}],["将上下文序列作为k和v",{"2":{"444":1}}],["将上三角部分全部mask掉",{"2":{"71":1}}],["将编码器的输出memory和之前解码器的所有输出作为参数",{"2":{"428":1}}],["将编码器生成的上下文作为解码器的初始化参数",{"2":{"283":1}}],["将输入输出操作抽象为字节序列的流动",{"2":{"1810":1,"1828":1}}],["将输入的数据进行放大",{"2":{"1371":1}}],["将输入句子通过遮盖词语",{"2":{"1317":1}}],["将输入分割成块",{"2":{"940":1,"959":1}}],["将输入数据转换为模型可以处理的形式",{"2":{"674":1}}],["将输入拆分为单词",{"2":{"545":1}}],["将输入序列转换为嵌入矩阵",{"2":{"515":1}}],["将输入送给编码器",{"2":{"428":1}}],["将输出的",{"2":{"473":1}}],["将输出进行拼接",{"2":{"213":1}}],["将",{"2":{"399":2,"460":1,"498":1,"763":1,"764":1,"943":1,"944":4,"961":1,"1085":1,"1226":1,"1316":1,"1329":1,"1340":1,"1343":1,"1344":1,"1604":1,"1611":1,"1623":1,"1624":1,"1629":1,"1645":2,"1706":2,"1729":1,"1817":2,"1835":2,"1873":1,"1933":1}}],["将每一步的损失求和",{"2":{"899":1}}],["将每个源文件编译成目标文件",{"2":{"1916":1}}],["将每个进程的部分和加到总和中",{"2":{"1594":1}}],["将每个位置的最佳idx记录下来",{"2":{"1330":1}}],["将每个参数组的学习率通过线性变化的小乘法因子进行衰减",{"2":{"1238":1}}],["将每个参数组的学习率按一个小的常数因子进行衰减",{"2":{"1237":1}}],["将每个参数组的学习率按",{"2":{"1235":1,"1239":1}}],["将每个参数组的学习率乘以指定函数中给定的因子",{"2":{"1234":1}}],["将每个参数组的学习率设置为初始学习率乘以给定函数",{"2":{"1233":1}}],["将每个单词的注意力范围扩展到整个序列",{"2":{"734":1}}],["将每个token的正向和反向lstm输出拼接起来",{"2":{"717":1}}],["将每个词与序列中的其他词联系起来",{"2":{"259":1}}],["将每维dct系数的展平序列压缩为最终的动作标记序列",{"2":{"637":1}}],["将每行的第一个词都改为1",{"2":{"383":1}}],["将残差连接应用于任何具有相同大小的子层",{"2":{"344":1}}],["将残差流引导向mlp认为正确的输出的方向",{"2":{"122":1}}],["将残差流逐渐形成",{"2":{"122":1}}],["将防止除以零的微小值eps保存为类实例的属性",{"2":{"343":1}}],["将批次中不同样本同一维特征视为相同分布",{"2":{"316":1}}],["将1",{"2":{"315":1}}],["将在继承章节介绍",{"2":{"1677":1}}],["将在后续课程中详细介绍",{"2":{"1673":1,"1675":1}}],["将在",{"2":{"315":1,"1312":1}}],["将这个整体添加到词典",{"2":{"576":1}}],["将这种位置编码与词向量相加",{"2":{"512":1}}],["将这些单词实体映射到背后的概念",{"2":{"510":1}}],["将这些神经元删掉",{"2":{"143":1}}],["将这一次的预测结果和之前的拼到一块",{"2":{"428":1}}],["将这样的思想拓展到了梯度的反向传播中",{"2":{"298":1}}],["将隐向量拼接成h",{"2":{"289":1}}],["将原来的三维空间投影到方差最大且线性无关的两个方向或者说将原矩阵进行单位正交基变换以保留最大的信息量",{"2":{"1370":1}}],["将原始数据转换成为能够被机器学习来有效开发的一种形式",{"2":{"1455":1}}],["将原始预测值转换为类别概率",{"2":{"847":1}}],["将原始模型的输出划分成n",{"2":{"288":1}}],["将原先用",{"2":{"184":1}}],["将序列内部的关系和序列之间的关系进行深度融合",{"2":{"287":1}}],["将传统编码器",{"2":{"287":1}}],["将对齐系数进行汇总",{"2":{"267":1}}],["将源序列的信息",{"2":{"267":1}}],["将源序列依次输入编码器",{"2":{"267":1}}],["将例句从英文翻译成中文时",{"2":{"246":1}}],["将记忆作为当前信息的上下文",{"2":{"231":1}}],["将过去信息压缩到长期神经记忆模块中",{"2":{"230":1}}],["将长期记忆模块作为深度神经网络的一层",{"2":{"229":1}}],["将长期记忆视为当前信息的上下文",{"2":{"229":1}}],["将存储在",{"2":{"221":1}}],["将块与块之间的信息逐步传递",{"2":{"216":1}}],["将x和mask传递给当前编码器层",{"2":{"522":1}}],["将xk和xv加载进缓存",{"2":{"201":1}}],["将x的分布规范化成在固定区间范围的标准分布",{"2":{"310":1}}],["将x的每个元素表达为最大元素xmaxxmaxx",{"2":{"191":1}}],["将x的shape由",{"2":{"36":1}}],["将k和v作为可学习参数设计了",{"2":{"731":1}}],["将keys和values根据n",{"2":{"201":1}}],["将k",{"2":{"201":1}}],["将query和key",{"2":{"502":1}}],["将q",{"2":{"201":1}}],["将qktqktqk^t进行拆解",{"2":{"176":1}}],["将归一化之后的结果经过输出层",{"2":{"201":1}}],["将最后一个token",{"2":{"431":1}}],["将最后一个transformerblock的输出进行归一化",{"2":{"201":1}}],["将最后一个维度变成",{"2":{"36":1}}],["将最后一个维度变成d",{"2":{"29":1}}],["将32个transformerblock存于modulelist",{"2":{"201":1}}],["将掩码张量和scores张量每个位置一一比较",{"2":{"199":1}}],["将掩码矩阵加到注意力分数上",{"2":{"63":1}}],["将将最后两个维度进行转置",{"2":{"199":1}}],["将注意力分数计算为两个单独的softmax",{"2":{"500":1}}],["将注意力分数经过softmax归一化后的结果称为注意力权重",{"2":{"178":1,"267":1}}],["将注意力应用在序列转换的源序列和目标序列之间就是对齐机制",{"2":{"277":1}}],["将注意力机制应用在一个序列内部",{"2":{"261":1}}],["将注意力机制应用在序列转换的源序列和目标序列之间",{"2":{"261":1}}],["将注意力机制应用在连续的局部块上",{"2":{"204":1}}],["将注意力头视为混合专家机制",{"2":{"42":1}}],["将向量转换成输出序列",{"2":{"885":1}}],["将向量",{"2":{"172":1}}],["将权重替换为可学习的激活函数",{"2":{"155":1}}],["将gpt",{"2":{"147":1}}],["将glu中原始的sigmoid激活函数替换为其他的激活函数",{"2":{"103":1}}],["将会得到未定义的值",{"2":{"1728":1}}],["将会得到一个形状为",{"2":{"36":1}}],["将会非常庞大",{"2":{"698":1}}],["将会预测出目标对象",{"2":{"145":1}}],["将新关系",{"2":{"145":1}}],["将包含主语",{"2":{"145":1}}],["将主语实体",{"2":{"122":1}}],["将维度降低回原始维度",{"2":{"99":1}}],["将当前位置之后位置的注意力分数设为一个很小的值",{"2":{"50":1}}],["将下层的输出作为本层的query",{"2":{"39":1}}],["将head数放在前面",{"2":{"36":1}}],["将意力分数矩阵放到一块连续的物理内存中",{"2":{"35":1}}],["例句中的注意力矩阵",{"2":{"757":1}}],["例句是5个正式token",{"2":{"450":1}}],["例如点",{"2":{"2009":1}}],["例如两个类需要互相访问对方的私有成员",{"2":{"1776":1}}],["例如运算符重载",{"2":{"1776":1}}],["例如运行平均值",{"2":{"1214":1}}],["例如记录错误日志",{"2":{"1764":1}}],["例如用户输入错误",{"2":{"1764":1}}],["例如违反前提条件",{"2":{"1762":1}}],["例如逻辑错误",{"2":{"1762":1}}],["例如通知用户",{"2":{"1761":1}}],["例如通过",{"2":{"1223":1}}],["例如通过索引或转置创建",{"2":{"1123":1}}],["例如学生",{"2":{"1728":1}}],["例如学习率",{"2":{"1137":1,"1143":1,"1147":1,"1221":1}}],["例如重载流运算符",{"2":{"1712":1}}],["例如简单的",{"2":{"1709":1}}],["例如缓冲区",{"2":{"1678":1}}],["例如释放对象占用的资源",{"2":{"1676":1}}],["例如释放资源",{"2":{"1674":1}}],["例如内部状态",{"2":{"1674":1}}],["例如内存",{"2":{"1407":1}}],["例如姓名",{"2":{"1657":1}}],["例如过多的递归调用",{"2":{"1648":1}}],["例如链表",{"2":{"1647":1}}],["例如栈区",{"2":{"1647":1}}],["例如函数参数",{"2":{"1646":1}}],["例如网络请求",{"2":{"1645":1}}],["例如网页抓取",{"2":{"1566":1}}],["例如电话",{"2":{"1645":1}}],["例如基本类型之间的转换",{"2":{"1629":1}}],["例如限制索引在一定范围内",{"2":{"1607":1}}],["例如将函数调用与函数定义连接起来",{"2":{"1604":1}}],["例如将掩码操作与softmax操作融合在一起",{"2":{"941":1,"960":1}}],["例如查找mpicc",{"2":{"1533":1}}],["例如键盘",{"2":{"1411":1}}],["例如自动摘要",{"2":{"1317":1}}],["例如自监督学习",{"2":{"1126":1}}],["例如句子分类",{"2":{"1315":1}}],["例如句子的长短造成的影响",{"2":{"326":1}}],["例如随机遮盖其中的词语",{"2":{"1315":1}}],["例如可以根据上下文预测被遮盖掉的词语",{"2":{"1312":1}}],["例如可以采用",{"2":{"706":1}}],["例如本文",{"2":{"1185":1}}],["例如误差上升而不下降",{"2":{"1179":1}}],["例如贝叶斯优化",{"2":{"1175":1}}],["例如测试数据与训练数据重叠",{"2":{"1164":1}}],["例如使用智能指针",{"2":{"1671":1}}],["例如使用",{"2":{"1161":1}}],["例如激活函数",{"2":{"1141":1}}],["例如提交给竞赛",{"2":{"1139":1}}],["例如提供少量实例来帮助",{"2":{"542":1}}],["例如时间长度",{"2":{"1138":1}}],["例如购买新硬件或重写训练工作流以实现多gpu",{"2":{"1134":1}}],["例如架构超参数",{"2":{"1130":1}}],["例如层数",{"2":{"1129":1,"1137":1}}],["例如工作流实施和优化",{"2":{"1126":1}}],["例如数据处理和模型评估",{"2":{"1121":1}}],["例如数学",{"2":{"221":1}}],["例如深度神经网络具有大量的隐藏层和参数",{"2":{"1012":1}}],["例如浮点乘加和指数",{"2":{"973":1}}],["例如应用于",{"2":{"941":1,"960":1}}],["例如softmax",{"2":{"941":1,"960":1}}],["例如src为`",{"2":{"385":1,"398":1}}],["例如概率",{"2":{"904":1}}],["例如每天的股票价格等等",{"2":{"878":1}}],["例如relu",{"2":{"838":1}}],["例如哲学",{"2":{"754":1}}],["例如上下文化信息",{"2":{"729":1}}],["例如word2vec论文中",{"2":{"713":1}}],["例如bresenham画线算法",{"2":{"2009":1}}],["例如bert和roberta",{"2":{"711":1}}],["例如bpe",{"2":{"564":1}}],["例如对于",{"2":{"674":1}}],["例如对称性或等变性",{"2":{"505":1}}],["例如同一个单词被用不同的方式进行分割",{"2":{"613":1}}],["例如hello",{"2":{"595":1}}],["例如llama3的词汇量从llama2的32k增加到128k",{"2":{"561":1}}],["例如下图所示",{"2":{"561":1}}],["例如`",{"2":{"557":1}}],["例如清理特殊符号",{"2":{"545":1}}],["例如从一个层的输出到另一个层的输入",{"2":{"505":1}}],["例如残差网络都是通过堆叠一系列的转换块",{"2":{"494":1}}],["例如target为",{"2":{"399":1}}],["例如c4",{"2":{"367":1}}],["例如添加数据增强",{"2":{"1155":1}}],["例如添加",{"2":{"351":1}}],["例如第1本书第6页",{"2":{"340":1}}],["例如有大量的重复图像",{"2":{"337":1}}],["例如在计算斐波那契数列时",{"2":{"1646":1}}],["例如在使用标准库容器时",{"2":{"1615":1}}],["例如在出现过拟合问题时使用新的正则化器",{"2":{"1140":1}}],["例如在验证数据上避免更新",{"2":{"1122":1}}],["例如在词嵌入中",{"2":{"702":1}}],["例如在句子",{"2":{"628":1}}],["例如在上图示例中",{"2":{"316":1}}],["例如在前面",{"2":{"316":1}}],["例如输出模型可以是softmax层",{"2":{"263":1}}],["例如插值",{"2":{"222":1}}],["例如风格lora和人物lora可以同时在一个模型上应用起来分别生效",{"2":{"222":1}}],["例如模型剪枝",{"2":{"152":1}}],["例如出现次数降低到100次",{"2":{"147":1}}],["例如具体术语",{"2":{"123":1}}],["例如国家名称",{"2":{"122":1}}],["例如",{"0":{"1981":1},"2":{"34":1,"99":1,"137":1,"141":1,"147":1,"189":2,"204":1,"209":1,"220":1,"221":1,"222":1,"224":1,"343":1,"400":1,"403":2,"428":1,"477":1,"536":1,"560":3,"579":1,"582":1,"594":1,"595":2,"612":1,"613":1,"629":1,"680":1,"685":1,"689":1,"692":1,"709":1,"712":1,"722":1,"732":1,"762":1,"838":2,"941":1,"960":1,"968":2,"973":1,"976":3,"977":1,"983":1,"1082":1,"1085":1,"1086":1,"1114":3,"1115":3,"1117":2,"1120":1,"1127":1,"1130":1,"1132":3,"1133":1,"1134":4,"1137":3,"1139":1,"1141":1,"1143":6,"1144":1,"1146":2,"1149":3,"1150":2,"1152":2,"1153":1,"1154":1,"1155":4,"1156":1,"1158":1,"1159":1,"1161":2,"1175":3,"1180":3,"1182":1,"1183":2,"1184":1,"1185":1,"1186":1,"1211":1,"1214":1,"1222":1,"1227":1,"1228":1,"1231":1,"1312":3,"1313":1,"1317":2,"1320":1,"1323":1,"1341":1,"1413":4,"1589":1,"1603":3,"1604":1,"1607":4,"1611":9,"1612":2,"1613":2,"1614":2,"1615":1,"1616":3,"1620":1,"1621":2,"1623":1,"1628":2,"1629":3,"1631":1,"1634":2,"1638":1,"1645":1,"1648":1,"1649":1,"1650":2,"1671":1,"1678":1,"1698":1,"1700":1,"1704":2,"1705":3,"1712":2,"1728":4,"1729":5,"1814":2,"1832":2,"1866":1,"1867":1,"1914":1,"1933":1,"1997":2,"2105":1}}],["例子中",{"2":{"1729":1}}],["例子中我们的语料库很小",{"2":{"585":1}}],["例子",{"2":{"1712":2}}],["例子2更倾向于从合作的角度来看",{"2":{"5":1}}],["例子1是从专家的专家角度来看",{"2":{"5":1}}],["通俗讲解",{"0":{"2115":1}}],["通俗地讲就是具备学习功能",{"2":{"1456":1}}],["通知方式",{"2":{"1645":1}}],["通信域定义了进程组及其作用范围",{"2":{"1577":1}}],["通信域",{"0":{"1577":1}}],["通信",{"2":{"1575":1}}],["通信模型",{"2":{"1573":1}}],["通信与块级注意力和前馈计算重叠",{"2":{"975":1}}],["通信互相传递",{"2":{"974":1}}],["通道",{"2":{"783":1}}],["通道表示不同的特征图",{"2":{"773":1}}],["通道的概念",{"2":{"773":1}}],["通用的指针",{"2":{"1611":1}}],["通用格式onnx的保存",{"0":{"1271":1},"1":{"1272":1,"1273":1,"1274":1}}],["通用且易于理解的框架",{"2":{"499":1}}],["通用结构",{"0":{"262":1},"1":{"263":1,"264":1,"265":1}}],["通常不会一下子就完成所有目标",{"2":{"2102":1}}],["通常与模板元编程一起使用",{"2":{"1912":1}}],["通常与屏幕关联",{"2":{"1811":3,"1829":3}}],["通常与键盘关联",{"2":{"1811":1,"1829":1}}],["通常返回对象的引用",{"2":{"1712":1}}],["通常返回一个新的对象",{"2":{"1712":1}}],["通常指缓冲区溢出",{"2":{"1648":1}}],["通常指代以斜坡函数及其变种为代表的非线性函数",{"2":{"840":1}}],["通常为",{"2":{"1607":1}}],["通常为一个常数或者让模型自适应学习得到",{"2":{"108":1}}],["通常占用",{"2":{"1607":9}}],["通常系统已经自带",{"2":{"1605":1}}],["通常和mpi",{"2":{"1575":1}}],["通常称为rank",{"2":{"1573":1}}],["通常称为出租车距离或城市街区距离",{"2":{"692":1}}],["通常缩写为mlp",{"2":{"1464":1}}],["通常假设当前时刻的状态只与前一个时刻的状态相关",{"2":{"1326":1}}],["通常我们会使用线性链crf来建模ner任务",{"2":{"1322":1}}],["通常需要重写",{"2":{"1763":1}}],["通常需要处理深拷贝",{"2":{"1712":1}}],["通常需要提供示例输入来触发模型的执行并捕获跟踪",{"2":{"1292":1}}],["通常需要在特定领域的数据集上进行实证评估",{"2":{"692":1}}],["通常用于需要访问两个操作数私有成员的情况",{"2":{"1712":1}}],["通常用于定义类的接口",{"2":{"1677":1}}],["通常用于更新循环控制变量的值",{"2":{"1621":1}}],["通常用于初始化循环控制变量",{"2":{"1621":1}}],["通常用于调试和打印模块的可读表示",{"2":{"1214":1}}],["通常用于将非数值数据转化为数值形式",{"2":{"680":1}}],["通常将其传递给优化器",{"2":{"1214":1}}],["通常将",{"2":{"1183":1}}],["通常建议将数据文件在多台主机之间进行分片",{"2":{"1169":1}}],["通常表明训练工作流中存在错误",{"2":{"1149":1}}],["通常表现为实际应用中输出不准确或者错误的结果",{"2":{"121":1}}],["通常来说",{"2":{"1131":1}}],["通常被认为对超参数的选择相当鲁棒",{"2":{"1059":1}}],["通常被称为kv",{"2":{"981":1}}],["通常被称为convection",{"2":{"498":2}}],["通常被称为diffusion",{"2":{"498":2}}],["通常情况下最优学习率和模型结构有关",{"2":{"1143":1}}],["通常情况下",{"2":{"941":1,"960":1,"1152":1,"1228":1,"1638":1}}],["通常都是极其复杂的",{"2":{"908":1}}],["通常叫做输入",{"2":{"770":1}}],["通常有两种选择",{"2":{"706":1}}],["通常在头文件中放置函数和变量的声明",{"2":{"1628":1}}],["通常在使用欧式距离度量之前",{"2":{"692":1}}],["通常在代码实现中把这个输出叫做memory",{"2":{"518":1}}],["通常是4或8字节",{"2":{"1680":1}}],["通常是数据",{"2":{"1674":1}}],["通常是方法",{"2":{"1674":1}}],["通常是屏幕",{"2":{"1673":1}}],["通常是键盘",{"2":{"1673":1}}],["通常是",{"2":{"1667":1}}],["通常是一个字典",{"2":{"1214":1}}],["通常是一个softmax神经网络层",{"2":{"899":1}}],["通常是一个较大的数字",{"2":{"700":1}}],["通常是整数",{"2":{"702":1}}],["通常是词汇表的大小",{"2":{"702":1}}],["通常是二维或三维",{"2":{"691":1}}],["通常是文本数据中的单词或字符",{"2":{"677":1}}],["通常是高维且稀疏",{"2":{"676":1}}],["通常是softmax",{"2":{"185":1}}],["通常低维且稠密",{"2":{"676":1}}],["通常通过神经网络或优化算法学习",{"2":{"676":1}}],["通常使用后要加一个contiguous",{"2":{"658":1}}],["通常每个动作块包含30",{"2":{"637":1}}],["通常n的选取无法包含训练集中的所有词",{"2":{"565":1}}],["通常内容是数值0或者随机值",{"2":{"249":1}}],["通常会将指针设置为",{"2":{"1647":1}}],["通常会选择",{"2":{"1605":1}}],["通常会影响验证集的性能",{"2":{"1186":1}}],["通常会使用大规模的语料库",{"2":{"505":1}}],["通常会使用交叉熵损失函数来配合softmax函数",{"2":{"180":1}}],["通常会禁用dropout",{"2":{"392":1}}],["通常会包括学术文献",{"2":{"367":1}}],["通常会在序列的末尾添加一些特殊的填充符号",{"2":{"50":1}}],["通常",{"2":{"34":1,"694":1,"977":1,"1127":1,"1133":1,"1147":1,"1150":1,"1164":1,"1197":1,"1211":1,"1678":1,"1709":2,"1728":1}}],["通过贪心算法",{"2":{"2135":1}}],["通过贪心思维",{"2":{"2126":1}}],["通过理解和掌握这种思维方式",{"2":{"2108":1}}],["通过字符串初始化bitset",{"2":{"2062":1}}],["通过字典学习分解语言模型",{"2":{"156":1}}],["通过字典学习和稀疏自编码器",{"2":{"137":1}}],["通过编程的方式实现艺术的效果",{"2":{"2010":1}}],["通过编辑键值对来修改llm中的事实信息",{"2":{"145":1}}],["通过映射图像纹理增加物体表面的细节",{"2":{"2009":1}}],["通过示例理解这些新特性的实际应用与意义",{"2":{"1876":1}}],["通过示例演示如何避免多继承中的冗余继承问题",{"2":{"1869":1}}],["通过公共继承树的共享避免冗余",{"2":{"1869":1}}],["通过cache类的public接口访问",{"2":{"1867":1}}],["通过cp",{"2":{"976":1}}],["通过实际代码示例掌握算法的使用方法",{"2":{"1731":1}}],["通过实验",{"2":{"127":1}}],["通过合理地组织函数",{"2":{"1729":1}}],["通过合适的语法和结构来表达复杂的逻辑和操作",{"2":{"504":1}}],["通过合适的设计",{"2":{"504":1}}],["通过键访问并赋值",{"2":{"1725":1}}],["通过下标访问字符串元素",{"2":{"1713":2}}],["通过函数指针调用",{"2":{"1706":2}}],["通过函数指针",{"2":{"1706":1}}],["通过指定具体的类型参数来实例化类模板",{"2":{"1700":1}}],["通过指针访问成员",{"2":{"1635":1}}],["通过注册回调函数来扩展程序的功能",{"2":{"1645":1}}],["通过注意力机制得到新的输出a",{"2":{"727":1}}],["通过注意力块进行计算",{"2":{"620":1}}],["通过注意力头将输入起始位置的信息复制到了最末位置",{"2":{"437":1}}],["通过注意头的中间表示的dla",{"2":{"479":1}}],["通过回调函数通知程序",{"2":{"1645":1}}],["通过传递不同的回调函数",{"2":{"1645":1}}],["通过类名访问静态成员函数",{"2":{"1639":1}}],["通过类名访问静态成员变量",{"2":{"1639":1}}],["通过安装",{"2":{"1589":1,"1605":1}}],["通过调用系统的",{"2":{"1589":1}}],["通过调整模型的参数来最小化损失函数",{"2":{"1023":1}}],["通过mpi",{"2":{"1576":1}}],["通过密码和名字查询用户",{"2":{"1485":1}}],["通过sql语句可以满足操作数据库的所有需求",{"2":{"1479":1}}],["通过sonar",{"2":{"629":1}}],["通过文档和源代码",{"2":{"1479":1}}],["通过框架可以减少重复代码",{"2":{"1479":1}}],["通过统计学的标准数学方法我们能够得到大量的可以用函数来表达的局部结构空间",{"2":{"1456":1}}],["通过统计文本中字符对的频率进而不断合并文本数据中最频繁出现的字符或字符序列",{"2":{"575":1}}],["通过随机遮盖掉输入中的文本片段进行预训练",{"2":{"1317":1}}],["通过扩大原始模型和训练集创造了",{"2":{"1316":1}}],["通过根据上文预测下一个单词的预训练任务",{"2":{"1316":1}}],["通过三处变化使得",{"2":{"1315":1}}],["通过升级训练数据来改进多语言预训练",{"2":{"1315":1}}],["通过修改预训练方案可以进一步提高性能",{"2":{"1315":1}}],["通过修改大模型权重并利用这些权重来让大模型完全整合知识",{"2":{"141":1}}],["通过微调可以激发出模型在预训练过程中获得的知识",{"2":{"1313":1}}],["通过微调参数使模型适用于新任务",{"2":{"1313":1}}],["通过微调注意力结构",{"2":{"745":1}}],["通过trace",{"2":{"1269":1}}],["通过t步cot",{"2":{"480":1}}],["通过逐步调整学习率来",{"2":{"1242":1}}],["通过逐步添加噪声",{"2":{"634":1}}],["通过wrapper的方式给step方法加补丁",{"2":{"1227":1}}],["通过w^q",{"2":{"36":1}}],["通过名称获取",{"2":{"1214":1}}],["通过名称获取parameter",{"2":{"1214":1}}],["通过名称获取子模块",{"2":{"1214":1}}],["通过查阅代码解决",{"2":{"1203":1}}],["通过自举",{"2":{"1177":1}}],["通过网络读取训练数据时可能会发生这种情况",{"2":{"1161":1}}],["通过网络的层级传播梯度时",{"2":{"838":1}}],["通过运行更多的短时间的实验",{"2":{"1157":1}}],["通过连续性在当前点定义梯度",{"2":{"1115":1}}],["通过连接所有层的路由权重",{"2":{"739":1}}],["通过从根节点到叶节点追溯这个图",{"2":{"1113":1}}],["通过考虑历史梯度的方向和幅度",{"2":{"1032":1}}],["通过考虑历史梯度的平均方向",{"2":{"1031":1,"1032":1}}],["通过积累历史梯度信息",{"2":{"1032":1}}],["通过首先淘汰叶子节点",{"2":{"986":1}}],["通过简单地在命令行中设置context",{"2":{"976":1}}],["通过跨卡的",{"2":{"974":1}}],["通过结合低秩键值联合压缩",{"2":{"956":1}}],["通过结合规则清洗和去重程序对文档质量进行严格评估",{"2":{"369":1}}],["通过存储输出o和softmax归一化统计信息",{"2":{"946":1,"966":1}}],["通过以上过程我们发现其中涉及的原理",{"2":{"908":1}}],["通过设置大量的翻译规则",{"2":{"907":1}}],["通过设计合适的mask",{"2":{"70":1}}],["通过初始化decoder参数",{"2":{"898":1}}],["通过门向细胞状态添加或移除信息",{"2":{"863":1}}],["通过添加驱动程序计算机中的硬件就能正常的工作",{"2":{"794":1}}],["通过绝对位置编码来实现相对位置编码",{"2":{"767":1}}],["通过线性偏置项",{"2":{"765":1}}],["通过线性变换和球面投影将这四个点进行了正确分类",{"2":{"320":1}}],["通过不断排查发现",{"2":{"2064":1}}],["通过不断合并来产生最终的词表",{"2":{"598":1}}],["通过不同的参数化分别计算词的上下文相关性和位置相关性",{"2":{"764":1}}],["通过矩阵",{"2":{"760":1}}],["通过加权求和可以平衡rw和hs嵌入的贡献",{"2":{"739":1}}],["通过加权插值来组合多个预训练的",{"2":{"225":1}}],["通过提供dao层",{"2":{"1479":1}}],["通过提供更稳定的训练信号",{"2":{"310":1}}],["通过提示词工程",{"2":{"736":1}}],["通过提示工程的方法",{"0":{"736":1},"2":{"736":1}}],["通过增加潜在注意力层来提高表征能力",{"2":{"735":1}}],["通过最大化同一序列的两个不同表示之间的相似性",{"2":{"734":1}}],["通过在序列开头添加特殊的",{"2":{"1316":1}}],["通过在预训练期间使用知识蒸馏",{"2":{"1315":1}}],["通过在输入块q",{"2":{"964":1}}],["通过在将每个块的输出乘以正确的归一化因子之前进行缩放并将它们相加",{"2":{"942":1,"959":1}}],["通过在特定区域内对特征进行",{"2":{"813":1}}],["通过在确定的范围内裁剪相对位置",{"2":{"759":1}}],["通过在模型的嵌入层中引入可学习的参数来学习位置信息的表示",{"2":{"749":1}}],["通过在少而精的下游任务labeled",{"2":{"726":1}}],["通过在不同",{"2":{"561":1}}],["通过训练来让模型自己学习如何关联",{"2":{"712":1}}],["通过训练可以学习到类别之间的语义相似性",{"2":{"702":1}}],["通过互相传递信息来更新自己的值",{"2":{"709":1}}],["通过乘一个",{"2":{"701":1}}],["通过词嵌入层将输入x编码为向量",{"2":{"701":1}}],["通过词表将token转换成计算机更容易处理的数字信息",{"2":{"545":1}}],["通过梯度下降学习到更有意义的词向量表示",{"2":{"699":1}}],["通过相关性",{"2":{"689":1}}],["通过相似度计算函数来计算注意力得分向量e",{"2":{"264":1}}],["通过位置编码为模型提供序列中每个单词的位置信息",{"2":{"674":1}}],["通过优化预测的下一个概念与真实的下一个概念的距离来优化参数",{"2":{"632":1}}],["通过减少周期之间的时间并允许并行运行更多实验来提高调整效率",{"2":{"1137":1}}],["通过减少信息熵",{"2":{"612":1}}],["通过减少internal",{"2":{"310":1}}],["通过轻量级的编码器和解码器模块将任意字节组分组为潜在的patch表示",{"2":{"612":1}}],["通过直接建模原始字节流",{"2":{"610":1}}],["通过直接插入新的键值关联的rank",{"2":{"145":1}}],["通过层数的增加",{"2":{"510":1}}],["通过严格的数学分析",{"2":{"507":1}}],["通过单子来定义模型必须满足的约束",{"2":{"505":1}}],["通过聚焦于",{"2":{"485":1}}],["通过损失函数进行量化差异",{"2":{"484":1}}],["通过串行的方式去一步步推理",{"2":{"480":1}}],["通过变化程度定位到对最终输出最重要的位置",{"2":{"475":1}}],["通过输入翻译开始符来启动解码器",{"2":{"445":1}}],["通过掩码可以确保后面的词不会参与前面词的计算",{"2":{"443":1}}],["通过掩码可以单独调节每一个源元素与每一个目标元素之间的注意力强度",{"2":{"409":1}}],["通过掩码来控制模型在计算注意力分数时的关注范围",{"2":{"439":1}}],["通过计算端点之间的增量差值",{"2":{"2018":1}}],["通过计算向量之间的余弦相似度",{"2":{"692":1}}],["通过计算查询和键向量的点积",{"2":{"355":1}}],["通过计算相似性得出权重最后加权求和",{"2":{"170":1,"271":1}}],["通过可学习的参数和以及归一化函数",{"2":{"354":1}}],["通过使用线性扩展的稀疏注意力形式",{"2":{"1317":1}}],["通过使用更大的数据集进行预训练",{"2":{"1316":1}}],["通过使用",{"2":{"1161":1,"1214":1}}],["通过使用之前在大数据集上经过训练的预训练模型",{"2":{"1009":1}}],["通过使用预训练模型初始化decoder的参数",{"2":{"898":2}}],["通过使用批归一化等技术",{"2":{"838":1}}],["通过使用这些",{"2":{"463":1}}],["通过使用超参数lamda来控制两个归一化层的比例",{"2":{"348":1}}],["通过使其更适合gpu单元的方式进一步加速计算",{"2":{"185":1}}],["通过把确切分类目标从0",{"2":{"1016":1}}],["通过把一部分不重复的复杂信息损失掉",{"2":{"321":1}}],["通过把序列元素两两直接比较",{"2":{"291":1}}],["通过多层的计算后参数可能开始出现过大或过小的情况",{"2":{"309":1}}],["通过多头注意力计算后",{"2":{"36":1}}],["通过残差学习的重构",{"2":{"301":1}}],["通过并行训练两个子网络",{"2":{"298":1}}],["通过a提供的权重来求和lstm隐藏状态h",{"2":{"289":1}}],["通过attention进一步转化simple",{"2":{"209":1}}],["通过attention函数计算出attention结果",{"2":{"36":1}}],["通过拓展隐状态的长度",{"2":{"272":1}}],["通过上面的例子",{"2":{"861":1}}],["通过上面的原理分析",{"2":{"262":1}}],["通过上一步产生的token和这一步的输入来预测这一步的输出",{"2":{"414":1}}],["通过上下文来预测中心词",{"2":{"50":1}}],["通过句子中的其它词对我们关注词的语义进行推断",{"2":{"259":1}}],["通过叠加多层卷积区去把局部感受野进行扩大",{"2":{"247":1}}],["通过学习大量数据来提取特征和模式",{"2":{"1455":1}}],["通过学习前面单词序列的统计规律就可以预测下一个单词",{"2":{"238":1}}],["通过学习一个字典",{"2":{"137":1}}],["通过额外的注意力来增强自注意力机制用于序列推荐",{"2":{"233":1}}],["通过额外关注注意力权重来增强自注意力机制的有效性",{"2":{"209":1}}],["通过有选择地调整模型权重的关键组成部分",{"2":{"225":1}}],["通过将数据设为私有",{"2":{"1677":1}}],["通过将数据的生产者和消费者分割为单独的warp",{"2":{"973":1}}],["通过将学习率减小2",{"2":{"1245":1}}],["通过将",{"2":{"1211":1,"1312":1}}],["通过将这些值转换为介于",{"2":{"866":1}}],["通过将正负值均匀分布在激活函数的输出范围内",{"2":{"838":1}}],["通过将输入张量重塑为一维张量来对其进行扁平化",{"2":{"828":1}}],["通过将卷积核进行翻转",{"2":{"779":1}}],["通过将knew",{"2":{"623":1}}],["通过将问题抽象成范畴论的语言",{"2":{"505":1}}],["通过将梯度矩阵映射到词汇空间",{"2":{"484":1}}],["通过将梯度矩阵映射到词汇空间来揭示",{"2":{"148":1}}],["通过将激活值先经过transformer的最终归一化层",{"2":{"482":1}}],["通过将矩阵a和lstm隐藏状态h相乘得到句子嵌入",{"2":{"289":1}}],["通过将负成分映射到零来消除它们",{"2":{"213":1}}],["通过将query与key中携带的信息摘要相比较",{"2":{"169":1}}],["通过点积这个提取特征的操作将输入序列中的每个词与其他词关联起来",{"2":{"167":1}}],["通过其独特的结构设计和权重表示方式",{"2":{"155":1}}],["通过用记忆层替换一个或多个",{"2":{"154":1}}],["通过预测文本中被遮盖的词语和判断一个文本是否跟随另一个来进行预训练",{"2":{"1315":1}}],["通过预先设定的最大相对位置k来强化模型对以当前词为中心的左右各k个词的注意力计算",{"2":{"759":1}}],["通过预计算和内存查找",{"2":{"153":1}}],["通过预训练找到每层组合分段线性函数",{"2":{"505":1}}],["通过预训练",{"2":{"116":1}}],["通过引入可学习的向量",{"2":{"153":1}}],["通过引入内存层替代全连接层",{"2":{"152":1}}],["通过引入图论方法",{"2":{"92":1}}],["通过分析注意力权重",{"2":{"512":1}}],["通过分析",{"2":{"489":1}}],["通过分析反向传播中的梯度矩阵",{"2":{"148":1,"484":1}}],["通过分析这些点的信息",{"2":{"122":1}}],["通过求解约束最小二乘问题",{"2":{"145":1}}],["通过求解",{"2":{"145":1}}],["通过一个路由机制来判断是否使用辅助记忆",{"2":{"143":1}}],["通过保留重要的边就可以构造出关于这个事实的回路",{"2":{"130":1}}],["通过保留被归类为主要类别的注意力头",{"2":{"20":1}}],["通过消融节点",{"2":{"130":1}}],["通过对象访问静态成员函数",{"2":{"1639":1}}],["通过对象访问静态成员变量",{"2":{"1639":1}}],["通过对l",{"2":{"1003":1}}],["通过对当前小批次的均值和方差进行归一化",{"2":{"314":1}}],["通过对信息流的分析",{"2":{"122":1}}],["通过对词语之间依存关系的分析",{"2":{"20":1}}],["通过它可以增加特征学习能力",{"2":{"117":1}}],["通过第二个线性层w2",{"2":{"113":1}}],["通过构建非平凡的反例",{"2":{"91":1}}],["通过排序",{"2":{"90":1}}],["通过这一过程",{"2":{"727":1}}],["通过这一层",{"2":{"525":1}}],["通过这一方法",{"2":{"148":1,"484":1}}],["通过这种方法",{"2":{"2130":1}}],["通过这种方式",{"2":{"488":1,"536":1,"621":1}}],["通过这种分解的方式",{"2":{"698":1}}],["通过这种分析",{"2":{"485":1}}],["通过这些特性",{"2":{"1901":1}}],["通过这些先验约束",{"2":{"242":1}}],["通过这些权重矩阵可以把原始高维向量分解成",{"2":{"12":1}}],["通过这些权重矩阵的转换",{"2":{"9":1}}],["通过这个操作",{"2":{"70":1,"172":1}}],["通过重用预训练的模型提供增量模型缩放所需的灵活性",{"2":{"621":1}}],["通过重叠相邻head",{"2":{"41":1}}],["通过重叠head来改进多head自注意力",{"2":{"41":1}}],["通过重塑",{"2":{"35":1}}],["通过重塑结果矩阵以消除",{"2":{"35":1}}],["通过全连接层的线性变换把拼合好的输出进行有机融合",{"2":{"35":1}}],["通过观察大量样本的attention矩阵我们发现",{"2":{"14":1}}],["通过海量的对照语料训练集训练出来的",{"2":{"12":1}}],["通过",{"2":{"10":1,"17":1,"210":1,"246":1,"265":1,"985":1,"1087":1,"1110":1,"1317":1,"1985":1,"1993":1,"1994":1}}],["通过处理大量数据并自我学习",{"2":{"9":1}}],["通过反向传播和梯度下降",{"2":{"1":1}}],["点对点通信",{"2":{"1573":1}}],["点击展开",{"2":{"1132":1,"1133":1,"1134":1,"1135":1,"1136":1,"1143":1,"1144":1,"1145":1,"1147":1,"1148":1,"1149":1,"1150":1,"1151":1,"1156":1,"1158":1,"1159":1,"1163":1,"1164":1,"1165":1,"1171":1,"1172":1,"1173":1,"1174":1,"1175":1,"1176":1,"1177":1,"1178":1,"1185":1,"1186":1,"1187":1}}],["点乘的结果都处在一个合理的范围内",{"2":{"187":1}}],["点乘注意力能够捕捉查询和键之间的相似度",{"2":{"175":1}}],["点乘注意力和加法注意力的性能差异不大",{"2":{"175":1}}],["点乘操作可以通过矩阵乘法高效地在硬件上并行化",{"2":{"175":1}}],["点乘结果的差异越来越大",{"2":{"175":1}}],["点乘结果的上限越来越高",{"2":{"175":1}}],["点乘结果再通过reshape和转置整理为8个头在第2维上的拼接",{"2":{"32":1}}],["点之间的褐色连线代表这他们之间的信息流动",{"2":{"130":1}}],["点积并非总是最合适的选择",{"2":{"692":1}}],["点积为1∗10+1∗10=20",{"2":{"692":1}}],["点积的结果很大程度上依赖于向量的大小",{"2":{"692":1}}],["点积的一致性",{"2":{"212":1}}],["点积运算的几何意义是两个向量的夹角",{"2":{"692":1}}],["点积或标量积的公式如下",{"2":{"692":1}}],["点积",{"2":{"692":3}}],["点积大幅度增长",{"2":{"647":1,"924":1}}],["点积结果是一个反映所有输入token之间关系的矩阵",{"2":{"464":1}}],["点积结果的分布会趋近于陡峭",{"2":{"187":1}}],["点积时会导致显著的信息丢失",{"2":{"213":1}}],["点积中的",{"2":{"212":1}}],["点积方差正比于维度数量",{"2":{"189":1}}],["点积计算的是对齐后的长度的乘积",{"2":{"176":1}}],["点积是两个向量的夹角",{"2":{"176":1}}],["点积注意力函数",{"0":{"174":1},"1":{"175":1,"176":1},"2":{"157":1}}],["点积上施加掩码",{"2":{"70":1}}],["点积自注意力",{"2":{"16":1}}],["逻辑推理",{"0":{"2105":1}}],["逻辑非和按位取反",{"2":{"1635":1}}],["逻辑非",{"2":{"1619":1,"1630":1}}],["逻辑或",{"2":{"1619":1,"1630":1,"1729":1}}],["逻辑与",{"2":{"1619":1,"1630":1,"1635":1,"1729":1}}],["逻辑运算符",{"2":{"1619":1,"1630":1}}],["逻辑运算简图",{"2":{"963":1}}],["逻辑回归",{"2":{"1461":1}}],["逻辑维度",{"0":{"414":1},"1":{"415":1,"416":1}}],["逻辑和src",{"2":{"380":1}}],["逻辑和src类似",{"2":{"380":1}}],["逻辑",{"0":{"61":1,"69":1},"1":{"62":1,"63":1,"70":1,"71":1,"72":1},"2":{"49":2}}],["逻辑矩阵形状是",{"2":{"30":1}}],["逻辑角度",{"0":{"28":1},"2":{"0":1}}],["线段",{"2":{"2009":1}}],["线段长度代表向量的大小",{"2":{"680":1}}],["线性代数",{"2":{"2009":1}}],["线性可分与线性不可分",{"2":{"1462":1}}],["线性相乘",{"2":{"1398":1}}],["线性相关奇点",{"2":{"305":1}}],["线性项导数计算",{"2":{"1393":1}}],["线性连接层",{"0":{"1446":1}}],["线性连接",{"2":{"1392":1}}],["线性体现在线性序列中的元素之间存在明确的顺序关系",{"2":{"1322":1}}],["线性crf的定义",{"0":{"1322":1}}],["线性回归或逻辑回归",{"2":{"1014":1}}],["线性模型",{"2":{"1011":1}}],["线性神经网络的问题",{"2":{"838":1}}],["线性插值法及相关改进源码阅读及相关记录",{"2":{"768":1}}],["线性的偏置意味着当相对距离很大时",{"2":{"765":1}}],["线性随机注意力",{"2":{"512":1}}],["线性组合",{"2":{"485":1}}],["线性层和softmax层",{"2":{"471":1}}],["线性层的输入维度也是第二个线性层的输出维度",{"2":{"113":1}}],["线性层的维度是",{"2":{"30":1}}],["线性变换层",{"0":{"803":1},"1":{"804":1,"805":1}}],["线性变换",{"2":{"466":1,"1388":1,"1389":1}}],["线性变化就是用wq",{"2":{"161":1}}],["线性化公式可以获得o",{"2":{"210":1}}],["线性attention的秩更低",{"2":{"180":1}}],["线性乘积下秩不会超过d",{"2":{"180":1}}],["线性注意力会降低注意力运算的效果",{"2":{"232":1}}],["线性注意力会导致权重分布更加均匀且熵更低",{"2":{"212":1}}],["线性注意力则可以把复杂度控制在线性增加范围",{"2":{"214":1}}],["线性注意力通过递归更新键值矩阵的乘积",{"2":{"210":1}}],["线性注意力的目标是使用",{"2":{"210":1}}],["线性注意力",{"0":{"210":1},"2":{"157":1,"212":1,"512":1}}],["线性表示假设",{"2":{"137":1}}],["线程是进程中的更小执行单元",{"2":{"1563":1}}],["线程",{"2":{"34":1,"1563":1}}],["线程中的底层低级实现",{"2":{"34":1}}],["线条越粗",{"2":{"265":1,"277":1}}],["线条越粗表示attention的权重越大",{"2":{"18":1}}],["线条的粗细就是权重",{"2":{"265":1}}],["线条的粗细分布就叫做",{"2":{"265":1}}],["线条的粗细表示注意力权重的强度",{"2":{"18":1,"20":1}}],["所指向的内存地址中存储的值",{"2":{"1611":1}}],["所占用的内存大小",{"2":{"1607":1}}],["所贡献的内容将保留其相关版权",{"2":{"1197":1}}],["所使用的gpu数量增加一倍",{"2":{"1134":1}}],["所理解的英语结构的复杂性",{"2":{"437":1}}],["所在的label设置为0",{"2":{"399":1}}],["所谓数据的生成",{"2":{"1371":1}}],["所谓相对位置",{"2":{"759":1,"1339":1}}],["所谓动态词向量就是在训练一个特定任务的同时",{"2":{"706":1}}],["所谓",{"2":{"334":1}}],["所谓masked",{"2":{"71":1}}],["所做都是",{"2":{"312":1}}],["所评价的",{"2":{"279":1}}],["所携带的信息会被忽略",{"2":{"253":1}}],["所用到的模型就是编码器",{"2":{"241":1}}],["所对应的索引",{"2":{"423":1}}],["所对应的内容",{"2":{"164":1}}],["所对应的权重值变高",{"2":{"163":1}}],["所示",{"2":{"154":1,"231":1,"320":4}}],["所需的步骤数减少一半",{"2":{"1134":1}}],["所需的计算分为两个部分",{"2":{"624":1}}],["所需要修改的神经元数目",{"2":{"143":1}}],["所需线性层更少",{"2":{"29":1}}],["所经过回路",{"2":{"130":1}}],["所取代",{"2":{"98":1}}],["所幸现在的精调框架大多能够通过",{"2":{"88":1}}],["所有位初始为0",{"2":{"2062":1}}],["所有解读和参悟都是我个人理解",{"2":{"2052":1}}],["所有算法通常进行",{"2":{"2019":1}}],["所有算法主要以斜率为",{"2":{"2014":1}}],["所有权转移到",{"2":{"1695":1}}],["所有权重梯度求解",{"2":{"1450":1}}],["所有条件都为真",{"2":{"1619":1}}],["所有mpi函数之后必须调用",{"2":{"1575":1}}],["所有处理器共享相同的内存空间",{"2":{"1568":1}}],["所有实现会更加简单",{"2":{"1479":1}}],["所有激活梯度求解",{"2":{"1450":1}}],["所有参数应该是",{"2":{"1221":1}}],["所有流行的优化算法的更新规则是什么",{"0":{"1187":1},"1":{"1188":1,"1189":1,"1190":1,"1191":1,"1192":1,"1193":1}}],["所有在残差前的f",{"2":{"1180":1}}],["所有超参数都将是目标超参数",{"2":{"1143":1}}],["所有这些模式都可以通过上下文管理器和装饰器进行切换",{"2":{"1118":1}}],["所有非叶张量将自动具有",{"2":{"1117":1}}],["所有weight的权重衰减系数相同吗",{"2":{"1014":1}}],["所有共享gpt代码路径的模型也应该能够受益于cp",{"2":{"976":1}}],["所有张量必须具有相同的形状",{"2":{"825":1}}],["所有subword的出现都是独立的",{"2":{"599":1}}],["所有token都收敛到共识均衡",{"2":{"507":1}}],["所有token都会渐近地趋于收敛",{"2":{"507":1}}],["所有嵌入向量组合在一起形成嵌入矩阵",{"2":{"431":1}}],["所有单词可以同时流入编码器中",{"2":{"415":1}}],["所有样本×所有h×所有w所有样本×所有h×所有w所有样本",{"2":{"325":1}}],["所有句子的同一位置的token的embedding做归一化",{"2":{"323":1}}],["所有信息都会通过",{"2":{"301":1}}],["所有的解读其实都是",{"2":{"2052":1}}],["所有的增删改操作都需要提交事务",{"2":{"1488":1}}],["所有的事情",{"2":{"1479":1}}],["所有的模型权重都被随机初始化",{"2":{"1313":1}}],["所有的优化器都实现了",{"2":{"1223":1}}],["所有的提交",{"2":{"1198":1}}],["所有的计算都需要将计算对象转换成数值才行",{"2":{"677":1}}],["所有的输入向量共同参与了这个过程",{"2":{"519":1}}],["所有的文本序列就会有相同的长度",{"2":{"376":1}}],["所有的上下文都存储在列表中",{"2":{"273":1}}],["所有的序列建模都在做如下操作",{"2":{"273":1}}],["所有输出值的和应该等于1",{"2":{"180":1}}],["所有输出的和为1",{"2":{"178":1}}],["所有可能的平滑映射组成的空间是无限维的",{"2":{"116":1}}],["所有其他令牌都可以在固定层数内直接或间接参与",{"2":{"93":1}}],["所有",{"2":{"29":1,"318":1,"323":1,"325":1,"326":1,"700":1,"1130":1,"1891":1}}],["所以大家不用太怕",{"2":{"2112":1}}],["所以返回值类型是",{"2":{"1729":1}}],["所以务必谨慎",{"2":{"1611":1}}],["所以即使对象不需要永久保存",{"2":{"1477":1}}],["所以也是数学统计学方法的一种实际应用",{"2":{"1456":1}}],["所以也有一种说法是attention是基于图的一个信息传递机制",{"2":{"201":1}}],["所以叫做变分推断",{"2":{"1377":1}}],["所以直接贝叶斯这个方法报废",{"2":{"1377":1}}],["所以直接用矩阵乘法来实现会很浪费算力",{"2":{"1344":1}}],["所以增强了人像的生成效果",{"2":{"1363":1}}],["所以给它们都分配一个独立的位置编码",{"2":{"1340":1}}],["所以本实验将聚焦在线性链crf来探讨",{"2":{"1322":1}}],["所以通常不必要支持预期提前停止",{"2":{"1166":1}}],["所以有必要对于针对每个batch",{"2":{"1135":1}}],["所以它们可以共用一个位置编码",{"2":{"1340":1}}],["所以它们在反向传播中的",{"2":{"1117":1}}],["所以它们的",{"2":{"135":1}}],["所以并不影响有效特征的提取",{"2":{"1019":1}}],["所以如果要改进seq2seq结构",{"2":{"892":1}}],["所以如果能靠增大模长∥∥kj∥∥∥kj∥∥k",{"2":{"176":1}}],["所以数据的幅度会随着模型层数的增加不断扩张",{"2":{"840":1}}],["所以最大池化更多保留些图像的纹理信息",{"2":{"814":1}}],["所以最终src",{"2":{"382":1}}],["所以共享矩阵不合理",{"2":{"764":1}}],["所以显然就不好像ape那样直接加到输入上了",{"2":{"757":1}}],["所以相对位置编码通常也有着优秀的表现",{"2":{"744":1,"1338":1}}],["所以研究人员直接搬用了simcse的无监督对比学习",{"2":{"734":1}}],["所以一旦模型训练完成",{"2":{"715":1}}],["所以一般采用以下两种方式进行转换",{"2":{"731":1}}],["所以一般不需要此步骤",{"2":{"553":1}}],["所以一般tgt处理时",{"2":{"381":1}}],["所以一般称之为volumetric",{"2":{"315":1}}],["所以一般称之为spatial",{"2":{"315":1}}],["所以一般称之为temporal",{"2":{"315":1}}],["所以positionalencoding类的输入是embedding的输出",{"2":{"704":1}}],["所以pre",{"2":{"334":2}}],["所以当在山谷附近时",{"2":{"1034":1}}],["所以当加上",{"2":{"701":1}}],["所以当需要增加模型规模时无法重用以前的小规模模型",{"2":{"618":1}}],["所以两者维度相同",{"2":{"698":1}}],["所以此操作也是一个整数",{"2":{"698":1}}],["所以此时天然已经获取了历史译文的内容",{"2":{"536":1}}],["所以在图中我们可以看到除了第一步",{"2":{"902":1}}],["所以在处理拼写错误",{"2":{"696":1}}],["所以在预测第一个输出",{"2":{"409":1}}],["所以他们距离物理学家就相对较远",{"2":{"685":1}}],["所以无论如何",{"2":{"1134":1}}],["所以无论在什么地方",{"2":{"246":1}}],["所以无法表达数学家和物理学家之间的密切相关性",{"2":{"685":1}}],["所以词表大小要取一个合适的中间值",{"2":{"582":1}}],["所以把原有单词从词表中驱除",{"2":{"582":1}}],["所以按单词粒度来切分所构造的词典太过庞大",{"2":{"565":1}}],["所以是最完整的",{"2":{"564":1}}],["所以是将输入x与dropout后的子层输出结果相加作为最终的子层连接输出",{"2":{"344":1}}],["所以解码器的输入之一是",{"2":{"528":1}}],["所以解码器有两个输入",{"2":{"524":1}}],["所以使用了掩码技术",{"2":{"525":1}}],["所以z1和z2各自独立通过全连接神经网络",{"2":{"519":1}}],["所以z1和z2各自都有从x1和x2得来的信息",{"2":{"519":1}}],["所以栈的其它encoderlayer的输入是上一个encoderlayer的输出",{"2":{"518":1}}],["所以可以解决一词多义的问题",{"2":{"715":1}}],["所以可以使用现成的微分方程求解器来进行计算",{"2":{"492":1,"496":1}}],["所以可以采用最相似的token来近似计算得到最终的",{"2":{"204":1}}],["所以基于transformer的大语言模型",{"2":{"490":1}}],["所以模型需要对所有它知道的单词均按可能性打分",{"2":{"473":1}}],["所以batch",{"2":{"460":1}}],["所以batchnorm需要平衡小批次统计量和整体样本统计量之间的关系",{"2":{"316":1}}],["所以输出out的维度也是变化的",{"2":{"428":1}}],["所以配合掩码",{"2":{"416":1}}],["所以会导致用teacher",{"2":{"411":1}}],["所以会带来信息遗失问题",{"2":{"253":1}}],["所以所以公式中的",{"2":{"399":1}}],["所以特征会更加聚拢",{"2":{"399":1}}],["所以依然要计算",{"2":{"380":1}}],["所以还是对模型处理带来负面影响",{"2":{"377":1}}],["所以降低深度导致无形中层数变少",{"2":{"334":1}}],["所以降维可以显著减少计算量",{"2":{"21":1}}],["所以ln和句子长度",{"2":{"326":1}}],["所以对整个batch归一化不适合图像风格化中",{"2":{"809":1}}],["所以对20米以内的猎物",{"2":{"713":1}}],["所以对embedding进行归一化是必要操作",{"2":{"326":1}}],["所以对于",{"2":{"78":1}}],["所以推理过程的行为和训练一致",{"2":{"326":1}}],["所以推理效果受限于信息压缩的能力",{"2":{"256":1}}],["所以卷积核数=通道数",{"2":{"325":1}}],["所以节省了额外的存储空间",{"2":{"322":1}}],["所以选择不同的normalization方法",{"2":{"321":1}}],["所以和句子长度以及batch",{"2":{"318":1}}],["所以非常适合nlp领域",{"2":{"317":1}}],["所以非常适合翻译场景",{"2":{"248":1}}],["所以就相当于每一层求导时都加上了一个常数项",{"2":{"304":1}}],["所以就是q",{"2":{"36":1}}],["所以更新较好层的输入质量可能会很糟糕",{"2":{"296":1}}],["所以没有解决并行问题",{"2":{"284":1}}],["所以没有一个单一窗口可以把这两个词建立起依赖关系",{"2":{"247":1}}],["所以rnn网络难以学习远距离依赖关系",{"2":{"255":1}}],["所以cnn一般在长依赖关系建模场景使用较少",{"2":{"247":1}}],["所以cnn的卷积感受视野是局部的",{"2":{"247":1}}],["所以方差变大时",{"2":{"191":1}}],["所以这个score越大",{"2":{"903":1}}],["所以这个高阶语义向量序列具有更加丰富和完整的语义",{"2":{"516":1}}],["所以这操作是一个整数",{"2":{"698":1}}],["所以这是交叉注意力",{"2":{"538":1}}],["所以这是掩码多头自注意力",{"2":{"538":1}}],["所以这些路径在流经ffn时可以并行计算",{"2":{"419":1}}],["所以这里不能再使用ptr",{"2":{"1907":1}}],["所以这里的criterion就是labelsmoothing类的实例",{"2":{"399":1}}],["所以这里要保持一致",{"2":{"66":2,"380":1,"382":1}}],["所以这部分不会参与损失计算",{"2":{"399":1}}],["所以这样算大致的复杂度只是o",{"2":{"180":1}}],["所以郁达夫",{"2":{"169":1}}],["所以需要非线性线将它们分成各自的类别",{"2":{"1462":1}}],["所以需要",{"2":{"511":1}}],["所以需要根据模型的输出",{"2":{"473":1}}],["所以需要反转这个映射",{"2":{"473":1}}],["所以需要把这些头的输出进行拼接并且通过线性层来融合",{"2":{"465":1}}],["所以需要把有限的精力集中在重点信息上",{"2":{"169":1}}],["所以需要多步来预测序列中所有的单词",{"2":{"413":1}}],["所以需要在输入句子的最开始添加起始符",{"2":{"528":1}}],["所以需要在真值中添加噪声",{"2":{"399":1}}],["所以需要在通道维度执行归一化",{"2":{"325":1}}],["所以需要对softmax进行一定的改造来适应实际任务",{"2":{"184":1}}],["所以需要一个方阵来进行",{"2":{"79":1}}],["所以接下来我们从不同理解的角度来阐释",{"2":{"162":1}}],["所以论文",{"2":{"111":1}}],["所以带有swiglu激活函数的ffn模块一共有三个权重矩阵",{"2":{"109":1}}],["所以不再需要掩码",{"2":{"529":1}}],["所以不能很好地表征位置信息",{"2":{"512":1}}],["所以不需要",{"2":{"380":1}}],["所以不需要在ffn处再进行交互",{"2":{"101":1}}],["所以不同的请求之间没有任何相关性",{"2":{"17":1}}],["所以不同的请求之间可以共享",{"2":{"17":1}}],["所以tokenizer很难并行",{"2":{"576":1}}],["所以target",{"2":{"83":1}}],["所以transformer使用多头对一个向量切分不同的维度来捕捉不同的模式",{"2":{"12":1}}],["所以transformer能够更好地映射到我们的高并行计算架构",{"2":{"1":1}}],["所以第三维也是",{"2":{"79":1}}],["所以第二维是1",{"2":{"79":1,"198":1}}],["所以只需要在最后一维做掩码就行了",{"2":{"79":1}}],["所以",{"2":{"78":1,"101":1,"135":2,"170":2,"192":1,"318":1,"325":1,"333":2,"334":1,"506":1,"516":1,"575":1,"580":1,"700":1,"808":1,"896":1,"1003":1,"1004":1,"1340":1,"2054":3,"2117":1}}],["所以编码器的掩码就只是padding",{"2":{"78":1}}],["所以encoder和decoder都有padding",{"2":{"78":1}}],["所以decoder这一条q可以看到context",{"2":{"72":1}}],["所以y中实际信息的时间步向前错了一位",{"2":{"71":1}}],["所以计算机把数字视为语言",{"2":{"679":1}}],["所以计算t时刻概率时只用到了t",{"2":{"70":1}}],["所以计算时把它们放在同一个维度上",{"2":{"36":1}}],["所以计算时也可以把它们放在同一个维度上",{"2":{"34":1}}],["所以每个矩阵中的每一个向量都包含了本序列其它位置上的编码信息",{"2":{"537":1}}],["所以每个输入词xixix",{"2":{"260":1}}],["所以每个时刻的全连接层是可以独立并行计算的",{"2":{"101":1}}],["所以每个时刻都可以",{"2":{"79":1,"198":1}}],["所以每个词在计算的过程中都是可以看到句子中所有词的",{"2":{"57":1}}],["所以每次decoder只能看到之前自己生成的token和prompt",{"2":{"57":1}}],["所以我们的attention机制不应该把注意力放在这些位置上",{"2":{"933":1}}],["所以我们认为它们本身也是相似的",{"2":{"713":1}}],["所以我们希望可以通过向量来表达",{"2":{"685":1}}],["所以我们继续探寻",{"2":{"678":1}}],["所以我们要找出这个v维向量中",{"2":{"899":1}}],["所以我们要找到所有非填充",{"2":{"55":1}}],["所以我们要先拿出来说一下",{"2":{"612":1}}],["所以我们也往往用的数量来评价一个tokenizer的好坏程度",{"2":{"584":1}}],["所以我们用llama3中的代码作为示例来进行分析",{"2":{"570":1}}],["所以我们用除以有效token数目来进行平均",{"2":{"398":1}}],["所以我们在此处主要看两者的差异点和推理的独有特性",{"2":{"425":1}}],["所以我们在训练的时候",{"2":{"58":1}}],["所以我们会把源序列拆分之后组装成矩阵",{"2":{"391":1}}],["所以我们提前把后续会讲到的一些概念提前做一下说明",{"2":{"363":1}}],["所以我们可以给这些维度一个较高的分数",{"2":{"690":1}}],["所以我们可以认为",{"2":{"332":1}}],["所以我们可以先算k⊤vk⊤vk^⊤v",{"2":{"180":1}}],["所以我们看看为何cv选择bn",{"2":{"325":1}}],["所以我们需要一种机制来依据不同的上下文聚焦于不同的信息",{"2":{"260":1}}],["所以我们需要进行一些处理",{"2":{"54":1,"933":1}}],["所以我们接下来就从这些概念入手进行分析",{"2":{"235":1}}],["所以我们再重点介绍下dca",{"2":{"204":1}}],["所以我们好奇为什么transformer依然使用自注意力呢",{"2":{"160":1}}],["所以我们将",{"2":{"145":1}}],["所以要使用非线性线将其分类",{"2":{"1462":1}}],["所以要broadcast",{"2":{"1330":1}}],["所以要对predict进行log操作",{"2":{"399":1}}],["所以要区分开来",{"2":{"288":1}}],["所以要用mask来进行掩盖当前时刻之后的位置信息",{"2":{"39":1}}],["所以要确保d",{"2":{"23":1}}],["所以query",{"2":{"36":1,"198":1,"199":1}}],["所以mask也要相应的把自己拓展成4维",{"2":{"36":1}}],["所以涵盖了词嵌入",{"2":{"24":1}}],["所以512",{"2":{"23":1}}],["各种证明和推导一定是必要的吗",{"2":{"2117":1}}],["各种相关论文",{"2":{"162":1}}],["各类技术问题和踩坑",{"0":{"2071":1}}],["各类计算机知识体系",{"2":{"2001":1}}],["各行各业的分析等",{"2":{"2039":1}}],["各位同学",{"2":{"1601":1}}],["各层激活值不为0",{"2":{"998":1}}],["各层激活值不会出现饱和现象",{"2":{"998":1}}],["各层输出近似正态分布",{"2":{"843":1}}],["各项nlp",{"2":{"911":1}}],["各显神通",{"2":{"746":1}}],["各大模型服务商",{"2":{"711":1}}],["各路大神都有很精彩的见解",{"2":{"542":1}}],["各自都运行",{"2":{"1702":1}}],["各自的发展也是有声有色",{"2":{"291":1}}],["各自有不同的分工",{"2":{"288":1}}],["各司其职",{"2":{"84":1,"115":1,"1614":1}}],["各个进程依次执行不同的步骤",{"2":{"1578":1}}],["各个层对状态z的梯度的方差要保持一致",{"2":{"999":1}}],["各个层的激活的方差要保持一致",{"2":{"999":1}}],["各个word就组合成了一个相对低维空间上的一组向量",{"2":{"714":1}}],["各个token之间没有进行交互",{"2":{"101":1}}],["各个注意力头彼此完全独立的工作",{"2":{"44":1}}],["各个头的权重事实上肯定不同",{"2":{"10":1}}],["各注意力头没有单独的线性层",{"2":{"29":1}}],["切割方式",{"2":{"564":1}}],["切换到第一个",{"2":{"1556":1}}],["切换到上一个文件",{"2":{"1556":1}}],["切换到下一个文件",{"2":{"1556":1}}],["切换",{"0":{"357":1},"2":{"293":1}}],["切成块",{"2":{"180":1}}],["切断它从未来获得信息的通路",{"2":{"70":1}}],["切片使我们能够在一个cuda核函数中实现我们的算法",{"2":{"944":1,"963":1}}],["切片的方式计算softmax",{"0":{"943":1,"961":1}}],["切片的统计数据",{"2":{"315":2}}],["切片和重计算",{"2":{"942":1,"959":1}}],["切片",{"2":{"29":1}}],["切分成token是依据词表进行匹配的过程",{"2":{"563":1}}],["切分并非是直接在物理层面上简单的把词嵌入切分成h份",{"2":{"27":1}}],["切分",{"2":{"16":2}}],["切分数据",{"0":{"27":1},"1":{"28":1,"29":1,"30":1},"2":{"0":1}}],["都到19岁左右了",{"2":{"2054":1}}],["都没有让我完全理解孔子想传达的思想",{"2":{"2054":1}}],["都没有时间登录博客和微信",{"2":{"235":1}}],["都能提供一致的开发体验",{"2":{"1605":1}}],["都能获得上下文的全局视图",{"2":{"41":1}}],["都需要进行审核",{"2":{"1198":1}}],["都与max",{"2":{"1155":1}}],["都起作用",{"2":{"1117":1}}],["都可以看作是在运用算法",{"2":{"2096":1}}],["都可以使用相同的",{"2":{"1810":1,"1828":1}}],["都可以达到处理任意长度文本的需求",{"2":{"1339":1}}],["都可以像往常一样工作",{"2":{"976":1}}],["都可以作为一组向量键",{"2":{"145":1}}],["都预测出概率最大的那个词",{"2":{"901":1}}],["都输入完才结束",{"2":{"897":1}}],["都送给下一步作为输入",{"2":{"894":1}}],["都希望将",{"2":{"844":1}}],["都喜欢喝咖啡",{"2":{"690":1}}],["都类似",{"2":{"667":1}}],["都对应一个反向的函数",{"2":{"661":1,"1104":1}}],["都有对应的",{"2":{"1671":1}}],["都有提高",{"2":{"846":1}}],["都有助于使行列式为正",{"2":{"542":1}}],["都有",{"2":{"394":1}}],["都首先是接一个",{"2":{"344":1}}],["都听得见它们的啼唱",{"2":{"246":1}}],["都必须要再训练一次",{"2":{"222":1}}],["都明确指向",{"2":{"131":1}}],["都只能表示输入和输出之间的线性关系",{"2":{"102":1}}],["都只能对包括自己在内的前面所有词进行attention",{"2":{"71":1}}],["都会经过一定的推理和判断",{"2":{"2105":1}}],["都会在后边叠加一个非线性的激活函数",{"2":{"838":1}}],["都会在其输入的基础之上吸收更多上下文信息来丰富自己的表示",{"2":{"437":1}}],["都会分别额外添加一个表示位置m和位置n相对位置信息的仅依赖于m",{"2":{"757":1}}],["都会先压缩为embedding",{"2":{"698":1}}],["都会将当前时刻之前所有的预测结果作为输入来对下一个时刻的输出进行预测",{"2":{"537":1}}],["都会存在秩崩溃现象",{"2":{"93":1}}],["都会用到padding",{"2":{"78":1}}],["都是为了优化资源的使用",{"2":{"2106":1}}],["都是我们自己制定的",{"2":{"2097":1}}],["都是用",{"2":{"1602":1}}],["都是尽最大可能让进程去占用资源",{"2":{"1565":1}}],["都是一步步的指引",{"2":{"2097":1}}],["都是一种资源",{"2":{"1411":1}}],["都是一个资源",{"2":{"1411":1}}],["都是一个向量",{"2":{"161":1}}],["都是一个双层",{"2":{"126":1}}],["都是一个完整的单隐层全连接网络",{"2":{"28":1}}],["都是将",{"2":{"1377":1}}],["都是多维的",{"2":{"1343":1}}],["都是试验方差的潜在来源",{"2":{"1152":1}}],["都是sparse的",{"2":{"1086":1}}],["都是sonar空间中的点云",{"2":{"636":1}}],["都是独立同分布的",{"2":{"1004":1}}],["都是独立运算的",{"2":{"101":1}}],["都是考虑了所有的输入向量才生成出来的",{"2":{"920":1}}],["都是只编码了该特定词的含义",{"2":{"715":1}}],["都是向量的一个维度",{"2":{"690":1}}],["都是leaf",{"2":{"661":1,"1104":1}}],["都是字母",{"2":{"580":1}}],["都是来自下层输入或者是原始输入",{"2":{"533":1}}],["都是做输入序列各个时间步信息的特征抽取",{"2":{"510":1}}],["都是输入的序列",{"2":{"442":1}}],["都是维度等于",{"2":{"341":1}}],["都是经过了反复的叠加产生的",{"2":{"325":1}}],["都是有用的",{"2":{"231":1}}],["都是通过一系列的诸如嵌入",{"2":{"161":1}}],["都是在将模型的输出向",{"2":{"122":1}}],["都是最开始输入",{"2":{"117":1}}],["都是标准的下三角矩阵",{"2":{"89":1}}],["都是同一个",{"2":{"82":1}}],["都是相同的值x",{"2":{"39":1}}],["都是相同的输入值x或者下层",{"2":{"38":1}}],["都拆成了h个",{"2":{"28":1}}],["同学们要同时使用",{"2":{"1651":1}}],["同学们",{"2":{"1644":1}}],["同步接收消息",{"2":{"1575":1}}],["同步发送消息",{"2":{"1575":1}}],["同步通信",{"2":{"1574":1}}],["同步",{"2":{"1425":1}}],["同步如何实现",{"0":{"1414":1},"1":{"1415":1,"1416":1,"1417":1,"1418":1,"1419":1,"1420":1,"1421":1,"1422":1,"1423":1,"1424":1}}],["同步分为进程同步和资源同步",{"2":{"1408":1}}],["同步是什么",{"0":{"1408":1}}],["同torch",{"2":{"1083":1}}],["同质的特征空间上去",{"2":{"722":1}}],["同义词",{"2":{"696":1}}],["同态和自然变换等",{"2":{"505":1}}],["同理可求的",{"2":{"1394":1}}],["同理可求得",{"2":{"1393":1,"1394":1}}],["同理可得",{"2":{"1388":1,"1389":1,"1395":1}}],["同理",{"2":{"337":1,"700":1,"1342":1,"1487":1,"1488":1,"1611":1}}],["同等参数量下的优化难度被降低",{"2":{"242":1}}],["同一请求的不同块可以由不同节点同时处理",{"2":{"977":1}}],["同一份",{"2":{"935":1,"951":1}}],["同一个batch中的输入拥有相同的均值和方差",{"2":{"808":1}}],["同一个batch会包含有多个文本序列",{"2":{"53":1}}],["同一个窗口内的词语会有相似的更新",{"2":{"714":1}}],["同一分支上的模型关系更密切",{"2":{"540":1}}],["同一句中的词之间有关联",{"2":{"318":1}}],["同一批量内",{"2":{"316":1}}],["同一框中的张量与计算相关联",{"2":{"210":1}}],["同样可以使用",{"2":{"1728":1}}],["同样可以在某个子空间中表达某些细分的语义逻辑",{"2":{"28":1}}],["同样沿用了",{"2":{"1344":1}}],["同样转置",{"2":{"1330":1}}],["同样的想法是一个",{"2":{"1004":1}}],["同样的情况也发生在gpu1和gpu3之间",{"2":{"976":1}}],["同样基于注意力分数计算的分解",{"2":{"762":1}}],["同样使用语言模型来挑选子词",{"2":{"601":1}}],["同样调用函数f",{"2":{"249":1}}],["同样",{"2":{"29":1,"629":1}}],["同时允许进一步继承和扩展",{"2":{"1857":1}}],["同时允许下游任务利用组合表示",{"2":{"739":1}}],["同时继承自",{"2":{"1664":1}}],["同时执行父类构造函数",{"2":{"1653":1}}],["同时结合了",{"2":{"1317":1}}],["同时使用",{"2":{"1317":1}}],["同时需要最小的代码更改",{"2":{"1293":1}}],["同时需要padding",{"2":{"78":1}}],["同时产出模块的名称和模块本身",{"2":{"1214":2}}],["同时产出buffers的名称和参数本身",{"2":{"1214":1}}],["同时产出参数的名称和参数本身",{"2":{"1214":1}}],["同时进行",{"2":{"1214":1}}],["同时进一步了解问题",{"2":{"1139":1}}],["同时避免在训练步数的数量上过度浪费",{"2":{"1155":1}}],["同时优化冗余超参数",{"2":{"1142":1}}],["同时深化对问题的理解",{"2":{"1139":1}}],["同时做如下假设",{"2":{"1000":1}}],["同时从前一个主机",{"2":{"975":1}}],["同时从前一个主机接收键",{"2":{"975":2}}],["同时减少非矩阵的计算量",{"2":{"972":1}}],["同时将k和v",{"2":{"944":1}}],["同时将输出限制在一定范围内",{"2":{"334":1}}],["同时计算",{"2":{"944":1,"963":1}}],["同时引入了负半轴的定义使得整体输出均值接近0",{"2":{"843":1}}],["同时引入了两组具有",{"2":{"621":1}}],["同时这种操作方式也常常丢失了一些特征图中的细节信息",{"2":{"814":1}}],["同时又能完成归一化操作",{"2":{"807":1}}],["同时the",{"2":{"735":1}}],["同时最小化与批次中其他序列表示的相似性",{"2":{"734":1}}],["同时一些简单的元素级别的向量表示加法可以有意义地组合单词",{"2":{"713":1}}],["同时可以控制pairwise距离的失真",{"2":{"684":1}}],["同时可以作为",{"2":{"128":1}}],["同时更新",{"2":{"662":1}}],["同时实现了与从头开始训练的",{"2":{"621":1}}],["同时支持动态参数扩展",{"2":{"617":1}}],["同时在推理和长尾数据泛化方面也取得了定性改进",{"2":{"611":1}}],["同时增加模型大小和patch大小",{"2":{"611":1}}],["同时原来的2个子词都被消解",{"2":{"582":1}}],["同时原来的2个子词中一个保留",{"2":{"582":1}}],["同时原来的2个子词还保留",{"2":{"582":1}}],["同时我们也标记出该单词出现的次数",{"2":{"579":1}}],["同时它能很方便将token总数量控制在一个手动设置的数目内",{"2":{"576":1}}],["同时对相近词能更好地处理",{"2":{"567":1}}],["同时对输入的中心部分进行几乎线性的变换",{"2":{"360":1}}],["同时尽可能将结果中",{"2":{"567":1}}],["同时为生成任务提供基础",{"2":{"541":1}}],["同时通过序列掩码来限制注意范围",{"2":{"535":1}}],["同时防止访问未来的token",{"2":{"464":1}}],["同时保证这些结论与最终长时间运行相关",{"2":{"1157":1}}],["同时保留了预训练模型所学到的通用语言表示能力",{"2":{"898":1}}],["同时保留了对字节级信息的访问",{"2":{"614":1}}],["同时保留已训练的参数token",{"2":{"620":1}}],["同时保留多样化的类别",{"2":{"368":1}}],["同时保持其他参数组之间一致时",{"2":{"1222":1}}],["同时保持整体宏观布局不变",{"2":{"501":1}}],["同时保持模型性能",{"2":{"437":1}}],["同时保持模型的简单",{"2":{"29":1}}],["同时保持数值稳定性和对称性",{"2":{"180":1}}],["同时保持了模型的性能和灵活性",{"2":{"151":1}}],["同时保持计算成本",{"2":{"150":1}}],["同时用来计算注意力向量",{"2":{"288":1}}],["同时前瞻和回顾",{"0":{"278":1}}],["同时向前预测和向后回顾",{"2":{"272":1}}],["同时还能保持快速推理",{"2":{"228":1}}],["同时仅引入最少量的附加参数",{"2":{"224":1}}],["同时抑制其他部分的信号",{"2":{"221":1}}],["同时ffn也提供了存储知识的场所",{"2":{"120":1}}],["同时attention",{"2":{"88":1}}],["同时推荐一本作者的书籍",{"2":{"48":1}}],["同时获得了多头注意力的效果",{"2":{"29":1}}],["同时",{"2":{"21":1,"77":1,"142":1,"245":1,"291":1,"351":1,"352":1,"355":1,"391":1,"400":1,"505":1,"676":1,"843":1,"1144":1,"1646":1}}],["同时考虑诸多问题",{"2":{"12":1}}],["同时也包括关联权重和池化层",{"2":{"769":1}}],["同时也基于语料来训练词向量了",{"2":{"706":1}}],["同时也为更复杂的计算和解码阶段做好了准备",{"2":{"518":1}}],["同时也提供了一个理论框架",{"2":{"495":1}}],["同时也能理解各个词汇间的关联",{"2":{"5":1}}],["同时也可以留意周围的环境声",{"2":{"3":1}}],["维基百科",{"2":{"1425":2,"1564":1}}],["维扩展到多维",{"0":{"1343":1}}],["维中",{"2":{"1176":1}}],["维护成本",{"2":{"1134":1}}],["维护开销可以忽略不计",{"2":{"986":1}}],["维输出值",{"2":{"926":1}}],["维效果更好",{"2":{"926":1}}],["维等",{"2":{"700":1}}],["维空间中",{"2":{"684":1}}],["维空间就可以塞下n个向量",{"2":{"684":1}}],["维的图片编码为2维的高斯分布",{"2":{"1373":1}}],["维的query",{"2":{"926":1}}],["维的词向量相比",{"2":{"698":1}}],["维的词向量相比512",{"2":{"698":1}}],["维的嵌入矩阵",{"2":{"674":1}}],["维的矩阵",{"2":{"341":1}}],["维的向量表示",{"2":{"700":1}}],["维的向量",{"2":{"36":1,"325":1,"700":1}}],["维特征",{"2":{"313":2}}],["维特征记作",{"2":{"313":1}}],["维",{"2":{"28":1,"341":1,"698":1,"700":2}}],["维度平面上的向量的几何性质",{"2":{"1342":1}}],["维度扩展",{"2":{"1330":1}}],["维度扩增到4d",{"2":{"113":1}}],["维度上的",{"2":{"967":1}}],["维度上进行归一化",{"2":{"321":1}}],["维度空间中",{"2":{"473":1}}],["维度再拆分为",{"2":{"420":1}}],["维度改变",{"2":{"419":1}}],["维度的",{"2":{"341":1}}],["维度的数据求均值和标准差",{"2":{"337":1}}],["维度灾难",{"2":{"176":1}}],["维度缩减回d",{"2":{"113":1}}],["维度增加",{"2":{"41":1}}],["维度来完成的",{"2":{"35":1}}],["维度",{"2":{"24":1,"31":1,"337":1,"676":1,"1343":1}}],["维度没有变化",{"2":{"10":1}}],["原神团队",{"2":{"1939":1}}],["原材料",{"2":{"1729":2}}],["原则",{"2":{"1671":1,"1723":1,"1764":1}}],["原则上出现的越少越好",{"2":{"584":1}}],["原则上",{"2":{"222":1,"316":1,"1139":1,"1153":1}}],["原地函数将引发错误",{"2":{"1123":1}}],["原地操作可能会覆盖计算梯度所需的值",{"2":{"1123":1}}],["原地操作",{"2":{"1087":1}}],["原地取绝对值",{"2":{"1087":1}}],["原地取余操作",{"2":{"1085":1}}],["原地左移操作",{"2":{"1085":1}}],["原地更新",{"2":{"1083":1}}],["原本的公式只有一个翻译模型",{"2":{"908":1}}],["原因是子任务内部是局部性的",{"2":{"1590":1}}],["原因",{"2":{"715":1}}],["原因就在于向量的几个优秀特性",{"2":{"680":1}}],["原因如下",{"2":{"57":1,"898":1}}],["原文章",{"2":{"672":1}}],["原论文图上的输入是token",{"2":{"445":1,"454":1}}],["原先自回归推理先后依赖被解除",{"2":{"416":1}}],["原先的真值向量是",{"2":{"399":1}}],["原版作者又发表了第二版论文",{"2":{"302":1}}],["原有信息可能被新信息覆盖或者被稀释",{"2":{"253":1}}],["原有的随机权重",{"2":{"209":1}}],["原始transformer中的相对位置表达能力是在计算注意力阶段被破坏的",{"2":{"757":1}}],["原始无位置编码",{"2":{"744":1}}],["原始网络相当于变成一个更瘦更稀疏的网络",{"2":{"393":1}}],["原始公式中的逻辑值代表在",{"2":{"352":1}}],["原始论文链接",{"2":{"1382":1,"1383":1}}],["原始论文的实现机制是",{"2":{"523":1}}],["原始论文layernorm在最后",{"2":{"344":1}}],["原始论文中给出transformer的定义如下",{"2":{"235":1}}],["原始模型结构如下图所示",{"2":{"288":1}}],["原始自注意力机制中",{"2":{"204":1}}],["原始词嵌入",{"2":{"28":1}}],["原来是每个词都要计算一次",{"2":{"185":1}}],["原来每层有固定的h个注意力头",{"2":{"43":1}}],["原创",{"2":{"156":1,"233":1}}],["原生softmax的计算公式如下",{"2":{"54":1}}],["原理以及如何正确使用",{"2":{"1627":1}}],["原理详解",{"0":{"1321":1},"1":{"1322":1,"1323":1,"1324":1}}],["原理简图",{"2":{"1089":1}}],["原理简介",{"0":{"926":1}}],["原理概述",{"0":{"1059":1,"1088":1,"1089":1},"1":{"1060":1,"1061":1,"1062":1,"1089":1,"1090":1}}],["原理及思想介绍",{"0":{"959":1}}],["原理介绍",{"0":{"940":1}}],["原理图示",{"2":{"1079":1}}],["原理图所示",{"2":{"878":1}}],["原理图",{"0":{"1352":1},"2":{"835":1,"902":1}}],["原理我们知道",{"2":{"167":1}}],["原理与创新",{"0":{"153":1},"2":{"96":1}}],["原理",{"0":{"6":1,"159":1,"258":1,"393":1,"408":1,"781":1,"874":1,"902":1,"956":1,"1342":1},"1":{"7":1,"8":1,"9":1,"10":1,"11":1,"12":1,"13":1,"14":1,"15":1,"16":1,"17":1,"18":1,"19":1,"20":1,"21":1,"160":1,"161":1,"162":1,"163":1,"164":1,"165":1,"166":1,"167":1,"168":1,"169":1,"170":1,"259":1,"260":1,"261":1},"2":{"0":1,"157":1,"807":1,"810":1,"814":1,"846":1,"1233":1,"1234":1,"1235":1,"1236":1,"1237":1,"1238":1,"1239":1,"1240":1,"1241":1,"1242":1,"1244":1,"1245":1,"1246":1,"1247":1}}],["产品",{"2":{"1124":1,"1729":1}}],["产品经理负责整体项目规划和需求分析",{"2":{"5":1}}],["产生梯度消失问题",{"2":{"994":1}}],["产生最终值",{"2":{"926":1}}],["产生不同的结果",{"2":{"898":1}}],["产生下一层的输出",{"2":{"620":1}}],["产生一个上三角矩阵",{"2":{"934":1}}],["产生一个新的表示",{"2":{"517":1}}],["产生一个one",{"2":{"180":1}}],["产生的超参数的不同值时",{"2":{"1150":1}}],["产生的所有隐状态都传给解码器",{"2":{"444":1}}],["产生的字符yt−1yt−1y",{"2":{"241":1}}],["产生整个源语言序列的编码表示",{"2":{"427":2}}],["产生",{"2":{"26":1,"926":1}}],["矩阵求导公式",{"2":{"1398":1}}],["矩阵反向传播公式",{"2":{"1398":1}}],["矩阵型运算",{"2":{"1081":1}}],["矩阵分割为",{"2":{"944":1}}],["矩阵化",{"2":{"923":1}}],["矩阵里面一个单词所在的那一行就是该单词的一个向量表示",{"2":{"714":1}}],["矩阵中的每一个值都会减小",{"2":{"701":1}}],["矩阵把输入字节序列",{"2":{"614":1}}],["矩阵保持不变",{"2":{"542":1}}],["矩阵可以在解码过程中重复用于后面的标记token",{"2":{"542":1}}],["矩阵增加一行",{"2":{"530":1}}],["矩阵h",{"2":{"489":1}}],["矩阵计算",{"2":{"405":1}}],["矩阵列是句子",{"2":{"319":1}}],["矩阵行是token的embedding",{"2":{"319":1}}],["矩阵对可以解释为联想记忆块",{"2":{"231":1}}],["矩阵是低秩矩阵",{"2":{"204":1}}],["矩阵和向量运算是计算机图形学的核心工具",{"2":{"2009":1}}],["矩阵和",{"2":{"173":1}}],["矩阵如下图所示",{"2":{"89":1}}],["矩阵需要变成如下图所示的块对角矩阵形式",{"2":{"89":1}}],["矩阵必须是",{"2":{"83":1}}],["矩阵的参数量",{"2":{"935":1,"951":1}}],["矩阵的每一行就表示在对memory",{"2":{"537":1}}],["矩阵的每一行由相同的值组成",{"2":{"485":1}}],["矩阵的每一行都是独立运算",{"2":{"101":1}}],["矩阵的列可能是句子之中的单词",{"2":{"263":1}}],["矩阵的构造方法",{"2":{"83":1}}],["矩阵的第i行",{"2":{"71":1}}],["矩阵的转置矩阵",{"2":{"71":1}}],["矩阵的形状从",{"2":{"35":1}}],["矩阵的形状",{"2":{"31":1}}],["矩阵乘法",{"2":{"944":2,"963":2}}],["矩阵乘以kk",{"2":{"71":1}}],["矩阵乘向量",{"2":{"17":1}}],["矩阵示例如下",{"2":{"70":1}}],["矩阵每个值都是一个",{"2":{"62":1}}],["矩阵",{"2":{"30":1,"70":1,"71":4,"352":1,"396":1,"542":1,"935":1,"951":1,"1086":1,"1330":1}}],["矩阵仍然是三个单一矩阵",{"2":{"30":1}}],["矩阵形状",{"2":{"28":1}}],["矩阵将被分割到多个注意头中",{"2":{"28":1}}],["矩阵相乘",{"2":{"26":1}}],["图形变换",{"2":{"2009":1}}],["图形硬件加速和gpu编程",{"2":{"2009":1}}],["图形学包含大量的数学理论",{"2":{"2010":1}}],["图形学技术不断创新",{"2":{"2010":1}}],["图形学在影视动画",{"2":{"2010":1}}],["图形学中的数学",{"2":{"2009":1}}],["图形学基础",{"2":{"2009":1}}],["图形学",{"2":{"2001":1}}],["图形渲染工程师",{"2":{"1938":1}}],["图形接口",{"2":{"1937":1}}],["图形处理器",{"2":{"795":1}}],["图3中只设置了三列",{"2":{"1323":1}}],["图优化",{"2":{"1288":1}}],["图2显示了在imagenet上训练的resnet",{"2":{"1150":1}}],["图",{"0":{"1290":1},"2":{"1147":1,"1150":1}}],["图在每次迭代时都是从头开始重新创建的",{"2":{"1113":1}}],["图b",{"2":{"975":1}}],["图a",{"2":{"975":1}}],["图如下",{"2":{"778":1}}],["图示如下",{"2":{"1312":2}}],["图示",{"2":{"773":1,"802":1,"804":1,"805":1,"841":1,"842":1,"935":1,"938":1,"951":1,"954":1,"1025":1,"1026":1,"1027":1,"1235":1,"1236":1,"1238":1,"1239":1,"1241":1,"1242":1,"1243":1,"1244":1}}],["图示卷积相对于mlp",{"2":{"773":1}}],["图示卷积是conv2d",{"2":{"773":1}}],["图示中未表达出",{"2":{"31":1}}],["图文混合等等多种类型的数据格式",{"2":{"682":1}}],["图文详解decoder解码器原理",{"2":{"95":1,"543":1}}],["图左侧是编码器栈",{"2":{"515":1}}],["图右侧则显示了相应电势",{"2":{"507":1}}],["图灵机用于衡量其他系统的计算能力",{"2":{"504":1}}],["图灵机被认为是所有可计算过程的最终抽象",{"2":{"504":1}}],["图灵完备的系统之间是等价的",{"2":{"504":1}}],["图灵完备性",{"2":{"504":1}}],["图灵完备性质",{"0":{"504":1}}],["图下方则是模型对应预测输出的概率分布",{"2":{"398":1}}],["图像处理等需要对大规模数据进行并行处理的场景",{"2":{"1578":1}}],["图像处理中的注意力机制",{"2":{"292":1}}],["图像的预处理",{"2":{"1250":1}}],["图像的跟目录",{"2":{"1250":1}}],["图像翻转",{"2":{"1015":1}}],["图像被视为独立的个体",{"2":{"850":1}}],["图像如下",{"2":{"843":1}}],["图像搜索",{"2":{"696":1}}],["图像等之间的语义关系",{"2":{"680":1,"682":1}}],["图像",{"2":{"676":1,"823":1,"844":1,"845":1,"846":1,"1360":1}}],["图像常见的归一化操作有bn",{"2":{"325":1}}],["图像不同位置用这个卷积核执行卷积以后的数据分布是稳定的",{"2":{"325":1}}],["图像不同位置使用的卷积核的参数都共享的",{"2":{"325":1}}],["图+公式",{"2":{"292":1}}],["图对应起来",{"2":{"267":1}}],["图片思路来源",{"2":{"407":1}}],["图片",{"2":{"233":1}}],["图解rope旋转位置编码及其特性",{"2":{"768":1}}],["图解",{"2":{"233":1,"961":1}}],["图书馆",{"2":{"169":1}}],["图例",{"0":{"199":1},"2":{"157":1}}],["图上a是从x⋅k⊤px⋅kp⊤x",{"2":{"621":1}}],["图上蓝色部分是解码器",{"2":{"537":1}}],["图上具体分为三步",{"2":{"464":1}}],["图上x就是",{"2":{"263":1}}],["图上几个标志的说明如下",{"2":{"130":1}}],["图上",{"2":{"122":3}}],["图上展示了将多个句子组成一个batch时会遇到的情况",{"2":{"53":1}}],["图上限定为2个头",{"2":{"24":1}}],["图中绿色部分",{"2":{"629":1}}],["图中蓝色部分",{"2":{"629":1}}],["图中分为两部分",{"2":{"519":1}}],["图中的每个点都对应于一次试验",{"2":{"1147":1}}],["图中的每个方块都是一个简单的rnn单元",{"2":{"249":1}}],["图中的v",{"2":{"537":1}}],["图中的",{"2":{"436":1}}],["图中简化了输入",{"2":{"406":1}}],["图中上方是目标概率分布",{"2":{"398":1}}],["图中编码器和解码器通过一个中间隐状态c来完成信息交互",{"2":{"241":1}}],["图中",{"2":{"18":1}}],["图中红色",{"2":{"17":1}}],["梳理如下图所示",{"2":{"24":1}}],["结尾",{"2":{"1704":2,"1715":2}}],["结尾的",{"2":{"1713":1,"1715":1}}],["结尾的文本文件",{"2":{"1628":1}}],["结尾的一小组文本的平均值",{"2":{"145":1}}],["结束当前循环迭代",{"2":{"1631":1}}],["结束值",{"2":{"1594":1}}],["结束进程",{"2":{"1523":1}}],["结束",{"2":{"976":2,"1594":1}}],["结构化绑定可以与",{"2":{"1921":1}}],["结构化绑定",{"0":{"1921":1},"2":{"1920":1,"1932":1,"1933":1}}],["结构化的循环",{"0":{"1621":1}}],["结构体的应用场景",{"2":{"1728":1}}],["结构体变量的",{"2":{"1728":1}}],["结构体就是一种自定义的数据类型",{"2":{"1728":1}}],["结构体",{"2":{"1728":2,"1729":1}}],["结构体和联合体成员访问",{"2":{"1635":1}}],["结构体或对象",{"2":{"1563":1}}],["结构的纯",{"2":{"1315":1}}],["结构的模型",{"2":{"1312":1}}],["结构的性能较之传统有了显著提升",{"2":{"498":1}}],["结构与",{"2":{"1312":1}}],["结构用于序列标注",{"2":{"1312":1}}],["结构图如下",{"2":{"864":1}}],["结构如下",{"0":{"859":1}}],["结构详解",{"0":{"852":1},"1":{"853":1,"854":1,"855":1,"856":1,"857":1,"858":1,"859":1}}],["结构篇",{"2":{"740":1}}],["结构",{"0":{"517":1,"525":1,"1356":1},"2":{"1619":2,"2049":1}}],["结构中的",{"2":{"498":1}}],["结构中随处可见",{"2":{"394":1}}],["结构会过度倾向于恒等分支",{"2":{"334":1}}],["结构无形地增加了模型的宽度而降低了模型的深度",{"2":{"334":1}}],["结构对输入上下文特征单独做了",{"2":{"7":1}}],["结果取决于系统位数",{"2":{"2059":1}}],["结果他的回答让我有些意外",{"2":{"2054":1}}],["结果就为真",{"2":{"1619":1}}],["结果就是词表存储过大",{"2":{"681":1}}],["结果才为真",{"2":{"1619":1}}],["结果会被截断为整数部分",{"2":{"1607":1}}],["结果",{"2":{"1374":1}}],["结果是它们之间相隔的元素的个数",{"2":{"1633":1}}],["结果是",{"2":{"1231":1}}],["结果发现会有错误",{"2":{"2070":1}}],["结果发现只有这些api",{"2":{"2070":1}}],["结果发现它的效果非常好",{"2":{"735":1}}],["结果发现聚合的新分布的最高预测词在百分之六十六的情况下语义发生了剧变",{"2":{"306":1}}],["结果向量维度",{"2":{"676":1}}],["结果的方差是两者的累积和",{"2":{"314":1}}],["结果表明",{"2":{"228":1}}],["结果你到了商店",{"2":{"163":1}}],["结果为",{"2":{"36":2,"1646":1,"2059":1}}],["结论",{"0":{"187":1,"996":1,"2126":1},"2":{"157":1,"772":1,"846":1,"1465":1,"1484":1}}],["结合第二句",{"2":{"2054":1}}],["结合我们的生活就知道",{"2":{"2054":1}}],["结合人工智能算法",{"2":{"2011":1}}],["结合性",{"2":{"1635":1}}],["结合共享内存和分布式内存模型",{"2":{"1568":1}}],["结合缩放的动量使用没有明确的理论动机",{"2":{"1059":1}}],["结合绝对位置编码的公式",{"2":{"759":1,"760":1,"761":1,"762":1,"763":1,"764":1,"765":1}}],["结合",{"2":{"637":1,"1764":1,"1921":1}}],["结合inter",{"2":{"287":1}}],["结合原输入一起提供给模型",{"2":{"239":2}}],["结合了引用传递的效率和值传递的安全性",{"2":{"1729":1}}],["结合了运算和赋值",{"2":{"1712":1}}],["结合了",{"2":{"1316":1}}],["结合了前两者的特点",{"2":{"540":1}}],["结合了局部",{"2":{"204":1}}],["结合了空洞注意力和局部注意力机制",{"2":{"204":1}}],["结合的优势",{"2":{"90":1}}],["结合哈佛代码中的具体函数从整体上把多头注意力的计算过程",{"2":{"24":1}}],["保持运算符的自然语义",{"2":{"1712":1}}],["保持梯度一致性指的是在深度神经网络中",{"2":{"838":1}}],["保持一致",{"2":{"691":1}}],["保持输出分布的连续性",{"2":{"621":1}}],["保持输出结果的维度",{"2":{"343":1}}],["保持维度与输入一致",{"2":{"343":2}}],["保持了每个模块的一致性",{"2":{"331":1}}],["保持其他层不变",{"2":{"154":1}}],["保持其他知识不变",{"2":{"142":1}}],["保存联系人的",{"2":{"1808":1}}],["保存文件",{"2":{"1544":1}}],["保存并退出",{"2":{"1518":1,"1544":2}}],["保存到可永久保存的存储设备中",{"2":{"1477":1}}],["保存图片",{"2":{"1283":1}}],["保存图像",{"2":{"1253":1}}],["保存onnx",{"0":{"1272":1}}],["保存和加载模型的静态图",{"0":{"1268":1},"1":{"1269":1,"1270":1}}],["保存训练中的状态",{"0":{"1266":1}}],["保存与加载模型",{"0":{"1261":1},"1":{"1262":1,"1263":1,"1264":1}}],["保存当前优化状态的字典",{"2":{"1227":1}}],["保存每次评估的足够信息",{"2":{"1164":1}}],["保存检查点并追溯选择最佳检查点",{"0":{"1166":1},"2":{"1125":1}}],["保存输入",{"2":{"1114":1}}],["保存计算当前张量梯度所需要的计算图中的其他张量",{"2":{"1111":1}}],["保存计算当前张量梯度所需要的计算图中的自身张量",{"2":{"1111":1}}],["保存哪些东西",{"2":{"1109":1}}],["保存模型到文件",{"2":{"1297":1}}],["保存模型静态图",{"0":{"1269":1}}],["保存模型状态后",{"2":{"1260":1}}],["保存模型的状态",{"0":{"1258":1}}],["保存模型",{"0":{"1262":1},"2":{"1109":1,"1283":1}}],["保存",{"2":{"1099":1,"1114":1,"1262":1,"2070":1}}],["保存中间tensor",{"2":{"1099":1}}],["保存中间",{"0":{"1099":1}}],["保存成",{"2":{"971":1}}],["保存平滑标签后的label",{"2":{"399":1}}],["保存起来",{"2":{"267":1}}],["保存着它从训练数据中学到的精髓",{"2":{"221":1}}],["保存此次解码应该看到的",{"2":{"71":1}}],["保留第四项",{"2":{"764":1}}],["保留了正余弦频率的取值以及位置信息与语义信息的交互",{"2":{"760":1}}],["保留了hs和rw嵌入的独立信息",{"2":{"739":1}}],["保留了高维度潜藏的逻辑信息",{"2":{"515":2}}],["保留每个token的特征信息",{"2":{"735":1}}],["保留随意注意力并使用其他技巧",{"2":{"732":1}}],["保留个性",{"2":{"684":1}}],["保留前x",{"2":{"602":1}}],["保留主要成分的大模型微调",{"2":{"233":1}}],["保留",{"2":{"135":1,"337":1,"512":1,"902":1}}],["保留归因得分大于归因阈值",{"2":{"135":1}}],["保留最确定性的结果",{"2":{"4":1}}],["保证的复制消除",{"2":{"1931":1}}],["保证前",{"2":{"1751":1}}],["保证只有一个",{"2":{"1695":1}}],["保证在多主机之间使用相同的随机数生成器种子",{"2":{"1169":1}}],["保证管道只在一台主机上进行日志记录和检查点",{"2":{"1169":1}}],["保证了在优化中梯度的可计算性",{"2":{"838":1}}],["保证多层网络不退化成单层线性网络",{"2":{"838":1}}],["保证数据分布一致",{"2":{"809":1}}],["保证每一次数据经过归一化后还保留原有学习来的特征",{"2":{"807":1}}],["保证每个位置只能看到前面的tokens",{"2":{"59":1}}],["保证推理成本",{"2":{"734":1}}],["保证各个单位拥有相对完整和独立的语义",{"2":{"547":1}}],["保证模型性能的提升",{"2":{"349":1}}],["保证填满",{"2":{"90":1}}],["保证",{"2":{"23":1}}],["注册的钩子可以用于在",{"2":{"1227":1}}],["注册的钩子可以用于在进行",{"2":{"1227":1}}],["注册的钩子可用于在返回state",{"2":{"1227":1}}],["注册的钩子可用于在进行state",{"2":{"1227":1}}],["注册的钩子函数可用于在进行state",{"2":{"1214":1}}],["注册",{"2":{"1214":3}}],["注册一个",{"2":{"1227":2}}],["注册一个状态字典后置钩子",{"2":{"1227":1}}],["注册一个状态字典前置钩子",{"2":{"1227":1}}],["注册一个优化器",{"2":{"1227":2}}],["注册一个后置钩子函数",{"2":{"1214":1}}],["注册一个不被视为模型参数的缓冲区",{"2":{"1214":1}}],["注册一个梯度累加之后的反向钩子函数",{"2":{"1083":1}}],["注册反向传播钩子函数",{"2":{"1213":1}}],["注册反向钩子函数",{"2":{"1083":1}}],["注册前向钩子函数",{"2":{"1212":1}}],["注释2",{"2":{"1082":1,"1377":1}}],["注释1",{"2":{"1082":1}}],["注释",{"2":{"838":2,"840":1,"844":1,"845":1,"943":1,"961":1,"977":1,"999":1,"1086":1,"1242":1,"1377":1,"1455":1,"1460":1,"1472":1}}],["注释中",{"2":{"23":1}}],["注解",{"2":{"779":1}}],["注意运算符的优先级和结合性",{"2":{"1712":1}}],["注意括号",{"2":{"1705":1}}],["注意加括号",{"2":{"1632":1}}],["注意区分赋值运算符",{"2":{"1630":1}}],["注意整数除法",{"2":{"1630":1}}],["注意事项",{"2":{"1615":1,"1905":1,"1922":1,"1929":1}}],["注意点",{"2":{"1486":1}}],["注意这里过滤了含有水印的图片",{"2":{"1363":1}}],["注意这个",{"2":{"340":1}}],["注意横轴的刻度是以对数的形式展示",{"2":{"1181":1}}],["注意选择冗余超参数的搜索空间",{"2":{"1142":1}}],["注意不是充分条件",{"2":{"998":1}}],["注意到tanh",{"2":{"995":1}}],["注意到不同部分之间的相关性",{"2":{"442":1}}],["注意头的dla",{"2":{"479":1}}],["注意头会存储明显的语言特征",{"2":{"131":1}}],["注意src",{"2":{"382":1}}],["注意是除以",{"2":{"315":1}}],["注意",{"2":{"24":1,"67":1,"172":1,"323":1,"427":1,"520":1,"530":2,"582":1,"584":1,"897":1,"986":1,"1000":2,"1027":1,"1155":1,"1156":1,"1179":1,"1222":1,"1225":1,"1242":1,"1243":1,"1322":1,"1350":1,"1373":1,"1502":1,"1607":2,"1611":1,"1619":1,"1623":1,"1624":1,"1633":1,"1704":1,"1723":1,"1724":1,"1728":1,"1924":1,"2019":1,"2027":1,"2052":1,"2059":1,"2088":1}}],["注意力架构",{"0":{"732":1}}],["注意力矩阵被限制为下三角形式",{"2":{"542":1}}],["注意力满秩问题",{"2":{"542":1}}],["注意力被用在三个地方",{"2":{"535":1}}],["注意力匹配",{"2":{"524":1}}],["注意力本来就是全局查询操作",{"2":{"511":1}}],["注意力图之间的差异",{"2":{"500":1}}],["注意力层只能访问句子中位于它之前的词语",{"2":{"1316":1}}],["注意力层都可以访问到原始输入句子中的所有词语",{"2":{"1315":1}}],["注意力层",{"2":{"535":1}}],["注意力层的目标是在",{"2":{"461":1}}],["注意力层要用的掩码",{"2":{"451":1}}],["注意力层或者ffn层",{"2":{"343":1}}],["注意力计算",{"2":{"431":1}}],["注意力计算的复杂度与维度的平方成正比",{"2":{"21":1}}],["注意力计算的复杂度降低",{"2":{"21":1}}],["注意力发展历史",{"0":{"280":1},"1":{"281":1,"282":1,"283":1,"284":1,"285":1,"286":1,"287":1,"288":1,"289":1,"290":1,"291":1}}],["注意力是一种重大解锁",{"2":{"279":1}}],["注意力允许依据输入序列和输出序列的不同部分进行相关度计算",{"2":{"277":1}}],["注意力之间的相关性",{"2":{"209":1}}],["注意力分数加上了一个线性的bias",{"2":{"765":1}}],["注意力分数会通过",{"2":{"464":1}}],["注意力分数通过相似度计算函数得到",{"2":{"268":1}}],["注意力分数是衡量序列中不同单词对当前单词的重要性程度的指标",{"2":{"268":1}}],["注意力分数",{"0":{"268":1}}],["注意力分布的高信息熵",{"2":{"212":1}}],["注意力分布",{"2":{"204":1,"265":1}}],["注意力分散",{"2":{"181":1}}],["注意力分散在这几个源元素上",{"2":{"168":1}}],["注意力汇聚",{"2":{"204":1}}],["注意力里面的线性计算一般不使用偏置单元",{"2":{"201":1}}],["注意力中k",{"2":{"201":1}}],["注意力的思想可以改写为如下公式",{"2":{"271":1}}],["注意力的核心思想就是帮助模型为输入的不同部分分配不同的权重",{"2":{"262":1}}],["注意力的优势就越明显",{"2":{"217":1}}],["注意力的效率问题",{"2":{"217":1}}],["注意力的平方复杂度降低为线性",{"2":{"216":1}}],["注意力的熵随着长度的变化更加平稳",{"2":{"194":1}}],["注意力的计算公式中",{"2":{"186":1}}],["注意力的本质思想可以改写为如下公式",{"2":{"170":1}}],["注意力作用就是让盐",{"2":{"163":1}}],["注意力模型的内部流程",{"2":{"267":1}}],["注意力模型的内部流程如下图所示",{"2":{"264":1}}],["注意力模型的输出是上下文向量c",{"2":{"263":1}}],["注意力模型",{"0":{"264":1},"2":{"263":1}}],["注意力模块稀疏化",{"2":{"512":1}}],["注意力模块帮助记忆了解哪些信息是有用的",{"2":{"231":1}}],["注意力模块在存储关系知识方面也发挥了重要作用",{"2":{"131":1,"136":1}}],["注意力模块",{"0":{"131":1,"438":1},"1":{"439":1,"440":1,"441":1,"442":1,"443":1,"444":1},"2":{"96":1}}],["注意力模式只需要是准强连通的",{"2":{"93":1}}],["注意力掩码和layernorm在transformer中的作用",{"2":{"95":1}}],["注意力掩码对秩崩溃的影响分析",{"2":{"92":1}}],["注意力权重表示每个输入单词对于当前输出单词的重要程度",{"2":{"267":1}}],["注意力权重的计算方式如下图所示",{"2":{"213":1}}],["注意力权重矩阵",{"2":{"209":1}}],["注意力权重细化",{"0":{"209":1},"2":{"157":1}}],["注意力权重",{"0":{"269":1},"2":{"34":1,"169":1,"213":1}}],["注意力头7",{"2":{"131":1}}],["注意力头5",{"2":{"131":2}}],["注意力头l14h7是一个移动",{"2":{"130":1}}],["注意力头l14h13是一个关系头",{"2":{"130":1}}],["注意力头之后的mlp充当了每个头输出的",{"2":{"122":1}}],["注意力头对",{"2":{"122":1}}],["注意力头",{"0":{"144":1},"2":{"96":1}}],["注意力头间的有限方式",{"2":{"46":1}}],["注意力头功能的重复冗余浪费了参数和计算资源等一些弊端",{"2":{"44":1}}],["注意力头数量",{"2":{"23":1}}],["注意力头的维度可以根据实际情况进行改变",{"2":{"19":1}}],["注意力会更倾向于关注token自己本身或者其他的比较单一的模式",{"2":{"20":1}}],["注意力",{"2":{"5":1,"217":1,"257":1,"268":1,"284":1,"501":1,"1315":1}}],["注意力机制中对注意力权重会施加dropout",{"2":{"394":1}}],["注意力机制中可能需要的mask掩码张量",{"2":{"36":1}}],["注意力机制真相",{"2":{"292":1}}],["注意力机制只是一种思想",{"2":{"280":1}}],["注意力机制在处理长序列时",{"2":{"279":1}}],["注意力机制需要计算序列中每个元素对其他每个元素的关系",{"2":{"279":1}}],["注意力机制并非完美方案",{"2":{"279":1}}],["注意力机制确保每个解码步骤都由最相关的上下文片段提供信息",{"2":{"279":1}}],["注意力机制根据输入来计算出输出应该更关注哪些输入",{"2":{"276":1}}],["注意力机制不仅查找最佳匹配",{"2":{"265":1}}],["注意力机制的感知域是整个句子",{"2":{"415":1}}],["注意力机制的提出不仅是技术上的突破",{"2":{"279":1}}],["注意力机制的计算总体可以分为两步",{"2":{"267":1}}],["注意力机制的任务应该是找到解码器当前隐向量和编码器所有隐向量之间的相互关系",{"2":{"267":1}}],["注意力机制的输入是一个序列或者集合",{"2":{"261":1}}],["注意力机制的本质可以用一句话来总结",{"2":{"259":1}}],["注意力机制的本质是上下文决定一切",{"2":{"258":1}}],["注意力机制计算过程就是序列中元素交换信息的过程",{"2":{"261":1}}],["注意力机制就是这种资源分配机制",{"2":{"260":1}}],["注意力机制就是这个查询过程",{"2":{"164":1}}],["注意力机制也是一种资源分配方式",{"2":{"260":1}}],["注意力机制",{"0":{"234":1,"257":1},"1":{"235":1,"236":1,"237":1,"238":1,"239":1,"240":1,"241":1,"242":1,"243":1,"244":1,"245":1,"246":1,"247":1,"248":1,"249":1,"250":1,"251":1,"252":1,"253":1,"254":1,"255":1,"256":1,"257":1,"258":2,"259":2,"260":2,"261":2,"262":2,"263":2,"264":2,"265":2,"266":2,"267":2,"268":2,"269":2,"270":2,"271":2,"272":2,"273":2,"274":2,"275":2,"276":2,"277":2,"278":2,"279":2,"280":1,"281":1,"282":1,"283":1,"284":1,"285":1,"286":1,"287":1,"288":1,"289":1,"290":1,"291":1,"292":1},"2":{"233":1,"292":1}}],["注意力机制更像是一个关系运算",{"2":{"174":1}}],["注意力机制聚焦的过程体现在权重系数上",{"2":{"169":1}}],["注意力机制会以加权求和的模式对数据进行加工",{"2":{"270":1}}],["注意力机制会从输入中选择性提取信息",{"2":{"261":1}}],["注意力机制会",{"2":{"168":1}}],["注意力机制可以选择性地关注和处理相关信息",{"2":{"275":1}}],["注意力机制可以从不同角度进行理解",{"2":{"258":1}}],["注意力机制可以类比为片上指令缓存",{"2":{"231":1}}],["注意力机制可以作为短期记忆",{"2":{"226":1}}],["注意力机制可以被解释为将多个局部信息源路由到一个局部表征的全局树结构中",{"2":{"169":1}}],["注意力机制可以追溯到美国心理学之父威廉",{"2":{"163":1}}],["注意力机制可以捕捉序列中的上下文关系",{"2":{"101":1}}],["注意力机制接受查询",{"2":{"158":1}}],["注意力机制本质就是使用",{"2":{"169":1}}],["注意力机制本质上是对value的线性变换",{"2":{"117":1}}],["注意力机制本身支持并行",{"2":{"34":1}}],["注意力机制是寻找sequence内部",{"2":{"535":1}}],["注意力机制是transformer",{"2":{"515":1}}],["注意力机制是transformer模型的心脏",{"2":{"438":1}}],["注意力机制是",{"2":{"355":1}}],["注意力机制是一种相对关系建模",{"2":{"276":1}}],["注意力机制是一种资源分配方案",{"2":{"258":1}}],["注意力机制是站在某个输入对象的立场上看待上下文和它之间的关系",{"2":{"276":1}}],["注意力机制是动态权重",{"2":{"276":1}}],["注意力机制是动态产生权重",{"2":{"276":1}}],["注意力机制是信息交换",{"2":{"258":1}}],["注意力机制是在同一特征空间内",{"2":{"99":1}}],["注意力机制是使用q去找相关的k",{"2":{"3":1}}],["注意力机制看起来很美好",{"2":{"3":1}}],["注",{"2":{"24":1,"105":1,"243":1,"309":1,"313":1,"318":1,"406":1,"445":1,"454":1,"474":1,"528":1,"542":1,"580":1,"598":1,"699":1,"764":1,"767":1,"1162":1,"1164":2,"1961":1}}],["更像是一种随机应变",{"2":{"2117":1}}],["更根据不同的情况更新yi+1y",{"2":{"2023":1}}],["更优雅",{"2":{"1932":1}}],["更优雅的代码",{"2":{"1913":1}}],["更灵活的数据存储方式",{"2":{"1932":1}}],["更灵活的错误处理机制",{"2":{"1761":1}}],["更严格的表达式求值顺序",{"2":{"1931":1}}],["更简洁的遍历容器",{"2":{"1897":1}}],["更简洁",{"2":{"1884":1}}],["更常用于实现具有私有成员的抽象数据类型",{"2":{"1728":1}}],["更明确",{"2":{"1629":1}}],["更安全易用",{"2":{"1713":1}}],["更安全",{"2":{"1624":1,"1629":1}}],["更方便的方式来间接操作变量",{"2":{"1612":1}}],["更长的整型",{"2":{"1607":1}}],["更接近人类语言",{"2":{"1603":1}}],["更易单元测试",{"2":{"1479":1}}],["更易维护",{"2":{"1479":1}}],["更易于管理和扩展",{"2":{"1848":1}}],["更易于管理的token",{"2":{"456":1}}],["更易于管理的语义片段的过程",{"2":{"456":1}}],["更易于维护的代码",{"2":{"1765":1}}],["更易于超参优化",{"2":{"329":1}}],["更一般的",{"2":{"1464":1}}],["更容易理解的步骤",{"2":{"1729":1}}],["更容易理解",{"2":{"1729":1}}],["更容易理解和编写",{"2":{"1603":1}}],["更容易",{"2":{"1313":1}}],["更复杂的算法也有其自身的缺陷",{"2":{"1175":1}}],["更复杂的搜索算法可能并不总能正确处理不可行的点",{"2":{"1175":1}}],["更广泛的工程考虑",{"2":{"1161":1}}],["更不用说100",{"2":{"1157":1}}],["更详细的描述如下",{"2":{"1145":1}}],["更改设置",{"2":{"2094":1}}],["更改文件所有者",{"2":{"1513":1}}],["更改是否自动梯度过程中应记录此module中参数",{"2":{"1214":1}}],["更改batch",{"0":{"1135":1},"2":{"1135":1}}],["更改这个方向",{"2":{"137":1}}],["更通用",{"2":{"1130":1}}],["更糟糕的是",{"2":{"1127":1}}],["更清晰的程序",{"2":{"1112":1}}],["更快地接近最优解",{"2":{"1031":1}}],["更快的训练来改善结果",{"2":{"1154":1}}],["更快的训练速度",{"2":{"346":1}}],["更快的更新速度",{"2":{"1026":1}}],["更快的神经缩放规律",{"2":{"155":1}}],["更节省空间",{"2":{"921":1}}],["更符合直觉也更符合文本的特性",{"2":{"767":1}}],["更符合上下文的目标词",{"2":{"525":1}}],["更为高效",{"2":{"1891":1}}],["更为重要的是相对位置编码",{"2":{"755":1}}],["更为平滑",{"2":{"314":1}}],["更加强大和易用",{"2":{"1603":1}}],["更加体现出了研究人员的",{"2":{"1338":1}}],["更加充分利用了上下文信息",{"2":{"715":1}}],["更加节省存储空间",{"2":{"694":1}}],["更加节省算力和存储",{"2":{"262":1}}],["更注重数据表示的直接性",{"2":{"676":1}}],["更高效地处理生活中的各种挑战",{"2":{"2108":1}}],["更高效的",{"2":{"1932":1}}],["更高效的代码",{"2":{"1901":1}}],["更高的精度",{"2":{"1607":1}}],["更高",{"2":{"1607":1}}],["更高端的教学模式是老师只给适量的引导",{"2":{"897":1}}],["更高压缩率的tokenizer",{"2":{"638":1}}],["更高层次的上下文和语言特征",{"2":{"477":1}}],["更纯粹",{"2":{"638":1}}],["更靠近在抽象空间的",{"2":{"628":1}}],["更是对ai未来发展路径的前瞻性探索",{"2":{"625":1}}],["更是北国的特产",{"2":{"246":1}}],["更小的词表",{"2":{"594":1}}],["更有意义的错误信息",{"2":{"1763":1}}],["更有意义且独立的部分",{"2":{"221":1}}],["更有效的方法是从简单的配置开始",{"2":{"1139":1}}],["更有语义的子词",{"2":{"580":1}}],["更大的模型通常会按比例增加头的数量",{"2":{"937":1,"953":1}}],["更大的模型应该配备更大的词表",{"2":{"561":1}}],["更大的词表大小增强了模型理解更多样化文本的能力",{"2":{"561":1}}],["更大的嵌入空间可分性以及在下游任务中更好的性能相关",{"2":{"351":1}}],["更丰富的上下文表示",{"2":{"517":1}}],["更具体地说",{"2":{"508":1}}],["更具体一点",{"2":{"194":1}}],["更可以从计算理论的角度出发",{"2":{"504":1}}],["更强大的",{"2":{"1644":1}}],["更强大的表示",{"2":{"9":1}}],["更强调从单个神经元角度进行分析",{"2":{"475":1}}],["更适合学习和解读",{"2":{"432":1}}],["更适合短文本计算",{"2":{"247":1}}],["更好更快地提取稀疏特征",{"2":{"840":1}}],["更好更快地学习",{"2":{"406":1}}],["更好地训练呢",{"2":{"320":1}}],["更好",{"2":{"301":1}}],["更好的标记过时api",{"2":{"1913":1}}],["更好的资源管理",{"2":{"1913":1}}],["更好的区分开每个位置",{"2":{"1336":1}}],["更好的长度外推性",{"2":{"768":1}}],["更好的梯度稳定性",{"2":{"621":1}}],["更好的模型性能",{"2":{"346":1}}],["更好的数值稳定性",{"2":{"346":1}}],["更好的反映了原始分值中的相对置信度",{"2":{"180":1}}],["更好的泛化能力",{"2":{"21":1,"594":1}}],["更反映了人类认知的深刻影响",{"2":{"279":1}}],["更准确地学习序列之间的关系",{"2":{"277":1}}],["更多回合可能更有意义",{"2":{"1159":1}}],["更多的是工程上的原因",{"2":{"326":1}}],["更多的参数通常允许模型覆盖更多知识维度",{"2":{"119":1}}],["更多吸收了",{"2":{"261":2}}],["更少步数的训练意味着每次训练运行得更快并且使用更少的资源",{"2":{"1137":1}}],["更少",{"2":{"260":1}}],["更持久的记忆",{"2":{"226":1}}],["更重要的是gap极大减少了网络参数",{"2":{"816":1}}],["更重要的是让每个词的向量化数值更加均衡",{"2":{"334":1}}],["更重要的是",{"2":{"180":1,"504":1,"629":1}}],["更新进度",{"0":{"2110":1}}],["更新日志",{"0":{"2040":1,"2050":1},"1":{"2041":1,"2042":1,"2043":1,"2044":1,"2045":1,"2046":1,"2047":1,"2048":1,"2049":1}}],["更新操作",{"2":{"1933":1}}],["更新系统和安装软件包",{"0":{"1537":1}}],["更新weight",{"2":{"1398":1}}],["更新module的状态",{"2":{"1214":1}}],["更新运行时的均值和方差",{"2":{"1211":1}}],["更新权重",{"2":{"1202":2}}],["更新剪辑",{"2":{"1179":1}}],["更新门的功能类似于lstm中的遗忘门和输入门",{"2":{"874":1}}],["更新门",{"2":{"874":1}}],["更新",{"2":{"709":1,"714":1,"1395":1,"1492":1,"1933":1}}],["更新词汇表",{"2":{"592":1}}],["更新后的词表如下",{"2":{"583":1}}],["更新后的频数表如下",{"2":{"583":1}}],["更新后的子词表如下",{"2":{"583":1}}],["更新学习率",{"2":{"385":1,"1243":1}}],["更新的模板为",{"2":{"1231":1}}],["更新的幅度也很小",{"2":{"991":1}}],["更新的每一步都由特征学习率",{"2":{"352":1}}],["更新的时候还是只处理",{"2":{"144":1}}],["更新量也会很小",{"2":{"333":1}}],["更新规则和输出规则",{"2":{"273":1}}],["更新解码器状态隐状态hi",{"2":{"267":1}}],["更新模型中的权重",{"2":{"148":1}}],["更新参数并且返回",{"2":{"423":1}}],["更新参数",{"2":{"142":1}}],["更细粒度的模型操纵形式",{"2":{"139":1}}],["更相关的输出结果",{"2":{"138":1}}],["由字母",{"2":{"1729":1}}],["由不同数据类型组成的数据集合",{"2":{"1728":1}}],["由编译器或操作系统预先设定",{"2":{"1648":1}}],["由编码3和8组成",{"2":{"449":1}}],["由编码3和6组成",{"2":{"449":1}}],["由",{"2":{"1603":1}}],["由intel提供",{"2":{"1569":1}}],["由我们选择超参数的程序引起的结果变化",{"2":{"1152":1}}],["由方差和期望的关系",{"2":{"1003":1}}],["由此",{"2":{"998":1}}],["由贝叶斯公式",{"2":{"908":1}}],["由上图左右对比可知",{"2":{"839":1}}],["由上图",{"2":{"839":2}}],["由上可见",{"2":{"341":1}}],["由前向语言模型和后向语言模型组成",{"2":{"717":1}}],["由一个对",{"2":{"614":1}}],["由最终的线性变换和softmax层最终转换为概率分布",{"2":{"526":1}}],["由神经元和其他称为星形胶质细胞",{"2":{"487":1}}],["由节点的线性依赖性产生的奇异性",{"2":{"305":1}}],["由给定层中节点的排列对称引起的重叠奇异性",{"2":{"305":1}}],["由解码器来决定编码后的序列中哪个部分最重要",{"2":{"284":1}}],["由这个序列派生出来q",{"2":{"161":1}}],["由vinod",{"2":{"104":1}}],["由karpathy对transformer架构的讨论引发的思考",{"2":{"47":1}}],["由于我们希望尽可能多的小孩得到满足",{"2":{"2151":1}}],["由于我们不再关心最大化我们对优化问题的经验",{"2":{"1153":1}}],["由于前面我看的manage",{"2":{"2070":1}}],["由于需要安装vim",{"2":{"2070":1}}],["由于链表不是连续存储",{"2":{"1799":1}}],["由于链式法则的应用",{"2":{"296":1}}],["由于联合体的成员共享内存",{"2":{"1728":1}}],["由于数组传参会退化为指针",{"2":{"1667":1}}],["由于数据量大大缩减",{"2":{"1370":1}}],["由于其高效的执行速度和对硬件的直接控制能力",{"2":{"1602":1}}],["由于其随机性采样和快速更新的特点",{"2":{"1026":1}}],["由于注意力机制",{"2":{"1317":1}}],["由于没有平行对照文本",{"2":{"1315":1}}],["由于模型已经在大量数据上进行过预训练",{"2":{"1313":1}}],["由于动态图可以根据实际输入数据生成计算图",{"2":{"1287":1}}],["由于动量的引入",{"2":{"1031":1,"1032":1}}],["由于该调度器是递归定义的",{"2":{"1243":1}}],["由于该函数对于位置变量而言是连续的",{"2":{"751":1}}],["由于样本方差的原因",{"2":{"1186":1}}],["由于你使用的是非自适应随机搜索",{"2":{"1175":1}}],["由于很难知道我们什么时候采样足够",{"2":{"1148":1}}],["由于运行实验的成本很高",{"2":{"1146":1}}],["由于使用的是一小批次的样本",{"2":{"1027":1}}],["由于使用的是kldivloss",{"2":{"399":1}}],["由于使用随机采样的梯度",{"2":{"1026":1}}],["由于使用整个数据集的梯度",{"2":{"1025":1}}],["由于bgd使用整个训练数据集的梯度",{"2":{"1025":1}}],["由于内存限制",{"2":{"986":1}}],["由于这是一个常见的模式",{"2":{"1117":1}}],["由于这些块在内存中无需连续",{"2":{"982":1}}],["由于这个语言模型是根据context",{"2":{"894":1}}],["由于这个限制的存在",{"2":{"879":1}}],["由于碎片化和过度保留",{"2":{"981":1}}],["由于长上下文推理只是最近出现",{"2":{"977":1}}],["由于长上下文预填具有丰富的并行性",{"2":{"977":1}}],["由于减少内存读写",{"2":{"964":1}}],["由于不需要保存大小为𝑁×𝑁的大型矩阵s和p",{"2":{"964":1}}],["由于一些或大部分操作是内存密集型的",{"2":{"941":1,"960":1}}],["由于分支有时候会较多",{"2":{"904":1}}],["由于独特的设计结构",{"2":{"862":1}}],["由于relu在",{"2":{"840":1}}],["由于rl是针对奖励值进行优化而非next",{"2":{"224":1}}],["由于激活函数是没有上界的",{"2":{"840":1}}],["由于零中心激活函数的输出均值为零",{"2":{"838":1}}],["由于偏置",{"2":{"765":1}}],["由于自注意力层中的额外计算步骤",{"2":{"746":1}}],["由于自然语言一般更依赖于相对位置",{"2":{"744":1,"1338":1}}],["由于采用了类似bert的编码器",{"2":{"727":1}}],["由于基于",{"2":{"696":1}}],["由于它不需要返回任何值",{"2":{"1729":1}}],["由于它可能不是最短路径",{"2":{"692":1}}],["由于它们依赖于运行时统计信息",{"2":{"327":1}}],["由于它们对依赖关系的精确建模以及有限的上下文窗口",{"2":{"231":1}}],["由于pattention层的多功能设计",{"2":{"623":1}}],["由于在数据可预测时动态选择长patch",{"2":{"611":1}}],["由于字节层面比字符粒度更低一层",{"2":{"608":1}}],["由于每次更新只使用一个样本或一小批样本的梯度",{"2":{"1026":1}}],["由于每次保留",{"2":{"601":1}}],["由于每个主机只拥有一个key",{"2":{"975":1}}],["由于每个附加token都会改变每个其他token的嵌入",{"2":{"746":1}}],["由于每个patch都通过一个大的transformer步骤进行处理",{"2":{"613":1}}],["由于每个位置只关注之前的token",{"2":{"542":1}}],["由于每个头的计算是并行进行的",{"2":{"34":1}}],["由于每个头工作在较低维度的空间中",{"2":{"21":1}}],["由于因果注意力机制",{"2":{"542":1}}],["由于因果掩码",{"2":{"542":1}}],["由于存在大量的冗余参数",{"2":{"512":1}}],["由于差分注意力往往具有更稀疏的模式",{"2":{"503":1}}],["由于梯度矩阵的维度高且难以全面分析",{"2":{"485":1}}],["由于对`",{"2":{"399":1}}],["由于对于单层全连接网络",{"2":{"28":1}}],["由于ln保留了输入张量的维度",{"2":{"359":1}}],["由于所有",{"2":{"352":1}}],["由于所有参数都是随机初始化的",{"2":{"332":1}}],["由于单位方差",{"2":{"343":1}}],["由于transformer的层数开始加深",{"2":{"335":1}}],["由于参数的分布不同",{"2":{"309":1}}],["由于网络权重的不断更新",{"2":{"309":1}}],["由于hthth",{"2":{"240":1}}],["由于卷积网络天然能够建模相对位置关系并打破",{"2":{"232":1}}],["由于上下文有限但依赖关系建模精确",{"2":{"226":1}}],["由于并非所有参数都会更新",{"2":{"192":1}}],["由于exp函数",{"2":{"180":1}}],["由于d≪n",{"2":{"180":1}}],["由于",{"2":{"176":1,"382":1,"742":1,"756":2,"861":1,"943":1,"961":1,"986":1,"1316":1,"1343":1,"1344":1,"1611":1}}],["由于有效上下文长度的限制",{"2":{"151":1}}],["由于巨量参数的存在",{"2":{"138":1}}],["由于swiglu的原因",{"2":{"109":1}}],["由于填充词位置的分数是非常大的负数",{"2":{"63":1}}],["由于将不同head的",{"2":{"41":1}}],["由于多头注意力机制能够从多个角度分析输入数据",{"2":{"21":1}}],["由单个注意头产生的上下文向量被拼接为一个向量",{"2":{"16":1}}],["优先考虑胃口最小的小孩",{"2":{"2151":1}}],["优先完成最紧急的任务",{"0":{"2132":1},"1":{"2133":1,"2134":1,"2135":1}}],["优先推荐使用",{"2":{"1715":1}}],["优先级",{"2":{"1635":1}}],["优先使用",{"2":{"1632":2}}],["优势2",{"0":{"983":1}}],["优势1",{"0":{"982":1}}],["优势与劣势",{"0":{"600":1,"603":1}}],["优势的同时",{"2":{"512":1}}],["优势",{"0":{"495":1,"561":1,"696":1},"2":{"600":1,"603":1,"840":1,"1579":1}}],["优秀的研发工作对基础技术的进步贡献远胜过那些我们常认为是",{"2":{"284":1}}],["优劣",{"0":{"279":1,"411":1,"512":1,"593":1,"608":1,"746":1},"1":{"594":1,"595":1},"2":{"741":1}}],["优于moe",{"2":{"156":1}}],["优于",{"2":{"108":1}}],["优化时间",{"2":{"2108":1}}],["优化较好",{"2":{"2026":1}}],["优化dda算法",{"2":{"2021":1}}],["优化方向",{"0":{"2020":1}}],["优化用于intel架构",{"2":{"1569":1}}],["优化目标不再是noise",{"2":{"1363":1}}],["优化目标是最小化交叉熵",{"2":{"612":1}}],["优化掉",{"2":{"1144":1,"1150":1}}],["优化输入管道",{"0":{"1161":1},"2":{"1125":1}}],["优化算法",{"0":{"1040":1,"1047":1},"1":{"1041":1,"1042":1,"1043":1,"1044":1,"1045":1,"1046":1,"1048":1,"1049":1,"1050":1},"2":{"1158":1}}],["优化器还支持指定每个参数的选项",{"2":{"1222":1}}],["优化器及学习率配置",{"2":{"1215":1}}],["优化器选择",{"2":{"1202":1}}],["优化器超参数",{"2":{"1137":1,"1158":1}}],["优化器",{"0":{"1058":1},"2":{"1130":1}}],["优化器使用梯度下降等迭代方法来更新模型的参数",{"2":{"1021":1}}],["优化器的状态",{"2":{"1226":1}}],["优化器的选择通常是一个目标超参数或固定超参数",{"2":{"1143":1}}],["优化器的目标是通过调整模型的参数",{"2":{"1021":1}}],["优化器的动量项有关",{"2":{"666":1}}],["优化thread",{"2":{"972":1}}],["优化attention部分thread",{"2":{"972":1}}],["优化性能",{"2":{"739":1}}],["优化都很难进行下去",{"2":{"337":1}}],["优化策略",{"0":{"203":1},"1":{"204":1,"205":1,"206":1,"207":1},"2":{"157":1}}],["优化",{"0":{"202":1},"1":{"203":1,"204":1,"205":1,"206":1,"207":1,"208":1,"209":1,"210":1,"211":1,"212":1,"213":1,"214":1,"215":1,"216":1,"217":1,"218":1,"219":1,"220":1,"221":1,"222":1,"223":1,"224":1,"225":1,"226":1,"227":1,"228":1,"229":1,"230":1,"231":1,"232":1},"2":{"157":1,"1144":1}}],["优化与演进",{"0":{"149":1,"345":1},"1":{"150":1,"151":1,"152":1,"153":1,"154":1,"155":1,"346":1,"347":1,"348":1,"349":1,"350":1,"351":1,"352":1,"353":1,"354":1,"355":1,"356":1,"357":1,"358":1,"359":1,"360":1},"2":{"96":1,"293":1}}],["优化的过程即执行在高维空间的朗之万动力学",{"2":{"506":1}}],["优化的",{"2":{"83":1}}],["优化的方法如下",{"2":{"20":1}}],["优点如下",{"2":{"608":1}}],["优点",{"0":{"21":1,"250":1,"594":1},"2":{"0":1,"565":1,"566":1,"1607":1,"1632":1,"1646":1,"1761":1,"1776":1,"1907":1,"1911":1,"1926":1,"1929":1}}],["每块饼干也有一个大小",{"2":{"2147":1}}],["每块不超过预填块大小",{"2":{"977":1}}],["每条消息附带一个标签",{"2":{"1576":1}}],["每条消息包含源进程id",{"2":{"1573":1}}],["每条句子记录两条最佳路径",{"2":{"1330":1}}],["每经过",{"2":{"1235":1}}],["每项研究中有多少试验是不可行",{"2":{"1146":1}}],["每轮实验都应该有一个明确的目标",{"2":{"1141":2}}],["每步的资源消耗",{"2":{"1134":1}}],["每步的时间应该是恒定的",{"2":{"1132":1}}],["每步时间",{"2":{"1132":1,"1133":1}}],["每张卡计算一个",{"2":{"974":1}}],["每行包含一个单词及其出现的次数",{"2":{"1933":1}}],["每行一个学生",{"2":{"1825":1,"1843":1}}],["每行最大值",{"2":{"970":1}}],["每行有",{"2":{"340":1}}],["每组元素对应的旋转角度",{"2":{"1345":1}}],["每组实验都有一个特定的目标",{"2":{"1146":1}}],["每组负责对相应的输入层进行卷积计算",{"2":{"775":1}}],["每组都会输出一个依据注意力权重来加权求和的向量",{"2":{"517":1}}],["每小时能跑53",{"2":{"713":1}}],["每四个字节进行跨步分组",{"2":{"613":1}}],["每种动物都有各自的特点和行为",{"2":{"1689":1}}],["每种模式有特定的功能和快捷键",{"2":{"1541":1}}],["每种方案会导致不同的patch数量",{"2":{"613":1}}],["每种架构变体都针对特定的学习目标和任务进行了定制",{"2":{"539":1}}],["每patch固定字节数",{"2":{"613":1}}],["每",{"2":{"510":1}}],["每当我们学习一个新的的领域的时候",{"2":{"402":1}}],["每做一次dropout",{"2":{"393":1}}],["每训练10个批量后会更新一次模型参数",{"2":{"372":1}}],["每份成为有",{"2":{"340":1}}],["每页只有一个字",{"2":{"340":1}}],["每页有",{"2":{"340":1}}],["每本有",{"2":{"340":1}}],["每头的输出会concat",{"2":{"201":1}}],["每头",{"2":{"201":1}}],["每层神经元与下层神经元全互连",{"2":{"1464":1}}],["每层由一个差分注意力模块和前馈网络模块连接形成",{"2":{"501":1}}],["每层两步",{"2":{"352":1}}],["每层注意力机制中都有自己的wq",{"2":{"201":1}}],["每层都有自己的注意力机制",{"2":{"201":1}}],["每层的调节阀数量可以有不同的变化组合",{"2":{"1466":1}}],["每层的梯度范数近似相等",{"2":{"335":1}}],["每层的总计算复杂度",{"2":{"160":1}}],["每层的输出相当于合并了数以千百计的激活记忆分布",{"2":{"126":1}}],["每次优先使用面值最大的硬币",{"2":{"2138":1}}],["每次优化一个小目标",{"2":{"2102":1}}],["每次先做最紧急的任务",{"2":{"2134":1}}],["每次调用都会泄漏",{"2":{"1671":1}}],["每次进入代码块时",{"2":{"1649":1}}],["每次递归调用都需要保存现场信息",{"2":{"1646":1}}],["每次设计新事物时",{"2":{"1151":1}}],["每次试验的验证目标值通常应该是它在训练期间达到的最佳值",{"2":{"1147":1}}],["每次更改模型或优化器时",{"2":{"1132":1}}],["每次更新参数时",{"2":{"991":1}}],["每次运行",{"2":{"1108":1}}],["每次运行都会利用数据加载器来加载数据",{"2":{"364":1}}],["每次都选最佳",{"0":{"2101":1}}],["每次都使用整个训练数据集的梯度相比",{"2":{"1026":1}}],["每次都会得到一个新的x",{"2":{"522":1}}],["每次参数更新时使用整个训练数据集的梯度",{"2":{"1025":1}}],["每次利用梯度下降更新参数的时",{"2":{"991":1}}],["每次执行我们习惯上称值为一个时间步",{"2":{"853":1}}],["每次嵌入向量更新后会将其范数裁剪到不超过",{"2":{"702":1}}],["每次在文档中生成一个概念",{"2":{"633":1}}],["每次合并一个",{"2":{"601":1}}],["每次合并后词表可能出现3种变化",{"2":{"582":1}}],["每次词表中选出两个subword合并成新的subword",{"2":{"598":1}}],["每次迭代都做同样事情",{"2":{"581":1}}],["每次迭代都相当于训练一个不同的子网络",{"2":{"393":1}}],["每次输入是变化的",{"2":{"515":1}}],["每次输出一个新token",{"2":{"426":1}}],["每次输出的上下文如下",{"2":{"267":1}}],["每次推理的序贯关系被打破",{"2":{"416":1}}],["每次预测新token都是一个分类任务",{"2":{"397":1}}],["每次预测一个token都需要o",{"2":{"184":1}}],["每次循环操作如下",{"2":{"36":1}}],["每一张图片名称",{"2":{"1250":1}}],["每一维代表是某个词的概率",{"2":{"899":1}}],["每一维度都是事先规定好的",{"2":{"687":1}}],["每一步都能让当前的结果尽可能大",{"2":{"2124":1}}],["每一步都做出当前最优的选择",{"2":{"2114":1,"2121":1}}],["每一步都将出现频数最大的一对",{"2":{"576":1}}],["每一步只增加少量时间",{"2":{"1134":1}}],["每一步会选择k个候选项作为下一步的扩展",{"2":{"904":1}}],["每一步",{"2":{"897":1,"901":1}}],["每一步的开始",{"2":{"902":1}}],["每一步的预测结果",{"2":{"894":1}}],["每一步的输出取决于先前的隐藏状态和当前的输入",{"2":{"254":1}}],["每一行代表一个字",{"2":{"709":1}}],["每一行代表一个token",{"2":{"168":1}}],["每一行代表对应字的位置信息",{"2":{"704":1}}],["每一行的512维向量代表一个字",{"2":{"704":1}}],["每一行存储的向量就是这个词",{"2":{"700":1}}],["每一行就是一个batch中的相关维度",{"2":{"323":1}}],["每一行减去每一行的均值然后除以每一行的标准差",{"2":{"323":1}}],["每一个词表中的词都有一个",{"2":{"700":1}}],["每一个方面都是一个维度",{"2":{"683":1}}],["每一个正向的函数",{"2":{"661":1,"1104":1}}],["每一个时间步都需要访问cache一次吗",{"2":{"656":1}}],["每一个encoderlayer输出的矩阵维度与输入完全一致",{"2":{"518":1}}],["每一个卷积层都会进行注意力操作",{"2":{"290":1}}],["每一个单词的query",{"2":{"172":1}}],["每一个维度的数值都是由几个词向量在这一维度的数值加权求和得来的",{"2":{"166":1,"170":1}}],["每一个key",{"2":{"128":1}}],["每一个key向量",{"2":{"125":1}}],["每一层由许多个可以控制水流流向与流量的调节阀",{"2":{"1466":1}}],["每一层的每个调节阀都通过水管与下一层的所有调节阀连接起来",{"2":{"1466":1}}],["每一层的神经元学到的东西都是一样的",{"2":{"992":1}}],["每一层的路由权重反映了对输入标记的推理选择",{"2":{"739":1}}],["每一层的计算复杂度",{"2":{"511":1}}],["每一层的输出又进一步是ffn的输出与残差的组合",{"2":{"129":1}}],["每一层都输出到上面的层",{"2":{"1464":1}}],["每一层都有两个子层",{"2":{"914":1}}],["每一层都会通过一个激活函数进行非线性转换",{"2":{"496":1}}],["每一层都会对输入进行不同级别的变换和抽象",{"2":{"437":1}}],["每一层都生成了一组嵌入",{"2":{"437":1}}],["每一层做各自的事情",{"2":{"437":1}}],["每一层有很多的卷积核",{"2":{"338":1}}],["每一层softmax",{"2":{"214":1}}],["每一层包含multi",{"2":{"97":1}}],["每一次",{"2":{"117":1}}],["每列下面数字代表剩余多少头",{"2":{"20":1}}],["每列代表不同修剪量",{"2":{"20":1}}],["每个小孩最多只能分到一块饼干",{"2":{"2147":1}}],["每个小孩都有一个对饼干大小的需求",{"2":{"2147":1}}],["每个键是唯一的",{"2":{"1807":1}}],["每个函数负责完成一个特定的任务",{"2":{"1729":1}}],["每个函数专注于完成一个特定的任务",{"2":{"1729":1}}],["每个参数由参数类型和参数名组成",{"2":{"1729":1}}],["每个参数组包含特定于优化器的元数据",{"2":{"1227":1}}],["每个成员都有自己的数据类型和名称",{"2":{"1728":1}}],["每个对象拥有独立的内存空间",{"2":{"1694":1}}],["每个类重载",{"2":{"1690":1}}],["每个类都继承自",{"2":{"1690":1}}],["每个非静态成员函数都有一个指向调用该函数的对象的指针",{"2":{"1638":1}}],["每个非矩阵乘法flop的成本是矩阵乘法flop的16倍",{"2":{"968":1}}],["每个单元存储一个元素",{"2":{"1623":1}}],["每个单词只能获得上文信息",{"2":{"720":1}}],["每个单词的嵌入就固定了",{"2":{"715":1}}],["每个单词的query",{"2":{"172":1}}],["每个单词会定位这个表中的某一行",{"2":{"700":1}}],["每个单词会据此把其它单词的信息吸收进来做信息融合",{"2":{"265":1}}],["每个单词与其他单词的距离相等",{"2":{"681":1}}],["每个单词都被映射到固定的",{"2":{"715":1}}],["每个单词都由一个向量表示",{"2":{"681":1}}],["每个单词都和它之前的单词相关",{"2":{"238":1}}],["每个单词可以使用一个index来进行唯一标识",{"2":{"679":1}}],["每个单词是128维度的向量",{"2":{"343":1}}],["每个单词和前面单词的关联度是不一样的",{"2":{"287":1}}],["每个房子都有一个唯一的地址",{"2":{"1611":1}}],["每个进程计算自己的部分和",{"2":{"1594":1}}],["每个进程负责的部分大小",{"2":{"1594":1}}],["每个进程处理数据的一部分",{"2":{"1578":1}}],["每个进程处理原始embedding的1",{"2":{"10":1}}],["每个进程执行相同的代码",{"2":{"1573":1}}],["每个处理器有独立的内存",{"2":{"1568":1}}],["每个文件权限用",{"2":{"1512":1}}],["每个文本都会分别映射成向量cicic",{"2":{"125":1}}],["每个标签",{"2":{"1322":1}}],["每个标记的查询",{"2":{"976":1}}],["每个循环迭代时",{"2":{"1241":1}}],["每个循环将初始振幅缩小一半",{"2":{"1241":1}}],["每个周期都会减小为上一个周期的",{"2":{"1239":1}}],["每个训练周期的步数",{"2":{"1242":1}}],["每个训练周期",{"2":{"1239":1}}],["每个调度器都会按顺序应用在前一个调度器得到的学习率上",{"2":{"1231":1}}],["每个问题都有自己的特性和计算资源限制",{"2":{"1145":1}}],["每个配置称为",{"2":{"1144":1}}],["每个模块有自己的头文件",{"2":{"1628":1}}],["每个模块都是由两个子模块组合而成",{"2":{"457":1}}],["每个模块都通过",{"2":{"222":1}}],["每个模型都有不同的超参数设置",{"2":{"1129":1}}],["每个原地操作都需要重新构建计算图",{"2":{"1123":1}}],["每个通大小为",{"2":{"1004":1}}],["每个通道在每次前向调用时都会独立地被置零",{"2":{"835":1}}],["每个通道的向量参数",{"2":{"360":1}}],["每个树边都带有一个标签",{"2":{"986":1}}],["每个节点维护一个引用计数器",{"2":{"986":1}}],["每个节点都表示其子节点的相对概率",{"2":{"184":1}}],["每个页面的大小等于一个token",{"2":{"986":1}}],["每个gpu的激活内存占用也小了cp倍",{"2":{"976":1}}],["每个gpu仅对序列的一部分进行计算",{"2":{"976":1}}],["每个gpu处理4k个标记",{"2":{"976":1}}],["每个gpu在前向过程中仅存储一个序列块的kv",{"2":{"976":1}}],["每个主机负责query",{"2":{"975":1}}],["每个主机负责运行与其指定块相对应的块并行注意力外部循环的一个元素",{"2":{"975":1}}],["每个主机将键",{"2":{"975":1}}],["每个主机持有一个查询块",{"2":{"975":1}}],["每个主机通过同时将用于注意力计算的key",{"2":{"975":1}}],["每个大小为",{"2":{"944":2}}],["每个block",{"2":{"944":3}}],["每个batch的均值与方差一直震荡",{"2":{"316":1}}],["每个组共享一个键头和值头",{"2":{"937":1,"953":1}}],["每个组",{"2":{"926":1}}],["每个self",{"2":{"920":1}}],["每个scaled",{"2":{"7":1}}],["每个群组包含相似的文本",{"2":{"906":1}}],["每个时间步为何没有矩阵相乘呢",{"2":{"857":1}}],["每个时刻的上下文向量都应不同",{"2":{"284":1}}],["每个特征图对应网络在输入数据中提取的一种特征",{"2":{"773":1}}],["每个特征图之间存在大量pattern的共现性",{"2":{"337":1}}],["每个特征维度间缺少区分性",{"2":{"765":1}}],["每个元素可以指向一个整数",{"2":{"1705":1}}],["每个元素都是一个键值对",{"2":{"1807":1}}],["每个元素都是一个",{"2":{"1807":1}}],["每个元素都有一个前继和后继元素",{"2":{"1322":1}}],["每个元素都会找到完成各自任务所需的最合适的query",{"2":{"172":1}}],["每个元素的位置嵌入向量会随着其与其他元素的位置关系而变化",{"2":{"757":1}}],["每个专家专注于输入的特定特征",{"2":{"739":1}}],["每个嵌入向量的维度为",{"2":{"702":1}}],["每个嵌入向量的维度",{"2":{"702":1}}],["每个属性",{"2":{"690":1}}],["每个字词对应一行的标签分数",{"2":{"1323":1}}],["每个字对应一个512维的随机向量",{"2":{"709":1}}],["每个字节都关注前面字节的固定窗口",{"2":{"614":1}}],["每个字符在词表中有唯一序号",{"2":{"545":1}}],["每个矩阵只有1行",{"2":{"530":1}}],["每个矩阵的形状应该是",{"2":{"109":1}}],["每个解码器层根据给定的输入向目标方向进行特征提取操作",{"2":{"533":1}}],["每个解码器层包括三个重要子模块",{"2":{"525":1}}],["每个解码步骤中",{"2":{"525":1}}],["每个重要子模块周围都有一个残差连接",{"2":{"525":1}}],["每个重要子模块周围有一个残差连接",{"2":{"517":1}}],["每个子层的输出是layernorm",{"2":{"914":1}}],["每个子层都有不同的功能和作用",{"2":{"525":1}}],["每个子模块的输出会传递到下一个子模块",{"2":{"525":1}}],["每个encoderlayer完成一次对输入的特征提取过程",{"2":{"523":1}}],["每个encoderlayer由以下模块组成",{"2":{"517":1}}],["每个层分为两个重要子模块",{"2":{"517":1}}],["每个由",{"2":{"485":1}}],["每个神经元都会激活大量不相关的",{"2":{"477":1}}],["每个神经元可以视作一个简单的分类器",{"2":{"116":1}}],["每个块包含一定数量的token的key",{"2":{"981":1}}],["每个块都是原始张量的视图",{"2":{"831":1}}],["每个块主要包括如下",{"2":{"461":1}}],["每个块放到不同gpu上进行计算",{"2":{"420":1}}],["每个块的大小固定",{"2":{"216":1}}],["每个维度一个很小的变化也可能引起很大的",{"2":{"326":1}}],["每个句子对应的两个矩阵相加得的新二维矩阵能更好更全面的表达语义",{"2":{"704":1}}],["每个句子对应的字向量是一个10",{"2":{"704":1}}],["每个句子是从数据集提取出来",{"2":{"451":1}}],["每个句子内所有位置的词",{"2":{"323":1}}],["每个句子即被视为批次中的一个样本",{"2":{"316":1}}],["每个句子又由不定长度的词构成",{"2":{"316":1}}],["每个样本通道数为",{"2":{"315":1}}],["每个样本就是完整的预测序列",{"2":{"57":1}}],["每个阶段的隐状态是列表的一项",{"2":{"273":1}}],["每个query都能从对应的key上获取相应的信息",{"2":{"270":1}}],["每个事件步的隐状态都基于编码器生成的同一个隐向量来构建",{"2":{"252":1}}],["每个rnn单元接收两个输入",{"2":{"249":1}}],["每个窗口都对其中的数据进行处理",{"2":{"247":1}}],["每个z向量可以视作该任务的专家",{"2":{"224":1}}],["每个输出项的计算和其他项的计算是独立的",{"2":{"519":1}}],["每个输出值都应该在0到1之间",{"2":{"180":1}}],["每个输入",{"2":{"1323":1}}],["每个输入最前面加",{"2":{"721":1}}],["每个输入仅仅依赖于对应的h",{"2":{"250":1}}],["每个输入单词的编码输出都会通过注意力机制引入其余单词的编码信息",{"2":{"166":1}}],["每个time",{"2":{"185":1}}],["每个transformer",{"2":{"172":1}}],["每个token对应的输入是token自身编码和其位置编码的融合",{"2":{"745":1}}],["每个token对应矩阵中的一个行",{"2":{"674":1}}],["每个token在embeddings做平均",{"2":{"735":1}}],["每个token在embedding层中都有一个对应的高维表示",{"2":{"706":1}}],["每个token可以了解自己与其它token的相关性",{"2":{"431":1}}],["每个token由一个唯一的数字表示",{"2":{"431":1}}],["每个token的embedding也是经过注意力操作",{"2":{"326":1}}],["每个token的注意力几乎全在自己身上",{"2":{"172":1}}],["每个token都有一个对应的高维表示",{"2":{"709":1}}],["每个token都会去词表中查询",{"2":{"545":1}}],["每个token都对应一个隐藏状态向量和记忆向量",{"2":{"287":1}}],["每个token都要和所有token作注意力计算",{"2":{"204":1}}],["每个token都搜集到本字和源序列之中其他哪几个字比较相关",{"2":{"200":1}}],["每个token都搜集到本字和目标序列之中其他哪几个字比较相关",{"2":{"39":1,"200":1}}],["每个token就包括了在token层面其感兴趣的信息",{"2":{"101":1}}],["每个token",{"2":{"99":1,"101":1}}],["每个这样的qk对都会产生一个期望为0",{"2":{"189":1}}],["每个这样的",{"2":{"161":1}}],["每个词的嵌入就被固定下来",{"2":{"715":1}}],["每个词在词典都有一个下标索引",{"2":{"679":1,"681":1}}],["每个词之间的依赖关系",{"2":{"649":1,"931":1}}],["每个词之间是可以直接关联的",{"2":{"415":1}}],["每个词汇都被表示为一个整数",{"2":{"545":1}}],["每个词可以直接关联",{"2":{"160":1}}],["每个词都分配一个id",{"2":{"565":1}}],["每个词都只能看到前面词的状态",{"2":{"57":1}}],["每个词都需要考虑上下文的关系",{"2":{"57":1}}],["每个key神经元都会触发人类可理解的浅输入模式",{"2":{"126":1}}],["每个key至少与一个人类可理解的输入模式高度相关",{"2":{"126":1}}],["每个位置的位置编码是固定的向量",{"2":{"745":1}}],["每个位置都可以自由地注意序列中的所有其他位置",{"2":{"525":1}}],["每个位置",{"2":{"101":1}}],["每个位置取最小值",{"2":{"72":1}}],["每个数据集又包含许多",{"2":{"89":1}}],["每个",{"2":{"82":1,"148":1,"154":2,"204":3,"463":1,"485":1,"1113":1,"1183":1,"1222":1,"1762":1}}],["每个head中的",{"2":{"41":2}}],["每个head就像是一个弱分类器",{"2":{"13":1}}],["每个分组对应一个注意力头",{"2":{"33":1}}],["每个线性层都有自己独立的权重",{"2":{"26":1}}],["每个线性层都具有d",{"2":{"23":1}}],["每个头只单独保留了一份",{"2":{"935":1,"951":1}}],["每个头的复杂度可以降低到",{"2":{"210":1}}],["每个头的注意力计算其实和单头注意力没啥区别",{"2":{"34":1}}],["每个头注意力对应的维度减少了",{"2":{"28":1}}],["每个头确实学到东西有所不同",{"2":{"20":1}}],["每个头都学习了某种固定的模式",{"2":{"20":1}}],["每个头都使用",{"2":{"16":1}}],["每个头可以专注学习输入的不同部分",{"2":{"16":1}}],["每个头将独立地进行注意力计算",{"2":{"9":1}}],["每个新形成的q在本质上都要求不同类型的相关信息",{"2":{"16":1}}],["每个向量对应到一个细分语义逻辑子空间",{"2":{"12":1}}],["每个注意头都有自己的可学习权重矩阵wqiwiqw",{"2":{"12":1,"16":1}}],["每个注意力头的行为由wqwqw^q",{"2":{"45":1}}],["每个注意力头互相独立的关注到不同的子空间上下文",{"2":{"12":1}}],["每个注意力头只关注输入序列中的一个独立子空间",{"2":{"9":1}}],["每个注意力头关注不同的表示子空间",{"2":{"5":1}}],["每个注意力头都有自己独立的一组q",{"2":{"5":1}}],["每个团队成员用自己的专业能力独立的对项目付出不同的贡献",{"2":{"5":1}}],["每个团队成员负责自己擅长的领域",{"2":{"5":1}}],["每个任务其实可以认为是一个独立的向量空间",{"2":{"4":1}}],["每个视角都能看到一些不同的细节",{"2":{"1":1}}],["该程序能够将一个整数的第n位进行翻转",{"2":{"2063":1}}],["该程序包含以下功能",{"2":{"1997":1}}],["该段子曰有三句",{"2":{"2054":1}}],["该站点的整体结构",{"0":{"2036":1},"1":{"2037":1,"2038":1}}],["该算法引入了直线的一般式方程",{"2":{"2021":1}}],["该算法主要亮点就是引入了增量思想",{"2":{"2016":1}}],["该lambda表达式接受一个容器元素的引用作为参数",{"2":{"1914":1}}],["该系统中有不同种类的动物",{"2":{"1689":1}}],["该系数会与第二个参数矩阵相乘",{"2":{"128":1}}],["该指针就变成了悬",{"2":{"1647":1}}],["该回调函数用于判断数组中的元素是否满足特定条件",{"2":{"1645":1}}],["该调度器会监测一个度量指标的数值",{"2":{"1245":1}}],["该类使用std",{"2":{"1902":1}}],["该类中有三种内置的策略",{"2":{"1241":1}}],["该类除了平滑标签外",{"2":{"399":1}}],["该类除了负责平滑标签外",{"2":{"399":1}}],["该类除了包含损失计算外",{"2":{"398":1}}],["该钩子可以就地修改state",{"2":{"1227":1}}],["该钩子将在调用",{"2":{"1227":1}}],["该钩子将在调用torch",{"2":{"1227":1}}],["该钩子将在优化器步骤之后调用",{"2":{"1227":1}}],["该钩子将在优化器步骤之前调用",{"2":{"1227":1}}],["该钩子将在每次调用",{"2":{"1214":1}}],["该对象将保存当前状态并根据计算得到的梯度更新参数",{"2":{"1221":1}}],["该预钩子将在每次调用",{"2":{"1214":1}}],["该图表明模型正在经历训练不稳定",{"2":{"1182":1}}],["该图展示的是训练开始时频繁更新评估的结果",{"2":{"1179":1}}],["该图展示了基数树在响应各种请求时的动态演变",{"2":{"986":1}}],["该项梯度就会对之前梯度有个修正",{"2":{"1034":1}}],["该项不需要学习",{"2":{"760":1}}],["该公式可最终化简为",{"2":{"1003":1}}],["该公式可以演变成标号6",{"2":{"498":1}}],["该策略在两个边界之间以恒定的频率循环调整学习率",{"2":{"1241":1}}],["该策略首先淘汰最近最少使用的叶子节点",{"2":{"986":1}}],["该策略基于普通transformer的",{"2":{"20":1}}],["该网络结构包含了多个卷积层和全连接层",{"2":{"840":1}}],["该网络输入为x",{"2":{"134":1}}],["该绝对值差是常量",{"2":{"765":1}}],["该相对位置偏差矩阵被添加到自注意力层中的查询矩阵和关键矩阵的乘积中",{"2":{"762":1}}],["该复值词向量函数以位置为变量来计算每个词在不同位置的词向量",{"2":{"751":1}}],["该工作使用可训练的嵌入形式作为位置编码",{"2":{"747":1}}],["该阶段在100m的文本对上训练",{"2":{"726":1}}],["该句子依然成立",{"2":{"714":1}}],["该句子10个单词",{"2":{"428":1}}],["该向量在与该单词对应的位置上为",{"2":{"681":1}}],["该向量中",{"2":{"681":1}}],["该向量包含了可供计算与学习的",{"2":{"241":1}}],["该团队希望",{"2":{"620":1}}],["该团队使用自回归llm",{"2":{"477":1}}],["该设计不仅用于token之间的交互",{"2":{"617":1}}],["该设计也可以看作是多头架构",{"2":{"231":1}}],["该窗口通常可以跨越动态",{"2":{"614":1}}],["该整数就是此token在词表中的唯一序号",{"2":{"550":1}}],["该功能或者模块有如下常见方式",{"2":{"549":1}}],["该因果性体现在上下文",{"2":{"536":1}}],["该因子可以反应输入序列各个位置对于解码器当前输出的影响力",{"2":{"256":1}}],["该输出会送入解码器堆栈中的每一个decoderlayer中",{"2":{"518":1}}],["该均衡是渐近稳定的",{"2":{"507":1}}],["该单元覆盖了隐藏层和最后一层之间的突触",{"2":{"489":1}}],["该前馈网络具有一个星形胶质细胞单元",{"2":{"489":1}}],["该可解释性研究旨在以逆向工程方式剖析ai模型",{"2":{"475":1}}],["该分布表示对词表中每个词匹配这个特征向量的概率",{"2":{"431":1}}],["该分布的预测会随着每层里的残差链接被不断校正",{"2":{"126":1}}],["该掩码是一个跟输入矩阵一样形状的矩阵",{"2":{"409":1}}],["该值是上一时刻预测出来的",{"2":{"407":1}}],["该值一般和d",{"2":{"343":1}}],["该数据不需要梯度下降",{"2":{"383":1}}],["该操作旨在通过α学习适当的缩放因子",{"2":{"359":1}}],["该现象会导致下面几个问题",{"2":{"309":1}}],["该特点可以解决长距离依赖问题",{"2":{"274":1}}],["该函数使用给定的lambda表达式处理容器中的每个元素",{"2":{"1914":1}}],["该函数返回满足条件的元素的个数",{"2":{"1645":1}}],["该函数返回token列表所对应的index列表",{"2":{"558":1}}],["该函数",{"2":{"1342":1}}],["该函数的局部变量",{"2":{"1648":1}}],["该函数的行为将会发生变化",{"2":{"1214":1}}],["该函数的作用是",{"2":{"557":1}}],["该函数用于在稀疏张量中合并重复的索引和值",{"2":{"1086":1}}],["该函数利用collate",{"2":{"375":1}}],["该函数一般接受key和value向量作为输入",{"2":{"268":1}}],["该函数旨在提高llm激活稀疏性",{"2":{"111":1}}],["该架构训练两个神经网络相互竞争",{"2":{"1472":1}}],["该架构通过指定token",{"2":{"446":1}}],["该架构的整体注意力掩码如上图所示",{"2":{"231":1}}],["该架构具有两个关键优势",{"2":{"231":1}}],["该架构包含三个分支",{"2":{"229":1}}],["该架构用创新的",{"2":{"151":1}}],["该神经记忆的优势在于可以快速并行训练",{"2":{"226":1}}],["该模块的",{"2":{"1214":1}}],["该模块的输入是一个索引列表",{"2":{"834":1}}],["该模块的作用是把解码器输出的隐向量从word",{"2":{"397":1}}],["该模块需要q=k=v",{"2":{"533":1}}],["该模块可以让数据更稳定",{"2":{"517":1}}],["该模块通过简单的正向传递",{"2":{"228":1}}],["该模块引入了一种自适应的遗忘机制",{"2":{"228":1}}],["该模块将源序列和目标序列进行对齐",{"2":{"525":1}}],["该模块将",{"2":{"228":1}}],["该模块将训练过程视为在线学习问题",{"2":{"228":1}}],["该模块能够在利用长远历史信息的同时",{"2":{"226":1}}],["该模型是ipc进程间通信",{"2":{"1568":1}}],["该模型使用特殊的前缀标记来指示源语言和目标语言",{"2":{"1317":1}}],["该模型大幅超越了",{"2":{"1315":1}}],["该模型对sonar空间进行量化",{"2":{"636":1}}],["该模型将编码器和解码器分离",{"2":{"635":1}}],["该模型将残差结构运用到attention层",{"2":{"349":1}}],["该模型引入了扩散模型",{"2":{"634":1}}],["该模型可以完美地从预训练阶段恢复模型状态",{"2":{"623":1}}],["该模型每层有12个注意头",{"2":{"437":1}}],["该模型的目标是生成v中向量的加权平均值",{"2":{"264":1}}],["该模型其他任务的性能会全面下降",{"2":{"222":1}}],["该模型有选择地为路由模块控制的每个令牌激活特定的专家",{"2":{"150":1}}],["该框架允许",{"2":{"225":1}}],["该机制还通过层次预填充",{"2":{"977":1}}],["该机制通过结合前向传播的输入和目标嵌入",{"2":{"485":1}}],["该机制考虑了记忆大小和数据惊讶程度",{"2":{"228":1}}],["该机制将相似度函数分解为特征映射的点积",{"2":{"210":1}}],["该机制针对",{"2":{"209":1}}],["该机制的目的是合并",{"2":{"209":1}}],["该机制只是将一组新的可训练矩阵应用于注意力权重矩阵",{"2":{"209":1}}],["该词具备词关联性",{"2":{"170":1}}],["该查询会通过query和存储器内所有元素key的地址进行相似性比较来寻址",{"2":{"164":1}}],["该地址总结了地址中value的特征",{"2":{"164":1}}],["该层对编码器堆栈的输出执行multi",{"2":{"915":1}}],["该层可以在通用嵌入任务中实现更具表现力的序列池",{"2":{"735":1}}],["该层可能非常庞大",{"2":{"296":1}}],["该层输出一个新的表示",{"2":{"525":1}}],["该层为序列中的每个单词生成嵌入向量",{"2":{"161":1}}],["该层通过内存查找表和局部敏感哈希",{"2":{"153":1}}],["该过程始于模型执行前向传播",{"2":{"484":1}}],["该过程类似cnn中",{"2":{"473":1}}],["该过程中的每一步预测都需要依赖上一步预测的结果",{"2":{"239":1}}],["该过程如图",{"2":{"154":1}}],["该过程通过更新ff1ff1ff",{"2":{"148":1}}],["该研究的动机在于扩展现有的可解释性方法",{"2":{"484":1}}],["该研究通过引入具有",{"2":{"154":1}}],["该研究在所有记忆层中使用共享记忆参数池",{"2":{"154":1}}],["该研究在多个",{"2":{"154":1}}],["该研究将键",{"2":{"154":1}}],["该组合是根据",{"2":{"154":1}}],["该组件将模型中的值矩阵复制一份作为辅助记忆",{"2":{"143":1}}],["该实验是在针对单问题数据集上训练和测试的",{"2":{"147":1}}],["该能力只与模型参数量有关",{"2":{"147":1}}],["该优化不会直接改变模型权重",{"2":{"145":1}}],["该技术通过对",{"2":{"138":1}}],["该技术借鉴了经典机器学习",{"2":{"137":1}}],["该mlp可以通过使用生成与头输出方向相一致或相反的向量来擦除或放大各个头的输出",{"2":{"122":1}}],["该方案将每个单词的位置k映射为一个唯一的位置向量pkpkp",{"2":{"748":1}}],["该方案通过参数token化实现了模型的高效扩展和计算优化",{"2":{"616":1}}],["该方案的缺点是",{"2":{"739":2}}],["该方案的优点是",{"2":{"739":2}}],["该方案的优势是在长上下文中的线性",{"2":{"273":1}}],["该方案的思路是",{"2":{"575":1}}],["该方案应该",{"2":{"309":1}}],["该方法显著提升了实时模拟的计算效率",{"2":{"2011":1}}],["该方法对于冻结模块的一部分以进行微调或单独训练模型的部分",{"2":{"1214":1}}],["该方法对各个头使用不同的投影矩阵",{"2":{"503":1}}],["该方法会in",{"2":{"1214":1}}],["该方法会把一个词切成更小的一块一块的子词",{"2":{"567":1}}],["该方法应返回一个表示对象状态的可序列化对象",{"2":{"1214":1}}],["该方法始终返回",{"2":{"1083":1}}],["该方法的一个显着优点是其可扩展性",{"2":{"762":1}}],["该方法的发展伴随着输出向量形式的转换",{"2":{"711":1}}],["该方法主要涉及了双向注意力机制改造",{"2":{"734":1}}],["该方法可以用来提供有关模块的附加信息",{"2":{"1214":1}}],["该方法可以像单词一样发挥作用",{"2":{"689":1}}],["该方法可以揭示信息储在mlp中的机制",{"2":{"148":1}}],["该方法将模型参数视为可学习的token",{"2":{"617":1}}],["该方法将网络梯度的计算转化为解一个ode",{"2":{"495":1}}],["该方法以三个字符为一组",{"2":{"595":1}}],["该方法仅用于demo",{"2":{"383":1}}],["该方法结合了gpu的特点进行了计算加速",{"2":{"185":1}}],["该方法在会在构建tgt的mask时使用",{"2":{"74":1}}],["该方向认为知识不是单独的存储在某一区域的",{"2":{"123":1}}],["该方向认为事实以键值对的形式存储在mlp中",{"2":{"123":1}}],["该方阵对角线与左下全为true",{"2":{"74":1}}],["该矩阵的形状跟注意力分布矩阵一样",{"2":{"70":1}}],["该种方式将8个注意头全部平铺在三维输入矩阵的第0维batch",{"2":{"32":1}}],["该论文将模型参数数量超过1b的语言模型称为llm",{"2":{"711":1}}],["该论文将需要存储的信息分别映射为key向量与value向量",{"2":{"125":1}}],["该论文的研究似乎也说明了",{"2":{"490":1}}],["该论文还发现",{"2":{"561":1}}],["该论文还提出了一种新的思路",{"2":{"484":1}}],["该论文还分析了如何去精简heads",{"2":{"20":1}}],["该论文对",{"2":{"320":1}}],["该论文提醒我们",{"2":{"181":1}}],["该论文提出了将网络分解为biased和centered两个子网络的思想",{"2":{"298":1}}],["该论文提出将将辅助记忆复制k次",{"2":{"143":1}}],["该论文提出使用glu的变种",{"2":{"103":1}}],["该论文主要提出了一种llm的编辑方法",{"2":{"145":1}}],["该论文采用ties",{"2":{"143":1}}],["该论文引入了两个组件辅助记忆模块和知识分片和合并机制",{"2":{"143":1}}],["该论文判断",{"2":{"130":1}}],["该论文发现所有注意力头之间捕捉的信息肯定是存在冗余的",{"2":{"19":1}}],["很相似",{"2":{"2102":1}}],["很直观明了",{"2":{"2022":1}}],["很高兴能和大家一起开启",{"2":{"1601":1}}],["很显然",{"2":{"1337":1}}],["很类似",{"2":{"1275":1,"2099":1}}],["很复杂",{"2":{"1136":1}}],["很少有情况下原地操作能够显著降低内存使用量",{"2":{"1123":1}}],["很可能产生",{"2":{"840":1}}],["很可能被表示为单个",{"2":{"595":1}}],["很自然的想法是",{"2":{"767":1}}],["很接近",{"2":{"334":1}}],["很容易被两个词中间的其它词干扰",{"2":{"246":1}}],["很难对高级黑盒优化算法进行benchmark测试",{"2":{"1175":1}}],["很难利用先前的试验结果",{"2":{"1175":1}}],["很难利用额外的条件信息",{"2":{"273":1}}],["很难理解内部的决策过程",{"2":{"512":1}}],["很难找到多个长度一样的句子",{"2":{"376":1}}],["很难得到稳定的统计量",{"2":{"316":1}}],["很难有效构建两者的依赖关系",{"2":{"253":1}}],["很难收敛",{"2":{"239":1}}],["很难只依靠单个头就可以同时捕捉到语法",{"2":{"13":1}}],["很像超参数",{"2":{"187":1}}],["很小",{"2":{"185":1}}],["很明显",{"2":{"168":1,"908":1}}],["很多经典算法的灵感就来源于我们生活中的需求",{"2":{"2107":1}}],["很多时候",{"2":{"2104":1}}],["很多人会感到有些害怕",{"2":{"2096":1}}],["很多编程语言中的抽象类也叫接口类",{"2":{"1693":1}}],["很多实际应用更关注生成的连贯性和语义丰富性",{"2":{"542":1}}],["很多非",{"2":{"512":1}}],["很多研究者在对transformer架构进行改进",{"2":{"512":1}}],["很多研究者正在探索知识蒸馏",{"2":{"512":1}}],["很多llm会在训练中采用了多种处理策略",{"2":{"369":1}}],["很多外推性好的改进某种意义上都是局部注意力的变体",{"2":{"204":1}}],["很多",{"2":{"135":1}}],["很多模型在ffn层都不用偏置了",{"2":{"99":1}}],["很多可以被砍掉",{"2":{"20":1}}],["很值得阅读和学习",{"2":{"48":1}}],["谓语",{"2":{"20":1}}],["模板构造函数",{"2":{"2063":1}}],["模板元编程",{"2":{"1961":1}}],["模板类型可以用在函数的任何位置",{"2":{"1699":1}}],["模板实例化",{"2":{"1698":1}}],["模板为我们提供了泛型编程的能力",{"2":{"1698":1}}],["模板",{"0":{"1697":1,"1698":1},"1":{"1698":1,"1699":1,"1700":1,"1701":1},"2":{"1632":1,"1917":1}}],["模板是",{"2":{"731":1}}],["模",{"2":{"1607":1}}],["模糊查询like语句该怎么写",{"2":{"1489":1}}],["模拟银行账户操作",{"2":{"1766":1}}],["模拟文件打开失败的情况",{"2":{"1763":1}}],["模拟每个批次的输入数据",{"2":{"1211":1}}],["模拟训练过程",{"2":{"1211":1}}],["模拟了不同数量的调整预算",{"2":{"1177":1}}],["模拟模型的表现越来越好",{"2":{"399":1}}],["模拟模型的输出",{"2":{"399":1}}],["模态敏感性",{"2":{"612":1}}],["模式简介",{"0":{"1541":1}}],["模式",{"0":{"1519":1},"2":{"1118":1,"1119":2,"1214":2,"1215":1,"1578":2,"1820":1,"1838":1}}],["模式和推断模式这样的上下文管理器",{"2":{"1116":1}}],["模式进行",{"2":{"629":1}}],["模式正经历着从",{"2":{"627":1}}],["模式相对的是",{"2":{"406":1}}],["模式随着层数的增加而慢慢固定",{"2":{"20":1}}],["模块文章",{"2":{"2046":2}}],["模块结构",{"2":{"2043":1}}],["模块并调整导航栏",{"2":{"2042":1}}],["模块化",{"2":{"1916":1,"1960":1}}],["模块化编程",{"2":{"1729":1}}],["模块实现并发任务",{"2":{"1566":1}}],["模块还可以包含其他模块",{"2":{"1206":1}}],["模块呢",{"2":{"1201":1}}],["模块的高级架构上",{"2":{"446":1}}],["模块的公式如下",{"2":{"173":1}}],["模块做进一步处理",{"2":{"445":1}}],["模块即指编码器",{"2":{"417":1}}],["模块",{"2":{"343":1,"445":1,"1315":1,"1316":1,"2043":3,"2045":1,"2049":1}}],["模块输出",{"2":{"145":1}}],["模型演示",{"0":{"1580":1}}],["模型大了约3倍",{"2":{"1363":1}}],["模型大小的发展趋势",{"2":{"1318":1}}],["模型后分别转换为特征向量",{"2":{"1360":1}}],["模型适合处理那些需要根据给定输入来生成新文本的任务",{"2":{"1317":1}}],["模型适合处理那些只涉及文本生成的任务",{"2":{"1316":1}}],["模型引入了两处架构变化",{"2":{"1315":1}}],["模型及它的常见变体如下",{"2":{"1315":1}}],["模型依然在",{"2":{"1315":1}}],["模型完全不需要人工标注数据",{"2":{"1312":1}}],["模型本质上都是预训练语言模型",{"2":{"1312":1}}],["模型层出不穷",{"2":{"1312":1,"1314":1}}],["模型迁移用于文本分类",{"2":{"1312":1}}],["模型跑通",{"0":{"1309":1,"1332":1}}],["模型准备",{"0":{"1295":1}}],["模型状态中都保存了什么内容呢",{"2":{"1260":1}}],["模型状态的保存",{"0":{"1256":1},"1":{"1257":1,"1258":1,"1259":1,"1260":1}}],["模型搭建",{"2":{"1215":2}}],["模型真正执行之处",{"2":{"1214":1}}],["模型选择就是选择训练期间看到的最佳检查点",{"2":{"1166":1}}],["模型参数初始值",{"2":{"1158":1}}],["模型参数量",{"2":{"363":1}}],["模型就会越来越接近其最佳性能",{"2":{"1157":1}}],["模型就根据来预测第一个字符",{"2":{"57":1}}],["模型类型等",{"2":{"1138":1}}],["模型配置",{"2":{"1137":1}}],["模型和优化器",{"2":{"1133":1}}],["模型和模拟神经元星形胶质细胞网络",{"2":{"489":1}}],["模型权重",{"2":{"1105":1}}],["模型遇到山谷不会自动减弱更新的幅度",{"2":{"1036":1}}],["模型复杂度较高",{"2":{"1012":1}}],["模型容易过拟合",{"2":{"1012":1}}],["模型容易关注到自身的位置上",{"2":{"3":1}}],["模型规模小还好",{"2":{"948":1,"978":1}}],["模型规模越大",{"2":{"217":1}}],["模型下载",{"2":{"930":1}}],["模型一经推出便取得",{"2":{"911":1}}],["模型都是自回归的",{"2":{"912":1}}],["模型都使用了这种激活函数",{"2":{"844":1}}],["模型都会变得不可用",{"2":{"115":1}}],["模型输入",{"2":{"745":2}}],["模型根据当前语境对多义词进行理解",{"2":{"715":1}}],["模型根据之前生成的元素预测当前元素的概率分布",{"2":{"239":1}}],["模型不再是简单的向量对应查表关系",{"2":{"715":1}}],["模型不需要接收编码器的信息输入",{"2":{"541":1}}],["模型加载的时候能通过",{"2":{"668":1}}],["模型保存和加载",{"0":{"668":1}}],["模型保存时候需要有",{"2":{"667":1}}],["模型里面涉及两种",{"2":{"650":1,"932":1}}],["模型以之前全部的概念为条件",{"2":{"633":1}}],["模型通常通过破坏给定的句子",{"2":{"1315":1}}],["模型通常会受益",{"2":{"1245":1}}],["模型通常将处理单个",{"2":{"624":1}}],["模型通过计算u和mimim",{"2":{"125":1}}],["模型无法直接访问底层字节特征",{"2":{"612":1}}],["模型应该动态分配计算资源",{"2":{"612":1}}],["模型应该把注意力聚焦在实际有意义的词上",{"2":{"55":1}}],["模型训练损失中不稳定的学习率",{"2":{"1182":1}}],["模型训练的时间复杂度较高",{"2":{"839":1}}],["模型训练的稳定性更强",{"2":{"334":1}}],["模型训练使用随机梯度下降",{"2":{"576":1}}],["模型要求",{"2":{"568":1}}],["模型很难学习到词与词",{"2":{"566":1}}],["模型很容易学会偷懒",{"2":{"58":1}}],["模型处理和后处理",{"2":{"551":1}}],["模型处理",{"0":{"554":1},"2":{"545":1}}],["模型处理之后输出out",{"2":{"381":1}}],["模型需要从数据中学习如何最好地利用这些信息",{"2":{"749":1}}],["模型需要额外设计输入输出配对的数据",{"2":{"542":1}}],["模型需要学习关于它自身参数的知识",{"2":{"142":1}}],["模型因其强大的零样本泛化能力而表现出色",{"2":{"542":1}}],["模型支持一直复用kv",{"2":{"542":1}}],["模型是否存在优化问题",{"2":{"1146":1}}],["模型是casual",{"2":{"542":1}}],["模型是一种神经科学模型",{"2":{"490":1}}],["模型是一种生成模型",{"2":{"239":1}}],["模型更适合生成任务",{"2":{"542":1}}],["模型更容易找到最优解",{"2":{"242":1}}],["模型没有显式的编码器模块",{"2":{"541":1}}],["模型只能处理一定长度内的文本",{"2":{"1317":1}}],["模型只能持续堆叠卷积网络",{"2":{"247":1}}],["模型只使用",{"2":{"1315":1,"1316":1}}],["模型只使用标准",{"2":{"541":1}}],["模型被推出",{"2":{"540":1}}],["模型采用如下方式调用",{"2":{"528":1}}],["模型剪枝",{"2":{"512":1}}],["模型作为示例来描述该架构",{"2":{"501":1}}],["模型开始反向传播",{"2":{"484":1}}],["模型组件归因",{"0":{"479":1}}],["模型将不断改进",{"2":{"1139":1}}],["模型将知道忽略",{"2":{"691":1}}],["模型将从特殊的起始序列标记开始依次生成输出序列",{"2":{"529":1}}],["模型将输入嵌入打包成",{"2":{"501":1}}],["模型将token",{"2":{"460":1}}],["模型将重点关注输入数据的哪些部分",{"2":{"277":1}}],["模型还需要对输入句子中每个token的位置信息进行编码",{"2":{"459":1}}],["模型调用的方式如下面代码所示",{"2":{"451":1}}],["模型所处理数据的维度",{"2":{"448":1}}],["模型结构超参数通常是目标或固定超参数",{"2":{"1143":1}}],["模型结构图",{"0":{"917":1,"1358":1}}],["模型结构详解及实现",{"2":{"513":1}}],["模型结构",{"0":{"435":1,"886":1,"1357":1},"1":{"436":1,"437":1,"887":1,"888":1,"889":1,"890":1}}],["模型结构如下图所示",{"2":{"288":1}}],["模型计算输入序列中所有位置的单词之间的注意力分数",{"2":{"417":1}}],["模型直接输出",{"2":{"409":1}}],["模型实际上可以关注到目标序列中",{"2":{"409":1}}],["模型也是采用该位置编码方式",{"2":{"1341":1}}],["模型也容易过拟合",{"2":{"1012":1}}],["模型也变得不稳定",{"2":{"407":1}}],["模型也能达到相同甚至更好的性能",{"2":{"358":1}}],["模型接受了",{"2":{"407":1}}],["模型必须经过5个时间步才能完成推理",{"2":{"405":1}}],["模型中",{"2":{"1326":1}}],["模型中的",{"2":{"1315":1,"1316":1}}],["模型中的所有子层以及嵌入层产生的输出维度都为dmodel",{"2":{"914":1}}],["模型中学到的词和短语表现出一种线性结构",{"2":{"713":1}}],["模型中不同层通常捕获不同类型的信息",{"2":{"402":1}}],["模型中使用了大量的如mqa",{"2":{"396":1}}],["模型收敛速度可能会变慢",{"2":{"400":1}}],["模型优化漫谈",{"2":{"361":2}}],["模型有更好的",{"2":{"333":1}}],["模型有两个选择",{"2":{"176":1}}],["模型确实越难训练",{"2":{"333":1}}],["模型越深",{"2":{"296":1}}],["模型越大",{"2":{"20":1}}],["模型得以衡量每个单词对其他单词的关注度",{"2":{"265":1}}],["模型",{"0":{"884":1,"1273":1},"2":{"241":1,"282":1,"333":1,"334":2,"349":1,"406":1,"490":1,"512":1,"513":1,"540":3,"614":1,"666":1,"885":1,"938":1,"954":1,"1155":1,"1242":1,"1312":6,"1315":2,"1316":2,"1317":3,"1363":1,"1472":1,"1573":1}}],["模型尚不成熟",{"2":{"239":1}}],["模型执行序列如下",{"2":{"239":1}}],["模型每次推理只会预测输出一个",{"2":{"239":1}}],["模型意外可以简单定义为它相对于输入的梯度",{"2":{"230":1}}],["模型学习一个能够记忆的函数",{"2":{"230":1}}],["模型分析传入的任务以了解其要求",{"2":{"225":1}}],["模型相比",{"2":{"220":1,"1315":1}}],["模型维度",{"0":{"417":1},"1":{"418":1,"419":1},"2":{"217":1}}],["模型对越靠后的层越敏感",{"2":{"401":1}}],["模型对这些键向量的选择更加明确和确定",{"2":{"194":1}}],["模型对于提示",{"2":{"145":1}}],["模型在分类等下游任务上取得了很好的效果",{"2":{"1316":1}}],["模型在处理任何单词时都会考虑到它的整个上下文",{"2":{"721":1}}],["模型在推理过程只需一次向前传播",{"2":{"542":1}}],["模型在同一个模块中处理输入序列与输出序列",{"2":{"542":1}}],["模型在没有任何tuning数据的情况下",{"2":{"542":1}}],["模型在模型足够大",{"2":{"542":1}}],["模型在自注意力机制中隐式完成对用户输入的分析",{"2":{"541":1}}],["模型在时间线上的垂直位置表示它们的发布日期",{"2":{"540":1}}],["模型在每层中计算所有头的attention",{"2":{"349":1}}],["模型在初始阶段就越接近一个恒等函数",{"2":{"332":1}}],["模型在整个解码过程中保持相关性的能力可能会减弱",{"2":{"256":1}}],["模型在第二步得到输入h1和",{"2":{"249":1}}],["模型在第一步得到输入h0和",{"2":{"249":1}}],["模型在多个输入之间分配了注意力",{"2":{"194":1}}],["模型在编码时",{"2":{"3":1}}],["模型架构上的显著胜利通常会转移",{"2":{"1158":1}}],["模型架构通常具有各种超参数",{"2":{"1129":1}}],["模型架构",{"0":{"215":1},"2":{"157":1,"1158":1}}],["模型解释新方向",{"2":{"156":1}}],["模型可能无法正确处理",{"2":{"1341":1}}],["模型可能无法有效地学习到更高层级的特征",{"2":{"512":1}}],["模型可能使用预训练的词向量来初始化嵌入矩阵",{"2":{"709":1}}],["模型可能收敛非常的慢",{"2":{"309":1}}],["模型可以使用",{"2":{"1317":1}}],["模型可以从预训练模型中继承这些有用的语言特征和知识",{"2":{"898":1}}],["模型可以通过反向传播来调整嵌入向量",{"2":{"686":1}}],["模型可以通过这种方法来让高频token稀释残差流中的正确回答",{"2":{"437":1}}],["模型可以利用这些单位来表示任何输入文本",{"2":{"594":1}}],["模型可以在回答问题时进行类比和推理",{"2":{"505":1}}],["模型可以计算注意力分数",{"2":{"463":1}}],["模型可以将query与keys正交对齐",{"2":{"320":1}}],["模型可以高效地处理大规模输入",{"2":{"34":1}}],["模型可解释性比较高",{"2":{"512":1}}],["模型可解释性",{"2":{"156":1}}],["模型编辑技术",{"2":{"156":1}}],["模型从内存中检索预存储的向量",{"2":{"153":1}}],["模型最终输出h5",{"2":{"249":1}}],["模型最终的",{"2":{"5":1}}],["模型最后一层普遍存在anti",{"2":{"122":1}}],["模型如何完成",{"2":{"122":1}}],["模型内部各组件之间的相互作用开始显现",{"2":{"119":1}}],["模型具有",{"2":{"89":1}}],["模型会对训练集的预测变得越来越自信",{"2":{"1155":1}}],["模型会对这三个权重矩阵进行随机初始化",{"2":{"172":1}}],["模型会隐藏句子中的某些单词",{"2":{"734":1}}],["模型会依据从数据中学习到的知识来调整这些embedding的数字以提高其任务执行能力",{"2":{"676":1}}],["模型会预测下一个概念",{"2":{"632":1}}],["模型会把刚预测的",{"2":{"529":1}}],["模型会把这些负无穷大值所对应的权重变成零",{"2":{"70":1}}],["模型会丢弃这个输出",{"2":{"407":1}}],["模型会使用softmax操作将注意力分数进行归一化得到注意力权重",{"2":{"269":1}}],["模型会不断调整和改变较大的数字",{"2":{"192":1}}],["模型会优先选择通过增大模长∥∥kj∥∥∥kj∥∥k",{"2":{"176":1}}],["模型会通过损失函数进行计算",{"2":{"168":1}}],["模型会用到后面",{"2":{"58":1}}],["模型则根据前",{"2":{"57":1}}],["模型的表征能力也会增加",{"2":{"1465":1}}],["模型的目标来完成预训练",{"2":{"1317":1}}],["模型的性能也在不断提高",{"2":{"1316":1}}],["模型的性能会急剧下降",{"2":{"296":1}}],["模型的探索在在很大程度上是由",{"2":{"1316":1}}],["模型的预训练通常围绕着预测句子中下一个单词展开",{"2":{"1316":1}}],["模型的预测",{"2":{"423":1}}],["模型的出现",{"2":{"1312":1}}],["模型的计算图是根据实际输入数据动态构建的",{"2":{"1287":1}}],["模型的计算图在执行前需要显式地进行编译和优化",{"2":{"1287":1}}],["模型的",{"2":{"1208":1}}],["模型的当前类的",{"2":{"1208":1}}],["模型的当前状态会定期保存在磁盘上",{"2":{"1166":1}}],["模型的收敛速度会变慢",{"2":{"1157":1}}],["模型的维度是多少",{"2":{"700":1}}],["模型的训练目标是预测下一个",{"2":{"542":1}}],["模型的任务适配性更好",{"2":{"542":1}}],["模型的整体架构和传统transformer",{"2":{"501":1}}],["模型的总体输入是若干个单词组成的句子",{"2":{"453":1}}],["模型的效果往往和模型的参数量成正比",{"2":{"437":1}}],["模型的效果好坏就是看看模型是否可以把下一个token分类到真值对应的token",{"2":{"397":1}}],["模型的输入依然是词嵌入",{"2":{"745":1}}],["模型的输入是src和tgt",{"2":{"381":1}}],["模型的输出序列可能和输入序列的顺序不一致",{"2":{"245":1}}],["模型的鲁棒性更强",{"2":{"331":1}}],["模型的最终预测越来越确定",{"2":{"306":1}}],["模型的最初几层会把所查询的关系",{"2":{"122":1}}],["模型的架构如下图所示",{"2":{"125":1}}],["模型的泛化能力得到提升",{"2":{"21":1}}],["模型的核心模块",{"2":{"1":1}}],["就只移动饼干的指针",{"2":{"2152":1}}],["就行",{"2":{"2118":1}}],["就行了",{"2":{"908":1}}],["就一句话",{"2":{"2114":1}}],["就把价格说的高高的",{"2":{"2051":1}}],["就把机器翻译这么大的一个工程给包下来",{"2":{"909":1}}],["就可能覆盖相邻的内存区域",{"2":{"1648":1}}],["就可以进行操作",{"2":{"1099":1}}],["就可以实现多卡的超长",{"2":{"974":1}}],["就可以实现在输出每一个元素的时候",{"2":{"70":1}}],["就可以达到我们的目的",{"2":{"934":1}}],["就可以表达出任意长度的相对位置",{"2":{"759":1,"1339":1}}],["就可以给出每个词在相应维度上的数值",{"2":{"712":1}}],["就可以在不同的维度上定义远近",{"2":{"679":1}}],["就可以构造lcm",{"2":{"628":1}}],["就可以提高模型在多个下游任务的性能",{"2":{"561":1}}],["就可以得到对应的neural",{"2":{"494":1}}],["就可以将激活值转换为词汇表中每个词的logit",{"2":{"482":1}}],["就可以将values转换为输出词表的分布",{"2":{"128":1}}],["就可以满足需求",{"2":{"409":1}}],["就可以改变模型对于这个概念的理解和处理",{"2":{"137":1}}],["就可以了",{"2":{"79":1,"198":1}}],["就可以激活出来让下游任务可以学习到",{"2":{"12":1}}],["就不能再指向其他变量",{"2":{"1612":1}}],["就不需要从零开始训练一个神经网络了",{"2":{"1009":1}}],["就不需要再编码了",{"2":{"592":1}}],["就永远绑定到最初引用的变量",{"2":{"1612":1}}],["就叫做",{"2":{"1377":1}}],["就需要了解它们的实际能力和局限性",{"2":{"1318":1}}],["就需要由输出函数",{"2":{"270":1}}],["就应该将其应用于baseline模型",{"2":{"1183":1}}],["就应该可以重现六个月前的研究",{"2":{"1175":1}}],["就很难计算出适合内存的batch",{"2":{"1132":1}}],["就",{"2":{"1110":1,"1619":1}}],["就失去了正则化的效果",{"2":{"1019":1}}],["就称为",{"2":{"908":1}}],["就终止beam",{"2":{"904":1}}],["就意味着候选结果会一直增多吗",{"2":{"904":1}}],["就意味着到当前这一步为止",{"2":{"903":1}}],["就没问题吗",{"2":{"896":1}}],["就表示对h0和x1各做了一次变换",{"2":{"878":1}}],["就有多少列",{"2":{"700":1}}],["就有望弥补当前",{"2":{"138":1}}],["就能感悟并实践",{"2":{"2054":1}}],["就能够准确检索",{"2":{"696":1}}],["就能把原始动作块压缩成数量少但更密集的动作token",{"2":{"637":1}}],["就能获取中间层的token了",{"2":{"147":1}}],["就输出token",{"2":{"563":1}}],["就探讨了大型语言模型",{"2":{"561":1}}],["就知道预测的下一个词是什么了",{"2":{"473":1}}],["就被映射成token",{"2":{"456":1}}],["就会发生名词歧义",{"2":{"1660":1}}],["就会发生过度拟合",{"2":{"1149":1}}],["就会导致内存泄漏",{"2":{"1647":1}}],["就会导致网络优化陷入困境",{"2":{"993":1}}],["就会按",{"2":{"1236":1}}],["就会造成翻译精度的下降",{"2":{"891":1}}],["就会造成数据泄漏的问题",{"2":{"409":1}}],["就会揭示出你的语料库中隐藏的结构",{"2":{"709":1}}],["就会使得词表变的很大",{"2":{"576":1}}],["就会越来越少",{"2":{"191":1}}],["就指出稀疏性在模型训练中的两个主要优势",{"2":{"393":1}}],["就指代猫",{"2":{"261":1}}],["就进行一次参数更新",{"2":{"385":1}}],["就采用了基于",{"2":{"347":1}}],["就调用到self",{"2":{"344":1}}],["就得到了所有通道的均值和方差",{"2":{"315":1}}],["就明确地提出了上述质疑",{"2":{"314":1}}],["就算能做到把输入缩放到",{"2":{"314":1}}],["就容易被忽略",{"2":{"253":1}}],["就下图来说",{"2":{"239":1}}],["就发现不同层的注意力分布有显著差异",{"2":{"204":1}}],["就代表三类低频词",{"2":{"185":1}}],["就对郁达夫有一个全面的了解",{"2":{"169":1}}],["就对此进行了研究",{"2":{"41":1}}],["就必须要了解transformers",{"2":{"162":1}}],["就必须在某种意义上记住",{"2":{"118":1}}],["就说明是一个新patch的开始",{"2":{"613":1}}],["就说明transformer在实现编码器和解码器上是存在优势的",{"2":{"242":1}}],["就说明",{"2":{"145":1}}],["就像做饭时的菜谱",{"2":{"2097":1}}],["就像给一本书加上了章节标题",{"2":{"1729":1}}],["就像声明",{"2":{"1728":1}}],["就像操作内置类型一样",{"2":{"1712":1}}],["就像你给你的房子起了个外号",{"2":{"1650":1}}],["就像你把你的房子的钥匙给了别人",{"2":{"1650":1}}],["就像你复印了一份文件给别人",{"2":{"1650":1}}],["就像向操作系统申请一块用于建造多栋房屋的土地的使用权",{"2":{"1647":1}}],["就像向操作系统申请一块土地的使用权",{"2":{"1647":1}}],["就像归还用于建造多栋房屋的土地的使用权",{"2":{"1647":1}}],["就像归还向操作系统申请的单块土地的使用权",{"2":{"1647":1}}],["就像归一化层也包含这些参数一样",{"2":{"360":1}}],["就像在镜子迷宫中找到通往下一个反射点的路",{"2":{"1646":1}}],["就像镜子迷宫的出口",{"2":{"1646":1}}],["就像使用原变量一样",{"2":{"1612":1}}],["就像拿到一个没有写任何地址的纸条",{"2":{"1611":1}}],["就像拿着地址纸条找到对应的房子",{"2":{"1611":1}}],["就像获取一个人的家庭住址",{"2":{"1611":1}}],["就像我们阅读文章一样",{"2":{"1604":1}}],["就像我们对编码器的输入所做的那样",{"2":{"515":1}}],["就像要去破译密码",{"2":{"1372":1}}],["就像卷积网络是专门用于处理网格化数据",{"2":{"850":1}}],["就像单向rnn和双向rnn的区别",{"2":{"721":1}}],["就像是相当使用独热编码来查表",{"2":{"694":1}}],["就像这些词从",{"2":{"334":1}}],["就像一个巨大的",{"2":{"1648":1}}],["就像一个后进先出的箱子",{"2":{"1648":1}}],["就像一个只会背诵但不懂意思的鹦鹉",{"2":{"627":1}}],["就像一个开关",{"2":{"103":1}}],["就像一栋高楼",{"2":{"309":1}}],["就像人类大脑通过互连的神经通路存储知识和处理信息一样",{"2":{"221":1}}],["就像人们第一次接触新信息一样",{"2":{"141":1}}],["就是迎接这些问题",{"2":{"2056":1}}],["就是专业",{"2":{"2051":1}}],["就是通过值传递给",{"2":{"1729":1}}],["就是通过数据训练来寻求作用力和运动方程从而达到构造满足要求的运动轨迹的过程",{"2":{"509":1}}],["就是m",{"2":{"1461":1}}],["就是经过样本训练后",{"2":{"1371":1}}],["就是由于换了clip模型",{"2":{"1363":1}}],["就是由google在2013年提出的word2vec",{"2":{"712":1}}],["就是比较邻近的位置",{"2":{"1340":1}}],["就是比encoderlayer多了一个src",{"2":{"533":1}}],["就是上一层传下来的梯度值",{"2":{"1099":1}}],["就是显而易见的",{"2":{"974":1}}],["就是基于这种切分的方法",{"2":{"974":1}}],["就是给在较短的序列后面填充",{"2":{"933":1}}],["就是每个头",{"2":{"926":1}}],["就是每个句子毕竟差距还是比较大的",{"2":{"323":1}}],["就是求y这个句子的概率",{"2":{"908":1}}],["就是x的最佳翻译了",{"2":{"908":1}}],["就是训练信号只来源于被mask掉的token",{"2":{"727":1}}],["就是直接使用token序号为2来进行查找",{"2":{"700":1}}],["就是从字符的整数索引为基础",{"2":{"694":1}}],["就是一个点一个点把坐标绘制出来",{"2":{"2016":1}}],["就是一个指向字符指针数组的指针",{"2":{"1650":1}}],["就是一个矩阵方程而已",{"2":{"700":1}}],["就是一个",{"2":{"694":1}}],["就是一个平移加缩放的线性操作",{"2":{"314":1,"322":1}}],["就是非常好的具备语义性的稠密向量示例",{"2":{"687":1}}],["就是玩具的组装说明书",{"2":{"524":1}}],["就是encoder模块里的一个独立的encoderlayer",{"2":{"519":1}}],["就是不合理的",{"2":{"512":1}}],["就是",{"2":{"505":1,"525":1,"722":1,"1344":1}}],["就是让概率密度函数朝着",{"2":{"1377":1}}],["就是让注意力机制在序列的本身中寻找关系",{"2":{"442":1}}],["就是让query",{"2":{"70":1}}],["就是传入了目标序列",{"2":{"410":1}}],["就是传入的参数",{"2":{"344":1}}],["就是加了一个系数a",{"2":{"347":1}}],["就是加权求和",{"2":{"169":1}}],["就是指",{"2":{"334":1}}],["就是对这一个batch中的每一个对应维度做归一化",{"2":{"323":1}}],["就是针对具体问题来选择在对应维度上进行处理",{"2":{"321":1}}],["就是把true",{"2":{"399":1}}],["就是把",{"2":{"329":1}}],["就是把第1个样本的第1个通道",{"2":{"315":1}}],["就是把输入数据x",{"2":{"310":1}}],["就是把哈佛代码中的掩码给细化了",{"2":{"84":1}}],["就是输入和输出相同",{"2":{"299":1}}],["就是输入矩阵",{"2":{"168":1}}],["就是使用",{"2":{"901":1}}],["就是使用cnn",{"2":{"242":1}}],["就是使用了2048个",{"2":{"116":1}}],["就是当前词只与距离它比较近的个词更加相关",{"2":{"242":1}}],["就是线性注意力的体现",{"2":{"210":1}}],["就是score",{"2":{"200":1}}],["就是论文公式提到的",{"2":{"198":1}}],["就是如果在已有的token基础上",{"2":{"194":1}}],["就是用来表达𝑖",{"2":{"176":1}}],["就是李宁商品本身",{"2":{"164":1}}],["就是在预训练好的模型上加些针对任务的层",{"2":{"718":1}}],["就是在原有的知识库上进一步拓展训练",{"2":{"623":1}}],["就是在页面上返回的商品描述",{"2":{"164":1}}],["就是在输入小于0时",{"2":{"104":1}}],["就是你在搜索栏输入的查询内容",{"2":{"164":1}}],["就是要打破这个局面",{"2":{"627":1}}],["就是要查找的位置",{"2":{"164":1}}],["就是要结合填充词对应的掩码和未来词汇相关的掩码",{"2":{"74":1,"382":1}}],["就是模型对于某个概念的理解和表示",{"2":{"137":1}}],["就是权重发生了变化",{"2":{"123":1}}],["就是神经元之间相互链接的关系改变了",{"2":{"123":1}}],["就是transformer论文的实现",{"2":{"98":1}}],["就是产生最后的",{"2":{"83":1}}],["就是源语言的padding",{"2":{"72":1}}],["就是多个样本并行计算",{"2":{"36":1}}],["就用",{"2":{"67":1}}],["就越能带来平均效果上的收益",{"2":{"20":1}}],["目下的",{"2":{"683":1}}],["目的进程编号",{"2":{"1590":1}}],["目的",{"2":{"676":1}}],["目的是使用sae来增加神经元的可解释性",{"2":{"475":1}}],["目的是要解决序列长度过长导致内存使用量过大的问题",{"2":{"420":1}}],["目的是将过去的数据存储为键",{"2":{"230":1}}],["目标进程id",{"2":{"1573":1}}],["目标",{"0":{"2148":1},"2":{"1315":1,"1657":1,"1917":1,"1997":1}}],["目标超参数的每个设置的",{"2":{"1149":1}}],["目标超参数是",{"2":{"1143":1}}],["目标超参数是指",{"2":{"1143":1}}],["目标超参数",{"2":{"1142":1}}],["目标可以包括",{"2":{"1141":1}}],["目标token间的注意力和掩码注意力",{"2":{"541":1}}],["目标数据的嵌入表示",{"2":{"532":1,"533":1}}],["目标输入不同",{"2":{"426":1}}],["目标真值标签",{"2":{"399":1}}],["目标句子列表",{"2":{"451":1}}],["目标句子2",{"2":{"384":1,"558":1}}],["目标句子1",{"2":{"384":1,"558":1}}],["目标句余下几个时间步的部分",{"2":{"78":1}}],["目标语句掩码对应的变量叫做tgt",{"2":{"382":1}}],["目标语句需要padding",{"2":{"382":1}}],["目标语句",{"0":{"381":1}}],["目标语言文本嵌入层",{"2":{"454":1}}],["目标语言output",{"2":{"450":1}}],["目标语言的word",{"2":{"450":1}}],["目标语言的自注意力",{"2":{"82":1}}],["目标语言词表中单词数目",{"2":{"448":1}}],["目标语言词典",{"2":{"423":1}}],["目标语言词典大小",{"2":{"399":1}}],["目标语言序列",{"2":{"427":2}}],["目标语言分词器",{"2":{"423":1}}],["目标语言句子的掩码",{"2":{"380":1}}],["目标语言句子真值列表",{"2":{"380":1}}],["目标语言句子列表",{"2":{"380":1}}],["目标词表",{"2":{"375":2}}],["目标序列的掩码",{"2":{"532":1,"533":1}}],["目标序列的每个元素把自己的特征总结到一个向量query之中",{"2":{"265":1}}],["目标序列和目标序列掩码",{"2":{"450":1}}],["目标序列掩码",{"2":{"450":2}}],["目标序列首先加一个句首标记",{"2":{"427":1}}],["目标序列也被转换为真值标签传递给优化器",{"2":{"391":1}}],["目标序列会输入给解码器",{"2":{"391":1}}],["目标序列",{"2":{"391":1,"450":2}}],["目标序列中每个元素会用自己的query去和目标序列每个元素的key计算得到对齐系数",{"2":{"265":1}}],["目标序列每个元素提出了query",{"2":{"265":1}}],["目标序列所有元素的key构成了键矩阵k",{"2":{"265":1}}],["目标序列所有元素的query构成了查询矩阵q",{"2":{"265":1}}],["目标是",{"2":{"235":1}}],["目标是在保证效果不变坏的前提下",{"2":{"162":1}}],["目标隐状态之中每个token与",{"2":{"200":1}}],["目标隐状态作为",{"2":{"200":1}}],["目标物品",{"2":{"163":1}}],["目标实体的可能性开始渐渐上升",{"2":{"130":1}}],["目标实体的排名和概率",{"2":{"130":1}}],["目前是大模型相对位置编码中应用最广的方式之一",{"2":{"1341":1}}],["目前还没有任何能够令人信服的证据表明batch",{"2":{"1186":1}}],["目前还没有任何算法可以在没有人工指导的情况下有效地搜索这个空间",{"2":{"1139":1}}],["目前尚不清楚如何构建一组严格的实验来自信地回答最佳的lr",{"2":{"1171":1}}],["目前batch",{"2":{"1168":1}}],["目前很少有人记录下那些深度学习中获得良好结果的实际方法",{"2":{"1127":1}}],["目前很多llm采用了",{"2":{"346":1}}],["目前很多框架已经可以精确计算",{"2":{"106":1}}],["目前已经有很多更好的激活函数",{"2":{"1460":1}}],["目前已经生成了",{"2":{"378":1}}],["目前已被",{"2":{"1083":1}}],["目前并没有明确的胜出者",{"2":{"874":1}}],["目前的想法是",{"2":{"846":1}}],["目前的工作中",{"2":{"123":1}}],["目前这些位置编码会让模型过分在意局部信息",{"2":{"746":1}}],["目前有两者针对检索模型的预训练策略",{"2":{"727":1}}],["目前有两种处理连续数据生成的主要方法",{"2":{"636":1}}],["目前主流的语言模型的预训练任务都是token级别的",{"2":{"727":1}}],["目前预训练有两种方法",{"2":{"718":1}}],["目前llm所作的是根据之前的单词来预测下一个单词",{"2":{"689":1}}],["目前llm主要是基于词表的分词方式",{"2":{"549":1}}],["目前为止",{"2":{"682":1}}],["目前解码器已经输出了",{"2":{"537":1}}],["目前也有很多方法来解决这个问题",{"2":{"511":1}}],["目前也有不同的解释",{"2":{"437":1}}],["目前较为常用的神经网络",{"2":{"494":1}}],["目前大多数llm使用左填充",{"2":{"378":1}}],["目前我们知道了上下文的重要性",{"2":{"260":1}}],["目前微调过程会产生灾难性遗忘和泛化性低的问题",{"2":{"222":1}}],["目前",{"2":{"142":1,"241":1,"484":1,"1047":1,"1127":1,"1315":1,"1603":1}}],["目前每个样本实际包括了整个句子",{"2":{"58":1}}],["目前在逻辑上已经把每个query",{"2":{"33":1}}],["目前可以看到的趋势是",{"2":{"20":1}}],["目录操作",{"2":{"1930":1}}],["目录",{"0":{"1125":1,"1768":1,"1904":1,"1920":1},"2":{"0":1,"49":1,"96":1,"157":1,"293":1,"741":1,"1506":1,"1999":4}}],["已不使用",{"2":{"2050":1}}],["已删",{"2":{"2043":1}}],["已知条件",{"2":{"2018":1}}],["已在siggraph",{"2":{"2011":1}}],["已到达输入末尾",{"2":{"1814":1,"1832":1}}],["已选择并实现适当的评估指标",{"2":{"1128":1}}],["已被证明是一种有效且实用的深度神经网络优化算法",{"2":{"1047":1}}],["已成为编码位置信息的主要方法",{"2":{"756":1}}],["已得到广泛支持和证明",{"2":{"756":1}}],["已得到作者许可",{"2":{"48":1}}],["已有能运行且得到不错结果的训练工作流",{"2":{"1138":1}}],["已有的位置编码主要刻画两个token之间的相对距离",{"2":{"761":1}}],["已有的token依旧能同样地聚焦到原来的token上",{"2":{"194":1}}],["已有论文证明头数目不是越多越好",{"2":{"20":1}}],["已经很不错了",{"2":{"2054":1}}],["已经为空",{"2":{"1911":1}}],["已经天然的具备了",{"2":{"1478":1}}],["已经将学习率针对该步数进行了调整",{"2":{"1137":1}}],["已经有一个工作流设置用来进行训练和评估",{"2":{"1128":1}}],["已经有很多尝试将多个逐元素操作融合在一起",{"2":{"941":1,"960":1}}],["已经完成了梯度清0",{"2":{"1098":1}}],["已经被很多目前最为领先的模型所采用",{"2":{"844":1}}],["已经持有了输入序列",{"2":{"536":1}}],["已经翻译好的部分",{"2":{"535":1}}],["已经集成了单词",{"2":{"122":1}}],["已经知道了序列的所有信息",{"2":{"78":1}}],["已经获得了全量信息",{"2":{"72":1}}],["已经能够关注位置信息",{"2":{"20":1}}],["研究领域的模型",{"2":{"1313":1}}],["研究中的试验次数会对结果产生重大影响",{"2":{"1177":1}}],["研究在",{"2":{"1150":1}}],["研究的目的是使用目标超参数的不同值运行训练流程",{"2":{"1144":1}}],["研究的是当某一系统或者函数趋于无穷大或某一特定值时",{"2":{"507":1}}],["研究",{"2":{"1144":1,"1569":1}}],["研究了rw和hs嵌入的不同之处",{"2":{"739":1}}],["研究了实体和关系之间的差异",{"2":{"136":1}}],["研究难点在于",{"2":{"738":1}}],["研究团队得出结论",{"2":{"692":1}}],["研究团队使用",{"2":{"621":1}}],["研究团队引入了",{"2":{"616":1}}],["研究团队还探索了多种捷径",{"2":{"302":1}}],["研究都循着",{"2":{"512":1}}],["研究还展示了构造的",{"2":{"504":1}}],["研究表明",{"2":{"504":1}}],["研究一种略有不同类型的机器学习模型",{"2":{"488":1}}],["研究界普遍认为网络架构需要更多层",{"2":{"296":1}}],["研究社区提出了稀疏注意力",{"2":{"210":1}}],["研究动机",{"0":{"222":1},"2":{"157":1}}],["研究者证明",{"2":{"504":1}}],["研究者能够更深入地理解",{"2":{"485":1}}],["研究者能够更全面地理解信息在模型中的流动",{"2":{"484":1}}],["研究者也指出",{"2":{"485":1}}],["研究者利用这种双重性",{"2":{"485":1}}],["研究者专注于mlp层",{"2":{"485":1}}],["研究者往往使用大模型对数据进行分类",{"2":{"368":1}}],["研究者希望能够明确地理解模型如何在多层次上进行信息存储和记忆",{"2":{"148":1,"484":1}}],["研究者们对relu",{"2":{"842":1}}],["研究者们正在寻找各种方法来进行优化",{"2":{"512":1}}],["研究者们假设特定类型的神经网络可以看作是离散的微分方程",{"2":{"492":1}}],["研究者们一直在为神经网络寻找生物学上的解释",{"2":{"486":1}}],["研究者们提出了一种新的优化策略",{"2":{"144":1}}],["研究者们进一步提出了一个新的假设",{"2":{"144":1}}],["研究者们观察到mhsa部分包含的知识比ffn部分有更多的变化和动态性",{"2":{"144":1}}],["研究者认为通过改变关系知识来修改实体知识在理论上是可能的",{"2":{"136":1}}],["研究者提出了以下研究问题",{"2":{"136":1}}],["研究人员和从业者致力于开发和改进相应的算法和技术",{"2":{"906":1}}],["研究人员和工程师通常会尝试两种方法",{"2":{"874":1}}],["研究人员在所有编码器模块中都使用了",{"2":{"844":1}}],["研究人员在这方面发挥了极大的聪明才智",{"2":{"746":1}}],["研究人员通过修改自注意力计算的过程",{"2":{"757":1}}],["研究人员通常需要混合使用不同的数据源来进行llm预训练",{"2":{"367":1}}],["研究人员通常会对隐藏层的大小做一个缩放",{"2":{"109":1}}],["研究人员自然想到",{"2":{"757":1}}],["研究人员开始探索使用统计方法学习文本嵌入的方法",{"2":{"711":1}}],["研究人员主要依靠手动设计的特征来表示文本",{"2":{"711":1}}],["研究人员就提出让词嵌入成为模型中的参数",{"2":{"708":1}}],["研究人员给出了各种猜测",{"2":{"560":1}}],["研究人员用数学方法证明",{"2":{"542":1}}],["研究人员也引入仅编码器和仅解码器的架构",{"2":{"540":1}}],["研究人员把transformer论文中的实现叫做post",{"2":{"523":1}}],["研究人员做了深入的研究",{"2":{"437":1}}],["研究人员修改了注意力算子的实现",{"2":{"377":1}}],["研究人员提出了flashattention",{"2":{"973":1}}],["研究人员提出了teacher",{"2":{"406":1}}],["研究人员提出了大量的修改方案",{"2":{"351":1}}],["研究人员提出了动态决定注意力头怎样组合",{"2":{"46":1}}],["研究人员决定使用一个定长的状态机来作为输入和输出之间的桥梁",{"2":{"241":1}}],["研究人员一直没有停止优化的脚步",{"2":{"111":1}}],["研究人员找到的一个方法就是在训练时使用掩码矩阵",{"2":{"62":1}}],["研究人员注意到",{"2":{"45":1}}],["研究人员对多头注意力做了深入的分析",{"2":{"20":1}}],["研究背景和动机",{"0":{"227":1},"2":{"157":1}}],["研究背景",{"0":{"2":1,"44":1,"212":1,"219":1,"351":1,"738":1},"1":{"3":1,"4":1,"5":1,"220":1,"221":1},"2":{"0":2,"157":2,"293":1}}],["进去第二家",{"2":{"2051":1}}],["进去说明了情况",{"2":{"2051":1}}],["进程可以在不同的通信域内工作",{"2":{"1577":1}}],["进程的id",{"2":{"1573":1}}],["进程和通信",{"0":{"1573":1}}],["进程之间的通信通过消息传递实现",{"2":{"1572":1}}],["进程",{"2":{"1563":1,"1589":1}}],["进程管理",{"0":{"1521":1},"1":{"1522":1,"1523":1,"1524":1}}],["进程互斥",{"2":{"1425":1}}],["进程我们前面已经提及过了",{"2":{"1408":1}}],["进程同步指多个进程在特定点会合",{"2":{"1408":1}}],["进程同步我们下面会详细讲述",{"2":{"1408":1}}],["进程同步是为了协调多个进程或线程对共享资源的访问",{"2":{"1407":1}}],["进程同步",{"0":{"1405":1},"1":{"1406":1,"1407":1,"1408":1,"1409":1,"1410":1,"1411":1,"1412":1,"1413":1,"1414":1,"1415":1,"1416":1,"1417":1,"1418":1,"1419":1,"1420":1,"1421":1,"1422":1,"1423":1,"1424":1,"1425":1}}],["进展",{"2":{"692":1}}],["进入vmware",{"2":{"2094":1}}],["进入doccano容器",{"0":{"2067":1}}],["进入下一次循环",{"2":{"1631":1}}],["进入块选择模式",{"2":{"1551":1}}],["进入可视模式",{"2":{"1551":1}}],["进入列选择",{"2":{"1541":1}}],["进入行选择模式",{"2":{"1551":1}}],["进入行选择",{"2":{"1541":1}}],["进入字符选择模式",{"2":{"1551":1}}],["进入字符选择",{"2":{"1541":1}}],["进入插入模式",{"2":{"1541":1}}],["进入",{"2":{"1519":1,"1999":1}}],["进入退出",{"0":{"1518":1}}],["进入指定目录",{"2":{"1509":1}}],["进入和退出目录",{"2":{"1509":1}}],["进入到输出层之后",{"2":{"530":1}}],["进入2023年",{"2":{"103":1}}],["进化",{"2":{"220":1}}],["进一步提升了其灵活性",{"2":{"1906":1}}],["进一步",{"2":{"1441":1}}],["进一步地",{"2":{"1342":1}}],["进一步放大",{"2":{"1316":1}}],["进一步影响损失及回传的梯度",{"2":{"990":1}}],["进一步表明神经元工作的稀疏性",{"2":{"841":1}}],["进一步的分析显示",{"2":{"739":1}}],["进一步训练",{"2":{"734":1}}],["进一步发现使用充足数据时",{"2":{"732":1}}],["进一步精进下游任务的效果",{"2":{"726":1}}],["进一步优化了目标词的表示",{"2":{"525":1}}],["进一步丰富和优化生成序列的表示",{"2":{"525":1}}],["进一步说",{"2":{"318":1}}],["进一步处理注意力分数",{"2":{"264":1}}],["进一步为了使得中间层是256的整数倍",{"2":{"109":1}}],["进一步压缩最终输出的整合矩阵的维度",{"2":{"19":1}}],["进阶主题",{"0":{"1500":1}}],["进阶",{"0":{"86":1,"1362":1,"1593":1},"1":{"87":1,"88":1,"89":1,"90":1,"91":1,"92":1,"93":1,"94":1,"1363":1,"1364":1,"1594":1},"2":{"49":1}}],["进而把它们转换成相应的权重",{"2":{"1009":1}}],["进而学习到最适合特定任务的位置编码",{"2":{"749":1}}],["进而获取单词对应的向量",{"2":{"713":1}}],["进而导致在训练过程中出现梯度消失或梯度爆炸",{"2":{"701":1}}],["进而导致模型在某些较少见的token上的训练不足",{"2":{"562":1}}],["进而承载更多信息",{"2":{"682":1}}],["进而和原有参数一起组合成新的",{"2":{"623":1}}],["进而更新之前",{"2":{"516":1}}],["进而可以将transformer想象成可微自旋系统的集合",{"2":{"508":1}}],["进而给出答案",{"2":{"505":1}}],["进而建立起与隐藏层强相关的微分方程",{"2":{"496":1}}],["进而影响最后的推理效果",{"2":{"480":1}}],["进而为",{"2":{"474":1}}],["进而解决深度学习神经网络在用小数据集训练时常见的两大问题",{"2":{"393":1}}],["进而需要对单个句子进行归一化操作",{"2":{"318":1}}],["进而得到注意力权重a",{"2":{"264":1}}],["进而得到记忆网络的整体是对每个键值对的加权求和",{"2":{"125":1}}],["进而在encoderdecoder类的forward",{"2":{"703":1}}],["进而在上层完成长距离信息依赖计算",{"2":{"247":1}}],["进而在辅助记忆上进行编辑",{"2":{"143":1}}],["进而会把点积结果推向softmax函数的梯度平缓区",{"2":{"187":1}}],["进而预测下一个token",{"2":{"45":1}}],["进而从多角度捕捉上下文和微妙之处",{"2":{"5":1}}],["进行创建账户",{"2":{"2068":1}}],["进行中",{"2":{"2043":2}}],["进行越界检查",{"2":{"1713":1}}],["进行安全的向下转型",{"2":{"1683":1}}],["进行释放",{"2":{"1669":1}}],["进行缩进",{"2":{"1559":1}}],["进行缩放",{"2":{"187":1,"970":1,"1241":1}}],["进行传递",{"2":{"1459":1}}],["进行比较",{"2":{"1443":1}}],["进行衰减",{"2":{"1235":1,"1239":1}}],["进行梯度截断会发生什么",{"2":{"1184":1}}],["进行一次学习率扫描",{"2":{"1179":1}}],["进行调整",{"2":{"1174":2}}],["进行评估",{"2":{"1162":1}}],["进行短时间的训练来找到较佳的模型和优化器超参数",{"2":{"1157":1}}],["进行设置",{"2":{"1117":1}}],["进行左移位运算",{"2":{"1087":1}}],["进行格式化时调用",{"2":{"1083":1}}],["进行操作",{"2":{"1083":4,"1214":1}}],["进行更新",{"2":{"988":1}}],["进行更细致的处理",{"2":{"554":1}}],["进行初始化",{"2":{"1897":1}}],["进行初始化赋值的过程",{"2":{"988":1}}],["进行初步的文本分割",{"2":{"553":1}}],["进行迭代",{"2":{"975":1}}],["进行指数缩放",{"2":{"944":1}}],["进行分解",{"2":{"943":2,"961":2}}],["进行联合训练",{"2":{"689":1}}],["进行建模",{"2":{"627":1}}],["进行大规模模型训练",{"2":{"623":1}}],["进行交互",{"2":{"621":1,"623":1}}],["进行token化",{"2":{"613":1}}],["进行编码",{"2":{"587":1,"727":1}}],["进行了补充",{"2":{"977":1}}],["进行了简化",{"2":{"812":1}}],["进行了检测",{"2":{"562":1}}],["进行了量身定制",{"2":{"209":1}}],["进行softmax操作",{"2":{"537":1}}],["进行掩码操作",{"2":{"537":1}}],["进行预测",{"2":{"537":1}}],["进行处理",{"2":{"532":1,"1214":1}}],["进行注意力计算",{"2":{"525":1}}],["进行组装",{"2":{"524":1}}],["进行解码",{"2":{"450":1}}],["进行再次解码",{"2":{"445":1}}],["进行优化步骤",{"0":{"1223":1}}],["进行优化",{"2":{"420":1}}],["进行优化来扩展其功能空间",{"2":{"144":1}}],["进行有计划的学习",{"2":{"411":1}}],["进行训练的warmup",{"2":{"1183":1}}],["进行训练使得模型能适配双向注意力机制",{"2":{"734":1}}],["进行训练",{"2":{"382":1,"406":1,"908":1}}],["进行正则化",{"2":{"326":1}}],["进行多头自注意力计算",{"2":{"289":1}}],["进行对齐",{"2":{"277":1}}],["进行对性能和存储的极致压缩",{"2":{"162":1}}],["进行加权求和",{"2":{"267":1}}],["进行计算",{"2":{"249":2,"776":1,"976":1}}],["进行局部信息计算",{"2":{"247":1}}],["进行推理来解决具体问题",{"2":{"225":1}}],["进行专家间的组合推理",{"2":{"225":1}}],["进行线性计算",{"2":{"201":1}}],["进行转换",{"2":{"161":1,"1159":1}}],["进行查找",{"2":{"154":1}}],["进行排序",{"2":{"135":1,"2152":1}}],["进行相乘",{"2":{"128":1}}],["进行相同的计算",{"2":{"34":1}}],["进行筛选",{"2":{"125":1}}],["进行归一化",{"2":{"125":1,"321":1,"343":1,"354":1,"356":1}}],["进行",{"2":{"89":1,"664":1,"938":1,"954":1}}],["进行第一步处理环节的逆操作",{"2":{"36":1}}],["进行dropout操作时置0比率",{"2":{"23":1}}],["让生活变得更加简单有序",{"2":{"2108":1}}],["让我感受到了不同的商家之间到底有多大明显的差距",{"2":{"2051":1}}],["让我们简单步入",{"0":{"2121":1},"1":{"2122":1,"2123":1,"2124":1,"2125":1,"2126":1,"2127":1,"2128":1,"2129":1,"2130":1,"2131":1,"2132":1,"2133":1,"2134":1,"2135":1,"2136":1,"2137":1,"2138":1,"2139":1,"2140":1}}],["让我们的生活更加有趣",{"2":{"2108":1}}],["让我们继续从上面讲解的部分接下来进行总结和实际应用场景的探讨",{"2":{"1864":1}}],["让我们继续使用自定义立方体类的状况",{"2":{"1789":1}}],["让我们仔细区分",{"2":{"1614":1}}],["让我们一起编写并运行你的第一个",{"2":{"1606":1}}],["让我们计算两对向量之间的点积",{"2":{"692":1}}],["让我们看看多头自注意力机制如何计算",{"2":{"462":1}}],["让我们看看每个步骤是如何完成的",{"2":{"455":1}}],["让我们通过一个真实例子来理解加权求和操作",{"2":{"169":1}}],["让我们回到具体任务上来分析",{"2":{"165":1}}],["让你更懂得如何从底层实现酷炫的视觉效果",{"2":{"2010":1}}],["让不同的子类类型可以以一种统一的方式进行处理",{"2":{"1848":1}}],["让读者更容易把握整体结构",{"2":{"1729":1}}],["让编译器根据初始化列表推断",{"2":{"1634":1}}],["让编码器对源序列进行编码",{"2":{"450":1}}],["让程序做出判断",{"0":{"1619":1}}],["让程序也能实现摄氏温度到华氏温度的转换",{"2":{"1608":1}}],["让q接近真实的后验",{"2":{"1377":1}}],["让module",{"2":{"1214":1}}],["让机器学习模型泛化得更好的最好办法是使用更多的数据进行训练",{"2":{"1015":1}}],["让所有的头之间",{"2":{"935":1,"951":1}}],["让老师来指导一下",{"2":{"896":1}}],["让老鸟也可以通过阅读本系列来了解一些新观点",{"2":{"235":1}}],["让结构简单的网络甚至超过sota性能",{"2":{"843":1}}],["让研究人员绞尽脑汁的transformer位置编码",{"2":{"768":1}}],["让bert可以处理超长文本",{"2":{"768":1}}],["让自注意力分布更加关注相对距离较小",{"2":{"765":2}}],["让自注意力机制在处理每个位置时",{"2":{"274":1}}],["让这些向量可以互相交流",{"2":{"709":1}}],["让这些初始子词将逐渐被合并成更长",{"2":{"580":1}}],["让神经网络自己学习特征的表示",{"2":{"708":1}}],["让神经网络层数的瓶颈得以突破",{"2":{"286":1}}],["让h和e解耦有利于减少参数量",{"2":{"698":1}}],["让数据连续",{"2":{"658":1}}],["让数据能够尽量恢复本身表达能力",{"2":{"309":1}}],["让",{"2":{"623":1}}],["让x逐次在每个解码器层流通",{"2":{"532":1}}],["让transformer避免简单并行推理",{"2":{"480":1}}],["让更多的参数来承载文本中深层次的信息",{"2":{"437":1}}],["让解码器来预测下一个token",{"2":{"428":1}}],["让梯度可以更直接地回传",{"2":{"332":1}}],["让它们不向后面的层传递信号",{"2":{"1017":1}}],["让它们可以根据输入动态组合",{"2":{"43":1}}],["让它随着训练过程更新",{"2":{"749":1,"1337":1}}],["让它不至于跑太远",{"2":{"326":1}}],["让注意力机制完成多项工作",{"2":{"289":1}}],["让注意力机制专注于当前上下文",{"2":{"226":1}}],["让两个光标分别移动在源序列和目标序列中",{"2":{"284":1}}],["让序列中两个词之间的关系距离更近",{"2":{"256":1,"272":1}}],["让网络能记住之前输入的信息",{"2":{"248":1}}],["让卷积网络依靠深度来弥补全局信息的缺失",{"2":{"247":1}}],["让新预测的单词和之前单词连成整个句子后最合理",{"2":{"238":1}}],["让新同学可以入门transformer",{"2":{"235":1}}],["让新知识和模型中现有知识之间形成联系",{"2":{"141":1}}],["让llm对输入prompt进行分类",{"2":{"225":1}}],["让模型通过数据自己学习位置信息来分辨不同位置的token",{"2":{"745":1}}],["让模型自己设计",{"0":{"708":1}}],["让模型只能看到目标序列的一部分",{"2":{"409":1}}],["让模型判断更精准",{"2":{"262":1}}],["让模型能够动态地融合时间上下文和序列的历史信息",{"2":{"248":1,"249":1}}],["让模型按照新的流程来执行",{"2":{"231":1}}],["让模型补全",{"2":{"130":1}}],["让模型可以捕捉到更加丰富的特征和信息",{"2":{"21":1}}],["让模型可能从多种维度去理解输入句子的含义",{"2":{"12":1}}],["让下一层能够继续计算",{"2":{"116":1}}],["让每个元素消化整合自己的信息",{"2":{"101":1,"466":1}}],["让每个注意力头之间都互相独立",{"2":{"19":1}}],["让人难以理解",{"2":{"77":1}}],["让小模型能打两倍大的模型",{"2":{"47":1}}],["让目标序列之中",{"2":{"39":1,"200":1}}],["只允许向前遍历",{"2":{"1721":1}}],["只在能够提高代码可读性和直观性的情况下使用",{"2":{"1712":1}}],["只在最后一个维度",{"2":{"341":1}}],["只拷贝指针成员变量的值",{"2":{"1694":1}}],["只执行一次",{"2":{"1621":1}}],["只进行了两次",{"2":{"1364":1}}],["只用了",{"2":{"1364":1}}],["只用很少的标注数据就达到了最佳性能",{"2":{"1312":1}}],["只依赖于位置编号k",{"2":{"1335":1}}],["只依靠单次的前向传播就能在",{"2":{"148":1}}],["只与当前位置的输入",{"2":{"1322":1}}],["只与一部分",{"2":{"204":1}}],["只差了一个转置",{"2":{"1004":1}}],["只存储一份",{"2":{"974":1}}],["只选择其中最大的k个保留",{"2":{"902":1}}],["只具有整体意义和相对意义",{"2":{"707":1}}],["只让模型学习其它的参数",{"2":{"706":1}}],["只含一个",{"2":{"681":1}}],["只学习pytorch的",{"2":{"669":1}}],["只见树木",{"2":{"626":1}}],["只不过我们是用大脑而不是计算机",{"2":{"2100":1}}],["只不过",{"2":{"1344":1}}],["只不过是",{"2":{"714":1}}],["只不过embedding嵌入模型经过矩阵算法的优化",{"2":{"711":1}}],["只不过使用的query",{"2":{"533":1}}],["只不过不同句子的特征长度不一定相同",{"2":{"316":1}}],["只使用了中文句子",{"2":{"529":1}}],["只需重新编译修改过的文件",{"2":{"1916":1}}],["只需保留第一轮的基础",{"2":{"1159":1}}],["只需将",{"2":{"1117":1}}],["只需在last循环缩放一次",{"2":{"969":1}}],["只需通过调整词汇表和嵌入矩阵即可",{"2":{"696":1}}],["只需通过精心设计的prompt",{"2":{"504":1}}],["只需要知道如何通知它完成",{"2":{"1645":1}}],["只需要有限个位置编码",{"2":{"1339":1}}],["只需要在将它们保存在检查点之前进行同步",{"2":{"1168":1}}],["只需要在最后的一层后面进行一次",{"2":{"184":1}}],["只需要更新context",{"2":{"898":1}}],["只需要最后一层的输出o5",{"2":{"860":1}}],["只需要o",{"2":{"684":1}}],["只需要固定长度的句子嵌入空间的编码器和解码器",{"2":{"628":1}}],["只需要让其关注自身及i",{"2":{"409":1}}],["只需要一个标量输入即可完成门控操作",{"2":{"108":1}}],["只需要考虑padding",{"2":{"84":1}}],["只需要把输入目标序列的下一元素作为输出就可以了",{"2":{"58":1}}],["只需要大概1",{"2":{"19":1}}],["只展示前三步中单个输出的损失计算",{"2":{"399":1}}],["只剩余一半的神经元是可以激活的",{"2":{"396":1}}],["只考虑自己",{"2":{"322":1}}],["只跟踪每个词对应的",{"2":{"161":1}}],["只保留一个实例",{"2":{"1806":1}}],["只保留对分词和模型训练有意义的内容",{"2":{"552":1}}],["只保留了缩放",{"2":{"346":1,"812":1}}],["只保留了q与前i个时间步的k的关系得分",{"2":{"71":1}}],["只保留达到这个阈值的神经元",{"2":{"135":1}}],["只要今天比昨天好",{"2":{"2056":1}}],["只要该容器支持迭代器",{"2":{"1914":1}}],["只要它们的参数列表不同",{"2":{"1707":1}}],["只要它保持相同的均匀性",{"2":{"1175":1}}],["只要我们把我们需要保存的信息",{"2":{"1275":1}}],["只要我们愿意",{"2":{"1154":1}}],["只要调整好所有超参数",{"2":{"1131":1}}],["只要有一个条件为真",{"2":{"1619":1}}],["只要有一个requires",{"2":{"1106":1}}],["只要有padding的地方",{"2":{"78":1}}],["只要每个块的统计数据正确组合以进行重新缩放",{"2":{"975":1}}],["只要奋力跳跃两次就可以捕到",{"2":{"713":1}}],["只要维度的数量足够多",{"2":{"712":1}}],["只要提示设计得足够巧妙",{"2":{"504":1}}],["只要给它足够的时间和资源",{"2":{"504":1}}],["只要给定足够的容量",{"2":{"296":1}}],["只要保证batch之间的图像是独立的",{"2":{"337":1}}],["只要梯度小",{"2":{"333":1}}],["只要序列中存在一个令牌",{"2":{"93":1}}],["只有在确实需要访问私有成员",{"2":{"1793":1}}],["只有在尝试编写这本",{"2":{"1127":1}}],["只有两个可能的值",{"2":{"1607":1}}],["只有使用这个技术的人有高低之别",{"2":{"1479":1}}],["只有满足了这些条件",{"2":{"1412":1}}],["只有偶尔在情况发生变化时",{"2":{"1128":1}}],["只有至少一个输入张量需要",{"2":{"1117":1}}],["只有非叶子节点才有意义",{"2":{"1110":1}}],["只有叶子节点才有",{"2":{"1110":1}}],["只有浮点",{"2":{"1106":1}}],["只有深度学习才有正则化吗",{"2":{"1011":1}}],["只有其中最好的k个候选项会被保留",{"2":{"904":1}}],["只有k个候选项会被保留",{"2":{"904":1}}],["只有bert表示在所有层中都受到左和右上下文的共同制约",{"2":{"721":1}}],["只有该单词对应的索引位置是1",{"2":{"681":1}}],["只有解码器的每个位置都能够获取输入序列中的所有位置的信息",{"2":{"536":1}}],["只有知道历史生成的内容",{"2":{"536":1}}],["只有一条路径",{"2":{"1330":1}}],["只有一小部分时间花在了",{"2":{"1140":1}}],["只有一小部分参数很重要",{"2":{"475":1}}],["只有一个输入序列",{"2":{"158":1}}],["只有源语言句子",{"2":{"381":1}}],["只有源语言输入的",{"2":{"83":1}}],["只有提供足够语料",{"2":{"367":1}}],["只有百分之三十四的情况新预测词语义与原预测词接近",{"2":{"306":1}}],["只有比较靠近输出的层才更新得比较好",{"2":{"296":1}}],["只有",{"2":{"83":1,"542":1,"1116":1,"1117":1}}],["只有self及其子模块的参数和缓存才能保证存在于state",{"2":{"1214":1}}],["只有self",{"2":{"66":1,"74":1,"382":1}}],["只是大家潜意识没有对其量化认识",{"2":{"2121":1}}],["只是现实的量化而已😄",{"2":{"2112":1}}],["只是需要慢慢自己领悟",{"2":{"2097":1}}],["只是希望自己能够不断成长并帮助到大家",{"2":{"2097":1}}],["只是create",{"2":{"2070":1}}],["只是想说自己的知己或知音难遇罢了",{"2":{"2054":1}}],["只是一个人生阶段罢了",{"2":{"2054":1}}],["只是一个规范",{"2":{"1569":1}}],["只是对编译器的建议",{"2":{"1709":1}}],["只是对训练和推理的对应的算子做不同的处理",{"2":{"662":1}}],["只是",{"2":{"1612":1}}],["只是用了它",{"2":{"1479":1}}],["只是用来填充序列长度",{"2":{"377":1}}],["只是数据流的方向改变了",{"2":{"1443":1}}],["只是为了好可视化",{"2":{"1373":1}}],["只是去掉了相对位置信息",{"2":{"761":1}}],["只是基于内容的寻址",{"2":{"758":1}}],["只是简单地将前后向信息拼接",{"2":{"720":1}}],["只是多加了一个mask码",{"2":{"650":1,"932":1}}],["只是字符的粒度不同",{"2":{"606":1}}],["只是因为传递参数的不同",{"2":{"449":1}}],["只是在每个transformer层计算多头注意力时",{"2":{"349":1}}],["只是在不同的层之间使用不同的参数",{"2":{"99":1}}],["只是把解码器的输出概率作为特征喂给了统计机器翻译",{"2":{"282":1}}],["只是加上了kv",{"2":{"76":1}}],["只是不同的注意力头在独属于其的逻辑部分上进行操作",{"2":{"29":1}}],["只能创建超管然后将别的超管作为员工添加到项目中",{"2":{"2064":1}}],["只能捕获已存在的变量",{"2":{"1907":1}}],["只能通过遍历访问元素",{"2":{"1799":1}}],["只能包含数据成员",{"2":{"1728":1}}],["只能存储",{"2":{"1684":1}}],["只能存储非负数",{"2":{"1607":1}}],["只能在定义它的函数内部访问",{"2":{"1649":1}}],["只能在其定义的代码块内部被访问",{"2":{"1649":1}}],["只能在构造函数初始化列表中进行初始化",{"2":{"1640":1}}],["只能放在赋值运算符的右边",{"2":{"1629":1}}],["只能被单一线程访问的设备",{"2":{"1413":1}}],["只能单个进程的访问",{"2":{"1412":1}}],["只能依赖小于i",{"2":{"915":1}}],["只能表征有限长度内的位置",{"2":{"749":1}}],["只能从位置开始预测",{"2":{"427":1}}],["只能老师带着学",{"2":{"411":1}}],["只能用到第i个元素之前的信息",{"2":{"409":1}}],["只能用来表达一些绝对信号",{"2":{"176":1}}],["只能以串行方式进行",{"2":{"239":1,"405":1}}],["只能获取某一部分的",{"2":{"78":1}}],["只能",{"2":{"74":1}}],["只能计算出第二个字与第一个字和第二个字的相关性",{"2":{"70":1}}],["只能看到目标序列的一部分",{"2":{"59":1}}],["只凭借人力难以做好",{"2":{"10":1}}],["𝑀",{"2":{"945":1,"965":1}}],["𝛂",{"2":{"739":1}}],["𝑃",{"2":{"721":2}}],["𝑉∗𝐻",{"2":{"698":2}}],["𝑉∗𝐸+𝐸∗𝐻",{"2":{"698":1}}],["𝑉∗𝐸",{"2":{"698":1}}],["𝑉",{"2":{"698":1}}],["𝑉=",{"2":{"145":1}}],["𝐸=𝐻",{"2":{"698":1}}],["𝐸",{"2":{"698":4}}],["𝑋",{"2":{"446":1}}],["𝐵",{"2":{"396":1}}],["𝐴",{"2":{"396":1}}],["𝛽",{"2":{"313":1}}],["𝛾",{"2":{"313":1}}],["𝑦=𝐹",{"2":{"301":2}}],["𝐻",{"2":{"301":1,"698":5}}],["𝑐ℎ",{"2":{"184":1}}],["𝑛是预测长度",{"2":{"194":1}}],["𝑛",{"2":{"184":5}}],["𝑂",{"2":{"184":2,"698":2,"946":1,"966":1}}],["𝑗𝑎",{"2":{"194":1}}],["𝑗",{"2":{"184":2,"194":1}}],["𝑗的交互",{"2":{"176":1}}],["𝑗|𝑖",{"2":{"176":2}}],["𝑤𝑛",{"2":{"721":2}}],["𝑤𝑖+1",{"2":{"721":1}}],["𝑤𝑖|𝑤𝑖+1",{"2":{"721":1}}],["𝑤𝑖|𝑤1",{"2":{"721":2}}],["𝑤𝑖−1",{"2":{"721":2}}],["𝑤",{"2":{"175":1,"184":2}}],["𝐺",{"2":{"145":1}}],["𝑥^",{"2":{"943":4,"961":4}}],["𝑥∈",{"2":{"943":1,"961":1}}],["𝑥𝑖",{"2":{"183":1}}],["𝑥",{"2":{"145":1,"183":1,"184":3,"301":3,"943":1,"944":2,"961":1,"963":2}}],["𝑥⋅𝑘",{"2":{"128":1}}],["𝑥⋅𝑘𝑖",{"2":{"128":1}}],["𝑡=",{"2":{"145":1}}],["𝑁^2",{"2":{"946":1,"966":1}}],["𝑁×𝐶×𝐻×𝑊",{"2":{"315":1,"340":1}}],["𝑁",{"2":{"141":1}}],["𝑑2",{"2":{"945":1,"965":1}}],["𝑑",{"2":{"141":2}}],["𝐿",{"2":{"141":1}}],["𝑘",{"2":{"140":4}}],["𝜃",{"2":{"140":6}}],["𝑝r𝑜𝑗",{"2":{"145":2}}],["𝑝",{"2":{"128":1,"145":1,"176":2}}],["𝑝𝑖𝑙",{"2":{"128":1}}],["𝑝𝑟𝑜𝑗",{"2":{"126":1}}],["𝑎𝑟𝑔𝑚𝑎𝑥",{"2":{"128":2}}],["𝑚",{"2":{"128":1,"141":1,"944":1,"946":1,"963":1,"966":1}}],["𝑚𝑖𝑙=𝑓",{"2":{"128":1}}],["𝑓𝑐",{"2":{"126":1}}],["𝑊",{"2":{"126":2,"145":3,"184":1}}],["𝑖^𝑙",{"2":{"128":1}}],["𝑖^𝑙=𝑓",{"2":{"128":1}}],["𝑖对应的value向量",{"2":{"125":1}}],["𝑖",{"2":{"125":2,"128":2,"145":1,"183":1,"194":1}}],["𝑙∗",{"2":{"145":1}}],["𝑙",{"2":{"122":2,"126":4,"145":4}}],["𝑜∗",{"2":{"145":2}}],["𝑜",{"2":{"122":2,"145":2}}],["𝑟",{"2":{"122":1,"396":2}}],["𝑠𝑜𝑓𝑡𝑚𝑎𝑥",{"2":{"183":2}}],["𝑠",{"2":{"122":1}}],["𝐾^𝑇",{"2":{"19":1}}],["𝑄𝑊",{"2":{"19":1}}],["提起",{"2":{"2096":1}}],["提成比例",{"2":{"1657":1}}],["提交事务",{"2":{"1486":1,"1487":1,"1488":1}}],["提及",{"2":{"657":1}}],["提示用户输入",{"2":{"1811":1,"1829":1}}],["提示用户输入年份",{"2":{"1729":1}}],["提示用户输入一个华氏温度值",{"2":{"1608":1}}],["提示用户选择转换类型",{"2":{"1608":1}}],["提示",{"0":{"1874":1},"2":{"983":1,"1619":1,"1642":1,"1657":1,"1825":1,"1843":1}}],["提示不再只是给定模型的一段简单文本",{"2":{"504":1}}],["提示πφπφπ",{"2":{"504":1}}],["提供默认值",{"2":{"1927":2}}],["提供函数",{"2":{"1916":1}}],["提供接口信息",{"2":{"1916":1}}],["提供方法来打开文件",{"2":{"1902":1}}],["提供清晰的异常信息",{"2":{"1764":1}}],["提供标准的排序和查找方式",{"2":{"1758":1}}],["提供的一份参考如下",{"2":{"1874":1}}],["提供的一个非常重要的层",{"2":{"702":1}}],["提供的基本数据类型",{"2":{"1728":1}}],["提供一个构造函数来初始化",{"2":{"1678":1}}],["提供一种新的看待模型的视角",{"2":{"620":1}}],["提供比",{"2":{"1607":1}}],["提供约",{"2":{"1607":2}}],["提供输入输出功能",{"2":{"1606":1}}],["提供代码补全",{"2":{"1605":1}}],["提供xml标签",{"2":{"1479":1}}],["提供初始信息",{"2":{"1438":1}}],["提供",{"2":{"1242":1}}],["提供给这个模块的state",{"2":{"1214":1}}],["提供数据的丰富性",{"2":{"1015":1}}],["提供更好的资源管理工具",{"2":{"1913":1}}],["提供更好的性能",{"2":{"976":1}}],["提供更精细的条件判断",{"2":{"1619":1}}],["提供更大的整数存储范围",{"2":{"1607":1}}],["提供更丰富的错误信息",{"2":{"1761":1}}],["提供更丰富",{"2":{"9":1}}],["提供语义信息",{"2":{"898":1}}],["提供信息带宽",{"2":{"510":1}}],["提供query",{"2":{"165":1}}],["提供key和value",{"2":{"165":1}}],["提供了跨平台的文件和目录操作接口",{"2":{"1932":1}}],["提供了更安全",{"2":{"1932":1}}],["提供了更大的架构灵活性",{"2":{"617":1}}],["提供了对并发编程的原生支持",{"2":{"1896":1}}],["提供了多种方式来控制输出的格式",{"2":{"1817":1,"1835":1}}],["提供了四个预定义的标准",{"2":{"1811":1,"1829":1}}],["提供了一组用于操作文件和目录的类和函数",{"2":{"1930":1}}],["提供了一个字符串的非拥有视图",{"2":{"1929":1}}],["提供了一些非常常见且高效的数据处理函数",{"2":{"1734":1}}],["提供了一种类型安全的联合体",{"2":{"1926":1}}],["提供了一种标准化的方式来将子模块添加到模型中",{"2":{"1214":1}}],["提供了一种改变平均patch大小的简单机制",{"2":{"613":1}}],["提供了一种廉价的近似的bagging集成",{"2":{"393":1}}],["提供了丰富的成员函数",{"2":{"1715":1}}],["提供了各种方便的字符串操作函数",{"2":{"1713":1}}],["提供了以下优势",{"2":{"1713":1}}],["提供了三种常用的参数传递方式",{"2":{"1650":1}}],["提供了开发桌面应用程序",{"2":{"1433":1}}],["提供了几种根据训练轮数来调整学习率的方法",{"2":{"1230":1}}],["提供了两个主要优点",{"2":{"977":1}}],["提供了良好的处理基础",{"2":{"709":1}}],["提供了良好的灵活性和扩展性",{"2":{"567":1}}],["提供了比简单点积更有意义的相似性度量",{"2":{"692":1}}],["提供了增量模型扩展所需的灵活性",{"2":{"621":1}}],["提供了分词器所需的基础能力",{"2":{"591":1}}],["提供了通用且强大",{"2":{"294":1}}],["提供了宝贵的洞察力",{"2":{"221":1}}],["提供了廉价地存储和检索信息的专用容量",{"2":{"154":1}}],["提供了描述",{"2":{"145":1}}],["提供了最简单的非线性变换",{"2":{"117":1}}],["提取运算符",{"2":{"1824":1,"1842":1}}],["提取码",{"2":{"1302":1}}],["提取字符级别的特征",{"2":{"718":1}}],["提取共性",{"2":{"684":1}}],["提取输入句子的高级特征",{"2":{"540":1}}],["提取对象的所有关系",{"2":{"472":1}}],["提取数据以丰富预训练数据中的相应信息也很重要",{"2":{"367":1}}],["提取语义",{"2":{"277":2}}],["提取到的特征向量列表",{"2":{"263":1}}],["提取哪些信息",{"2":{"263":1}}],["提取的是局部特征",{"2":{"247":1}}],["提取特征图中响应最强烈的部分进入下一层",{"2":{"814":1}}],["提取特征和加权求和的特点",{"2":{"169":1}}],["提取特征",{"0":{"168":1},"2":{"157":1}}],["提取出来的特征向量",{"2":{"263":1}}],["提取出来",{"2":{"122":1}}],["提取步骤",{"0":{"122":1},"2":{"96":1}}],["提取更多语义信息",{"0":{"116":1},"2":{"96":1,"115":1}}],["提升编程和算法能力",{"2":{"2010":1}}],["提升编程技能和代码效率",{"2":{"1602":1}}],["提升图形的真实感",{"2":{"2009":1}}],["提升开发效率",{"2":{"1913":1}}],["提升程序性能",{"0":{"1709":1}}],["提升代码的灵活性和可扩展性",{"2":{"1645":1}}],["提升我们的编程基本功",{"2":{"1602":1}}],["提升计算性能",{"2":{"1565":1}}],["提升计算效率",{"2":{"322":1}}],["提升了从大规模互联网数据预训练的迁移能力",{"2":{"637":1}}],["提升了模型的鲁棒性",{"2":{"553":1}}],["提升了模型的推理速度",{"2":{"204":1}}],["提升模型性能",{"2":{"945":1,"965":1}}],["提升模型对句子理解能力",{"2":{"722":1}}],["提升模型在特定领域的性能",{"2":{"560":1}}],["提升模型效果和训练速度",{"2":{"533":1}}],["提升模型训练效率",{"2":{"434":1}}],["提升训练并行度",{"2":{"434":1}}],["提升到了",{"2":{"361":1}}],["提升效率",{"2":{"239":1}}],["提升transformer模型鲁棒性",{"2":{"233":1}}],["提升目标token的最高rank",{"2":{"130":1}}],["提升",{"2":{"47":1}}],["提出来一些观点很值得我们思考",{"2":{"754":1}}],["提出来的",{"2":{"748":1}}],["提出一种复值词向量函数生成绝对位置编码",{"2":{"751":1}}],["提出一种基于语言模型的分词算法",{"2":{"603":1}}],["提出pretended",{"2":{"736":1}}],["提出的一种能够将相对位置信息依赖集成到",{"2":{"1341":1}}],["提出的字节潜在",{"2":{"610":1}}],["提出的bahdanau",{"2":{"284":1}}],["提出的adaptive",{"2":{"185":1}}],["提出可以使用积分梯度法",{"2":{"134":1}}],["提出",{"2":{"104":1,"105":1,"349":1,"420":1,"736":1}}],["提出用可动态组合的多头注意力",{"2":{"43":1}}],["提出了翻译语言建模",{"2":{"1315":1}}],["提出了多查询注意力机制",{"2":{"956":1}}],["提出了leakyrelu",{"2":{"842":1}}],["提出了rms",{"2":{"812":1}}],["提出了基于moe来生成embedding",{"2":{"737":1}}],["提出了结合mlm",{"2":{"732":1}}],["提出了promptsth和promptsum",{"2":{"736":1}}],["提出了prompt",{"2":{"731":1}}],["提出了两个学习任务",{"2":{"721":1}}],["提出了大型概念模型",{"2":{"625":1}}],["提出了diff",{"2":{"500":1}}],["提出了稀疏探测",{"2":{"477":1}}],["提出了用自注意力机制来替换rnn对序列的编解码过程",{"2":{"434":1}}],["提出了动态tanh",{"2":{"358":1}}],["提出了一个简洁有效的解决方案",{"2":{"716":1}}],["提出了一个改进版本resnetv2",{"2":{"302":1}}],["提出了一种绝对位置编码的鲁棒性训练方法",{"2":{"751":1}}],["提出了一种高效的机器人动作tokenization方法",{"2":{"637":1}}],["提出了一种prepbn的新方法",{"2":{"348":1}}],["提出了一种新的简单的网络架构transformer",{"2":{"911":1}}],["提出了一种新的drelu函数",{"2":{"111":1}}],["提出了一种新颖的神经网络架构",{"2":{"350":1}}],["提出了一种新型的神经长期记忆模块",{"2":{"226":1}}],["提出了一种名为神经常微分方程的模型",{"2":{"493":1}}],["提出了一种名为",{"2":{"151":1}}],["提出了一种称为",{"2":{"148":1}}],["提出了一种通过计算梯度变化敏感性的知识归因方法用以定位知识存储的位置",{"2":{"143":1}}],["提出了高速路网络",{"2":{"298":1}}],["提出了三种不同的方法来自适应地选择",{"2":{"224":1}}],["提出了注意力权重细化",{"2":{"209":1}}],["提出了全新的解决方案",{"2":{"152":1}}],["提出了记忆网络的概念",{"2":{"125":1}}],["提出了gelu",{"2":{"106":1}}],["提出了混合头注意力",{"2":{"42":1}}],["提出了其它整合方式",{"2":{"19":1}}],["提高编译效率",{"2":{"1916":1,"1917":1}}],["提高可读性",{"2":{"1880":1}}],["提高读写效率",{"2":{"1810":1,"1828":1}}],["提高读取速度",{"2":{"206":1}}],["提高程序健壮性",{"2":{"1761":1}}],["提高程序的性能",{"2":{"1607":1}}],["提高效率",{"2":{"1632":1,"1729":1}}],["提高代码的可重用性和可维护性",{"2":{"1916":1}}],["提高代码的可读性",{"2":{"1729":2}}],["提高代码的可读性和安全性",{"2":{"1613":1}}],["提高代码的简洁性和可读性",{"2":{"1913":1}}],["提高代码的复用性",{"2":{"1729":1}}],["提高代码的灵活性",{"0":{"1707":1}}],["提高代码可读性和可维护性",{"2":{"1628":1}}],["提高代码可读性",{"2":{"1612":1,"1615":1,"1728":1,"1729":1,"1773":1,"1793":1}}],["提高资源利用率",{"2":{"1565":1}}],["提高开发效率和代码质量",{"2":{"1729":1}}],["提高开发效率",{"2":{"1479":1,"1918":1}}],["提高命名实体识别的准确率",{"2":{"1324":1}}],["提高系统的性能和效率",{"2":{"985":1}}],["提高复杂",{"2":{"985":1}}],["提高gpu利用率",{"2":{"982":1}}],["提高训练效率",{"2":{"512":1}}],["提高性能",{"2":{"257":1,"1629":1,"1888":1,"1929":1}}],["提高了字符串处理的效率",{"2":{"1932":1}}],["提高了灵活性",{"2":{"1926":1}}],["提高了代码的可读性和可维护性",{"2":{"1707":1}}],["提高了代码的健壮性和可维护性",{"2":{"1677":1}}],["提高了开发效率",{"2":{"1603":1,"1715":1}}],["提高了可维护性",{"2":{"1479":1}}],["提高了网络的稳定性和训练效果",{"2":{"331":1}}],["提高了泛化性能",{"2":{"301":1}}],["提高了llm的泛化和自适应能力",{"2":{"218":1}}],["提高了其区分强弱响应的能力",{"2":{"213":1}}],["提高模型性能的科学方法",{"0":{"1138":1},"1":{"1139":1,"1140":1,"1141":1,"1142":1,"1143":1,"1144":1,"1145":1,"1146":1,"1147":1,"1148":1,"1149":1,"1150":1,"1151":1,"1152":1,"1153":1},"2":{"1125":1}}],["提高模型性能",{"2":{"310":1}}],["提高模型的泛化能力和生成能力",{"2":{"898":1}}],["提高模型的泛化能力和性能",{"2":{"277":1}}],["提高模型的效果",{"2":{"172":1}}],["提高模型泛化能力和可移植性的目的",{"2":{"145":1}}],["提高模型容量",{"2":{"21":1}}],["提高计算效率",{"2":{"21":1}}],["越界会抛出",{"2":{"1713":1}}],["越界",{"2":{"1670":1}}],["越界访问可能被恶意利用",{"2":{"1670":1}}],["越来越焦虑😥",{"2":{"2134":1}}],["越来越长时间的实验逐渐降低了它不可用的风险",{"2":{"1159":1}}],["越来越多的",{"2":{"540":1}}],["越来越多的工作尝试提出能够实时修改模型的高效",{"2":{"138":1}}],["越低越好",{"2":{"1147":1}}],["越小越好",{"2":{"935":1,"951":1}}],["越高越好",{"2":{"903":1}}],["越高的层",{"2":{"309":1}}],["越自信",{"2":{"399":1}}],["越靠近输入",{"2":{"332":1}}],["越靠近前面的通道反而权重越小",{"2":{"332":1}}],["越靠近底层的注意力",{"2":{"20":1}}],["越不容易梯度消失",{"2":{"332":1}}],["越能提升模型的性能",{"2":{"296":1}}],["越能学习到复杂的特征",{"2":{"296":1}}],["越往前的输入",{"2":{"253":1}}],["越应该承担更多的对输入的预测",{"2":{"168":1}}],["越到顶层的注意力",{"2":{"20":1}}],["越长",{"2":{"17":1,"974":1}}],["越大越好",{"2":{"935":1,"951":1}}],["越大",{"2":{"17":1}}],["算",{"2":{"810":1}}],["算子列表",{"2":{"836":2}}],["算子以及",{"2":{"829":1}}],["算子会在连接操作之前对每个头进行归一化",{"2":{"503":1}}],["算子类型",{"2":{"113":1}}],["算力需求大",{"2":{"279":1}}],["算完一块q和一块ktktk^t之后",{"2":{"180":1}}],["算法设计策略",{"2":{"2115":1}}],["算法是我们生活中的无形助手",{"2":{"2108":1}}],["算法是计算机图形学中最简单的直线绘制算法之一",{"2":{"2017":1}}],["算法将越来越深刻地影响我们的生活",{"2":{"2107":1}}],["算法不仅仅属于科学家和程序员",{"2":{"2108":1}}],["算法不仅仅是解决计算机问题的工具",{"2":{"2103":1}}],["算法不仅存在于技术中",{"2":{"2107":1}}],["算法交织于生活",{"0":{"2107":1}}],["算法思维",{"2":{"2103":1,"2108":1}}],["算法思想",{"0":{"942":1,"2022":1}}],["算法就是解决一个问题或完成一项任务时所遵循的一系列步骤",{"2":{"2097":1}}],["算法其实并不是只有电脑和编程才需要的东西",{"2":{"2097":1}}],["算法其实没那么复杂",{"2":{"2096":1}}],["算法似乎与编程",{"2":{"2096":1}}],["算法进行优化",{"2":{"2023":1}}],["算法进行迭代",{"2":{"580":1}}],["算法步骤",{"0":{"2018":1,"2023":1}}],["算法步骤详述",{"2":{"944":1}}],["算法概述",{"0":{"2017":1}}],["算法经常简称为backprop",{"2":{"1438":1}}],["算法实现流程",{"0":{"1060":1}}],["算法实例",{"2":{"1048":1}}],["算法8",{"2":{"1059":1}}],["算法流程",{"0":{"1049":1,"1053":1}}],["算法流程需要两个循环",{"2":{"54":1,"179":1}}],["算法中",{"2":{"1047":1}}],["算法具有一些令人满意的理论性质",{"2":{"1042":1}}],["算法详述",{"0":{"1036":1}}],["算法原理图",{"0":{"1035":1}}],["算法原理",{"0":{"1034":1,"1042":1,"1064":1}}],["算法图示",{"0":{"1030":1}}],["算法图解",{"0":{"962":1}}],["算法过程",{"0":{"1029":1,"1038":1}}],["算法1的第10行",{"2":{"944":1,"963":1}}],["算法1的第3行",{"2":{"944":1,"963":1}}],["算法狗",{"2":{"638":1,"740":1}}],["算法复杂度比wordpiece",{"2":{"601":1}}],["算法考虑了句子的不同分词可能",{"2":{"601":1}}],["算法的另一个核心是效率",{"2":{"2106":1}}],["算法的本质其实很简单",{"2":{"2096":1}}],["算法的某些点是否有别的方法实现",{"2":{"2031":1}}],["算法的基本步骤是通过增量迭代",{"2":{"2017":1}}],["算法的第12行",{"2":{"944":1,"963":1}}],["算法的输出是的subword词表",{"2":{"602":1}}],["算法的输出是子词表",{"2":{"599":1}}],["算法的词表大小是从小到大变化",{"2":{"601":1}}],["算法的主要目标",{"2":{"581":1}}],["算法的下一步是寻找最频繁的字符对",{"2":{"581":1}}],["算法的应用",{"2":{"153":1}}],["算法先将每个文本词",{"2":{"576":1}}],["算法",{"0":{"179":1,"576":1,"599":1,"602":1,"1043":1},"2":{"157":1,"1042":1,"1048":1,"1440":1,"1441":1,"1472":1,"2031":1,"2043":2,"2096":1,"2097":2,"2100":1}}],["算法冷知识第1期",{"2":{"156":1}}],["算法用于将输入嵌入映射到内存中的特定位置",{"2":{"153":1}}],["算法取代传统的全连接层",{"2":{"153":1}}],["算术右移或逻辑右移",{"2":{"1630":1}}],["算术运算示例",{"2":{"1607":1}}],["算术运算",{"2":{"1607":1}}],["算术运算符重载",{"2":{"1712":1}}],["算术运算符",{"2":{"1607":1,"1630":1}}],["算术知识以及世界知识等",{"2":{"121":1}}],["算术强度越大",{"2":{"17":1}}],["算术强度不到",{"2":{"17":1}}],["算作句子成分",{"2":{"66":1}}],["算单个注意力差不多",{"2":{"14":1}}],["紫色虚线框部分",{"2":{"17":1}}],["紫色",{"2":{"17":1}}],["时序数据库开发",{"2":{"1951":1}}],["时停止",{"2":{"1813":1,"1831":1}}],["时自动释放内存",{"2":{"1695":1}}],["时就需要用到链式求导",{"2":{"1442":1}}],["时可被自动调用",{"2":{"1214":1}}],["时不能传参数",{"2":{"1208":1}}],["时隐式存在的模式",{"2":{"1119":1}}],["时的加速",{"2":{"1227":1}}],["时的大多数函数",{"2":{"1115":1}}],["时的卷积",{"2":{"774":1}}],["时记录了创建数据的所有操作",{"2":{"1113":1}}],["时触发",{"2":{"1085":2,"1208":1}}],["时被调用",{"2":{"1085":1}}],["时调用",{"2":{"1085":1}}],["时生效",{"2":{"1082":1}}],["时遇到了内存不足",{"2":{"976":1}}],["时梯度为",{"2":{"840":1}}],["时会输出为",{"2":{"840":1}}],["时间复杂度",{"0":{"2155":1}}],["时间复杂度较高",{"2":{"511":1}}],["时间管理",{"0":{"2106":1}}],["时间序列问题",{"2":{"878":1}}],["时间旅客",{"2":{"513":1}}],["时间步越小加的噪音越小",{"2":{"1350":1}}],["时间步",{"2":{"58":1,"405":1,"407":1,"427":1}}],["时所希望的",{"2":{"333":1}}],["时使用",{"2":{"79":1,"198":1,"1122":1}}],["时刻",{"2":{"245":1}}],["时刻之前的输出",{"2":{"59":1,"934":1}}],["时刻及其之后的输入",{"2":{"58":1}}],["时刻的输入",{"2":{"58":1}}],["时候",{"2":{"38":1,"39":1,"516":1}}],["时",{"2":{"17":2,"108":1,"189":2,"191":1,"230":1,"245":1,"268":1,"277":1,"301":3,"370":1,"409":1,"542":1,"561":1,"692":1,"698":2,"701":1,"702":1,"778":1,"843":1,"845":2,"935":1,"951":1,"994":1,"995":1,"1083":1,"1114":1,"1175":1,"1214":1,"1227":1,"1235":1,"1236":1,"1237":1,"1238":1,"1239":1,"1240":1,"1322":1,"1438":1,"1589":1,"1605":1,"1713":1,"2054":1}}],["b+tree",{"2":{"1952":1}}],["b+rmidb+rmidb+r",{"2":{"122":1}}],["bjarne",{"2":{"1603":1}}],["btz",{"2":{"1331":1}}],["bt+1=1−β2t+11−β1t+1b",{"2":{"1192":1,"1193":1}}],["b−organization​​+t​b−organization",{"2":{"1324":1}}],["b−organization+tb−organization",{"2":{"1324":1}}],["b−person​​+t​o",{"2":{"1324":1}}],["b−person+to",{"2":{"1324":1}}],["b−a",{"2":{"1007":2}}],["bmio",{"2":{"1309":1}}],["bmm",{"2":{"805":1,"1087":1,"1092":1}}],["bmm是用于批量矩阵乘法的函数",{"2":{"805":1}}],["b7",{"2":{"1308":1}}],["b6",{"2":{"1308":1}}],["b5",{"2":{"1308":1}}],["b4",{"2":{"1308":1}}],["b2=0",{"2":{"1395":1}}],["b2",{"2":{"1102":2,"1308":1,"1398":2}}],["bf16背后的想法是通过降低数字的精度来减少计算能力和将张量相乘所需的能源消耗",{"2":{"1075":1}}],["bf16矩阵乘法的最大理论吞吐量为312",{"2":{"968":1}}],["bfloat16",{"2":{"1075":2,"1087":1,"1214":2}}],["bgd的参数更新相对稳定",{"2":{"1025":1}}],["bgd的计算开销较大",{"2":{"1025":1}}],["bgd是批量梯度下降",{"2":{"1025":1}}],["bgd",{"0":{"1025":1,"1027":1},"2":{"1026":1,"1027":2}}],["bge希望做中文世界的通用embedding模型",{"2":{"724":1}}],["bge",{"0":{"724":1},"1":{"725":1,"726":1,"727":1}}],["b图上的黑色的点即是导致失活的特征点",{"2":{"1019":1}}],["bp",{"0":{"1441":1,"1443":1},"2":{"1398":2,"1404":1,"1441":1}}],["bp时",{"2":{"994":1}}],["bpt",{"2":{"975":1}}],["bpe就不是增量patch化方案",{"2":{"612":1}}],["bpe按频率来选择合并的token对",{"2":{"598":1}}],["bpe的劣势是",{"2":{"595":1}}],["bpe的词汇表包含了从单个字符到较长的子词单位",{"2":{"594":1}}],["bpe的优点如下",{"2":{"594":1}}],["bpe算法原理及代码实现",{"2":{"638":1}}],["bpe算法通过训练",{"2":{"576":1}}],["bpe算法的主要步骤如下",{"2":{"576":1}}],["bpe遵循一种贪婪的策略来尽可能取得最优的解决方案",{"2":{"576":1}}],["bpe是在2015年由google在论文",{"2":{"574":1}}],["bpe",{"0":{"574":1,"592":1},"1":{"575":1,"576":1,"577":1,"578":1,"579":1,"580":1,"581":1,"582":1,"583":1,"584":1,"585":1,"586":1,"587":1,"588":1,"589":1,"590":1,"591":1,"592":1,"593":1,"594":1,"595":1},"2":{"554":1,"567":1,"569":7,"571":1,"575":2,"580":1,"581":3,"587":1,"594":1,"595":1,"596":1,"604":1,"605":1,"608":1,"613":1,"637":2,"638":9}}],["bcast",{"2":{"1573":1,"1575":1}}],["bcb",{"2":{"944":1}}],["bc=m4d",{"2":{"944":1}}],["b​2​​=0",{"2":{"1395":1}}],["b​1​​=0",{"2":{"1395":1}}],["b​t+1​​=​1−β​1​t+1​​​​√​1−β​2​t+1​​​​​​​",{"2":{"1192":1,"1193":1}}],["b​c​​",{"2":{"944":1}}],["b​c​​=​4d​​m​​",{"2":{"944":1}}],["b​r​​",{"2":{"944":4}}],["b​r​​=min",{"2":{"944":1}}],["b1=0",{"2":{"1395":1}}],["b1",{"2":{"762":1,"1102":2,"1308":1,"1398":2}}],["b节",{"2":{"637":1}}],["bbpe不会为其分配专门的token",{"2":{"608":1}}],["bbpe从原理上和bpe类似",{"2":{"607":1}}],["bbpe是从utf8编码入手的",{"2":{"607":1}}],["bbpe",{"0":{"605":1},"1":{"606":1,"607":1,"608":1},"2":{"608":1,"638":1}}],["bbb",{"2":{"105":1,"109":1,"2018":1}}],["b部分则展示了在写入阶段",{"2":{"489":1}}],["b3",{"2":{"429":1,"1308":1}}],["b0",{"2":{"429":1,"1308":1}}],["büsche",{"2":{"370":1,"557":1}}],["bn3",{"2":{"1215":2}}],["bn2",{"2":{"1215":2}}],["bn1",{"2":{"1215":2}}],["bn注重对每个batch进行归一化",{"2":{"809":1}}],["bn中则针对不同神经元输入计算均值和方差",{"2":{"808":1}}],["bn不适用于深度不固定的网络",{"2":{"808":1}}],["bn示意图",{"2":{"807":1}}],["bn会固定每句话的第n个位置",{"2":{"341":1}}],["bn会对",{"2":{"323":1}}],["bn要操作的这个特征维度是",{"2":{"325":1}}],["bn是在batch上",{"2":{"341":1}}],["bn是以行为单位",{"2":{"323":1}}],["bn是对一个batch",{"2":{"322":1}}],["bn通过计算在batch维度上的通道平均值和方差来对激活",{"2":{"321":1}}],["bn利用一个",{"2":{"316":1}}],["bn并非是万能良药",{"2":{"316":1}}],["bn缓解的是variance",{"2":{"314":1}}],["bn",{"2":{"314":2,"316":2,"322":1,"338":1,"340":2,"341":2,"343":4,"361":2,"807":6,"808":1,"1308":4}}],["bn认为相同维的特征具有相同分布",{"2":{"314":1}}],["bresenham算法",{"0":{"2026":1},"1":{"2027":1,"2028":1,"2029":1,"2030":1}}],["break",{"2":{"592":1,"1215":1,"1254":1,"1621":1,"1627":1,"1631":1,"1814":1,"1832":1,"1922":2}}],["breaking",{"2":{"429":1}}],["brb",{"2":{"944":4}}],["br=min",{"2":{"944":1}}],["brownsearch",{"2":{"740":2}}],["broadcast",{"2":{"1081":1,"1087":2,"1329":6,"1330":8}}],["broadcasting",{"2":{"79":1,"198":1}}],["broadly",{"2":{"20":1}}],["brain",{"2":{"845":1,"2073":1}}],["brain的vaswani等人在论文",{"2":{"235":1}}],["bradley",{"2":{"513":1}}],["brauwers",{"2":{"292":1}}],["buddy",{"2":{"1674":1,"1685":2}}],["buf",{"2":{"1590":3}}],["buffered",{"2":{"723":4}}],["buffer",{"2":{"723":2,"1208":2,"1211":5,"1214":3,"1648":1,"1813":3,"1821":4,"1831":3,"1839":4}}],["buffers键",{"2":{"1214":1}}],["buffers哪些有哪些没有",{"2":{"664":1}}],["buffers",{"0":{"1211":1},"2":{"664":1,"1208":4,"1214":5}}],["builtins",{"2":{"1085":7}}],["builds",{"2":{"2075":1}}],["building",{"2":{"487":1,"557":2,"2086":1}}],["build",{"2":{"374":1,"428":1,"557":10,"591":3,"1309":1,"1481":1,"1968":2,"1969":2,"1996":1,"1999":4,"2073":1,"2078":1}}],["buchatskaya",{"2":{"638":1}}],["bucket",{"2":{"204":1}}],["but",{"2":{"399":1,"591":2,"1303":1,"1329":1,"2087":1}}],["bushes",{"2":{"370":1,"557":1}}],["bubble",{"2":{"89":1,"90":1}}],["b爱a",{"2":{"172":1}}],["bhn",{"2":{"156":2}}],["b显示",{"2":{"131":1}}],["birth",{"2":{"1659":1}}],["bird",{"2":{"1654":3,"1690":1,"1691":6,"1869":6}}],["birch",{"2":{"638":1}}],["bileschi",{"2":{"1194":2}}],["bindings",{"0":{"1921":1},"2":{"1920":1,"1933":1}}],["bin",{"2":{"1506":1,"1507":1,"1820":1,"1838":1,"1987":1,"1999":1}}],["binaryvalue",{"2":{"1910":2}}],["binaryfile",{"2":{"1820":4,"1838":4}}],["binary",{"0":{"1755":1},"2":{"1096":1,"1732":1,"1755":2,"1758":1,"1820":2,"1838":2,"1910":3,"1989":1,"2059":10}}],["bins",{"2":{"1087":3}}],["bincount",{"2":{"1087":1}}],["bij=f",{"2":{"766":1}}],["bijbijb",{"2":{"766":1}}],["bidirectional",{"2":{"719":1,"747":1,"1312":1}}],["bibib",{"2":{"614":2}}],["big",{"2":{"567":1,"2077":1}}],["bigbird",{"2":{"204":1,"1317":2}}],["biao",{"2":{"361":1}}],["bias项的神奇作用",{"2":{"768":1}}],["bias方案",{"2":{"766":1}}],["bias做了一些改进工作",{"2":{"411":1}}],["bias",{"2":{"110":3,"411":1,"553":1,"757":1,"758":3,"760":1,"768":1,"808":1,"895":1,"988":1}}],["bias=config",{"2":{"110":3}}],["bias=false",{"2":{"8":4,"114":3,"201":5,"503":4,"1217":3,"1283":1}}],["biases",{"2":{"8":2,"765":2,"768":1}}],["bits",{"2":{"2062":3,"2063":4}}],["bitset>",{"2":{"2062":3,"2063":1}}],["bitset",{"0":{"2062":1},"2":{"2062":9,"2063":4}}],["bitwise",{"2":{"1087":22}}],["bit",{"2":{"147":2,"1081":1,"1087":1,"2062":1,"2063":1}}],["bi",{"2":{"76":1,"1082":2,"1315":1,"1386":1}}],["bat",{"2":{"1659":7,"1869":7}}],["batches",{"2":{"1215":1}}],["batched",{"2":{"17":1}}],["batch2",{"2":{"1087":2}}],["batch1",{"2":{"1087":2}}],["batch大小",{"2":{"382":1}}],["batch的代码如下",{"2":{"380":1}}],["batch类最重要的作用就是生成句子的掩码",{"2":{"382":1}}],["batch类的关键成员函数如下",{"2":{"380":1}}],["batch类的代码中设定掩码有两步",{"2":{"79":1}}],["batch类会把一个batch的源语言句子和目标语言句子整合在一起",{"2":{"379":1}}],["batch类",{"0":{"379":1},"1":{"380":1,"381":1,"382":1,"383":1}}],["batch情况下表现不太好的劣势",{"2":{"338":1}}],["batch统计量和其贡献的梯度都会呈现一定的不稳定性",{"2":{"316":1}}],["batch里的若干数据进行调整",{"2":{"316":1}}],["batchnorm的实现细节",{"0":{"1168":1},"2":{"1125":1}}],["batchnorm是在batch上",{"2":{"807":1}}],["batchnorm是最早出现",{"2":{"312":1}}],["batchnorm使用训练时的统计均值和方差数据直接计算",{"2":{"348":1}}],["batchnorm只使用在训练过程中学习的统计数据",{"2":{"327":1}}],["batchnorm在批方向计算均值和方差",{"2":{"322":1}}],["batchnorm针对同一特征",{"2":{"322":1}}],["batchnorm3d",{"2":{"315":1}}],["batchnorm2d的作用一致",{"2":{"343":2}}],["batchnorm2d",{"0":{"807":1},"2":{"315":2,"361":1,"807":2,"1122":1,"1215":3}}],["batchnorm1d",{"2":{"315":3}}],["batchnorm最开始被提出的时候宣称能缓解",{"2":{"314":1}}],["batchnorm将同一维度的所有特征视为一个分布并将其标准化",{"2":{"313":1}}],["batchnorm将不同样本相同维度的特征处理为相同的分布",{"2":{"313":1}}],["batchnorm",{"0":{"312":1},"1":{"313":1,"314":1,"315":1,"316":1},"2":{"293":1,"315":6,"341":1,"807":1,"1122":1,"1211":1}}],["batch对齐问题就会浮出水面",{"2":{"87":1}}],["batch=2",{"2":{"34":1,"80":1}}],["batching",{"2":{"17":3,"89":2,"986":1}}],["batch",{"0":{"1025":1,"1136":1},"2":{"17":3,"23":1,"24":1,"25":1,"28":2,"30":2,"31":2,"34":6,"35":3,"36":19,"65":3,"66":4,"76":1,"79":2,"83":9,"84":3,"87":2,"88":2,"90":3,"141":1,"198":5,"199":8,"201":2,"314":1,"315":6,"316":7,"322":3,"323":2,"325":2,"326":5,"337":1,"338":3,"346":1,"348":1,"361":5,"364":3,"372":1,"375":11,"380":5,"381":7,"382":2,"383":4,"384":6,"385":17,"398":2,"399":12,"402":1,"410":7,"420":3,"423":6,"424":3,"428":1,"450":2,"451":5,"460":2,"518":2,"520":11,"522":1,"528":5,"530":15,"532":1,"533":1,"538":4,"558":8,"640":2,"665":1,"700":5,"704":2,"726":1,"808":2,"834":1,"1025":1,"1027":7,"1083":2,"1087":1,"1131":3,"1132":1,"1133":2,"1134":6,"1136":3,"1162":1,"1164":4,"1168":3,"1211":21,"1215":14,"1216":10,"1218":9,"1241":2,"1242":2,"1251":3,"1280":1,"1283":1,"1296":1,"1299":2,"1303":1,"1308":3,"1328":13,"1329":15,"1330":23,"1332":1,"1345":7}}],["bad",{"2":{"1647":1,"1762":1,"1814":2,"1832":2,"1926":1}}],["baddbmm",{"2":{"1087":2}}],["bag",{"2":{"714":1}}],["basic",{"0":{"2086":1}}],["basictokenizer",{"2":{"592":1}}],["basictokenizer是bpe算法的实现",{"2":{"592":1}}],["bashmkdir",{"2":{"1968":1,"1969":1}}],["bashmpirun",{"2":{"1589":1}}],["bashmpicc",{"2":{"1589":1,"1594":1}}],["bash请输入一个年份",{"2":{"1729":1}}],["bashg++",{"2":{"1916":1}}],["bashgcc",{"2":{"1589":1}}],["bashgrep",{"2":{"1516":1}}],["bash",{"2":{"1584":5,"2066":1,"2067":1}}],["bashssh",{"2":{"1583":1}}],["bashsudo",{"2":{"1513":1,"1537":2}}],["bashvim",{"2":{"1543":1}}],["bashtail",{"2":{"1539":1}}],["bashtouch",{"2":{"1510":1}}],["bashwhich",{"2":{"1533":1}}],["bashkill",{"2":{"1523":1}}],["bashcmake",{"2":{"1987":1}}],["bashcat",{"2":{"1510":1,"1515":1}}],["bashcp",{"2":{"1510":1}}],["bashls",{"2":{"1507":1}}],["bashusage",{"2":{"1308":1}}],["bashpython",{"2":{"1303":2,"1304":1,"1306":1,"1307":2}}],["baseline的学习率也应该同样得到很好的调整",{"2":{"1150":1}}],["base",{"0":{"632":1,"639":1,"1224":1,"1358":1},"1":{"640":1,"641":1,"642":1,"643":1,"644":1,"645":1,"646":1,"647":1,"648":1,"649":1,"650":1,"651":1,"652":1,"653":1,"654":1,"1225":1,"1226":1,"1227":1},"2":{"333":1,"372":1,"423":1,"450":1,"564":2,"571":2,"580":1,"591":1,"632":2,"703":1,"1082":1,"1110":1,"1183":10,"1222":2,"1240":1,"1241":1,"1308":1,"1309":1,"1646":1,"1663":10,"1683":8,"1849":5,"1853":5,"1857":6,"1861":5,"1863":3,"2006":5}}],["based",{"0":{"633":1,"1898":1},"2":{"122":1,"156":2,"210":1,"285":1,"292":1,"437":1,"513":2,"718":1,"726":1,"758":1,"975":1,"1713":1,"1897":1}}],["balance",{"2":{"1677":6,"1766":1,"1873":2,"1874":11}}],["bal",{"2":{"513":1}}],["bark",{"2":{"1674":3,"1866":2}}],["bar",{"2":{"1489":2}}],["barry",{"2":{"638":1}}],["bard等",{"2":{"490":1}}],["bart",{"2":{"76":2,"569":1,"1312":1,"1317":1}}],["ba",{"2":{"429":1,"1059":1}}],["backslash",{"2":{"1616":1}}],["backpropagation",{"2":{"1439":1,"2086":1}}],["backends",{"2":{"1215":1}}],["backend",{"2":{"1086":1,"1107":1,"1208":1,"1305":1,"1306":1,"1307":2,"1308":5}}],["backwad",{"2":{"964":1}}],["backward",{"0":{"964":1,"971":1},"2":{"148":1,"385":1,"483":1,"513":1,"661":2,"964":1,"1039":1,"1082":1,"1083":1,"1092":1,"1094":1,"1095":2,"1096":2,"1097":1,"1098":3,"1099":2,"1100":2,"1101":3,"1102":1,"1104":2,"1105":1,"1108":1,"1114":1,"1117":1,"1202":1,"1205":2,"1208":6,"1213":5,"1214":9,"1215":1,"1218":1,"1223":3,"1231":2,"1244":1,"1295":1,"1398":5,"2086":4}}],["backbone选择",{"0":{"729":1}}],["back",{"0":{"1390":1},"1":{"1391":1,"1392":1,"1393":1,"1394":1},"2":{"395":3,"1330":1,"1404":1,"1438":1,"1442":1,"1713":3,"1714":5,"1719":6,"1720":2,"1722":2,"1723":1,"1726":3,"1797":1,"1799":1,"1800":1,"1825":1,"1843":1,"1883":1,"1891":1}}],["bahdanau在邮件中说得非常好",{"2":{"284":1}}],["bahdanau将这种软搜索设计为",{"2":{"284":1}}],["bahdanau的一次灵感闪现中",{"2":{"284":1}}],["bahdanau的思路发展历程",{"2":{"284":1}}],["bahdanau退而求其次",{"2":{"284":1}}],["bahdanau",{"0":{"284":1},"2":{"284":1,"285":2,"292":2}}],["baidu",{"2":{"1302":1,"1566":1}}],["bai",{"2":{"156":4,"233":1}}],["bankaccount",{"2":{"1677":2,"1766":1}}],["bank",{"2":{"691":2}}],["banana",{"2":{"257":1,"277":1,"516":1,"1715":1}}],["banxian",{"2":{"156":1}}],["bandwidth",{"2":{"78":1}}],["blinn",{"2":{"2009":1}}],["blt动态地将字节分组为patch",{"2":{"614":1}}],["blt结合了字节n",{"2":{"614":1}}],["blt由三个模块组成",{"2":{"614":1}}],["blt架构",{"0":{"614":1}}],["blt需要决定当前字节序列的步骤是否处于patch边界",{"2":{"612":1}}],["blt对基于token化模型的关键改进就是重新定义了词汇表大小和计算之间的权衡",{"2":{"612":1}}],["blt根据下一个字节预测的熵对数据进行分段",{"2":{"612":1}}],["blt没有固定的patch词汇表",{"2":{"612":1}}],["blt使训练和推理效率都得到提升",{"2":{"611":1}}],["blt在处理噪声输入和字符级任务",{"2":{"611":1}}],["blt可以在保持相同推理flops预算的情况下",{"2":{"611":1}}],["blt通过基于熵的patch划分方法",{"2":{"611":1}}],["blt",{"2":{"610":2,"614":1}}],["bleu",{"2":{"399":1,"935":1,"951":1}}],["blue",{"2":{"370":1,"429":1,"557":1}}],["black",{"2":{"557":1}}],["blauen",{"2":{"370":1}}],["blank>",{"2":{"66":1,"375":1,"380":1,"557":2,"558":1,"679":1}}],["blob",{"2":{"361":1}}],["blog",{"2":{"95":1,"387":1,"429":1,"513":2,"768":2}}],["blocking",{"2":{"1087":7,"1214":3}}],["block结合用于计算自注意力",{"2":{"975":1}}],["block在主机之间轮转",{"2":{"975":1}}],["block所需的时间",{"2":{"975":1}}],["block发送给下一个主机",{"2":{"975":1}}],["block之间的注意力时",{"2":{"975":1}}],["block之间的attention可以按任何顺序计算",{"2":{"975":1}}],["block与一个key",{"2":{"975":1}}],["block与一组key",{"2":{"975":1}}],["block有效地发送到下一个主机",{"2":{"975":1}}],["block操作的置换不变性属性",{"2":{"975":1}}],["block的累积会导致内存使用增加",{"2":{"975":1}}],["blockwise",{"2":{"974":2,"975":1}}],["block连接",{"2":{"721":1}}],["block通过注意力机制将这些embedding向量转换成若干特征向量",{"2":{"431":1}}],["blocksize",{"2":{"1086":3,"1087":3}}],["blocks内部warp级别的工作模式",{"2":{"972":1}}],["blocks的并行化计算",{"2":{"972":1}}],["blocks",{"0":{"354":1},"2":{"293":1,"981":2,"1364":1,"1729":1}}],["block都有自己的wt",{"2":{"172":1}}],["block里面的qkv矩阵",{"2":{"162":1}}],["block",{"0":{"356":1,"982":1},"2":{"89":2,"90":1,"97":1,"216":2,"293":1,"329":1,"944":1,"974":1,"975":6,"1019":1,"1364":1}}],["block中",{"2":{"71":1}}],["bset2",{"2":{"2062":10}}],["bset1",{"2":{"2062":14}}],["bsr",{"2":{"1086":1,"1087":1}}],["bsc",{"2":{"1086":1,"1087":1}}],["bsz",{"2":{"76":2,"201":12,"503":6,"1345":1}}],["bs",{"2":{"65":1,"201":8,"384":3,"558":3,"1345":1,"2063":4}}],["box",{"2":{"1772":8}}],["bob",{"2":{"1725":9,"1750":1,"1807":3,"1921":1}}],["bonus",{"2":{"1657":1}}],["body",{"2":{"1621":1}}],["board",{"2":{"1275":1}}],["both",{"2":{"1254":1,"2073":1,"2077":1}}],["bottom",{"2":{"135":2,"334":2}}],["boost",{"2":{"1977":1,"1991":3}}],["boosting",{"0":{"955":1},"1":{"956":1,"957":1}}],["bookcorpus",{"2":{"1316":1}}],["boom",{"2":{"740":1}}],["boolalpha",{"2":{"1817":3,"1835":3}}],["bool",{"2":{"84":1,"395":4,"557":1,"572":4,"700":1,"702":4,"1082":2,"1085":3,"1086":13,"1087":155,"1208":7,"1214":27,"1227":6,"1329":1,"1330":3,"1607":3,"1645":3,"1674":1,"1713":1,"1726":1,"1729":2,"1736":1,"1737":1,"1738":1,"1750":1,"1755":1,"1817":3,"1835":3,"1902":1,"1927":2,"2063":1}}],["boolean",{"2":{"62":1,"1436":1,"1607":1}}],["bow",{"2":{"676":1}}],["borgeaud",{"2":{"638":1}}],["boldsymbol",{"2":{"343":10,"1180":3}}],["boltzmann",{"2":{"104":1,"156":1}}],["boy",{"2":{"257":2,"557":1}}],["bos>`",{"2":{"557":1}}],["bos>i",{"2":{"408":5}}],["bos>我吃了一个苹果",{"2":{"398":1}}],["bos>和",{"2":{"384":1,"558":1}}],["bos>",{"2":{"380":3,"383":1,"384":2,"385":1,"428":5,"558":2}}],["bos>新年好",{"2":{"380":3}}],["bos",{"2":{"80":2,"408":1,"571":2,"572":4}}],["bounded",{"2":{"1413":1}}],["bound=dict",{"2":{"1214":1}}],["boundary",{"2":{"553":1}}],["bound",{"2":{"17":5}}],["by=kx+b推导而来",{"2":{"2017":1}}],["by=",{"2":{"1284":1}}],["bypass",{"2":{"1086":1}}],["bytetensor",{"2":{"1328":1,"1329":1,"1330":1}}],["bytepiece",{"2":{"638":1}}],["bytes",{"2":{"591":4,"592":10,"607":1,"1227":1,"1607":3,"1821":1,"1839":1,"1930":1}}],["byte",{"2":{"567":1,"575":1,"605":1,"607":1,"610":2,"638":4,"1087":1}}],["by",{"2":{"41":1,"95":1,"122":4,"151":1,"156":1,"160":1,"257":1,"284":1,"292":2,"298":1,"361":2,"393":1,"513":1,"557":4,"572":2,"592":1,"702":6,"764":1,"885":1,"1090":1,"1100":1,"1216":1,"1217":1,"1218":1,"1227":1,"1254":1,"1283":1,"1303":1,"1304":1,"1650":3,"1729":3,"1750":2,"1762":1,"2073":1,"2075":1,"2079":1}}],["beg",{"2":{"1821":2,"1839":2,"1902":1}}],["beginner",{"2":{"740":1}}],["beginning",{"2":{"557":1,"572":1}}],["begin",{"2":{"694":3,"698":1,"943":1,"961":1,"1615":1,"1645":1,"1713":3,"1714":1,"1718":1,"1719":5,"1720":5,"1721":5,"1722":5,"1724":2,"1725":3,"1736":1,"1737":1,"1738":1,"1739":1,"1741":2,"1742":2,"1743":1,"1744":1,"1746":1,"1747":1,"1749":1,"1750":1,"1751":2,"1752":2,"1754":2,"1755":1,"1756":3,"1883":6,"1897":1,"1914":2,"1922":2,"1933":2}}],["become",{"2":{"1284":1}}],["becomes",{"2":{"1284":1,"1329":1}}],["because",{"2":{"261":2,"591":1,"1254":2,"2073":1,"2076":1,"2083":1}}],["benchmark",{"2":{"1304":1,"1308":1,"1315":2}}],["ben",{"2":{"1175":1}}],["bengio",{"2":{"429":1,"543":1,"999":1,"1176":1}}],["being",{"2":{"1086":1,"1891":1}}],["beam",{"0":{"900":1,"902":1,"904":1},"1":{"901":1,"902":1,"903":1,"904":1},"2":{"901":1,"902":2,"904":2}}],["beats",{"2":{"122":1}}],["beat",{"2":{"122":2}}],["bellm将llm的最后几层改为双向注意力",{"2":{"732":1}}],["below",{"2":{"38":1,"39":1,"82":1,"326":1,"344":1,"523":1,"533":1,"1254":1}}],["beyond",{"2":{"638":2,"760":1,"768":1}}],["best",{"2":{"542":1,"592":1,"1179":1,"1305":1,"1330":13}}],["behavior",{"2":{"507":1,"513":1,"1254":1}}],["behind",{"2":{"429":1}}],["berkeley",{"2":{"740":1}}],["bernoulli",{"2":{"395":1,"1087":4}}],["bereiten",{"2":{"370":1}}],["bert+crf模型架构实现ner任务",{"2":{"1320":1}}],["bert4torch",{"2":{"1309":3}}],["bert深度解析",{"2":{"740":1}}],["bert和t5使用mean",{"2":{"731":1}}],["bert和openai",{"2":{"721":1}}],["bertembeddings",{"2":{"723":1}}],["bert也使用特殊token",{"2":{"721":1}}],["bert是双向的transformer",{"2":{"721":1}}],["bert使用双向transformer",{"2":{"721":1}}],["bert的输入的编码向量是3种embedding特征element",{"2":{"722":1}}],["bert的关键特点是其双向性",{"2":{"721":1}}],["bert的初始标准差为什么是0",{"2":{"361":2}}],["bert会根据当前输入的上下文动态调整每个词的嵌入",{"2":{"721":1}}],["bert为词生成的嵌入不仅仅依赖于该词本身",{"2":{"719":1}}],["bert采用的是右填充",{"2":{"378":1}}],["bert性能优化之",{"2":{"47":1}}],["bert",{"0":{"671":1,"719":1,"1310":1,"1319":1},"1":{"720":1,"721":1,"722":1,"723":1,"1320":1,"1321":1,"1322":1,"1323":1,"1324":1,"1325":1,"1326":1},"2":{"18":2,"20":4,"47":2,"204":1,"437":3,"513":1,"569":1,"670":1,"676":1,"719":1,"720":1,"721":1,"722":1,"740":3,"747":1,"749":1,"763":1,"768":1,"844":1,"945":1,"965":1,"1309":1,"1310":2,"1312":3,"1315":9,"1317":1,"1320":3,"1337":1,"1404":1}}],["bedienen",{"2":{"370":1}}],["between",{"2":{"136":1,"156":1,"160":1,"688":1,"764":1,"977":1}}],["beta2",{"2":{"1143":1,"1144":1}}],["beta1",{"2":{"1143":1,"1144":1}}],["beta=0",{"2":{"108":1}}],["beta",{"2":{"107":2,"108":4,"110":1,"313":2,"343":4,"347":1,"360":1,"640":1,"807":2,"808":2,"809":2,"810":2,"845":1,"1086":2,"1087":3,"1130":2,"1174":3,"1192":6,"1193":8,"1340":2}}],["beta为1时的swish激活函数",{"2":{"103":1}}],["betas=",{"2":{"83":1,"402":1,"423":1,"424":1}}],["betterthan",{"2":{"638":1}}],["better",{"0":{"610":1,"968":1},"1":{"611":1,"612":1,"613":1,"614":1,"615":1,"969":1,"970":1,"971":1,"972":1},"2":{"47":1,"233":1,"610":1,"947":1}}],["before",{"2":{"47":1,"1205":2,"1215":1,"1633":1,"1715":1,"2086":1}}],["be",{"2":{"20":1,"47":1,"76":1,"160":1,"334":1,"399":1,"429":1,"504":1,"513":1,"557":1,"572":3,"573":1,"591":4,"592":2,"702":1,"723":1,"736":1,"802":1,"981":1,"1216":1,"1217":1,"1218":1,"1245":1,"1254":2,"2078":1}}],["b",{"2":{"19":1,"46":1,"105":8,"109":7,"113":4,"145":2,"209":1,"217":1,"224":1,"320":1,"325":2,"341":3,"343":11,"361":1,"364":3,"385":3,"395":3,"423":6,"479":1,"510":2,"519":2,"540":1,"592":1,"762":1,"765":1,"766":2,"807":1,"808":3,"809":1,"810":1,"819":2,"820":3,"826":2,"833":2,"943":1,"944":10,"961":1,"986":1,"1007":4,"1086":2,"1093":2,"1095":1,"1096":1,"1098":7,"1099":3,"1192":1,"1193":1,"1279":1,"1299":2,"1308":4,"1320":3,"1324":7,"1389":1,"1395":2,"1398":1,"1547":1,"1607":9,"1612":3,"1614":5,"1629":4,"1630":6,"1645":4,"1648":4,"1650":21,"1660":4,"1663":2,"1683":6,"1687":8,"1698":6,"1699":2,"1702":1,"1706":4,"1707":6,"1709":3,"1729":3,"1750":2,"1762":3,"1772":3,"1778":2,"1786":2,"1788":4,"1789":5,"1825":2,"1843":2,"1883":2,"1905":5,"1906":4,"1916":6,"1931":2,"1933":2,"1999":22,"2005":4,"2006":1,"2021":1,"2059":6,"2060":5,"2062":1}}],["使基类的接口在派生类的外部不可见",{"2":{"1857":1}}],["使代码结构更清晰",{"2":{"1761":1}}],["使代码更加清晰易懂",{"2":{"1729":1}}],["使代码更易于理解和维护",{"2":{"1728":1}}],["使成员函数不依赖于类的具体对象而存在",{"2":{"1649":1}}],["使类的所有对象共享同一个变量",{"2":{"1649":1}}],["使系统的设计更清晰",{"2":{"1479":1}}],["使同时尽可能缩短评估时间",{"2":{"1163":1}}],["使同一个",{"2":{"90":1}}],["使同一个batch内所有序列的长度一致",{"2":{"53":1,"376":1}}],["使预测更极端",{"2":{"1016":1}}],["使网络各层的激活值和反向激活梯度的方差在传播过程中尽量保持一致",{"2":{"999":1}}],["使sm的利用率尽量打满",{"2":{"972":1}}],["使矩阵乘法速度更快",{"2":{"968":1}}],["使之成为一种颇具吸引力的深度学习结构",{"2":{"769":1}}],["使它们能够重新计算您的模型",{"2":{"1223":1}}],["使它们不与位置信息纠缠",{"2":{"745":1}}],["使它们生成更准确",{"2":{"138":1}}],["使每个token都能访问序列中的所有其他token",{"2":{"734":2}}],["使每个词元能够选择合适的注意力头",{"2":{"42":1}}],["使模型更加简洁且泛化性能更好",{"2":{"1012":1}}],["使模型对输入图像中的特征位置变化更加鲁棒",{"2":{"813":1}}],["使模型可以更有效地学习从输入到输出的映射",{"2":{"701":1}}],["使模型能够在更高的抽象层面上进行推理",{"2":{"628":1}}],["使模型能够在多",{"2":{"217":1}}],["使模型能够更有效地学习并降低对初始权重的敏感性",{"2":{"468":1}}],["使模型能够随时间增加新技能而不会出现灾难性遗忘",{"2":{"222":1}}],["使向量空间的几何结构与心理量的实证测量",{"2":{"691":1}}],["使这个弱点愈发凸显",{"2":{"511":1}}],["使这些模型能够处理不同长度的文本",{"2":{"376":1}}],["使输出权重不可识别",{"2":{"305":1}}],["使个体能够恢复失去的功能并适应新的思维方式或行动方式",{"2":{"220":1}}],["使llm能够适应未见过的任务",{"2":{"218":1}}],["使复杂度降到线性",{"2":{"204":1}}],["使其具有描述性",{"2":{"1728":1}}],["使其具备高效的学习和优化能力",{"2":{"294":1}}],["使其具备广泛的能力",{"2":{"222":1}}],["使其长度变为",{"2":{"1713":1}}],["使其输出相应动物的叫声",{"2":{"1690":1}}],["使其指向新分配的内存",{"2":{"1650":1}}],["使其只能在当前源文件中访问",{"2":{"1649":1}}],["使其逐渐接近循环结束的条件",{"2":{"1621":1}}],["使其逐渐稳定并达到更好的性能状态",{"2":{"1242":1}}],["使其成为更完整的第一节课后半部分内容",{"2":{"1606":1}}],["使其成为叶节点",{"2":{"1083":1}}],["使其能处理更普遍的函数关系类型",{"2":{"1464":1}}],["使其能够在找到凸碗状结构后快速收敛",{"2":{"1048":1}}],["使其能够表示更加复杂的函数关系",{"2":{"117":1}}],["使其不再与计算图相关联",{"2":{"1094":1}}],["使其分布一致",{"2":{"807":1}}],["使其在函数多次调用之间保持状态",{"2":{"1649":1}}],["使其在不确定性较高的提示条件下也能保持较高的嵌入质量",{"2":{"739":1}}],["使其在参数更新时不产生效果",{"2":{"650":1,"932":1}}],["使其更像其邻居的向量",{"2":{"709":1}}],["使其更加适应于当今深度学习的环境",{"2":{"155":1}}],["使其越来越不切实际",{"2":{"624":1}}],["使其达到最佳性能",{"2":{"542":1}}],["使其位于单位范数超球面上",{"2":{"352":1}}],["使其均值为0方差为1",{"2":{"313":1}}],["使其满足均值为",{"2":{"309":1}}],["使记忆层超越了概念验证阶段",{"2":{"154":1}}],["使",{"2":{"90":1,"1603":1}}],["使上三角阵的数据类型变为unit8",{"2":{"74":1}}],["使得尽可能多的小孩得到满足",{"2":{"2148":1}}],["使得尽可能多的小孩能得到满足",{"2":{"2147":1}}],["使得代码更灵活",{"2":{"1884":1}}],["使得代码可以重用",{"2":{"1848":1}}],["使得代码的逻辑更加模块化",{"2":{"1729":1}}],["使得代码结构更清晰",{"2":{"1628":1}}],["使得对象可以像函数一样被调用",{"2":{"1712":1}}],["使得对于任意输入",{"2":{"504":1}}],["使得自注意力层",{"2":{"1315":1}}],["使得验证误差最小",{"2":{"1175":1}}],["使得参数更新更加稳定",{"2":{"1032":1}}],["使得参数无法继续更新的问题",{"2":{"838":1}}],["使得以后不用人为设计激活函数了",{"2":{"845":1}}],["使得计算更复杂",{"2":{"746":1}}],["使得计算成本降低",{"2":{"183":1}}],["使得每个位置的token都可以看到其他位置的信息",{"2":{"734":1}}],["使得每一层的输入分布保持稳定",{"2":{"314":1}}],["使得增加隐层大小时而不用改变词嵌入的大小",{"2":{"698":1}}],["使得数据对象的某些属性被编码到其向量表示的几何属性中",{"2":{"690":1}}],["使得数据的分布稳定下来",{"2":{"310":1}}],["使得相似词的向量更加接近",{"2":{"686":1}}],["使得原本高维空间中的检索问题可以降低到o",{"2":{"684":1}}],["使得原先自回归推理先后依赖被解除",{"2":{"408":1}}],["使得完成",{"2":{"613":1}}],["使得其均值为0",{"2":{"517":1}}],["使得不同的token能够相互信息通信",{"2":{"446":1}}],["使得不同来源的知识的加权成为一个需要考虑的关键参数",{"2":{"141":1}}],["使得矩阵每一行代表一个时间步的输入",{"2":{"408":1}}],["使得前向传播和反向传播中的激活值和梯度值保持相近的方差",{"2":{"403":1}}],["使得在超过10万个四面体的弹性体上实现实时高保真物理仿真成为可能",{"2":{"2011":1}}],["使得在子类上调用的方法返回一个子类实例",{"2":{"1083":1}}],["使得在处理大型数据集时更加可行",{"2":{"1027":1}}],["使得在反向传播过程中梯度的变化更加平稳",{"2":{"403":1}}],["使得在计算注意力时",{"2":{"41":1}}],["使得输入变化不会太大",{"2":{"396":1}}],["使得输出权重w不可识别",{"2":{"305":1}}],["使得输出具有概率的物理意义",{"2":{"180":1}}],["使得u和v向量分别代表h与存储在wuwuw",{"2":{"356":1}}],["使得与计算的点积可以解释为单位范数向量之间的余弦相似度",{"2":{"355":1}}],["使得算法更加简洁",{"2":{"346":1}}],["使得训练更加稳定",{"2":{"333":1}}],["使得学习过早停止",{"2":{"309":1}}],["使得它可以对于任何数组类型求和",{"2":{"1702":1}}],["使得它有能力分辨不同位置的token",{"2":{"1334":1}}],["使得它们在线性空间的运算和群的操作同构",{"2":{"713":1}}],["使得它们的输出权重无法识别",{"2":{"305":1}}],["使得它不能访问未来的输入",{"2":{"39":1}}],["使得网络不收敛",{"2":{"991":1}}],["使得网络更容易被优化",{"2":{"814":1}}],["使得网络对这种缩放不敏感",{"2":{"320":1}}],["使得网络的表达能力没有看起来那么强大",{"2":{"305":1}}],["使得网络易于优化",{"2":{"301":1}}],["使得恒等映射成为可能",{"2":{"301":1}}],["使得注意力编码器也能像rnn一样对一个复杂的语句或者场景进行理解和解析",{"2":{"261":1}}],["使得当面临冗长且信息密集的输入序列时",{"2":{"256":1}}],["使得彼此可以连接",{"2":{"204":1}}],["使得所有value的权重总和为1",{"2":{"168":1}}],["使得序列中的每个token都能感知其他token",{"2":{"158":1}}],["使得内存需求不会因哈希表的引入而暴增",{"2":{"153":1}}],["使得特定任务头传递的",{"2":{"122":1}}],["使得模型有能力分辨不同位置的token",{"2":{"745":1}}],["使得模型可以生成高质量的句子级别的表征",{"2":{"734":1}}],["使得模型可以对数据进行更复杂的变换",{"2":{"117":1}}],["使得模型在处理多语言文本时更加高效",{"2":{"603":1}}],["使得模型对训练超参更具鲁棒性的同时",{"2":{"349":1}}],["使得模型更快地收敛",{"2":{"310":1}}],["使得模型能够处理序列数据并保留位置信息",{"2":{"674":1}}],["使得模型能够生成更准确",{"2":{"525":1}}],["使得模型能够从中学习到大量的范畴和相关信息",{"2":{"505":1}}],["使得模型能够从多个角度分析输入序列",{"2":{"1":1}}],["使得模型能够学习更多的特征表示",{"2":{"116":1}}],["使得模型能够学习到更复杂的表示",{"2":{"9":1}}],["使得深层网络更容易训练",{"2":{"104":1}}],["使得",{"2":{"59":1,"382":1,"977":1}}],["使用两个指针",{"2":{"2152":1}}],["使用模板参数来定义bitset的大小",{"2":{"2063":1}}],["使用位运算编写一个程序",{"2":{"2063":1}}],["使用位运算交换两个数",{"2":{"2060":1}}],["使用位运算进行权限控制",{"2":{"2060":1}}],["使用增量",{"2":{"2016":1}}],["使用增加梯度方差",{"2":{"1154":1}}],["使用最少的硬币",{"0":{"2136":1},"1":{"2137":1,"2138":1,"2139":1}}],["使用最多且最简单",{"2":{"2013":1}}],["使用最后一个token",{"2":{"731":1}}],["使用访问者模式",{"2":{"1926":1}}],["使用结构化绑定",{"2":{"1921":1,"1933":1}}],["使用结构体",{"2":{"1728":1}}],["使用变量",{"2":{"1917":1}}],["使用分隔符提高可读性",{"2":{"1910":1}}],["使用分词器从三个iterator之中获取token",{"2":{"557":1}}],["使用右值引用实现一个自己的字符串类",{"2":{"1900":1}}],["使用右积",{"2":{"216":1}}],["使用auto关键字让编译器推断变量类型",{"2":{"1878":1}}],["使用a对value进行加权平均",{"2":{"271":1}}],["使用a对value进行加权平均的线性变换",{"2":{"173":1}}],["使用public继承",{"2":{"1852":1}}],["使用操纵符",{"2":{"1817":1,"1835":1}}],["使用友元函数输出立方体信息",{"2":{"1791":1,"1792":1}}],["使用友元函数进行",{"2":{"1788":1}}],["使用友元函数重载输出流运算符",{"2":{"1712":1}}],["使用异常处理机制来处理以下情况",{"2":{"1766":1}}],["使用异常处理机制",{"2":{"1762":1}}],["使用给定的操作将范围内的元素应用到输出范围",{"2":{"1741":1}}],["使用联合体需要谨慎",{"2":{"1728":1}}],["使用联合体的注意事项",{"2":{"1728":1}}],["使用联合体c++",{"2":{"1728":1}}],["使用枚举类型",{"2":{"1728":1}}],["使用迭代器遍历",{"2":{"1714":1,"1718":2}}],["使用迭代器遍历字符串",{"2":{"1713":1}}],["使用迭代器初始化",{"2":{"1713":1}}],["使用成员函数重载",{"2":{"1712":1}}],["使用成本",{"2":{"1134":1}}],["使用函数指针进行计算",{"2":{"1710":1}}],["使用默认打印次数",{"2":{"1708":1}}],["使用默认宽度",{"2":{"1708":1}}],["使用行指针访问二维数组元素",{"2":{"1705":1}}],["使用继承和虚函数来实现动物的多态功能",{"2":{"1689":1}}],["使用动态内存分配创建一个指定大小的整数数组",{"2":{"1678":1}}],["使用动态词向量",{"2":{"706":1}}],["使用私有成员是实现封装性的关键",{"2":{"1677":1}}],["使用提取运算符",{"2":{"1673":1}}],["使用提示词让模型在最后一个token来总结全文语义",{"2":{"731":1}}],["使用插入运算符",{"2":{"1673":1}}],["使用流",{"2":{"1673":1}}],["使用内存泄漏检测工具",{"2":{"1671":1}}],["使用内置的",{"2":{"1083":1}}],["使用容器",{"2":{"1670":1}}],["使用虚继承",{"2":{"1665":2}}],["使用虚继承来解决菱形继承的问题",{"2":{"1664":1}}],["使用菱形继承",{"2":{"1664":1}}],["使用作用域分辨符指定调用b类的print",{"2":{"1660":1}}],["使用作用域分辨符指定调用a类的print",{"2":{"1660":1}}],["使用合适的访问权限控制成员的可见性和可访问性",{"2":{"1657":1}}],["使用合适的权重初始化策略可以有效控制梯度的大小",{"2":{"403":1}}],["使用递归方法计算斐波那契数列的第",{"2":{"1651":1}}],["使用递归计算斐波那契数列",{"2":{"1646":1}}],["使用回调函数进行排序",{"2":{"1645":1}}],["使用回调函数进行自定义排序",{"2":{"1645":1}}],["使用参数初始化表初始化所有成员变量",{"2":{"1642":1}}],["使用参数self",{"2":{"1214":1}}],["使用场景总结",{"0":{"1864":1}}],["使用场景",{"2":{"1641":1,"1650":1,"1852":1,"1856":1,"1860":1}}],["使用冒号",{"2":{"1641":1}}],["使用头文件保护符",{"2":{"1628":1}}],["使用双引号",{"2":{"1628":1}}],["使用双向lstm处理句子",{"2":{"289":1}}],["使用尖括号",{"2":{"1628":1}}],["使用数组名和下标来访问数组中的特定元素",{"2":{"1623":1}}],["使用数组可以更方便地组织和访问这些数据",{"2":{"1623":1}}],["使用关键字",{"2":{"1613":1}}],["使用关系r",{"2":{"122":1}}],["使用以下命令进行安装",{"2":{"1987":1}}],["使用以下命令将公钥复制到",{"2":{"1594":1}}],["使用以下公式将华氏温度转换为摄氏温度",{"2":{"1608":1}}],["使用标准命名空间",{"2":{"1606":1}}],["使用命令行操作",{"2":{"1605":1}}],["使用助记符表示机器指令",{"2":{"1604":1}}],["使用示例",{"2":{"1589":1}}],["使用共享内存模型",{"2":{"1569":1}}],["使用万能的map",{"2":{"1485":1}}],["使用感知机解决异非",{"2":{"1462":1}}],["使用感知机解决或",{"2":{"1462":1}}],["使用感知机解决与",{"2":{"1462":1}}],["使用感知机解决线性可分问题",{"0":{"1462":1}}],["使用特定任务的标注语料",{"2":{"1312":1}}],["使用jit",{"2":{"1292":1}}],["使用多项式函数来在给定的",{"2":{"1240":1}}],["使用方法",{"0":{"1230":1,"1231":1},"1":{"1231":1,"1232":1,"1233":1,"1234":1,"1235":1,"1236":1,"1237":1,"1238":1,"1239":1,"1240":1,"1241":1,"1242":1,"1243":1,"1244":1,"1245":1,"1246":1,"1247":1,"1248":1}}],["使用运行时的均值和方差进行归一化",{"2":{"1211":1}}],["使用运算符",{"2":{"1083":1}}],["使用梯度截断",{"2":{"1180":1}}],["使用梯度积累技术可以支持的更大的batch",{"2":{"1132":1}}],["使用1000步的学习率预热可以解决这种特殊的不稳定情况",{"2":{"1178":1}}],["使用2倍预算随机搜索作为baseline有多强大",{"2":{"1175":1}}],["使用不同的评估指标来评估这些结果",{"2":{"1175":1}}],["使用不同的权重矩阵为q和k提供了能力去捕捉不同的依赖关系",{"2":{"172":1}}],["使用不同的权重矩阵能够更好地区分这些不同的角色",{"2":{"172":1}}],["使用不同的权重矩阵生成的主要原因是",{"2":{"172":1}}],["使用已经运行的实验中的数据来评估每一组超参数的训练误差",{"2":{"1175":1}}],["使用一些简单的",{"2":{"1173":1}}],["使用一个线性层来生成",{"2":{"515":1}}],["使用当前批次的均值和方差对激活值进行归一化",{"2":{"1168":1}}],["使用比训练时更大的",{"2":{"1162":1}}],["使用适当的性能分析工具来诊性能受限的输入管道",{"2":{"1161":1}}],["使用性能分析工具并注意常见的一些问题",{"2":{"1161":1}}],["使用较少的训练步数可以解决早期训练的不稳定性",{"2":{"1158":1}}],["使用较为一般的数据集进行训练",{"2":{"224":1}}],["使用学习率预热",{"2":{"1180":1}}],["使用学习率搜索算法来确定",{"0":{"1156":1}}],["使用学习率衰减",{"2":{"1149":1}}],["使用黑盒优化工具来正确处理发散试验很重要",{"2":{"1153":1}}],["使用严格的统计测试对有限验证集上估计的验证错误率进行比较是很好的",{"2":{"1152":1}}],["使用isolation图检测更改是否有用",{"0":{"1150":1}}],["使用更好的优化器或更好的学习率更新策略",{"2":{"1155":1}}],["使用更多的验证数据",{"2":{"1149":1}}],["使用更大batch",{"2":{"1132":1}}],["使用常见的正则化技术减少过度拟合通常很简单",{"2":{"1149":1}}],["使用从柯西分布中抽取的数字填充张量",{"2":{"1087":1}}],["使用从伯努利分布中采样的样本",{"2":{"835":1}}],["使用与",{"2":{"1087":1}}],["使用指针传递的方式交换两个数组的元素",{"2":{"1650":1}}],["使用指定的大小",{"2":{"1087":1}}],["使用指数衰减平均以丢弃遥远过去的历史",{"2":{"1048":1}}],["使用移动平均引入了一个新的超参数ρ",{"2":{"1048":1}}],["使用如权重衰减等其他正则化策略能够防止这种情况",{"2":{"1016":1}}],["使用正则表达式应用的简单模式匹配变",{"2":{"1015":1}}],["使用正弦和余弦函数",{"2":{"750":1}}],["使用预训练的weight",{"0":{"1009":1}}],["使用超过一个8x",{"2":{"977":1}}],["使用先前生成的符号作为附加输入",{"2":{"912":1}}],["使用上一步的预测来作为下一步的输入吗",{"2":{"895":1}}],["使用隐藏状态来传递信息",{"2":{"874":1}}],["使用步长对张量进行切片",{"2":{"832":1}}],["使用gap代替全连接层",{"2":{"816":1}}],["使用gpu训练的利用率很低",{"2":{"576":1}}],["使用batchnorm",{"2":{"807":1}}],["使用偏差",{"2":{"762":1}}],["使用的人多",{"2":{"1479":1}}],["使用的复杂性",{"2":{"746":1}}],["使用的训练算法可以利用所有可能的分词结果",{"2":{"603":1}}],["使用公式函数或者可学习向量得到每个token的位置编码",{"2":{"745":1}}],["使用moe进行embedding",{"0":{"737":1},"1":{"738":1,"739":1}}],["使用mask来掩盖相乘结果矩阵",{"2":{"67":1}}],["使用门控机制",{"2":{"733":1}}],["使用门控机制将长期记忆与核心分支的信息融合",{"2":{"229":1}}],["使用各种pooling策略",{"2":{"732":1}}],["使用每个token的",{"2":{"731":1}}],["使用第一个token作为表征",{"2":{"731":1}}],["使用第二步产生的权重和v矩阵进行相乘以获得自注意力机制的最终输出",{"2":{"465":1}}],["使用该词的上下文来表示这个单词的特征",{"2":{"713":1}}],["使用lambda进行条件过滤",{"2":{"1883":1}}],["使用llm来生成embedding是近来的趋势",{"2":{"711":1}}],["使用lognlog⁡𝑛log⁡𝑛缩放注意力可以在一定程度上缓解这个问题",{"2":{"194":1}}],["使用稀疏梯度",{"2":{"702":1}}],["使用稀疏自编码器能从单层transformer模型中提取大量可解释的特征",{"2":{"137":1}}],["使用embedding来编码就是使用token",{"2":{"700":1}}],["使用encoderlayer对输入x进行逐层处理",{"2":{"522":1}}],["使用encoder类来实现编码器",{"2":{"522":1}}],["使用线性投影进行计算",{"2":{"620":1}}],["使用交叉注意力机制管理交互",{"2":{"617":1}}],["使用交叉注意力让源序列与目标序列对齐",{"2":{"39":1}}],["使用retain",{"2":{"1099":1}}],["使用reinforce方法",{"2":{"218":1}}],["使用r256×hεr256×hεr^",{"2":{"614":1}}],["使用下标运算符",{"2":{"1634":1}}],["使用下标来访问",{"2":{"1624":1}}],["使用下标访问",{"2":{"1624":1}}],["使用下标",{"2":{"614":1}}],["使用具有2字节上下文的小型cnn字节级模型对熵进行预测",{"2":{"613":1}}],["使用字节对编码",{"2":{"613":1}}],["使用小字节语言模型的动态熵patch",{"2":{"613":1}}],["使用torch",{"2":{"1214":1}}],["使用token时",{"2":{"612":1}}],["使用transformer来进行文本生成其实就是用模型来预测下一个词",{"2":{"431":1}}],["使用语言模型",{"2":{"604":2}}],["使用词表",{"0":{"558":1},"2":{"556":1}}],["使用突触前可塑性规则更新神经元与星形胶质细胞之间的权重",{"2":{"489":1}}],["使用固定位精度和o",{"2":{"480":1}}],["使用q和k矩阵的点积来决定了每个q与每个k的对齐程度",{"2":{"464":1}}],["使用点运算符",{"2":{"1728":1}}],["使用点积来计算注意力分数",{"2":{"464":1}}],["使用点乘会更加灵活方便计算",{"2":{"175":1}}],["使用0号gpu进行单机训练",{"2":{"422":1}}],["使用wqwqw^q",{"2":{"417":1}}],["使用平滑后的标签计算损失",{"2":{"399":1}}],["使用dropout会导致计算资源的增加和效率的降低",{"2":{"396":1}}],["使用dropout操作固然可以增加模型的泛化性",{"2":{"396":1}}],["使用dropout进行随机置0",{"2":{"113":1}}],["使用灵活的模板来平衡自然理解与结构一致性",{"2":{"369":1}}],["使用深度残差网络",{"2":{"301":1}}],["使用c++17的以下特性",{"2":{"1933":1}}],["使用cp",{"2":{"976":1}}],["使用clones",{"2":{"532":1}}],["使用clone",{"2":{"522":1}}],["使用cnn模型结合注意力机制进行操作",{"2":{"290":1}}],["使用cross",{"2":{"200":1}}],["使用端到端结构",{"2":{"283":1}}],["使用编码器decoder进行解码",{"2":{"450":1}}],["使用编码器encoder进行编码",{"2":{"450":1}}],["使用编码器",{"2":{"281":1}}],["使用列表的成本也会越来越高",{"2":{"273":1}}],["使用注意力权重w作为权重",{"2":{"267":1}}],["使用注意力机制融合这些信息",{"2":{"229":1}}],["使用注意力机制进行数据处理",{"2":{"229":1}}],["使用这种权重与原始信息相乘就得到了注意力处理后的加权信息",{"2":{"261":1}}],["使用这个意外分数",{"2":{"230":1}}],["使用神经网络来拟合",{"2":{"242":1}}],["使用神经长期记忆模块来存储和回忆历史信息",{"2":{"229":1}}],["使用可能更陡峭的",{"2":{"1173":1}}],["使用可训练参数a",{"2":{"343":1}}],["使用可训练的键值查找机制向模型添加额外的参数",{"2":{"154":1}}],["使用可学习但不依赖于数据的参数来编码任务相关知识",{"2":{"229":1}}],["使用依靠",{"2":{"225":1}}],["使用某个数据集对某个模型进行微调训练",{"2":{"222":1}}],["使用左积",{"2":{"216":1}}],["使用了一种双模型方法来解决这个问题",{"2":{"1315":1}}],["使用了一种块稀疏的注意力模式",{"2":{"204":1}}],["使用了",{"2":{"1210":1}}],["使用了数据增强技术",{"2":{"1154":1}}],["使用了较小的batch",{"2":{"1154":1}}],["使用了transformer架构",{"2":{"719":1}}],["使用了深度神经网络",{"2":{"231":1}}],["使用了nn",{"2":{"110":1}}],["使用size作为输入维度",{"2":{"344":1}}],["使用softmax",{"2":{"1016":1}}],["使用softmax操作将权值进行归一化",{"2":{"271":1}}],["使用softmax进行归一化",{"2":{"267":1}}],["使用softmax的输出作为权重",{"2":{"63":1}}],["使用svf和rl来学习",{"2":{"224":1}}],["使用svd将llm的",{"2":{"224":1}}],["使用svd分解训练",{"2":{"218":1}}],["使用self",{"2":{"200":1}}],["使用主要记忆",{"2":{"143":1}}],["使用fnn替代rnn有个好处就是可以避免参数稀疏化",{"2":{"119":1}}],["使用funtional中relu函数进行激活",{"2":{"113":1}}],["使用nn",{"2":{"113":1}}],["使用np",{"2":{"74":1}}],["使用unsqueeze",{"2":{"66":1,"382":1}}],["使用掩码矩阵把将补全的位置给掩盖掉",{"2":{"62":1}}],["使用掩码的自注意力机制就叫做掩码自注意力机制",{"2":{"50":1}}],["使用自动求导",{"2":{"1123":1}}],["使用自动微分进行梯度计算仅在每个使用的基本函数都是可微分的情况下有效",{"2":{"1115":1}}],["使用自产自销的策略",{"2":{"894":1}}],["使用自注意力机制把隐藏状态h进行转换",{"2":{"289":1}}],["使用自注意力找到输出序列自身内部的语义关系",{"2":{"39":1}}],["使用自己的信息预测自己",{"2":{"58":1}}],["使用",{"0":{"81":1,"586":1,"1989":1,"1990":1,"1991":1},"1":{"82":1,"83":1,"587":1,"588":1},"2":{"49":1,"77":3,"183":1,"356":1,"595":1,"624":1,"746":1,"776":1,"1214":2,"1313":1,"1320":2,"1364":1,"1524":1,"1554":1,"1584":1,"1589":1,"1594":1,"1606":1,"1607":5,"1612":3,"1615":2,"1620":2,"1621":3,"1623":1,"1628":1,"1638":2,"1647":5,"1656":2,"1668":2,"1669":1,"1671":1,"1683":2,"1693":2,"1695":1,"1698":1,"1699":1,"1700":1,"1709":1,"1712":1,"1713":2,"1714":1,"1715":1,"1762":1,"1764":2,"1766":1,"1797":1,"1813":1,"1820":3,"1831":1,"1838":3,"1873":2,"1891":2,"1897":1,"1907":1,"1911":1,"1912":1,"1914":1,"1916":3,"1917":1,"1933":1,"1965":1,"1969":1,"1984":1,"1997":1}}],["使用重叠",{"2":{"41":1}}],["使用权重矩阵wowow^o以确保生成的上下文向量恢复为原",{"2":{"16":1}}],["此项就是反向传播",{"2":{"1450":1}}],["此项其实就是前向传播",{"2":{"1450":1}}],["此项会最小化",{"2":{"145":1}}],["此调度器的默认行为遵循",{"2":{"1242":1}}],["此调度器不可链式使用",{"2":{"1242":1}}],["此衰减可以与来自调度器外部的学习率的其他变化同时发生",{"2":{"1235":1,"1236":1,"1237":1,"1238":1}}],["此过程假设不仅可以",{"2":{"1156":1}}],["此激活函数早在",{"2":{"844":1}}],["此激活函数的特点是随着",{"2":{"844":1}}],["此阶段未在任何文本对数据上训练",{"2":{"726":1}}],["此阶段涉及对",{"2":{"485":1}}],["此阶段涉及调整或者改变",{"2":{"148":1}}],["此刻的",{"2":{"683":1}}],["此刻需要注意的是",{"2":{"533":1}}],["此解耦方式使得输入数据可以与可变数量的参数",{"2":{"621":1}}],["此分词法会将单词拆分为单个字符和特殊符号",{"2":{"566":1}}],["此流程比较复杂",{"2":{"556":1}}],["此步骤会把段落等裁剪成短文本",{"2":{"545":1}}],["此步骤会把语料库中的文字去重",{"2":{"545":1}}],["此步骤对应下图的标号6",{"2":{"515":1}}],["此步骤对应下图的标号5",{"2":{"515":1}}],["此步骤对应下图的标号4",{"2":{"515":1}}],["此步骤对应下图的标号3",{"2":{"515":1}}],["此步骤对应下图的标号2",{"2":{"515":1}}],["此步骤对应下图的标号1",{"2":{"515":1}}],["此权重可以看做是q",{"2":{"537":1}}],["此输出是一次性全部输出",{"2":{"528":1}}],["此编码执行类似编码器中全局自注意力层的工作",{"2":{"525":1}}],["此循环的过程相当于输出的x经过了n个编码器层的逐步处理",{"2":{"522":1}}],["此论文在prompteol",{"2":{"736":1}}],["此论文使用其开源的sonar作为句子嵌入的解码器和编码器",{"2":{"629":1}}],["此论文提出了两种方法",{"2":{"287":1}}],["此论文针对这两个问题进行处理",{"2":{"287":1}}],["此论文也把知识抽象成如下三元组",{"2":{"122":1}}],["此问题在rnn和cnn中都存在",{"2":{"256":1}}],["此公式中",{"2":{"173":1}}],["此公式也可以转换为",{"2":{"109":3}}],["此后输入会被进一步逐层处理",{"2":{"501":1}}],["此后学习率会下降",{"2":{"372":1}}],["此后",{"2":{"154":1}}],["此",{"2":{"148":1,"676":1}}],["此类方法一般会使用一套习得的知识表示来对大模型的输出或中间结果进行增强或替换",{"2":{"141":1}}],["此函数相对简单",{"2":{"99":1}}],["此掩码用于解码器中的交叉注意力",{"2":{"84":1}}],["此逻辑是为了训练特殊打造",{"2":{"81":1}}],["此外在解码头的",{"2":{"1315":1}}],["此外还有一些其它方法",{"2":{"751":1}}],["此外",{"2":{"71":1,"93":1,"122":1,"131":1,"139":1,"148":1,"153":2,"217":1,"222":2,"355":1,"367":1,"477":1,"484":1,"485":1,"489":1,"501":1,"502":1,"507":1,"636":1,"692":2,"738":1,"739":2,"756":1,"761":1,"838":1,"937":1,"953":1,"957":1,"977":1,"985":1,"1137":1,"1143":1,"1151":1,"1186":1,"1214":1,"1364":1}}],["此处",{"2":{"1003":1}}],["此处有歧义",{"2":{"976":1}}],["此处有两层含义或者说思考的维度",{"2":{"684":1}}],["此处分类模糊",{"2":{"767":1}}],["此处不是三角函数而是学习出来的",{"2":{"722":1}}],["此处不再赘述",{"2":{"75":1}}],["此处作用是把词本身的信息投射到embedding空间",{"2":{"698":1}}],["此处重新展示是为了让读者更好理解",{"2":{"698":1}}],["此处重点突出了编码器和解码器内部以及之间的关联",{"2":{"515":1}}],["此处降维指的是降低向量的总体维度",{"2":{"682":1}}],["此处的",{"2":{"1184":1}}],["此处的w为",{"2":{"1004":1}}],["此处的核心技术是retromae训练策略",{"2":{"726":1}}],["此处的增维指的是用更多的维度来表示语义信息",{"2":{"682":1}}],["此处的矩阵乘ah和transformer的注意力计算方式有些近似",{"2":{"289":1}}],["此处同样表示使用线性投影计算的token和参数之间的交互",{"2":{"620":1}}],["此处交叉注意力k",{"2":{"615":1}}],["此处与",{"2":{"614":1}}],["此处去除了",{"2":{"582":1}}],["此处可以说是transformer最大的创新",{"2":{"535":1}}],["此处结合代码对交叉注意力再进行深入分析",{"2":{"534":1}}],["此处是把绝对位置编码",{"2":{"760":1}}],["此处是解码器的解码功能的实现",{"2":{"538":1}}],["此处是编码器层的编码功能",{"2":{"538":1}}],["此处是编码器的编码功能",{"2":{"538":1}}],["此处是src",{"2":{"533":1}}],["此处是作用对象的另一种解读",{"2":{"323":1}}],["此处进行了简化",{"2":{"520":1}}],["此处我们主要介绍",{"2":{"482":1}}],["此处假设是",{"2":{"431":1}}],["此处假定batch大小是8",{"2":{"379":1}}],["此处会设置batch",{"2":{"372":1}}],["此处会改变",{"2":{"28":1}}],["此处要求传入的",{"2":{"344":1}}],["此处参考了",{"2":{"280":1}}],["此处操作对于上图的标号2",{"2":{"199":1}}],["此处操作对于上图的标号1",{"2":{"199":1}}],["此处也回答了为何要区分q",{"2":{"172":1}}],["此处也会和注意力机制进行一定的比对",{"2":{"101":1}}],["此处搬运出现问题",{"2":{"80":1}}],["此处把layernorm放到了前面",{"2":{"344":1}}],["此处把创建掩码矩阵和应用掩码矩阵合二为一",{"2":{"67":1}}],["此处把batch类中关于mask的部分拿出来再进行分析",{"2":{"66":1,"74":1}}],["此处对应下图标号1",{"2":{"498":1}}],["此处对应图上序号8",{"2":{"35":1}}],["此处对应图上的序号7",{"2":{"35":1}}],["此处对应图上的序号6",{"2":{"35":1}}],["此处对应图上的序号5",{"2":{"35":1}}],["此处对应图上的序号4",{"2":{"32":1}}],["此处对应图上的序号3",{"2":{"31":1}}],["此处对应图上的序号2",{"2":{"27":1}}],["此处对应图上的序号1",{"2":{"26":1}}],["此处对于上图的标号4",{"2":{"199":1}}],["此处对于上图的标号3",{"2":{"199":1}}],["此处对于下图标号3",{"2":{"16":1}}],["此处对于下图的标号4",{"2":{"16":1}}],["此处对于下图的标号2",{"2":{"16":1}}],["此处对于下图的标号1",{"2":{"16":1}}],["此时取",{"2":{"2018":2}}],["此时访问",{"2":{"1728":1}}],["此时保存的模型中都包含哪些内容呢",{"2":{"1264":1}}],["此时保存后的模型可以直接给他人使用吗",{"2":{"1264":1}}],["此时eager",{"2":{"1214":1}}],["此时使用超参数一词是不合适的",{"2":{"1185":1}}],["此时因为后续是卷积核来进行特征的提取",{"2":{"1019":1}}],["此时经过多个前馈层",{"2":{"994":1}}],["此时输入的x就是图像的特征",{"2":{"882":1}}],["此时输入权重为零",{"2":{"305":1}}],["此时word",{"2":{"717":1}}],["此时就相当于得到了我们想要的",{"2":{"709":1}}],["此时的子词频数表如下",{"2":{"579":1}}],["此时token仅仅是字符串",{"2":{"545":1}}],["此时true",{"2":{"399":1}}],["此时batch类会提供数据和掩码",{"2":{"538":1}}],["此时这个输出向量可以看作是考虑了memory中各个位置编码信息的输出向量",{"2":{"537":1}}],["此时得到的结果是注意力权重",{"2":{"464":1}}],["此时还会加上位置编码信息",{"2":{"431":1}}],["此时还未出现",{"2":{"409":1}}],["此时训练速度很慢",{"2":{"402":1}}],["此时让学习率按指数的方式衰减",{"2":{"402":1}}],["此时先让学习率线性增长到某个最大的值",{"2":{"402":1}}],["此时模型很自信的说结果就是",{"2":{"399":1}}],["此时模型还不太会预测",{"2":{"399":1}}],["此时随机",{"2":{"394":1}}],["此时通过注意力机制",{"2":{"122":1}}],["此时swish简化为swish",{"2":{"110":1}}],["此时是把padding",{"2":{"67":1}}],["此时q是输入x",{"2":{"533":1}}],["此时q",{"2":{"36":1}}],["此时",{"2":{"10":1,"270":1,"529":1,"878":1,"898":1,"1153":1,"1462":1,"1728":1,"1911":1}}],["组装家具时的说明书",{"2":{"2097":1}}],["组装说明书和组装过程中的玩具",{"2":{"524":1}}],["组织大型项目",{"0":{"1978":1},"1":{"1979":1,"1980":1,"1981":1,"1982":1}}],["组织自定义头文件",{"2":{"1628":1}}],["组织机构等",{"2":{"906":1}}],["组和其他人可读执行",{"2":{"1513":1}}],["组成一个从前到后",{"2":{"1466":1}}],["组成",{"2":{"769":1,"785":1}}],["组成的可迭代对象",{"2":{"1226":1}}],["组成的tensor",{"2":{"1087":1}}],["组成的",{"2":{"249":1}}],["组内共用kv",{"2":{"652":1}}],["组合在一起",{"2":{"1674":1}}],["组合成一张有向无环图",{"2":{"785":1}}],["组合的意义不在于构成它的基本单元",{"2":{"446":1}}],["组合学习到的专家向量",{"2":{"224":1}}],["组合多个头的结果可以捕捉到不同子空间中的信息",{"2":{"21":1}}],["组合多头注意力可以捕捉多种依赖关系",{"2":{"9":1}}],["组件可能在多个任务中共享",{"2":{"224":1}}],["组件的信号",{"2":{"224":1}}],["组query",{"2":{"16":1}}],["拼接语",{"2":{"560":1}}],["拼接起来一起输入到解码器",{"2":{"529":1}}],["拼接的输出将通过ffn",{"2":{"466":1}}],["拼接成解码器的新输入",{"2":{"426":1}}],["拼接成一个超长的输入序列",{"2":{"377":1}}],["拼接相似度",{"2":{"175":3}}],["拼接在一个",{"2":{"89":1}}],["拼接之后",{"2":{"71":1}}],["拼接后的",{"2":{"19":1}}],["拼接",{"0":{"465":1},"2":{"16":2,"239":1,"285":1,"1930":1}}],["但时间有限",{"2":{"2133":1}}],["但你的预算有限",{"2":{"2129":1}}],["但尽管如此",{"2":{"2056":1}}],["但眼泪并不是因为难过而流",{"2":{"2054":1}}],["但孔子的这个",{"2":{"2054":1}}],["但孔老夫子所言的学可不是狭义的学",{"2":{"2054":1}}],["但依旧让我回味无穷",{"2":{"2056":1}}],["但依旧还是有点问题",{"2":{"2054":1}}],["但依然会存在末尾截断",{"2":{"90":1}}],["但老师这句话就是孔子真正意义上的广义的",{"2":{"2054":1}}],["但就我的理解来说答案是肯定的",{"2":{"2054":1}}],["但我希望大家的日程安排是顺其自然",{"2":{"2134":1}}],["但我觉得我们追求严谨并非是件好事",{"2":{"2117":1}}],["但我觉得理解这个",{"2":{"2054":1}}],["但我觉得",{"2":{"2054":1}}],["但我在成长的过程中会更加像第二家老板学习",{"2":{"2051":1}}],["但我们能够按顺序运行许多试验",{"2":{"1175":1}}],["但我们相信尝试一些",{"2":{"1171":1}}],["但我们不相信这在通常情况下是正确的",{"2":{"1158":1}}],["但我们通常不希望以这种方式选择学习率",{"2":{"1149":1}}],["但我们还涉及深度学习学习的其他方面",{"2":{"1126":1}}],["但我们建议您熟悉它",{"2":{"1112":1}}],["但我们希望𝐻𝑖不要有太大改变",{"2":{"194":1}}],["但我们仍然面临着模型如何存储知识以及如何利用和表达知识的问题",{"2":{"142":1}}],["但我们知道一个记忆网络是所有值向量的加权",{"2":{"129":1}}],["但没必要",{"2":{"2051":1}}],["但没有一个团队录取他",{"2":{"628":2}}],["但没有找到原始出处",{"2":{"339":1}}],["但没有表示相应的关系",{"2":{"135":1}}],["但针对不同的k",{"2":{"2031":1}}],["但类型安全",{"2":{"1928":1}}],["但更安全",{"2":{"1927":1}}],["但更加抽象和安全",{"2":{"1718":1}}],["但至少需要一个非递归的返回语句来启动推导",{"2":{"1905":1}}],["但至少意味着经过训练的模型在验证集上的性能比随机机会好得多",{"2":{"1137":1}}],["但资源未被销毁",{"2":{"1891":1}}],["但完全不希望暴露基类的接口",{"2":{"1868":1}}],["但输入了非数字字符",{"2":{"1814":1,"1832":1}}],["但输出的最后一个token对应的向量是512维的向量",{"2":{"473":1}}],["但提供了更多的功能和类型安全",{"2":{"1802":1}}],["但该特性应谨慎使用",{"2":{"1793":1}}],["但最终都需要在硬件上实现效果",{"2":{"2010":1}}],["但最终投影层的输出维度也更大",{"2":{"612":1}}],["但最好将枚举类型视为独立的类型",{"2":{"1728":1}}],["但要遵循命名规范",{"2":{"1728":1}}],["但要求完全相信某些东西会有所帮助也不是正确的要求",{"2":{"1152":1}}],["但现实世界的数据往往更加复杂",{"2":{"1728":1}}],["但被声明为类的友元",{"2":{"1712":1}}],["但内联函数是类型安全的",{"2":{"1709":1}}],["但长期运行的程序可能会耗尽内存",{"2":{"1671":1}}],["但受保护的成员可以访问",{"2":{"1654":1}}],["但过度使用全局变量会降低代码的模块化程度",{"2":{"1649":1}}],["但作用域仍然是局部的",{"2":{"1649":1}}],["但作为验证错误函数的人在循环计划是脆弱的并且不容易重现",{"2":{"1173":1}}],["但有时我们需要在程序运行的过程中动态地申请内存",{"2":{"1647":1}}],["但有一些对象是无论如何都不能丢失的",{"2":{"1477":1}}],["但有一些共同的特征",{"2":{"1227":1}}],["但某些问题的迭代实现可能比较复杂",{"2":{"1646":1}}],["但分号",{"2":{"1621":1}}],["但空格在字符串中也是常用的字符",{"2":{"1616":1}}],["但指针本身可以指向其他地址",{"2":{"1614":1}}],["但具体大小和精度取决于编译器和平台",{"2":{"1607":1}}],["但具有指定形状的张量",{"2":{"819":1}}],["但体积较大",{"2":{"1605":1}}],["但换来了更快的开发速度和更简洁的语法",{"2":{"1602":1}}],["但开发效率高",{"2":{"1602":1}}],["但会导致一定的同步开销",{"2":{"1574":1}}],["但扩展性有限",{"2":{"1568":1}}],["但同一时刻只能被一个进程或线程使用的资源",{"2":{"1412":1}}],["但同时一个前馈网络的输出还包括了来自前馈网络之前的残差信息",{"2":{"306":1}}],["但同时也带来注意力分数矩阵的低秩化削弱了表达能力",{"2":{"44":1}}],["但又不会聚成超小的小簇",{"2":{"1375":1}}],["但另一方面",{"2":{"1318":1}}],["但超参数这个术语在深度学习社区中已经变得极为通俗",{"2":{"1185":1}}],["但90",{"2":{"1184":1}}],["但每次都会有不一样的感触",{"2":{"2056":1}}],["但每执行一次训练都要进行一次评估",{"2":{"1179":1}}],["但每一类其实又能衍生出各种各样的变种",{"2":{"746":1}}],["但肯定有可能找到最先进的贝叶斯优化技术可以击败两倍预算随机搜索的搜索空间和问题",{"2":{"1175":1}}],["但往往仅试验方差就能在使用相同超参数设置的两个不同的训练模型之间产生统计上的显著差异",{"2":{"1152":1}}],["但往往不能满足第二个条件",{"2":{"180":1}}],["但套用杰弗里",{"2":{"1151":1}}],["但还有许多其他的行为可以通过检查训练曲线而变得明显",{"2":{"1149":1}}],["但还是有本质区别的",{"2":{"676":1}}],["但检查训练曲线是识别常见故障模式的简单方法",{"2":{"1149":1}}],["但代价是显而易见的",{"2":{"1148":1}}],["但值",{"2":{"1143":1}}],["但对着镜子又看不到自己",{"2":{"2056":1}}],["但对于某些类别的超参数",{"2":{"1143":1}}],["但对计算机却是困难的",{"2":{"678":1}}],["但通常会包含一些更复杂的任务",{"2":{"1317":1}}],["但通常",{"2":{"1157":1}}],["但通常应该假设优化器超参数必须单独调整",{"2":{"1143":1}}],["但通常我们的最大资源预算低于调整所有非目标超参数所需的计算资源",{"2":{"1143":1}}],["但通过声明友元函数或友元类",{"2":{"1769":1}}],["但通过",{"2":{"976":1,"1729":1}}],["但通过一些先进的技术和模型变种",{"2":{"413":1}}],["但实际存储可能只用一个比特位",{"2":{"1607":1}}],["但实际上这是不可能的",{"2":{"692":1}}],["但实际上每一个头",{"2":{"28":1}}],["但实践中这往往不实际",{"2":{"1139":1}}],["但深度学习在工程领域仍处于起步阶段",{"2":{"1127":1}}],["但启用推断模式将使得",{"2":{"1121":1}}],["但仍然偏向过程式编程",{"2":{"1603":1}}],["但仍然很底层",{"2":{"1603":1}}],["但仍希望在后续的梯度模式中使用这些计算的输出时",{"2":{"1120":1}}],["但仍显笨拙",{"2":{"284":1}}],["但仅对叶张量设置它才有意义",{"2":{"1117":1}}],["但仅进行原始训练步骤的一小部分α",{"2":{"938":1,"954":1}}],["但出于性能原因",{"2":{"1115":1}}],["但相对更稳定",{"2":{"1027":1}}],["但相较于bert模型直接生成向量来说",{"2":{"736":1}}],["但据我们所知",{"2":{"977":1}}],["但重新组织计算",{"2":{"975":1}}],["但重新设计了相对位置矩阵的构建",{"2":{"204":1}}],["但所取得的结果之间存在着巨大的差距",{"2":{"1127":1}}],["但所需的kv缓存显著减少",{"2":{"956":1}}],["但所有已知的实现都需要以速度换取内存",{"2":{"946":1,"966":1}}],["但总的运行时间还是加速的",{"2":{"945":1,"965":1}}],["但需要程序员自己管理",{"2":{"1648":1}}],["但需要更细致的同步控制",{"2":{"1574":1}}],["但需要有配对好的文本集才能训练出对应的模型",{"2":{"885":1}}],["但需要仔细使用和后续分析才能得出严格的结论",{"2":{"477":1}}],["但elu",{"2":{"843":1}}],["但处处",{"2":{"838":1}}],["但您不应依赖于复制与视图行为",{"2":{"819":1}}],["但保留重要的特征信息",{"2":{"813":1}}],["但惩罚过于绝对",{"2":{"766":1}}],["但确实有些场景下绝对位置信息更有帮助",{"2":{"763":1}}],["但都是无监督学习",{"2":{"734":1}}],["但目标函数其实是不同的",{"2":{"721":1}}],["但迁移后需要重新计算出输入的表征",{"2":{"718":1}}],["但随着特征数目的增加",{"2":{"708":1}}],["但随着稀疏自编码器的规模扩大",{"2":{"137":1}}],["但由于",{"2":{"1874":1}}],["但由于不稳定的影响",{"2":{"1178":1}}],["但由于其名称",{"2":{"1116":1}}],["但由于大大减少了对hbm的访问量",{"2":{"940":1,"962":1}}],["但由于它们的大小存在差异",{"2":{"692":1}}],["但由于神经元的交流方式",{"2":{"488":1}}],["但欧式距离并不是尺度不变的",{"2":{"692":1}}],["但向量之间的关系承载着关于心理测量的意义",{"2":{"691":1}}],["但runing",{"2":{"668":1}}],["但reshape和permute这些是不会变的",{"2":{"658":1}}],["但=none的话会释放显存",{"2":{"659":1}}],["但用了就会丢弃",{"2":{"659":1}}],["但数据共用",{"2":{"662":1}}],["但数据不连续不能使用view",{"2":{"658":1}}],["但数据的分布形状和范围保持不变",{"2":{"320":1}}],["但全部可能的文本句子",{"2":{"636":1}}],["但全连接层在计算负载中占据主导地位",{"2":{"152":1}}],["但抽象空间与语言或模态无关",{"2":{"628":1}}],["但缺乏真正的理解",{"2":{"627":1}}],["但角色相反",{"2":{"615":1}}],["但比仅使用纯字节更有效",{"2":{"606":1}}],["但整体上",{"2":{"576":1}}],["但并非总是如此",{"2":{"1114":1}}],["但并非最优解",{"2":{"155":1}}],["但并不一定在所有情况下都表现更好",{"2":{"838":1}}],["但并不意味着一定会提升下游任务效果",{"2":{"560":1}}],["但也有一些运算符是不能重载的",{"2":{"1712":1}}],["但也有一些问题适合用经典的rnn结构建模",{"2":{"879":1}}],["但也限制了某些高级操作的可能性",{"2":{"1611":1}}],["但也常常让新手感到困惑",{"2":{"1611":1}}],["但也可能在某些情况下带来性能上的开销",{"2":{"1602":1}}],["但也可能在处理不带换行符的多行输入时引入",{"2":{"553":1}}],["但也带来了内存泄漏等风险",{"2":{"1602":1}}],["但也许实际情况并非如此",{"2":{"1478":1}}],["但也不会离得太近",{"2":{"1375":1}}],["但也应该足够大",{"2":{"1165":1}}],["但也容易在训练数据上表现出很好的性能而在新数据上表现较差",{"2":{"1012":1}}],["但也容易产生过多的参数",{"2":{"1012":1}}],["但也会引入显著的开销",{"2":{"976":1}}],["但也会破坏模型学习生成所必需的更深层预测模式",{"2":{"542":1}}],["但也是很重要的",{"2":{"785":1}}],["但也包括向量上的动态计算",{"2":{"689":1}}],["但稍作改动",{"2":{"541":1}}],["但因为在自注意力机制中发生了信息交换",{"2":{"519":1}}],["但因为文字顺序不同",{"2":{"459":1}}],["但前提是至少三个神经元之间存在通信",{"2":{"488":1}}],["但不提供具体实现",{"2":{"1916":1}}],["但不希望外界知晓它使用了",{"2":{"1868":1}}],["但不希望用户直接调用原",{"2":{"1867":1}}],["但不希望对外暴露基类的接口",{"2":{"1867":1}}],["但不将其从流中移除",{"2":{"1813":1,"1831":1}}],["但不推荐",{"2":{"1708":1}}],["但不包括其子模块",{"2":{"1214":1}}],["但不是万无一失的",{"2":{"1175":1}}],["但不是全部",{"2":{"1042":1}}],["但不会显着减少验证误差",{"2":{"1154":1}}],["但不会过拟合训练数据",{"2":{"230":1}}],["但不限于",{"2":{"1130":1}}],["但不能访问派生类中新增的成员",{"2":{"1786":1}}],["但不能直接进行指针算术运算",{"2":{"1633":1}}],["但不能保证",{"2":{"1115":1}}],["但不能跨越文档边界",{"2":{"614":1}}],["但不保存",{"2":{"1106":1}}],["但不可否认bpe是一个性能非常高效的tokenizer算法",{"2":{"576":1}}],["但不同的特征服从不同的动态",{"2":{"477":1}}],["但一次只能存放一件",{"2":{"1728":1}}],["但一般不建议过度嵌套",{"2":{"1631":1}}],["但一般场景下",{"2":{"78":1}}],["但一旦训练完成",{"2":{"715":1}}],["但一旦训练好的好",{"2":{"335":1}}],["但一个词在结尾有一个",{"2":{"579":1}}],["但仿真",{"2":{"301":1}}],["但其背后涉及到内存管理和字符编码等概念",{"2":{"1704":1}}],["但其不提供任何训练吞吐量优势",{"2":{"1132":1}}],["但其学习过程有时会很慢",{"2":{"1028":1}}],["但其性能无法与mha相媲美",{"2":{"956":1}}],["但其性能总差于",{"2":{"329":1}}],["但其其整体偏置易随相对位置大小波动",{"2":{"766":1}}],["但其解码器架构限制了其在没有进一步表示微调的情况下作为嵌入模型的潜力",{"2":{"738":1}}],["但其可扩展性受到了严重限制",{"2":{"618":1}}],["但其引入噪声会导致模型训练的不稳定性",{"2":{"396":1}}],["但其没有明确的机制从结构中推理",{"2":{"287":1}}],["但其实并不是对transformer结构做了改变",{"2":{"218":1}}],["但却有一个明显缺陷",{"2":{"245":1}}],["但当下一个点y为y+1的时候要d",{"2":{"2027":1}}],["但当不稳定迫使我们使用太小的学习率时",{"2":{"1179":1}}],["但当前的",{"2":{"222":1}}],["但当键正在不断训练并需要重新索引时",{"2":{"154":1}}],["但在任何时刻只能存储其中一种类型的值",{"2":{"1926":1}}],["但在不再使用时忘记使用",{"2":{"1647":1}}],["但在不能替换的情况下",{"2":{"1168":1}}],["但在高并行条件下",{"2":{"1175":1}}],["但在多设备设置中",{"2":{"1168":1}}],["但在模型开发阶段往往是不切实际的",{"2":{"1163":1}}],["但在某些应用中",{"2":{"1152":1}}],["但在重复实验后发现",{"2":{"1152":1}}],["但在试验中将验证误差减少到固定数字时我们必须小心",{"2":{"1149":1}}],["但在每个优化器中运行良好的值范围通常相差几个数量级",{"2":{"1143":1}}],["但在初始配置中添加它们可能会浪费时间调整无用的功能和",{"2":{"1137":1}}],["但在大众开始记录它并讨论各个步骤之前",{"2":{"1127":1}}],["但在非矩阵乘法fp32上只有19",{"2":{"968":1}}],["但在实际应用中很少见",{"2":{"1704":1}}],["但在实现上却更为复杂",{"2":{"964":1}}],["但在实践中很难实现",{"2":{"222":1}}],["但在生成过程中",{"2":{"956":1}}],["但在超出训练长度后表现更好的原因",{"2":{"765":1}}],["但在捕获复杂的语义和句法结构方面仍然存在局限性",{"2":{"711":1}}],["但在嵌入经过适当归一化时可以发挥作用",{"2":{"692":1}}],["但在概念",{"2":{"676":1}}],["但在处理长文本和复杂概念时",{"2":{"626":1}}],["但在有足够的训练和模型规模的情况下",{"2":{"542":1}}],["但在数学上",{"2":{"490":1}}],["但在我们的设计中",{"2":{"360":1}}],["但在前向传播中",{"2":{"296":1}}],["但在表达能力方面仍不及基于",{"2":{"212":1}}],["但在低维情况下",{"2":{"155":1}}],["但使用两个不同的种子进行quasi",{"2":{"1152":1}}],["但使用不同的权重矩阵为q和k提供了更大的灵活性和表示能力",{"2":{"172":1}}],["但使用联想记忆",{"2":{"154":1}}],["但计算机却毫无头绪",{"2":{"167":1}}],["但之前在其他模型上效果不甚理想",{"2":{"160":1}}],["但大小有限",{"2":{"1648":1}}],["但大多数工作都局限于原始的深度为2",{"2":{"155":1}}],["但大部分头之间的差异没有我们想的那么大",{"2":{"20":1}}],["但如果你继续往上爬",{"2":{"2115":1}}],["但如果未传递任何对象",{"2":{"1214":1}}],["但如果这个奏效",{"2":{"1184":1}}],["但如果进行了调整",{"2":{"1158":1}}],["但如果将它理解为",{"2":{"765":1}}],["但如果batch之间有很强的相关性",{"2":{"337":1}}],["但如果完全移除mlp层",{"2":{"147":1}}],["但如果知识的训练不够充分",{"2":{"147":1}}],["但如果ggg是因果图",{"2":{"94":1}}],["但直接将它们视作新一代知识库仍然存在某些局限",{"2":{"121":1}}],["但特征与神经元并不对应",{"2":{"118":1}}],["但理论上需要无限多的自由度来表达完整的光滑映射",{"2":{"116":1}}],["但隐藏维度",{"2":{"116":1}}],["但它简单高效",{"2":{"2121":1}}],["但它能让你快速做出决定",{"2":{"2101":1}}],["但它能记住之前的内容",{"2":{"216":1}}],["但它带来的改进却非常实用",{"2":{"1913":1}}],["但它仍然包含一个尾零",{"2":{"1704":1}}],["但它的特点是先执行循环体",{"2":{"1620":1}}],["但它的模型大小使其难以部署在低延迟需求的环境中",{"2":{"1315":1}}],["但它是稳定排序",{"2":{"1750":1}}],["但它是成为",{"2":{"1611":1}}],["但它是模块的状态的一部分",{"2":{"1211":1}}],["但它只是因为更多的样本被正确预测了吗",{"2":{"1165":1}}],["但它发生在所有试验中时",{"2":{"1149":1}}],["但它比欧式距离直观性差",{"2":{"692":1}}],["但它们只是将状态与参数组关联起来的id",{"2":{"1227":1}}],["但它们通常会过于复杂",{"2":{"1159":1}}],["但它们仍然共享相同的存储",{"2":{"1114":1}}],["但它们的jaccard相似度和精确匹配率较低",{"2":{"739":1}}],["但它们本质上还是一个字一个字地",{"2":{"627":1}}],["但它们更容易发生秩崩溃",{"2":{"93":1}}],["但它也涵盖了我们在工作中遇到",{"2":{"1127":1}}],["但它也可能陷入糟糕的局部最小值中",{"2":{"1025":1}}],["但它也大大降低了任务的复杂性",{"2":{"260":1}}],["但它也存在一个问题",{"2":{"104":1}}],["但它可能不会在长时间内持续让我们感到惊讶",{"2":{"230":1}}],["但它对语言理解任务至关重要",{"2":{"224":1}}],["但它基本是一种",{"2":{"214":1}}],["但它不涉及到i",{"2":{"176":1}}],["但速率会减缓",{"2":{"92":1}}],["但稀疏或局部掩码注意力可以证明减缓崩溃速率",{"2":{"91":1}}],["但可以通过数组名来访问和修改数组元素的值",{"2":{"1704":1}}],["但可以通过该指针修改指向的内存地址处的值",{"2":{"1614":1}}],["但可以通过多线程或协程实现并行化",{"2":{"1566":1}}],["但可以节省内存",{"2":{"1607":1}}],["但可以基于其独特的id执行不同的任务",{"2":{"1573":1}}],["但可以为空",{"2":{"380":1}}],["但可以把它们看作是每个注意力头的逻辑上独立的wq",{"2":{"30":1}}],["但可能导致代码膨胀",{"2":{"1632":1}}],["但可能导致训练崩溃和较差的性能",{"2":{"348":1}}],["但可能是模型状态的一部分",{"2":{"1214":1}}],["但可能不能完全代表生产环境",{"2":{"1163":1}}],["但可能有很多例外",{"2":{"1158":1}}],["但可能会丢失一些细节",{"2":{"815":1}}],["但可能存在明显的负载不均衡",{"2":{"90":1}}],["但这只能帮你解决简单的问题",{"0":{"2127":1}}],["但这只是默认行为",{"2":{"1728":1}}],["但这只是一个请求",{"2":{"1709":1}}],["但这需要非常小心",{"2":{"1728":1}}],["但这需要对编码器进行仔细调整",{"2":{"542":1}}],["但这仅仅是一种约定",{"2":{"1728":1}}],["但这会降低代码的可读性",{"2":{"1619":1}}],["但这会给我们的架构增加复杂性",{"2":{"977":1}}],["但这样做的代价是引入了另一个超参数",{"2":{"1041":1}}],["但这样模型融合的效果应该至少不差于单个注意力",{"2":{"14":1}}],["但这通常需要手动配置和特定的调整",{"2":{"985":1}}],["但这可能会使计算",{"2":{"976":1}}],["但这里说是",{"2":{"779":1}}],["但这个看似可行的设计",{"2":{"712":1}}],["但这是一种不太有效的练习方式",{"2":{"542":1}}],["但这是通过强行截断窗口外的注意力",{"2":{"204":1}}],["但这些数据不会同时使用时",{"2":{"1728":1}}],["但这些人脸是从未出现过的全新的人脸",{"2":{"1371":1}}],["但这些实现是较新的",{"2":{"1228":1}}],["但这些方面并不详尽",{"2":{"1126":1}}],["但这些方法大多忽视了全连接层的计算瓶颈",{"2":{"152":1}}],["但这些向量中的数字意味着什么尚不清楚",{"2":{"690":1}}],["但这些模块对提高模型的整体表示能力非常重要",{"2":{"467":1}}],["但这些嵌入不再直接与单个词元相关",{"2":{"437":1}}],["但这不会引入任何额外的信息",{"2":{"307":1}}],["但这次输入的x是tgt",{"2":{"83":1}}],["但那时的信息交换仅限于每个",{"2":{"41":1}}],["但",{"2":{"17":1,"610":1,"839":1,"1630":1}}],["但是并不能通用",{"2":{"1470":1}}],["但是几乎无法控制生成序列的风格",{"2":{"1316":1}}],["但是依然可以按模型结构将它们大致分为三类",{"2":{"1312":1}}],["但是根据已有的结论我们可以提出一些猜测",{"2":{"1158":1}}],["但是否使用正则化技术往往是目标或固定超参数",{"2":{"1143":1}}],["但是会带来精度上的损失",{"2":{"938":1,"954":1}}],["但是随着序列长度的增加",{"2":{"904":2}}],["但是既然叫",{"2":{"901":1}}],["但是长期的记忆影响就很小",{"2":{"861":1}}],["但是激活函数如",{"2":{"838":1}}],["但是图像风格化中",{"2":{"809":1}}],["但是一般会采用这个加速库",{"2":{"796":1}}],["但是一方面也扼杀了翻译多样性的可能",{"2":{"411":1}}],["但是基本上都可以整理成如下形式",{"2":{"766":1}}],["但是基于马尔科夫假设的模型难以处理句子中的长距离依赖关系",{"2":{"242":1}}],["但是经过softmax变化之后",{"2":{"765":1}}],["但是position和token之间所包含的信息不同",{"2":{"764":1}}],["但是殊途同归",{"2":{"757":1}}],["但是距离",{"2":{"744":1,"757":1}}],["但是还不能生成高质量的句子级别的表征",{"2":{"734":1}}],["但是直接利用其生成文本表征的效果往往比较糟糕",{"2":{"730":1}}],["但是word2vec只是利用和挖掘了",{"2":{"715":1}}],["但是下面有一些该词的描述句子",{"2":{"713":1}}],["但是计算机并没有办法直接的从感知抽象出每个语言符号的意义",{"2":{"689":1}}],["但是计算量却小很多",{"2":{"14":1}}],["但是可能依然会出现类似独热编码的问题",{"2":{"684":1}}],["但是究竟多少维度适合",{"2":{"684":1}}],["但是ascii只能告诉我们这个词是什么",{"2":{"678":1}}],["但是数值计算的方式有很多",{"2":{"677":1}}],["但是他的材料",{"2":{"668":1}}],["但是backward对我们用户是不可见的",{"2":{"661":1,"1104":1}}],["但是现在的nlp语言模型无法直接的从感知中抽象出每个语言符号的意义",{"2":{"545":1}}],["但是训练时候降低了学习难度",{"2":{"542":1}}],["但是quasi",{"2":{"1175":1}}],["但是q",{"2":{"533":1}}],["但是每次推理之后",{"2":{"530":1}}],["但是为了保持与训练时的计算一致",{"2":{"525":1}}],["但是为了保持一致",{"2":{"81":1}}],["但是sublayerconnection的实现和论文略有不同",{"2":{"523":1}}],["但是当batch",{"2":{"807":1}}],["但是当网络的层数较深时",{"2":{"495":1}}],["但是当eieie",{"2":{"326":1}}],["但是简单的并行推理会导致模型无法提供深度信息",{"2":{"480":1}}],["但是我们并不知道他的原理是什么",{"2":{"1470":1}}],["但是我们可以看具体的例子",{"2":{"1177":1}}],["但是我们会发现一个问题",{"2":{"1036":1}}],["但是我们所期望的是语义搜索",{"2":{"678":1}}],["但是我们有必要看看研究人员如何从理论",{"2":{"474":1}}],["但是我们知道在",{"2":{"344":1}}],["但是劣势是缺少位置信息",{"2":{"457":1}}],["但是又有不同之处",{"2":{"443":1}}],["但是新词的来源不同",{"2":{"426":1}}],["但是token是通过压缩启发式方法生成的",{"2":{"612":1}}],["但是teacher",{"2":{"411":1}}],["但是transformer中的注意力分布其实是不均匀的",{"2":{"204":1}}],["但是却存在一个问题",{"2":{"409":1}}],["但是却破坏了模型原有的能力",{"2":{"222":1}}],["但是没啥用处",{"2":{"405":1}}],["但是没有一一对应的关系",{"2":{"245":1}}],["但是没有找到非常合适的系统的学习资料",{"2":{"235":1}}],["但是表述一定是一个词一个词说出来的",{"2":{"405":1}}],["但是效果等同于一个一个词输入到编码器进行串行解码",{"2":{"391":1}}],["但是nlp",{"2":{"376":1}}],["但是其语义相差甚远",{"2":{"679":1}}],["但是其连接了编码器和解码器",{"2":{"444":1}}],["但是其在训练的时候也需要时常关注梯度",{"2":{"335":1}}],["但是其存在一些问题",{"2":{"309":1}}],["但是它们依然可以被归纳到以下三种结构中",{"2":{"1314":1}}],["但是它们都有相同的事实信息",{"2":{"135":1}}],["但是它容易训练",{"2":{"335":1}}],["但是单独调整的话",{"2":{"334":1}}],["但是value仍然同时被用来编码上下文信息和下一个词的分布表示",{"2":{"288":1}}],["但是目前没有做到",{"2":{"284":1}}],["但是信息在过深的网络传播过程之中容易丢失",{"2":{"274":1}}],["但是第一个",{"2":{"259":1}}],["但是通常又有定语后置",{"2":{"252":1}}],["但是通过在不同的方向上叠加和组合这些维度",{"2":{"137":1}}],["但是面对几万字的小说",{"2":{"252":1}}],["但是深度就代表间接",{"2":{"247":1}}],["但是该方案中",{"2":{"245":1}}],["但是很难训练好",{"2":{"242":1}}],["但是这种训练任务难以让模型获得一个高质量的基于句子级别的句向量",{"2":{"727":1}}],["但是这还不够",{"2":{"260":1}}],["但是这个不是lora的特性",{"2":{"222":1}}],["但是这样的子词基本上无法表示相应的含义",{"2":{"580":1}}],["但是这样可以让模型不那么自信",{"2":{"399":1}}],["但是这样可能会有问题",{"2":{"172":1}}],["但是这样要看四遍",{"2":{"5":1}}],["但是从硬件的角度",{"2":{"185":1}}],["但是外面套了tanh和v",{"2":{"175":1}}],["但是如何在计算机中表示一个单词",{"2":{"678":1}}],["但是如何判断某个隐状态对当前生成词是否重要",{"2":{"267":1}}],["但是如何实现注意力机制",{"2":{"262":1}}],["但是如何实践",{"2":{"167":1}}],["但是如果直接拿来完成特定任务",{"2":{"1312":1}}],["但是如果直接使用score来挑选的话",{"2":{"904":1}}],["但是如果我们以单词为单位",{"2":{"576":1}}],["但是如果把",{"2":{"246":1}}],["但是如果输入的序列太长",{"2":{"53":1,"376":1,"933":1}}],["但是因为单个卷积核的长度一般比较小",{"2":{"247":1}}],["但是因为人的精力有限",{"2":{"169":1}}],["但是因为目标物品对应的权重高",{"2":{"163":1}}],["但是因为方便实现",{"2":{"69":1}}],["但是始终没有深入",{"2":{"162":1}}],["但是知易行难",{"2":{"160":1}}],["但是列表之中所有向量都考虑了其上下文关系",{"2":{"158":1}}],["但是编辑实体知识和关系知识并不完全等价",{"2":{"136":1}}],["但是集合中还有很多",{"2":{"135":1}}],["但是仍有很多问题没有被回答",{"2":{"122":1}}],["但是对于某些功能",{"2":{"256":1}}],["但是对于计算机或者对于模型来说",{"2":{"246":1}}],["但是对于自注意力来说",{"2":{"158":1}}],["但是对于",{"2":{"117":1}}],["但是在这个时间限制下的结论不一定适用于20",{"2":{"1157":1}}],["但是在最开始训练的时候需要每个参数有相应的初始值",{"2":{"988":1}}],["但是在中文里",{"2":{"566":1}}],["但是在模型变化剧烈时",{"2":{"402":1}}],["但是在",{"2":{"333":1}}],["但是在post",{"2":{"332":1}}],["但是在推理的输入只有一个实例",{"2":{"316":1}}],["但是在实际工作中不一定如此",{"2":{"468":1}}],["但是在实际中这并不可行",{"2":{"287":1}}],["但是在实际操作时还是需要加一个src",{"2":{"72":1}}],["但是在长上下文时",{"2":{"273":1}}],["但是在nlp的生成任务中",{"2":{"184":1}}],["但是在某些长序列任务中",{"2":{"175":1}}],["但是在处理复杂任务的时候",{"2":{"119":1}}],["但是在不同层中",{"2":{"99":1}}],["但是不保留",{"2":{"661":1,"1104":1}}],["但是不",{"2":{"90":1}}],["但是不需要防止",{"2":{"77":1}}],["但是后面的multiheadattention的qkv向量中的kv来自最后一层编码器的输入",{"2":{"72":1}}],["但是decoder",{"2":{"58":1}}],["但是decoder实质上是一个单向的自注意力结构",{"2":{"57":1}}],["但是多头注意力机制下",{"2":{"36":1}}],["但是多头中每个头的功能不同",{"2":{"13":1}}],["但是有一个点可以留意下",{"2":{"34":1}}],["但是有机融合是个复杂的情况",{"2":{"10":1}}],["但是性能不如大矩阵的方案",{"2":{"29":1}}],["但是特定的层会对特定的依存关系识别的比较好",{"2":{"20":1}}],["但是行数都是dmodeldmodeld",{"2":{"7":1}}],["但是无法有效反映外部丰富的世界",{"2":{"4":1}}],["但是",{"2":{"3":1,"5":1,"39":1,"118":1,"246":1,"247":1,"296":2,"299":1,"316":1,"332":1,"344":1,"405":1,"536":1,"560":1,"895":1,"908":1,"986":1,"1003":1,"1126":1,"1144":1,"1152":1,"1153":1,"1157":1,"1158":1,"1186":1,"2070":1}}],["但是也暴露出来了一些缺陷",{"2":{"3":1}}],["词或子词",{"2":{"700":1}}],["词的向量",{"2":{"694":1}}],["词的表示",{"2":{"694":1}}],["词的表示能通过简单的数学计算",{"2":{"689":1}}],["词义不相近的词量化值需要尽量",{"2":{"689":1}}],["词义相近词需要有",{"2":{"689":1}}],["词和embedding必须一一对应",{"2":{"689":1}}],["词和词之间的关系只能基于两个标量间的差值得到",{"2":{"679":1}}],["词与词之间复杂的关系便能在这一高维的空间中得到表达",{"2":{"679":1}}],["词与词之间的关系还不仅仅限于语义上的分类所导致的定位远近这么简单",{"2":{"4":1}}],["词是人类语言的抽象总结",{"2":{"678":1}}],["词是最自然的语言单元",{"2":{"565":1}}],["词袋模型",{"2":{"676":1}}],["词语分类",{"2":{"1315":1}}],["词语序列",{"2":{"885":1}}],["词语",{"2":{"627":1}}],["词缀",{"2":{"565":1}}],["词",{"2":{"565":1,"671":1,"689":2}}],["词性",{"2":{"512":1}}],["词典中有多少字",{"2":{"681":1}}],["词典中会得到5个词",{"2":{"567":1}}],["词典是无重复且有序的",{"2":{"679":1}}],["词典",{"2":{"591":1}}],["词典如何使用",{"2":{"558":1}}],["词典大小为6",{"2":{"399":1}}],["词典的大小",{"2":{"383":1,"702":1}}],["词元化",{"2":{"698":1}}],["词元分析器",{"2":{"454":1}}],["词元",{"2":{"363":1,"547":1,"548":1}}],["词向量嵌入发展综述",{"2":{"740":1}}],["词向量与embedding究竟是怎么回事",{"2":{"740":1}}],["词向量维度",{"2":{"700":1}}],["词向量只要能把相似单词聚在一起就完成了它的任务",{"2":{"698":1}}],["词向量是否越大越好",{"2":{"698":1}}],["词向量保存了词语的语言学信息",{"2":{"512":1}}],["词向量",{"2":{"428":1,"518":1}}],["词向量的维度应该怎么选择",{"2":{"740":1}}],["词向量的维度",{"2":{"326":1}}],["词向量经过注意力机制加权求和之后的表示",{"2":{"170":1}}],["词特征的维度由模型自行确定",{"2":{"316":1}}],["词在一篇文本中表达的意思通常与它的上下文有关",{"2":{"259":1}}],["词表id",{"2":{"700":1}}],["词表有多少词",{"2":{"700":1}}],["词表就是这两个矩阵之间的映射",{"2":{"694":1}}],["词表变化",{"2":{"604":1}}],["词表通常先增加后减小",{"2":{"582":1}}],["词表里多了一个新的子词",{"2":{"582":1}}],["词表总数通常先增大",{"2":{"576":1}}],["词表的embedding",{"2":{"698":1}}],["词表的数量可能增大",{"2":{"576":1}}],["词表的基本思想是基于词典匹配",{"2":{"550":1}}],["词表过大",{"2":{"565":1}}],["词表扩增可能会增加模型的计算和存储成本",{"2":{"560":1}}],["词表大小对模型扩展至关重要",{"2":{"561":1}}],["词表大小呈现增长趋势",{"2":{"559":1}}],["词表大小",{"0":{"559":1},"1":{"560":1,"561":1,"562":1},"2":{"701":1}}],["词表大小为6",{"2":{"399":1}}],["词表中只包含4个词",{"2":{"567":1}}],["词表中的序号",{"2":{"563":1}}],["词表中的每个词都有一条从二叉树根部到自身的路径",{"2":{"184":1}}],["词表中每个单词都有可能成为下一个单词",{"2":{"473":1}}],["词表中其它词的概率都应该是0",{"2":{"398":1}}],["词表是指llm能够理解和识别的唯一单词或token的集合",{"2":{"363":1,"550":1,"556":1}}],["词表",{"0":{"550":1,"556":1},"1":{"557":1,"558":1,"559":1,"560":1,"561":1,"562":1},"2":{"363":1}}],["词表太大怎么办",{"2":{"233":1}}],["词嵌入模型实战",{"2":{"740":1}}],["词嵌入已经成为文本嵌入学习的一个重大突破",{"2":{"711":1}}],["词嵌入维度",{"2":{"701":1}}],["词嵌入本质上是向量",{"2":{"692":1}}],["词嵌入就是将单词与一系列数字",{"2":{"691":1}}],["词嵌入是从高维稀疏向量转化而来的稠密向量",{"2":{"688":1}}],["词嵌入空间的稀疏性问题",{"2":{"562":1}}],["词嵌入的维度大小",{"2":{"533":1}}],["词嵌入的大小",{"2":{"198":1}}],["词嵌入",{"2":{"460":1,"671":1}}],["词嵌入向量生成过程",{"2":{"457":1}}],["词嵌入大小",{"2":{"423":1}}],["词嵌入层",{"2":{"201":1}}],["词数",{"2":{"199":4,"428":1}}],["词汇替换",{"2":{"1015":1}}],["词汇量为v",{"2":{"902":1}}],["词汇表的index",{"2":{"890":1}}],["词汇表的大小也会影响模型的处理速度",{"2":{"562":1}}],["词汇表的扩大使模型能够处理更广泛的语言",{"2":{"559":1}}],["词汇表通常超过50",{"2":{"559":1}}],["词汇表是llm在训练时使用的词元列表",{"2":{"456":1}}],["词汇表可能非常大",{"2":{"185":1}}],["词汇信息",{"2":{"135":1}}],["词这条路径上的第",{"2":{"184":1}}],["词必定最关心自己",{"2":{"172":1}}],["词法信息",{"2":{"13":1}}],["大更新",{"0":{"2042":1,"2045":1,"2048":1}}],["大疆创新",{"2":{"1944":1}}],["大写后的容器",{"2":{"1914":1}}],["大",{"2":{"1607":1}}],["大数据处理框架",{"2":{"1569":1}}],["大规模并行计算",{"2":{"1569":1}}],["大规模仿真等领域",{"2":{"1564":1}}],["大规模数据上",{"2":{"1312":1}}],["大致分为两个阶段",{"2":{"1320":1}}],["大致浏览即可",{"2":{"169":1}}],["大都采用自监督学习",{"2":{"1312":1}}],["大海捞针",{"2":{"768":1}}],["大概知道是个什么样的思想就行",{"2":{"2120":1}}],["大概两小时",{"2":{"2053":1}}],["大概是最常见的向量",{"2":{"681":1}}],["大概念模型",{"2":{"627":1,"628":1}}],["大大提高了代码的效率和可维护性",{"2":{"1729":1}}],["大大限制",{"2":{"624":1}}],["大大降低了成本",{"2":{"617":1}}],["大大降低",{"2":{"305":1}}],["大量不可行点可能表示训练代码中存在错误",{"2":{"1146":1}}],["大量的内存访问会导致较慢的实际执行时间",{"2":{"941":1,"960":1}}],["大量的参数还会导致模型发生过拟合问题",{"2":{"512":1}}],["大量信号被刻意的屏蔽",{"2":{"841":1}}],["大量信号被刻意的屏蔽了",{"2":{"840":1}}],["大量学术论文证明",{"2":{"13":1}}],["大方",{"2":{"407":1,"429":1,"543":1}}],["大型项目通常分为多个子模块",{"2":{"1979":1}}],["大型概念模型",{"2":{"629":1}}],["大型模型的巨量参数还会导致模型的优化过程更为困难",{"2":{"512":1}}],["大型网络在训练初期尚不稳定",{"2":{"402":1}}],["大型语言模型",{"2":{"460":1}}],["大型语言模型记忆机制分析与干预研究综述",{"2":{"156":1}}],["大型语言模型系列解读",{"2":{"156":1}}],["大型语言模型具有卓越的语境理解和融合新信息能力",{"2":{"151":1}}],["大型语言模型的强大能力离不开其对知识的记忆",{"2":{"118":1}}],["大幅提高了计算效率",{"2":{"346":1}}],["大权重代表在生成输出时",{"2":{"268":1}}],["大家可能会觉得这不是显而易见的吗",{"0":{"2127":1}}],["大家可以自己推导出对应点的坐标",{"2":{"2022":1}}],["大家可以通过下面pytorch的源码来了解到dropout的内部机制",{"2":{"395":1}}],["大家可以当作甜点",{"2":{"362":1}}],["大家可以看到圆形隐藏层中包含了前面所有的颜色",{"2":{"860":1}}],["大家可以看到",{"2":{"280":1}}],["大家可以看到在代码中两个变量的特性",{"2":{"77":1}}],["大家找找感觉能理解什么是",{"2":{"2118":1}}],["大家对下面的这些原理和策略的证明可以暂时跳过",{"2":{"2117":1}}],["大家想了解的话可以浏览器或者ai查一查贪心算法的严谨证明",{"2":{"2117":1}}],["大家根据自己的领悟去理解即可",{"2":{"2054":1}}],["大家再仔细想想",{"2":{"2022":1}}],["大家能够掌握",{"2":{"1932":1}}],["大家能够掌握c++14的新特性",{"2":{"1913":1}}],["大家好",{"2":{"1601":1,"1610":1}}],["大家最熟悉的例子就是",{"2":{"257":1}}],["大语言模型的快速发展对自然语言处理领域产生了革命性的影响",{"2":{"1318":1}}],["大语言模型的知识机理",{"2":{"156":1}}],["大语言模型之所以能够很好地回答问题",{"2":{"505":1}}],["大语言模型背后的神经科学机制",{"2":{"233":1,"513":1}}],["大脑同时被激活的神经元只有1",{"2":{"841":1}}],["大脑通过感官输入与参考系中的位置联系起来",{"2":{"754":1}}],["大脑通过不断收集和处理外部信息来构建内部模型",{"2":{"363":1}}],["大脑是使用参考系来管理所有知识",{"2":{"754":1}}],["大脑可以对世界做出更准确的预测",{"2":{"612":1}}],["大脑的目标是将其内部模型与感官输入之间的预测误差最小化",{"2":{"612":1}}],["大脑的主要目标是预测和控制外界的信息",{"2":{"363":1}}],["大脑里也有个transformer",{"2":{"513":1}}],["大脑如何表征知识",{"2":{"233":1}}],["大脑如何在社会中",{"2":{"233":1}}],["大脑",{"2":{"221":2,"224":2}}],["大脑中的参考系",{"0":{"754":1},"2":{"741":1}}],["大脑中似乎也存在一个类",{"2":{"490":1}}],["大脑中没有专门的记忆器官",{"2":{"129":1}}],["大脑中所有只是不会只存储在一个地方",{"2":{"129":1}}],["大部分情况下",{"2":{"1313":1}}],["大部分神经序列转导模型都有一个编码器",{"2":{"912":1}}],["大部分激活函数都是没有参数的",{"2":{"838":1}}],["大部分的activation",{"2":{"829":1}}],["大部分的大模型",{"2":{"387":1}}],["大部分数据基于海量未标记数据的管理",{"2":{"725":1}}],["大部分还是字母",{"2":{"582":1}}],["大部分时候",{"2":{"306":1}}],["大部分概率质量集中在初始标记",{"2":{"204":1}}],["大部分注意力头的pattern趋同",{"2":{"20":1}}],["大众文艺",{"2":{"169":4}}],["大象alpha",{"2":{"156":1,"740":2}}],["大模型榜单",{"2":{"1318":1}}],["大模型概述",{"2":{"1318":1}}],["大模型推理加速利器",{"0":{"948":1,"949":1,"978":1,"979":1},"1":{"980":1,"981":1,"982":1,"983":1}}],["大模型加速利器",{"0":{"939":1,"958":1},"1":{"940":1,"941":1,"942":1,"943":1,"944":1,"945":1,"946":1,"959":1,"960":1,"961":1,"962":1,"963":1,"964":1,"965":1,"966":1,"967":1}}],["大模型神器",{"0":{"936":1,"952":1},"1":{"937":1,"953":1}}],["大模型分布式训练并行技术",{"2":{"768":1}}],["大模型之嵌入与向量化的区别是什么",{"2":{"740":1}}],["大模型中的分词器tokenizer",{"2":{"638":2}}],["大模型中的知识存储",{"2":{"156":1}}],["大模型基础组件之分词器",{"2":{"638":2}}],["大模型时代",{"2":{"543":1}}],["大模型时代是否还需dropout",{"2":{"429":1}}],["大模型由于参数量大",{"2":{"512":1}}],["大模型参数微调",{"2":{"429":1}}],["大模型往往只支持2k或4k",{"2":{"279":1}}],["大模型的爆发",{"0":{"1318":1}}],["大模型的向量表征能力同样可以被激发出来",{"2":{"736":1}}],["大模型的幻觉",{"2":{"513":1}}],["大模型的认知框架看起来十分接近卡尔",{"2":{"363":1}}],["大模型的ffn有什么变化",{"2":{"156":1}}],["大模型的涌现现象是一个复杂且引人入胜的话题",{"2":{"119":1}}],["大模型承重墙",{"2":{"156":1}}],["大模型也有侧脑",{"2":{"156":1}}],["大模型在训练阶段时",{"2":{"12":1}}],["大小不能改变",{"2":{"1802":1}}],["大小可在运行时确定",{"2":{"1714":1}}],["大小固定",{"2":{"1714":1}}],["大小限制",{"2":{"1648":1}}],["大小和范围可能因系统和编译器而异",{"2":{"1607":1}}],["大小和操作",{"2":{"1108":1}}],["大小运算符",{"2":{"1607":1,"1712":1}}],["大小一样大",{"2":{"1164":1}}],["大小应该至少与用于训练的",{"2":{"1164":1}}],["大小",{"2":{"944":1,"1623":1,"1647":1,"2149":1}}],["大小无关",{"2":{"318":1,"326":1}}],["大小为",{"2":{"201":5,"944":2}}],["大小的布尔张量",{"2":{"66":1}}],["大小是d",{"2":{"23":1,"198":1}}],["大多数可以用递归解决的问题",{"2":{"1646":1}}],["大多数学习率调度器可以连续调用",{"2":{"1231":1}}],["大多数深度学习框架都支持模型检查点",{"2":{"1166":1}}],["大多数时候",{"2":{"1140":1}}],["大多数自动搜索算法依赖于人工设计的搜索空间",{"2":{"1139":1}}],["大多数超参数的最佳值对batch",{"2":{"1135":1}}],["大多数函数将使用",{"2":{"1115":1}}],["大多数模型在一开始会用随机数来初始化嵌入矩阵的每个向量",{"2":{"709":1}}],["大多数模型由多个这样的块组成",{"2":{"461":1}}],["大多数",{"2":{"335":1,"1712":1}}],["大多数情况下特别是企业级应用",{"2":{"1478":1}}],["大多数情况下人工神经网络能在外界信息的基础上改变内部结构",{"2":{"1456":1}}],["大多数情况下",{"2":{"241":1}}],["大多数方案是与moe相关的",{"2":{"150":1}}],["大多数是0",{"2":{"50":1}}],["大多依据batch",{"2":{"36":1}}],["大矩阵方式",{"2":{"32":1}}],["大于等于",{"2":{"1619":1,"1635":1}}],["大于两层的",{"2":{"1464":1}}],["大于梯度截断的阈值",{"2":{"1184":1}}],["大于max",{"2":{"1183":1}}],["大于",{"2":{"17":1,"1619":1,"1635":1}}],["单单一段话是理解不完的",{"2":{"2054":1}}],["单步调试时可能看不到独立的函数调用",{"2":{"1709":1}}],["单行定义多个变量",{"2":{"1629":1}}],["单行多个定义和连续赋值",{"2":{"1629":1}}],["单行",{"2":{"1619":1}}],["单引号",{"2":{"1616":2}}],["单精度浮点型",{"2":{"1607":1}}],["单主多从",{"2":{"1578":1}}],["单机的多处理器",{"2":{"1568":1}}],["单机训练代码如下",{"2":{"423":1}}],["单机训练代码",{"0":{"423":1}}],["单一处理机",{"2":{"1568":1}}],["单例",{"2":{"1500":1}}],["单标志法",{"0":{"1417":1}}],["单次",{"2":{"1153":1}}],["单向链表",{"2":{"1795":1,"1801":1}}],["单向性",{"2":{"1768":1,"1781":1}}],["单向和双向掩蔽",{"2":{"976":1}}],["单向自注意力是自回归模式",{"2":{"444":1}}],["单层网络能够保证是凸函数",{"2":{"838":1}}],["单层的前馈网络也足以表示任何函数",{"2":{"296":1}}],["单调性",{"2":{"838":1}}],["单维向量",{"2":{"681":1}}],["单塔扩散lcm由一个transformer主干组成",{"2":{"634":1}}],["单塔扩散lcm",{"0":{"634":1}}],["单字节token",{"2":{"591":1}}],["单看每一轮操作完",{"2":{"576":1}}],["单复数等语义关系和泛化性",{"2":{"565":1}}],["单子代数同态可以用来描述模型层之间的转换",{"2":{"505":1}}],["单子可以用来描述模型必须满足的约束",{"2":{"505":1}}],["单词的对应关系",{"2":{"764":1}}],["单词的位置与顺序定义了语法",{"2":{"744":1}}],["单词的",{"2":{"706":1}}],["单词的表",{"2":{"700":1}}],["单词的表示即为上述所有输出子词",{"2":{"587":1}}],["单词使用的复杂特征",{"2":{"689":1}}],["单词也具备多维度的信息",{"2":{"683":1}}],["单词之间都是正交的",{"2":{"681":1}}],["单词级别分词",{"2":{"564":1}}],["单词",{"2":{"564":1}}],["单词转换为",{"2":{"473":1}}],["单词和单词通过共享权值已经相互发生了一定程度的信息交换",{"2":{"172":1}}],["单元",{"2":{"334":1}}],["单注意力头",{"2":{"198":1}}],["单变量输入",{"2":{"155":1}}],["单独放在站点根目录",{"2":{"2043":1}}],["单独出现的概率更大",{"2":{"598":1}}],["单独一句话",{"2":{"326":1}}],["单独作为",{"2":{"89":1}}],["单独分组",{"0":{"33":1},"2":{"0":1}}],["单头情况下",{"2":{"507":1}}],["单头注意力下",{"2":{"36":1}}],["单头容易只关注自身的注意力权重",{"2":{"13":1}}],["单个字符用单引号",{"2":{"1704":1}}],["单个字符形式",{"2":{"576":1}}],["单个进程与另一个进程通信",{"2":{"1573":1}}],["单个序列",{"2":{"981":1}}],["单个神经元只对有限区域内的刺激作出反应",{"2":{"769":1}}],["单个神经元的dla",{"2":{"479":1}}],["单个",{"2":{"504":1,"922":1}}],["单个粒子的运动会用两个部分模拟",{"2":{"498":1}}],["单个句子举例如下",{"2":{"380":1}}],["单个头的注意力维度",{"2":{"23":1}}],["单个概念向量可以通过不同的函数进行投影",{"2":{"12":1}}],["cxx",{"2":{"1917":5,"1982":2}}],["cxxflags",{"2":{"1917":6}}],["c风格类型转换",{"2":{"1683":1}}],["cmakeenable",{"2":{"1986":1}}],["cmake支持集成测试",{"2":{"1986":1}}],["cmakeinstall",{"2":{"1987":1}}],["cmakeinclude",{"2":{"1976":1,"1989":1,"1990":1}}],["cmakeif",{"2":{"1985":1}}],["cmakeset",{"2":{"1982":1}}],["cmakefind",{"2":{"1977":1,"1991":1}}],["cmaketarget",{"2":{"1975":1,"1984":1}}],["cmakeadd",{"2":{"1973":1,"1974":1,"1993":1,"1994":1,"1999":2}}],["cmakeproject",{"2":{"1972":1}}],["cmakecmake",{"2":{"1966":1,"1971":1,"1980":1,"1999":1}}],["cmakelists",{"0":{"1966":1,"1980":1,"1981":1},"2":{"1965":1,"1981":1,"1997":3,"1999":7}}],["cmake",{"0":{"1963":1,"1964":1,"1970":1,"1971":1,"1983":1,"1988":1,"1992":1,"1997":1,"1999":1},"1":{"1964":1,"1971":1,"1972":1,"1973":1,"1974":1,"1975":1,"1976":1,"1977":1,"1984":1,"1985":1,"1986":1,"1987":1,"1989":1,"1990":1,"1991":1,"1993":1,"1994":1},"2":{"1605":2,"1963":1,"1965":1,"1968":1,"1969":1,"1971":1,"1981":1,"1982":2,"1989":1,"1993":1,"1996":3,"1997":3,"1999":3}}],["c语言",{"2":{"1603":1}}],["cj",{"2":{"1481":1}}],["c对激活函数z的偏导数",{"2":{"1450":1}}],["cqc",{"2":{"1361":1}}],["ckpt",{"2":{"1266":2,"1267":2}}],["ck×k×c",{"2":{"1003":1}}],["cycle的实现",{"2":{"1242":1}}],["cycle学习率策略在每个批次之后改变学习率",{"2":{"1242":1}}],["cycle学习率策略设置每个参数组的学习率",{"2":{"1242":1}}],["cycle策略将学习率从初始学习率逐渐退火到最大学习率",{"2":{"1242":1}}],["cycle",{"2":{"1241":1,"1284":1}}],["cyclical",{"2":{"1241":1}}],["cycliclr",{"0":{"1241":1},"2":{"1241":1}}],["c​q​​",{"2":{"1361":1}}],["c​p​​",{"2":{"1361":1}}],["c​p​​+c​q​​−2√​c​p​​c​q​​​​​",{"2":{"1361":1}}],["c​l​​=d​l−1​​",{"2":{"1003":1}}],["c​t​kv​​",{"2":{"957":1}}],["ctest",{"2":{"1996":1}}],["ctrl",{"2":{"1316":2,"1520":1,"1541":1,"1547":2,"1550":1,"1551":2,"1557":2}}],["ctx",{"2":{"1100":4}}],["ctkvc",{"2":{"957":1}}],["ctypes",{"2":{"1072":2,"1078":1}}],["ctype",{"2":{"395":1}}],["c×lc",{"2":{"1004":1}}],["c×​n​^​​",{"2":{"1004":1}}],["c×n^c",{"2":{"1004":1}}],["c×d",{"2":{"957":1}}],["c×h×w",{"2":{"340":1}}],["cpc",{"2":{"1361":1}}],["cp+cq−2cpcq",{"2":{"1361":1}}],["cp还可以与不同的注意力变体一起工作",{"2":{"976":1}}],["cp可以与tp",{"2":{"976":1}}],["cp可以更好地解决这些问题",{"2":{"976":1}}],["cp类似于环注意力",{"2":{"976":1}}],["cp需要跨gpu进行额外的allgather以收集完整的kv序列",{"2":{"976":1}}],["cp沿着序列维度分割网络输入和所有激活",{"2":{"976":1}}],["cp",{"2":{"976":3,"1510":1}}],["cpu适合串行计算",{"2":{"796":1}}],["cpu",{"2":{"795":1,"986":1,"1076":2,"1086":1,"1087":2,"1214":2,"1215":1,"1255":1,"1284":1,"1589":1,"1653":1,"2077":1}}],["cppif",{"2":{"1922":1}}],["cppint",{"2":{"1629":3,"1630":3,"1633":2,"1634":2,"1714":2,"1907":1,"1910":1}}],["cpp文件",{"2":{"1918":1}}],["cpp=",{"2":{"1917":1}}],["cppstd",{"2":{"1898":1,"1921":2,"1925":2}}],["cppauto",{"2":{"1879":1,"1907":1,"1924":1}}],["cppchar",{"2":{"1715":2}}],["cppclass",{"2":{"1653":1,"1655":1,"1656":1,"1712":1,"1770":1,"1772":1,"1774":1,"1778":1,"1779":1,"1784":1,"1866":1,"1867":1,"1868":1,"1869":1,"1902":1}}],["cppdelete",{"2":{"1714":1}}],["cpp返回类型",{"2":{"1712":1}}],["cppvoid",{"2":{"1634":1}}],["cpp怎么完成大模型推理的",{"2":{"513":1}}],["cpp",{"2":{"513":1,"977":5,"1082":1,"1628":2,"1632":2,"1633":1,"1645":1,"1646":1,"1647":1,"1648":1,"1649":1,"1650":1,"1654":1,"1665":1,"1691":1,"1698":1,"1699":1,"1700":1,"1701":1,"1704":1,"1705":1,"1706":2,"1707":1,"1708":1,"1709":1,"1710":1,"1712":1,"1713":4,"1714":1,"1715":6,"1718":1,"1719":6,"1720":4,"1721":4,"1722":4,"1723":1,"1724":3,"1725":4,"1736":1,"1737":1,"1738":1,"1739":1,"1741":1,"1742":1,"1743":1,"1744":1,"1746":1,"1747":1,"1749":1,"1750":1,"1751":1,"1752":1,"1754":1,"1755":1,"1756":1,"1788":1,"1789":1,"1791":1,"1797":2,"1799":1,"1800":1,"1801":1,"1802":1,"1803":1,"1805":2,"1806":2,"1807":2,"1811":1,"1813":1,"1816":1,"1817":1,"1820":2,"1821":1,"1824":1,"1825":1,"1829":1,"1831":1,"1834":1,"1835":1,"1838":2,"1839":1,"1842":1,"1843":1,"1849":1,"1853":1,"1857":1,"1861":1,"1883":1,"1887":1,"1891":1,"1895":1,"1897":1,"1902":1,"1905":3,"1906":3,"1907":1,"1908":1,"1909":2,"1910":1,"1911":2,"1916":8,"1917":10,"1918":1,"1921":1,"1922":2,"1923":6,"1924":1,"1925":1,"1926":1,"1927":1,"1930":2,"1933":1,"1966":1,"1973":1,"1974":1,"1980":1,"1981":1,"1986":1,"1997":2,"1999":9,"2059":5,"2060":2,"2061":1,"2062":3,"2063":1}}],["cpptemplate",{"2":{"395":1,"1699":1,"1700":1,"1908":1,"1912":1}}],["cbow模型在训练期间确实通过滑动窗口考虑了词的局部上下文",{"2":{"715":1}}],["cbow就是使用",{"2":{"714":1}}],["cbow架构会基于词的上下文来预测目标单词",{"2":{"714":1}}],["cbow",{"2":{"714":1}}],["cz",{"2":{"638":1}}],["cde",{"2":{"1713":2}}],["cdata",{"2":{"1082":1}}],["cd",{"2":{"575":2,"1309":1,"1332":2,"1509":4,"1968":1,"1969":1,"1999":1}}],["cddcdycdyc",{"2":{"575":1}}],["cdots+var",{"2":{"1002":1}}],["cdots+x",{"2":{"1002":1}}],["cdot",{"2":{"71":2,"106":1,"108":2,"109":1,"110":1,"128":1,"148":1,"188":1,"189":1,"270":1,"402":2,"485":2,"621":1,"692":2,"845":1,"1000":1,"1003":1,"1361":1}}],["c++28课",{"0":{"1962":1},"1":{"1963":1,"1964":1,"1965":1,"1966":1,"1967":1,"1968":1,"1969":1,"1970":1,"1971":1,"1972":1,"1973":1,"1974":1,"1975":1,"1976":1,"1977":1,"1978":1,"1979":1,"1980":1,"1981":1,"1982":1,"1983":1,"1984":1,"1985":1,"1986":1,"1987":1,"1988":1,"1989":1,"1990":1,"1991":1,"1992":1,"1993":1,"1994":1,"1995":1,"1996":1,"1997":1,"1998":1,"1999":1}}],["c++20",{"2":{"1603":2,"1960":1}}],["c++等规范应用扩展",{"2":{"1960":1}}],["c++函数名",{"2":{"1729":1}}],["c++返回值类型",{"2":{"1729":1}}],["c++enum",{"2":{"1728":2}}],["c++struct",{"2":{"1728":1,"1931":1}}],["c++template",{"2":{"1726":1,"1925":1}}],["c++课程",{"0":{"1681":1,"1703":1},"1":{"1682":1,"1683":1,"1684":1,"1685":1,"1686":1,"1687":1,"1688":1,"1704":1,"1705":1,"1706":1,"1707":1,"1708":1,"1709":1,"1710":1}}],["c++和相关的工具它是必要条件",{"2":{"1961":1}}],["c++和",{"2":{"1651":1}}],["c++的用时",{"2":{"1651":1}}],["c++ååclass",{"2":{"1641":1}}],["c++风格类型转换",{"2":{"1629":1,"1683":1}}],["c++const",{"2":{"1921":1}}],["c++cout",{"2":{"1623":1,"1624":1}}],["c++class",{"2":{"1638":1,"1639":1,"1640":1,"1641":1,"1674":1,"1693":3,"1694":2}}],["c++char",{"2":{"1624":1}}],["c++数据类型",{"2":{"1623":1}}],["c++for",{"2":{"1621":2}}],["c++do",{"2":{"1620":1}}],["c++while",{"2":{"1620":1}}],["c++if",{"2":{"1619":3}}],["c++int",{"2":{"1611":6,"1612":2,"1614":2,"1621":1,"1623":4,"1669":2,"1695":1,"1726":1,"1729":1}}],["c++auto",{"2":{"1615":1}}],["c++17引入了结构化绑定",{"2":{"1921":1}}],["c++17",{"0":{"1919":1},"1":{"1920":1,"1921":1,"1922":1,"1923":1,"1924":1,"1925":1,"1926":1,"1927":1,"1928":1,"1929":1,"1930":1,"1931":1,"1932":1,"1933":1},"2":{"1603":1,"1912":1,"1921":2,"1922":3,"1923":3,"1924":1,"1925":3,"1930":1,"1931":4,"1932":2,"1933":1}}],["c++14虽然是一个相对较小的更新",{"2":{"1913":1}}],["c++14",{"0":{"1903":1},"1":{"1904":1,"1905":1,"1906":1,"1907":1,"1908":1,"1909":1,"1910":1,"1911":1,"1912":1,"1913":1,"1914":1},"2":{"1603":1,"1905":3,"1906":3,"1907":2,"1908":1,"1909":1,"1910":2,"1911":1}}],["c++11为现代c++编程带来了大量的便利和性能提升",{"2":{"1901":1}}],["c++11",{"0":{"1875":1},"1":{"1876":1,"1877":1,"1878":1,"1879":1,"1880":1,"1881":1,"1882":1,"1883":1,"1884":1,"1885":1,"1886":1,"1887":1,"1888":1,"1889":1,"1890":1,"1891":1,"1892":1,"1893":1,"1894":1,"1895":1,"1896":1,"1897":1,"1898":1,"1899":1,"1900":1,"1901":1,"1902":1},"2":{"1603":1,"1611":1,"1615":1,"1629":1,"1630":1,"1713":1,"1879":1,"1902":1,"1905":3,"1906":3,"1907":2,"1911":1}}],["c++98",{"2":{"1603":1}}],["c++版本",{"2":{"1603":1}}],["c++编程语言",{"0":{"1600":1,"1617":1},"1":{"1601":1,"1602":1,"1603":1,"1604":1,"1605":1,"1606":1,"1607":1,"1608":1,"1618":1,"1619":1,"1620":1,"1621":1,"1622":1,"1623":1,"1624":1,"1625":1}}],["c++底层实现了一个dispather分发机制",{"2":{"662":1}}],["c++",{"0":{"1600":1,"1601":1,"1602":1,"1603":1,"1604":1,"1605":1,"1606":2,"1607":1,"1609":1,"1610":1,"1626":1,"1636":1,"1643":1,"1652":1,"1658":1,"1666":1,"1673":1,"1692":1,"1697":1,"1711":1,"1717":1,"1727":1,"1730":2,"1760":1,"1762":1,"1794":1,"1809":1,"1811":1,"1827":1,"1829":1,"1845":1,"1870":1,"1916":1,"1934":1,"2000":1,"2002":1},"1":{"1601":1,"1602":1,"1603":1,"1604":1,"1605":1,"1606":1,"1607":1,"1608":1,"1610":1,"1611":1,"1612":1,"1613":1,"1614":1,"1615":1,"1616":1,"1627":1,"1628":1,"1629":1,"1630":1,"1631":1,"1632":1,"1633":1,"1634":1,"1637":1,"1638":1,"1639":1,"1640":1,"1641":1,"1644":1,"1645":1,"1646":1,"1647":1,"1648":1,"1649":1,"1650":1,"1659":1,"1660":1,"1661":1,"1662":1,"1663":1,"1667":1,"1668":1,"1669":1,"1670":1,"1671":1,"1672":1,"1673":1,"1674":1,"1675":1,"1676":1,"1677":1,"1678":1,"1693":1,"1694":1,"1695":1,"1698":1,"1699":1,"1700":1,"1701":1,"1712":1,"1713":1,"1714":1,"1715":1,"1718":1,"1719":1,"1720":1,"1721":1,"1722":1,"1723":1,"1724":1,"1725":1,"1728":1,"1729":1,"1731":2,"1732":2,"1733":2,"1734":2,"1735":2,"1736":2,"1737":2,"1738":2,"1739":2,"1740":2,"1741":2,"1742":2,"1743":2,"1744":2,"1745":2,"1746":2,"1747":2,"1748":2,"1749":2,"1750":2,"1751":2,"1752":2,"1753":2,"1754":2,"1755":2,"1756":2,"1757":2,"1758":2,"1759":2,"1761":1,"1762":1,"1763":1,"1764":1,"1765":1,"1766":1,"1795":1,"1796":1,"1797":1,"1798":1,"1799":1,"1800":1,"1801":1,"1802":1,"1803":1,"1804":1,"1805":1,"1806":1,"1807":1,"1808":1,"1810":1,"1811":1,"1812":1,"1813":1,"1814":1,"1815":1,"1816":1,"1817":1,"1818":1,"1819":1,"1820":1,"1821":1,"1822":1,"1823":1,"1824":1,"1825":1,"1826":1,"1828":1,"1829":1,"1830":1,"1831":1,"1832":1,"1833":1,"1834":1,"1835":1,"1836":1,"1837":1,"1838":1,"1839":1,"1840":1,"1841":1,"1842":1,"1843":1,"1844":1,"1846":1,"1847":1,"1848":1,"1849":1,"1850":1,"1851":1,"1852":1,"1853":1,"1854":1,"1855":1,"1856":1,"1857":1,"1858":1,"1859":1,"1860":1,"1861":1,"1862":1,"1863":1,"1864":1,"1865":1,"1866":1,"1867":1,"1868":1,"1869":1,"1871":1,"1872":1,"1873":1,"1874":1,"1935":1,"1936":1,"1937":1,"1938":1,"1939":1,"1940":1,"1941":1,"1942":1,"1943":1,"1944":1,"1945":1,"1946":1,"1947":1,"1948":1,"1949":1,"1950":1,"1951":1,"1952":1,"1953":1,"1954":1,"1955":1,"1956":1,"1957":1,"1958":1,"1959":1,"1960":1,"1961":1,"2003":1,"2004":1,"2005":1,"2006":1,"2007":1,"2008":1},"2":{"567":1,"1084":2,"1589":1,"1601":3,"1602":16,"1603":7,"1604":5,"1605":6,"1606":3,"1607":5,"1608":2,"1610":4,"1611":4,"1612":2,"1616":1,"1618":1,"1619":1,"1620":1,"1621":2,"1622":1,"1623":3,"1624":4,"1625":2,"1627":1,"1628":1,"1629":1,"1634":1,"1644":3,"1647":1,"1648":2,"1649":1,"1650":1,"1651":1,"1652":1,"1659":1,"1660":1,"1661":1,"1662":1,"1663":1,"1666":2,"1667":2,"1668":2,"1670":1,"1671":1,"1672":2,"1673":3,"1674":1,"1675":1,"1676":1,"1677":2,"1678":2,"1680":1,"1683":4,"1684":2,"1685":2,"1687":2,"1688":2,"1695":3,"1696":1,"1703":2,"1704":2,"1708":1,"1709":3,"1710":1,"1712":2,"1713":1,"1714":1,"1715":1,"1727":1,"1728":8,"1729":11,"1731":1,"1734":1,"1761":1,"1762":2,"1763":1,"1765":1,"1769":1,"1792":1,"1798":1,"1803":1,"1805":1,"1807":1,"1810":2,"1811":1,"1814":1,"1817":1,"1818":1,"1826":2,"1828":2,"1829":1,"1832":1,"1835":1,"1836":1,"1844":2,"1846":1,"1874":1,"1912":1,"1914":1,"1916":3,"1918":3,"1926":1,"1927":1,"1928":2,"1929":2,"1931":1,"1932":2,"1964":1,"2003":1,"2004":1,"2005":1,"2006":4,"2007":1}}],["cctype>",{"2":{"1933":1}}],["ccol",{"2":{"1087":1}}],["cc",{"2":{"387":2}}],["ccc",{"2":{"105":1}}],["cstring",{"2":{"1715":1}}],["cstring>",{"2":{"1704":1,"1715":1,"1728":1,"1887":1}}],["cstr",{"2":{"1713":1,"1929":2}}],["csv",{"2":{"1250":1,"1254":2}}],["csr",{"2":{"1086":1,"1087":2}}],["csrc",{"2":{"1082":1}}],["csc",{"2":{"1086":1,"1087":1}}],["csstwo",{"2":{"370":1}}],["csdn博客",{"2":{"429":3}}],["csdn",{"2":{"95":1,"768":2}}],["c是feature的通道数",{"2":{"341":1}}],["ch",{"2":{"1619":9,"1813":5,"1816":2,"1821":5,"1831":5,"1834":2,"1839":5}}],["chmod",{"2":{"1513":3}}],["chrono",{"2":{"1895":1}}],["chrono>",{"2":{"1895":1}}],["chrome",{"2":{"1284":1}}],["chris",{"2":{"1194":1}}],["christopher",{"2":{"1124":1,"1195":1}}],["christian",{"2":{"429":1}}],["chen对文档演讲内容提出的宝贵建议",{"2":{"1194":1}}],["checkpermission",{"2":{"2060":3}}],["checkpoint",{"0":{"938":1,"954":1},"2":{"723":1,"938":2,"954":2,"1266":2,"1267":5,"1308":1}}],["checkout",{"2":{"1309":1,"1332":1}}],["check",{"2":{"592":1,"668":1,"1098":1,"1215":1,"1227":1,"2063":3,"2070":3}}],["choice",{"2":{"1608":4}}],["chown",{"2":{"1513":2}}],["cholesky",{"2":{"1087":3}}],["chongjie",{"2":{"429":1}}],["childfriendfunction",{"2":{"1784":3}}],["childprivate",{"2":{"1784":3}}],["child",{"2":{"1784":7,"1785":1,"2153":5}}],["children",{"2":{"591":2,"1214":2}}],["china",{"2":{"1309":1}}],["chinchilla",{"2":{"1159":1}}],["chinese",{"2":{"560":1,"638":1,"724":1,"1309":1,"1310":1,"1825":9,"1843":9}}],["chalf",{"2":{"1087":1}}],["changing",{"2":{"827":1,"834":1}}],["changepassword",{"2":{"2070":1}}],["change",{"2":{"820":1,"2075":1}}],["channel",{"2":{"321":1,"325":1,"326":1,"337":4,"1083":2,"1087":3}}],["channels维度上进行操作",{"2":{"315":1}}],["channels",{"2":{"315":1,"338":1,"810":3}}],["chapter6",{"2":{"638":2}}],["charlie",{"2":{"1750":1,"1807":2}}],["char",{"2":{"591":1,"1087":1,"1436":1,"1590":1,"1594":1,"1607":5,"1615":1,"1619":1,"1622":1,"1624":2,"1650":1,"1683":1,"1684":4,"1704":5,"1713":7,"1715":15,"1728":3,"1813":9,"1816":2,"1820":1,"1821":2,"1825":1,"1831":9,"1834":2,"1838":1,"1839":2,"1843":1,"1879":1,"1887":5,"1925":1,"1929":1,"1933":1}}],["char粒度的字母序列",{"2":{"576":1}}],["charset=utf8",{"2":{"1481":1}}],["chars",{"2":{"572":6}}],["characterencoding=utf8",{"2":{"1481":1}}],["characters",{"2":{"606":1,"1566":1,"1821":1,"1839":1}}],["character",{"2":{"564":1,"1607":1,"1616":1,"1713":3,"1821":2,"1839":2,"1929":1}}],["chart",{"2":{"399":1}}],["chainedscheduler",{"0":{"1246":1},"2":{"1246":1}}],["chain",{"2":{"480":1,"513":1,"736":1}}],["chat",{"2":{"768":1,"985":2}}],["chatretriever",{"2":{"731":1}}],["chat的分析",{"2":{"429":1}}],["chatgpt",{"2":{"569":1,"1425":1}}],["chatgpt是第一个真正意义的人工通用智能",{"2":{"429":1}}],["chatglm",{"2":{"347":1}}],["chunks",{"2":{"1087":2}}],["chunked",{"0":{"977":1},"2":{"977":1}}],["chunk",{"2":{"204":1,"1087":2}}],["cerr",{"2":{"1713":1,"1761":1,"1762":2,"1763":3,"1811":2,"1814":3,"1820":3,"1821":1,"1825":2,"1826":1,"1829":2,"1832":3,"1838":3,"1839":1,"1843":2,"1844":1,"1902":1,"1933":2,"2062":1,"2063":1}}],["ce",{"2":{"1713":1}}],["ceil",{"2":{"1087":2}}],["centos8及以上",{"2":{"1584":1}}],["centos7及以下",{"2":{"1584":1}}],["center",{"2":{"1087":1}}],["centered激活函数指的是在激活函数的输出中心值为零",{"2":{"838":1}}],["centered",{"2":{"838":1,"840":1}}],["centering",{"2":{"298":1,"320":1}}],["central",{"2":{"795":1}}],["cell",{"0":{"864":1,"867":1,"870":1},"1":{"865":1,"866":1,"867":1,"868":1,"869":1},"2":{"287":1,"490":2,"863":2,"864":2}}],["cvf",{"2":{"1535":1}}],["cvpr",{"2":{"429":1}}],["cv使用bn为主",{"2":{"325":1}}],["cv",{"0":{"325":1},"2":{"293":1,"808":1,"1015":1}}],["cv技术指南",{"2":{"95":1}}],["c4",{"2":{"267":6}}],["c3",{"2":{"267":6,"1712":3,"1792":2}}],["c2",{"2":{"267":6,"1712":3,"1789":3,"1792":7}}],["c10",{"2":{"395":1}}],["c1",{"2":{"267":6,"1712":3,"1774":8,"1789":3,"1792":7}}],["c=",{"2":{"241":1,"826":1}}],["citic",{"2":{"1949":1}}],["circulararea",{"2":{"1908":1}}],["circuit",{"0":{"480":1},"2":{"437":1,"475":1}}],["circuits的数学框架",{"2":{"156":1,"233":1}}],["circuits",{"2":{"130":2,"156":1,"233":1,"513":1}}],["circle",{"2":{"1656":3,"1678":1,"1688":7,"1693":6,"1779":8}}],["cin",{"0":{"1673":1,"1813":1,"1831":1},"2":{"1608":3,"1619":1,"1624":1,"1666":1,"1673":3,"1678":1,"1729":2,"1811":2,"1812":1,"1813":14,"1814":8,"1826":2,"1829":2,"1830":1,"1831":14,"1832":8,"1844":2}}],["cis=freqs",{"2":{"201":1,"1345":1}}],["cis",{"2":{"201":11,"1345":10}}],["cutting",{"2":{"2081":1}}],["cutmix就是将一部分区域cut掉但不填充0像素而是随机填充训练集中的其他数据的区域像素值",{"2":{"1015":1}}],["cuboid",{"2":{"1774":7,"1789":22,"1791":8,"1792":11}}],["cumsum",{"2":{"1087":4}}],["cumprod",{"2":{"1087":4,"1350":5}}],["cummin",{"2":{"1087":4}}],["cummax",{"2":{"1087":4}}],["cudnn不是必须的",{"2":{"796":1}}],["cudnn",{"0":{"796":1},"2":{"796":2}}],["cuda默认为true",{"2":{"1214":1}}],["cuda",{"0":{"795":1},"2":{"201":2,"422":1,"423":5,"662":1,"795":2,"796":1,"1076":3,"1083":3,"1086":1,"1087":2,"1214":1,"1215":11,"1227":1,"1228":1,"1255":2,"1263":1,"1284":2,"1569":1}}],["curlbashcurl",{"2":{"1528":1}}],["cur",{"2":{"1244":1,"1821":2,"1839":2}}],["curse",{"2":{"692":1}}],["currentdir",{"2":{"1930":3}}],["currently",{"2":{"1305":1}}],["current",{"2":{"387":1,"1083":1,"1179":1,"1215":1,"1243":2,"1308":1,"1329":2,"1330":1,"1874":1,"1930":2,"2086":1}}],["cue",{"2":{"163":2}}],["customimagedataset",{"2":{"1250":1}}],["customer",{"0":{"1100":1,"1206":1}}],["custom",{"2":{"114":1,"1206":2,"1993":2,"1994":2}}],["clr",{"2":{"1241":1}}],["clog",{"2":{"1811":1,"1826":1,"1829":1,"1844":1}}],["close",{"2":{"1282":1,"1283":1,"1481":1,"1485":1,"1486":1,"1487":1,"1488":1,"1820":4,"1821":2,"1825":2,"1838":4,"1839":2,"1843":2,"1930":1,"1933":1}}],["closure",{"2":{"1223":4,"1227":3}}],["clone可以clone",{"2":{"2038":1}}],["clone",{"2":{"383":2,"399":2,"1086":1,"1087":1,"1309":1,"1332":1,"1347":1}}],["clones",{"2":{"23":1,"83":1,"343":2,"344":1,"522":2,"523":1,"532":1,"533":1}}],["clearsessions",{"2":{"2070":1}}],["clear",{"2":{"1087":1,"1713":2,"1814":1,"1824":1,"1832":1,"1842":1}}],["cleaned",{"2":{"1933":3}}],["cleaner",{"2":{"387":1}}],["cleanword",{"2":{"1933":2}}],["clean",{"2":{"1917":4}}],["cleaning",{"2":{"370":1}}],["cl=dl−1c",{"2":{"1003":1}}],["clustering",{"2":{"724":1,"906":1}}],["clang",{"2":{"1605":2,"1960":1}}],["cla",{"2":{"1197":2}}],["clamp",{"2":{"1087":12}}],["clark",{"2":{"638":1}}],["classc",{"2":{"1781":2}}],["classb",{"2":{"1778":4,"1781":4}}],["classa",{"2":{"1778":6,"1781":4}}],["classname",{"2":{"1674":1}}],["classes",{"2":{"1254":1}}],["classtorch",{"2":{"1224":1}}],["classmethod",{"2":{"1083":1}}],["classifier",{"2":{"1222":2}}],["classification等任务",{"2":{"724":1}}],["classification",{"2":{"156":1,"167":2,"259":2,"724":1,"906":1,"1312":1,"2079":3}}],["classinstantier",{"2":{"110":1}}],["class",{"0":{"1224":1,"1925":1},"1":{"1225":1,"1226":1,"1227":1},"2":{"83":1,"113":1,"343":1,"346":1,"450":1,"503":1,"522":1,"591":1,"592":1,"701":1,"703":1,"723":1,"1082":2,"1100":1,"1211":1,"1212":1,"1213":1,"1215":2,"1216":4,"1217":3,"1218":5,"1227":1,"1250":1,"1254":4,"1257":1,"1295":2,"1345":1,"1436":1,"1481":4,"1485":1,"1486":1,"1487":1,"1488":1,"1654":2,"1655":1,"1656":2,"1659":3,"1660":6,"1661":4,"1662":4,"1663":2,"1665":4,"1674":2,"1675":1,"1676":1,"1677":1,"1683":4,"1685":4,"1688":6,"1691":4,"1693":3,"1696":1,"1698":1,"1700":2,"1701":1,"1712":1,"1728":5,"1763":1,"1769":1,"1770":1,"1778":2,"1779":2,"1784":1,"1788":1,"1789":1,"1791":1,"1792":1,"1825":1,"1843":1,"1849":4,"1853":2,"1857":2,"1861":2,"1866":2,"1867":1,"1868":1,"1869":3,"1874":3,"1887":1,"1891":1,"1902":1,"1920":1,"1923":4,"2006":5,"2063":1,"2086":1}}],["cls",{"2":{"204":2,"555":1,"721":1,"722":1,"727":1,"731":1,"764":3,"1083":1,"1086":1,"1087":3}}],["clion",{"2":{"1605":11}}],["clip−score",{"2":{"1360":2}}],["clip",{"0":{"1352":1,"1356":1,"1360":1},"2":{"1087":4,"1339":6,"1360":3,"1363":1}}],["clippedgeluactivation",{"2":{"110":1}}],["climbing",{"2":{"156":1,"370":1}}],["crud操作",{"0":{"1483":1},"1":{"1484":1,"1485":1,"1486":1,"1487":1,"1488":1,"1489":1}}],["crf的viterbi解码",{"0":{"1326":1}}],["crf的作用就是在所有可能的路径中",{"2":{"1320":1}}],["crf会据此解码出一串标签序列",{"2":{"1320":1}}],["crf",{"0":{"1319":1,"1321":1,"1325":1},"1":{"1320":1,"1321":1,"1322":2,"1323":2,"1324":2,"1325":1,"1326":2},"2":{"1309":1,"1326":1}}],["crop",{"2":{"1254":5}}],["crow",{"2":{"1087":1}}],["crossattnmidblock",{"2":{"1364":1}}],["crossattndownblock",{"2":{"1364":2}}],["crossentropyloss",{"2":{"399":1,"429":2,"1205":1,"1218":1,"1296":1,"2086":1}}],["crossentropy",{"2":{"399":1}}],["cross",{"0":{"649":1,"931":1},"2":{"78":1,"397":1,"444":1,"530":2,"535":1,"612":1,"1087":1,"1096":1,"1218":3,"1363":1}}],["credit",{"2":{"1874":6}}],["creditlimit",{"2":{"1873":2,"1874":3}}],["creditaccount",{"2":{"1873":3,"1874":6}}],["crelu",{"2":{"840":1}}],["createcachetable",{"2":{"2070":1}}],["createmytype",{"2":{"1931":2}}],["created",{"2":{"1930":2}}],["createresource",{"2":{"1891":2}}],["createarray",{"2":{"1706":2}}],["createonheap",{"2":{"1648":2}}],["createsuperuser的使用前面讲过了",{"2":{"2070":1}}],["createsuperuser",{"2":{"2069":2,"2070":1}}],["createstring",{"2":{"1887":2}}],["creates",{"2":{"557":1,"2086":1}}],["create",{"2":{"74":1,"76":1,"79":1,"84":1,"364":1,"375":2,"380":1,"423":1,"592":1,"1083":1,"1254":1,"1481":1,"1576":1,"1930":3,"2070":6,"2086":1}}],["crafting",{"2":{"513":1}}],["crawl",{"2":{"387":1,"1315":1}}],["criteration",{"2":{"1202":2,"1205":3}}],["criterion",{"2":{"83":3,"364":1,"385":1,"398":5,"399":3,"410":1,"423":4,"424":3,"472":4,"1218":2,"1244":1,"1295":2,"1296":2,"2086":2}}],["critical",{"2":{"591":1,"1412":1}}],["crit",{"2":{"399":4}}],["cae算法工程师",{"2":{"1958":1}}],["cae软件开发",{"2":{"1956":1}}],["cad",{"2":{"1956":1}}],["calculation",{"0":{"2076":1}}],["calculations",{"2":{"2075":1}}],["calculatorproject",{"2":{"1999":1}}],["calculator",{"2":{"1916":5,"1917":15,"1997":5,"1999":20}}],["calculates",{"2":{"2076":1}}],["calculatesalary",{"2":{"1657":3}}],["calculategrade",{"2":{"1825":2,"1843":2}}],["calculatevolume",{"2":{"1774":4}}],["calculate",{"2":{"1710":4,"2086":1}}],["calculatearea",{"2":{"1708":4,"1779":3}}],["calculated",{"2":{"1594":1,"1825":1,"1843":1}}],["calloc",{"2":{"1668":2}}],["callfunction",{"2":{"1645":7}}],["callback",{"0":{"1645":1},"2":{"1645":3}}],["called",{"2":{"1208":1,"1245":1,"1254":1,"1284":2,"2076":1}}],["calling",{"2":{"1086":1,"1087":1,"1100":1}}],["calls",{"0":{"1646":1},"2":{"558":1}}],["call",{"2":{"344":2,"398":1,"410":1,"472":1,"1101":2,"1208":4,"1214":7,"1254":4}}],["callable>",{"2":{"1914":1}}],["callable",{"2":{"344":3,"384":1,"558":1,"1082":1,"1087":6,"1208":9,"1214":11,"1227":8,"1250":1,"1254":2,"1914":1}}],["caught",{"2":{"1713":1,"1762":2,"1902":1,"2063":1}}],["cauchy",{"2":{"1087":1}}],["cause",{"2":{"572":1}}],["causal",{"0":{"476":1},"1":{"477":1,"478":1,"479":1},"2":{"50":1,"76":4,"89":2,"90":1,"94":1,"443":1,"475":1,"541":1,"542":1,"734":1,"735":1,"970":1,"1312":1}}],["cap",{"2":{"1713":1}}],["capacity",{"2":{"1713":2,"1714":2}}],["capture",{"0":{"1907":1},"2":{"1227":1,"1904":1}}],["caption",{"2":{"882":1}}],["capital",{"2":{"122":3}}],["cars",{"2":{"713":2}}],["car",{"2":{"713":2}}],["cai",{"2":{"638":1}}],["casual",{"2":{"732":1}}],["casas",{"2":{"638":1}}],["cast",{"2":{"573":1,"1226":1,"1611":1,"1623":1,"1629":5,"1683":4,"1685":1,"1704":2,"1705":5,"1820":1,"1838":1,"1928":4}}],["cased",{"2":{"723":1}}],["case",{"2":{"477":1,"513":1,"560":1,"592":1,"638":1,"1254":1,"1304":1,"1330":1,"1631":1,"1646":1,"1922":4}}],["caching",{"2":{"201":1}}],["cacheread",{"2":{"1867":2}}],["cache填满",{"2":{"986":1}}],["cache张量存储在non",{"2":{"986":1}}],["cache张量之间的映射关系",{"2":{"986":1}}],["cache重用模式",{"2":{"985":1}}],["cache划分为块",{"2":{"981":1}}],["cache带来了重大挑战",{"2":{"981":1}}],["cache具有以下特点",{"2":{"981":1}}],["cache的有效使用变得复杂",{"2":{"746":1}}],["cache时候当预测下一个时间步的时候与之前的做attention的时候",{"2":{"656":1}}],["cache",{"0":{"653":1,"948":1,"978":1,"985":1},"2":{"76":1,"83":1,"201":18,"273":1,"423":2,"542":1,"746":2,"768":1,"937":2,"948":1,"953":2,"978":1,"981":2,"985":7,"1867":4}}],["cat查看create",{"2":{"2070":1}}],["cat2",{"2":{"1675":2}}],["cat1",{"2":{"1675":2}}],["catage",{"2":{"1675":2}}],["catname",{"2":{"1675":2}}],["catch",{"2":{"1481":1,"1491":1,"1647":1,"1713":1,"1762":14,"1763":2,"1764":1,"1765":1,"1766":1,"1902":1,"2062":1,"2063":1}}],["cat的别名",{"2":{"825":1}}],["cat在句子前面加上",{"2":{"384":2,"558":2}}],["cat",{"2":{"65":1,"76":1,"83":1,"261":3,"384":2,"428":1,"472":1,"529":1,"558":2,"825":2,"840":1,"1510":1,"1675":5,"1690":1,"1691":6,"1866":3}}],["candidate",{"2":{"1330":2}}],["candidates",{"2":{"601":1,"638":1}}],["cannot",{"2":{"591":1}}],["can",{"2":{"20":1,"47":1,"160":1,"542":1,"543":1,"572":1,"591":3,"592":2,"736":1,"802":1,"1254":3,"2077":1,"2078":1}}],["cnclude",{"2":{"1590":1}}],["cn=k×k×c",{"2":{"1003":1}}],["cn",{"0":{"1429":1},"2":{"387":1,"1379":1,"1381":1,"1382":1,"1383":1,"1399":1,"1400":1,"1401":1,"1402":1}}],["cnn等模型",{"2":{"1334":1}}],["cnn网络中全连接层占据了很大的参数",{"2":{"816":1}}],["cnn存在一定的优势",{"2":{"290":1}}],["cnn或者全连接层的权重是静态权重",{"2":{"276":1}}],["cnn虽然可以借助增加卷积层数来扩大视野",{"2":{"274":1}}],["cnn和rnn都难以在源序列和目标序列之间做到完美对齐",{"2":{"256":1}}],["cnn和rnn方案",{"0":{"243":1},"1":{"244":1,"245":1,"246":1,"247":1,"248":1,"249":1,"250":1,"251":1,"252":1,"253":1,"254":1,"255":1,"256":1}}],["cnn的存在",{"2":{"291":1}}],["cnn的卷积操作可以提取重要特征",{"2":{"247":1}}],["cnn的本质是学习空间数据中的局部依赖关系",{"2":{"247":1}}],["cnn方案相比",{"2":{"273":1}}],["cnn方案",{"0":{"247":1}}],["cnn",{"2":{"242":1,"263":1,"274":1,"325":1,"511":1,"769":2,"1215":1,"1472":1}}],["cnn需要层叠多个卷积层才能捕捉词与词之间关系",{"2":{"160":1}}],["cnn中使用了不同的卷积核来关注图像中的不同特征",{"2":{"12":1}}],["cnblogs",{"2":{"47":1,"95":1,"156":1,"233":1,"292":1,"361":1,"387":1,"429":1,"513":1,"543":1,"638":1,"740":1,"768":1}}],["c",{"0":{"1715":1},"2":{"19":1,"105":8,"109":1,"131":1,"183":4,"224":1,"241":1,"315":2,"320":1,"325":4,"326":3,"337":1,"340":3,"341":6,"449":12,"461":5,"479":1,"540":1,"703":10,"724":1,"725":1,"740":1,"762":1,"807":1,"808":1,"809":1,"810":2,"820":3,"944":9,"957":5,"986":1,"1078":1,"1082":1,"1083":11,"1099":1,"1243":1,"1254":2,"1360":6,"1361":4,"1589":3,"1594":2,"1603":2,"1605":2,"1611":2,"1612":2,"1616":2,"1624":6,"1629":5,"1630":2,"1660":7,"1668":1,"1684":6,"1698":3,"1704":1,"1707":2,"1711":1,"1712":6,"1713":15,"1715":18,"1728":3,"1763":1,"1774":10,"1779":9,"1784":7,"1786":1,"1789":5,"1791":7,"1792":5,"1802":1,"1825":2,"1843":2,"1867":3,"1916":3,"1917":4,"1929":1,"1931":2,"1933":3,"2021":1,"2062":1}}],["coding",{"2":{"1440":1}}],["coderethan",{"2":{"1566":1,"2034":1,"2036":1}}],["code",{"0":{"1384":1},"2":{"449":1,"477":1,"768":1,"960":1,"1088":1,"1200":1,"1227":2,"1254":1,"1284":1,"1476":1,"1566":6,"1605":1,"1729":2,"2075":2,"2086":1}}],["cov",{"2":{"1087":1}}],["covariate",{"2":{"309":2,"310":1,"314":1,"361":2}}],["coalesce",{"2":{"1087":1}}],["coalesced",{"2":{"1086":2,"1087":1}}],["cookie",{"2":{"2153":4}}],["coordinate",{"2":{"1083":1,"1086":1}}],["coo",{"2":{"1083":2,"1086":1}}],["could",{"2":{"1902":1}}],["couldn",{"2":{"591":1}}],["cout",{"0":{"1673":1,"1678":1,"1816":1,"1834":1},"2":{"1606":2,"1607":15,"1608":6,"1611":4,"1616":3,"1619":4,"1620":1,"1621":4,"1623":1,"1624":5,"1625":6,"1633":5,"1634":4,"1638":1,"1639":1,"1640":1,"1645":15,"1646":6,"1647":5,"1648":5,"1649":5,"1650":10,"1653":2,"1654":2,"1659":3,"1660":3,"1661":2,"1662":1,"1663":5,"1665":14,"1666":1,"1667":5,"1668":4,"1670":2,"1672":1,"1673":6,"1674":6,"1675":3,"1676":2,"1677":2,"1678":8,"1680":4,"1683":4,"1684":3,"1685":4,"1687":4,"1688":6,"1691":8,"1693":3,"1694":6,"1695":7,"1698":2,"1699":2,"1700":2,"1701":1,"1704":6,"1705":6,"1706":5,"1707":6,"1708":5,"1709":2,"1712":2,"1713":24,"1714":11,"1715":8,"1718":2,"1719":12,"1720":8,"1721":8,"1722":8,"1723":2,"1724":6,"1725":5,"1726":1,"1728":9,"1729":8,"1736":1,"1737":1,"1738":1,"1739":2,"1741":2,"1742":2,"1743":2,"1744":2,"1746":1,"1747":2,"1749":2,"1750":1,"1751":2,"1752":4,"1754":2,"1755":1,"1756":2,"1761":1,"1762":2,"1772":1,"1774":2,"1778":1,"1779":2,"1784":4,"1788":2,"1789":3,"1791":1,"1792":1,"1797":7,"1799":3,"1800":3,"1801":3,"1802":3,"1803":1,"1805":1,"1806":5,"1807":4,"1811":3,"1813":6,"1814":2,"1815":1,"1816":4,"1817":10,"1820":1,"1821":4,"1824":3,"1825":1,"1826":2,"1829":3,"1831":6,"1832":2,"1833":1,"1834":4,"1835":10,"1838":1,"1839":4,"1842":3,"1843":1,"1844":2,"1849":1,"1853":1,"1857":1,"1861":1,"1866":3,"1867":1,"1868":2,"1869":5,"1874":4,"1883":7,"1887":6,"1891":4,"1895":1,"1897":2,"1898":1,"1902":2,"1905":1,"1906":3,"1907":2,"1908":2,"1909":2,"1910":2,"1911":4,"1912":3,"1914":10,"1921":3,"1922":5,"1923":1,"1924":2,"1925":4,"1926":5,"1927":7,"1928":2,"1929":4,"1930":8,"1931":2,"1933":2,"1999":4,"2003":2,"2004":2,"2005":1,"2006":7,"2007":1,"2008":1,"2059":6,"2060":3,"2061":1,"2062":10,"2063":1}}],["courses",{"2":{"2083":1}}],["course",{"2":{"638":2}}],["courville",{"2":{"543":1}}],["coupling",{"2":{"499":2}}],["counts=false",{"2":{"1083":2}}],["counts=none",{"2":{"590":1}}],["counts",{"2":{"590":8}}],["counter",{"2":{"557":3,"1110":1,"1868":7}}],["counter表示词汇表中的单词及其频次信息",{"2":{"557":1}}],["count",{"2":{"119":2,"422":1,"592":2,"1087":3,"1590":3,"1639":6,"1729":4,"1868":3,"1933":2}}],["cot的工作原理",{"2":{"513":1}}],["cot",{"2":{"480":1,"504":2}}],["cot赋予模型执行本质上串行计算的能力",{"2":{"480":1}}],["copysign",{"2":{"1087":4}}],["copy",{"0":{"1742":1},"2":{"449":2,"522":1,"658":1,"703":1,"1078":1,"1083":2,"1087":9,"1594":1,"1732":1,"1742":2,"1883":1,"1887":2,"1930":1,"1931":2,"2006":1}}],["copied",{"2":{"76":1}}],["correct",{"2":{"1215":4,"1295":3}}],["correction",{"2":{"1087":5}}],["corresponding",{"2":{"507":1,"558":1,"572":2}}],["corresponds",{"2":{"201":1}}],["corrcoef",{"2":{"1087":1}}],["corporations",{"2":{"2082":1}}],["corpora上进行",{"2":{"726":1}}],["corpus",{"2":{"908":1,"1309":1}}],["corda",{"2":{"233":2}}],["core中",{"2":{"420":1}}],["core",{"2":{"229":1,"343":1,"373":6,"522":1,"976":1,"2077":1}}],["cosh",{"2":{"1087":2}}],["cost",{"2":{"999":2}}],["cosineannealingwarmrestarts",{"0":{"1244":1},"2":{"1244":2}}],["cosineannealinglr",{"0":{"1243":1},"2":{"1243":1}}],["cosine",{"2":{"692":1,"740":1}}],["cos",{"2":{"176":8,"1087":2,"1243":1,"1299":1,"1336":1,"1345":2,"1360":3}}],["co",{"0":{"1430":1},"2":{"131":1,"387":1,"393":1,"638":2,"1309":1}}],["comes",{"2":{"1715":2}}],["com发送邮件",{"2":{"1196":1}}],["command",{"2":{"1986":1,"1993":4}}],["commands",{"2":{"1917":1}}],["commissionrate",{"2":{"1657":1}}],["commit",{"2":{"1486":1,"1487":1,"1488":1}}],["community",{"2":{"1605":1,"2087":1}}],["communicator",{"0":{"1577":1}}],["commutative",{"2":{"770":1}}],["comm",{"2":{"1575":2,"1577":1,"1590":9,"1594":5}}],["commonlooputils",{"2":{"1161":1}}],["common",{"2":{"592":1,"1315":1}}],["commoncrawl",{"2":{"387":2}}],["com",{"2":{"47":1,"95":1,"156":6,"233":1,"292":1,"361":3,"370":1,"373":1,"387":3,"429":4,"432":1,"503":1,"513":2,"543":1,"638":2,"713":1,"740":1,"768":8,"1195":1,"1197":1,"1302":1,"1309":1,"1332":1,"1347":1,"1476":1,"1481":6,"1485":3,"1486":1,"1487":1,"1526":1,"1528":1,"1566":1,"1605":3,"1989":1,"1990":1}}],["compare",{"2":{"1645":2,"1713":2,"1750":2}}],["comparedescending",{"2":{"1645":2}}],["compareascending",{"2":{"1645":2}}],["compilation",{"2":{"1604":1}}],["compilemessages",{"2":{"2070":1}}],["compile通过即时编译",{"2":{"1293":1}}],["compile是最新的方法",{"2":{"1293":1}}],["compile编译该模块的前向传播",{"2":{"1214":1}}],["compiled",{"2":{"1208":2}}],["compile",{"0":{"1293":1,"1299":1},"2":{"1208":2,"1214":2,"1227":1,"1299":3,"1984":2,"1985":2}}],["comprehend",{"2":{"2083":1}}],["comprehensive",{"2":{"141":1,"156":1,"711":1,"740":1}}],["compression",{"2":{"575":1}}],["complex32",{"2":{"1087":1}}],["complex",{"2":{"751":1,"1085":12,"1086":2,"1087":112,"1345":2,"1712":14}}],["complexity",{"2":{"160":1,"513":1,"945":1,"965":1}}],["completeness",{"2":{"504":1,"513":1}}],["complete",{"2":{"504":1}}],["components",{"2":{"233":1,"1991":1}}],["composed",{"2":{"1254":1}}],["compose``",{"2":{"1254":1}}],["compose",{"2":{"1215":1,"1254":4,"1283":1}}],["composable",{"2":{"43":2,"47":1}}],["compositionality",{"2":{"713":1,"740":1}}],["composition",{"2":{"40":1}}],["computing",{"2":{"1564":1}}],["computational",{"2":{"160":1,"1089":1}}],["computation",{"0":{"2075":1},"2":{"151":1,"160":1,"513":1,"1090":1,"2075":1}}],["computes",{"2":{"2086":1}}],["computers",{"2":{"688":1}}],["computer",{"0":{"1429":1,"1430":1},"2":{"429":1,"2077":1}}],["compute中",{"2":{"385":1}}],["compute",{"2":{"17":2,"67":1,"201":1,"235":1,"381":1,"385":3,"398":1,"399":2,"410":1,"423":2,"472":1,"638":1,"795":1,"933":1,"1087":1,"1328":1,"1329":2,"1330":1,"2086":1}}],["column",{"2":{"1329":2,"1330":1,"1332":3}}],["columnparallellinear和rowparallellinear是用于并行训练的两种并行线性层",{"2":{"201":1}}],["columnparallellinear",{"2":{"8":3,"114":2,"201":4}}],["col",{"2":{"1087":1,"1634":1}}],["color",{"2":{"1015":1,"1254":1}}],["colala",{"2":{"740":1}}],["collectstatic",{"2":{"2070":1}}],["collection",{"2":{"572":1}}],["collectives",{"2":{"508":1,"513":1}}],["collate",{"2":{"65":2,"375":4,"384":2,"558":3}}],["collaborate",{"2":{"19":1,"938":1,"954":1}}],["conjugate",{"2":{"1223":1}}],["conjugated",{"2":{"1082":1}}],["conj",{"2":{"1086":3,"1087":5}}],["con",{"2":{"769":1}}],["conference",{"2":{"740":1}}],["confidence",{"2":{"399":4}}],["configuration>",{"2":{"1481":2}}],["configuration",{"2":{"1481":1}}],["config",{"2":{"110":6,"364":2,"372":2,"422":6,"423":4,"723":11,"1481":3,"1987":1}}],["conclusion",{"0":{"2087":1},"2":{"2087":1}}],["concepts",{"2":{"688":2}}],["concept",{"2":{"625":2,"632":1,"638":1}}],["concat之类的结构",{"2":{"1001":1}}],["concat",{"0":{"824":1,"825":1},"1":{"825":1,"826":1,"827":1,"828":1},"2":{"10":3,"36":1,"739":1,"926":1,"927":1}}],["concatenated",{"2":{"1824":1,"1842":1}}],["concatenate",{"2":{"7":1,"19":1,"938":1,"954":1,"1216":1,"1218":1}}],["consuming",{"2":{"1304":1}}],["cons",{"2":{"1067":1}}],["consistency",{"2":{"985":1}}],["consider",{"2":{"160":1}}],["consecutive",{"2":{"590":3,"592":1,"1083":1}}],["constname",{"2":{"1921":1}}],["constid",{"2":{"1921":1}}],["constructor",{"2":{"1665":4,"1869":4,"1931":2,"2006":2}}],["constructed",{"2":{"1089":1,"1887":3}}],["construction",{"2":{"1086":1}}],["construct",{"2":{"449":1,"723":1}}],["constantlr",{"0":{"1237":1},"2":{"1237":1,"1246":1,"1247":1}}],["constants",{"0":{"1613":1,"1614":1},"2":{"702":1}}],["constant",{"2":{"402":1,"1613":1,"1729":1}}],["const",{"0":{"1614":1},"2":{"395":2,"1590":1,"1612":1,"1613":2,"1614":23,"1615":1,"1628":1,"1629":2,"1632":1,"1638":1,"1640":10,"1641":1,"1665":6,"1674":5,"1675":2,"1677":1,"1683":1,"1685":11,"1688":6,"1690":2,"1691":13,"1694":1,"1708":1,"1712":6,"1713":3,"1715":2,"1724":1,"1726":1,"1729":2,"1739":1,"1741":1,"1742":1,"1743":1,"1744":1,"1747":1,"1749":1,"1750":3,"1751":1,"1752":1,"1761":2,"1762":2,"1763":5,"1774":2,"1778":1,"1779":1,"1788":5,"1789":6,"1791":2,"1792":6,"1797":1,"1807":1,"1816":1,"1824":1,"1825":1,"1834":1,"1842":1,"1843":1,"1868":2,"1874":2,"1879":1,"1887":4,"1898":1,"1902":3,"1912":3,"1914":6,"1921":1,"1925":1,"1929":1,"1930":1,"1931":1,"1933":3,"2006":3,"2062":1,"2063":1}}],["constexpr",{"0":{"1924":1},"2":{"395":1,"1628":1,"1632":1,"1908":2,"1920":1,"1924":10,"1932":1}}],["connector",{"2":{"1481":1}}],["connect2",{"2":{"1205":2}}],["connect1",{"2":{"1205":4}}],["connect",{"2":{"296":1,"1205":1}}],["connection和mlp可以很好地阻止自注意力网络的这种",{"2":{"446":1}}],["connection则可以恢复网络的表达能力",{"2":{"305":1}}],["connection",{"2":{"296":1,"298":1,"300":1,"301":1,"302":1,"344":2,"446":3,"470":1,"517":1,"1217":1}}],["connections",{"0":{"298":1},"2":{"82":1,"293":1,"305":1,"344":1,"523":1,"529":1,"533":1}}],["connected",{"2":{"151":1}}],["convnext",{"2":{"1308":4}}],["conv3",{"2":{"1215":2}}],["conv3d",{"2":{"773":1}}],["conv2",{"2":{"1215":4,"1257":2}}],["conv2d",{"0":{"801":1},"2":{"801":3,"802":1,"1215":5,"1257":2,"1283":1}}],["conv1",{"2":{"1215":4,"1257":2,"1283":2}}],["convtranspose2d",{"0":{"802":1},"2":{"802":3}}],["conv的权重",{"2":{"770":1}}],["conversion",{"0":{"1682":1,"1683":1,"1684":1},"1":{"1683":1,"1684":1}}],["convergence",{"2":{"1242":1}}],["convert",{"2":{"1254":2,"1330":1}}],["convertedtemperature",{"2":{"1608":5}}],["converted",{"2":{"688":1}}],["converting",{"0":{"938":1,"954":1},"2":{"47":1}}],["conventional",{"2":{"1086":1}}],["convection",{"2":{"498":1}}],["convolution",{"0":{"775":1,"776":1,"778":2,"779":1,"780":1,"800":1},"1":{"781":1,"782":1,"801":1,"802":1},"2":{"235":2,"769":1,"911":1,"1404":1}}],["convolutional",{"2":{"105":1,"156":1,"290":2,"292":2,"747":1,"748":1,"769":2}}],["convolutions",{"0":{"777":1},"2":{"101":1}}],["condition",{"2":{"1087":2,"1621":1}}],["conditional",{"0":{"1319":1},"1":{"1320":1,"1321":1,"1322":1,"1323":1,"1324":1,"1325":1,"1326":1},"2":{"894":1,"898":1,"1316":1}}],["cond",{"2":{"76":3}}],["container",{"2":{"1914":4}}],["contains",{"2":{"1083":1}}],["containing",{"2":{"700":1,"834":2}}],["contenttypes",{"2":{"2070":2}}],["contents=none",{"2":{"1083":1}}],["content",{"2":{"758":3,"760":1,"1566":1,"1902":14}}],["contextualized",{"2":{"717":2}}],["contexthttps",{"2":{"233":1}}],["context",{"0":{"976":1},"2":{"89":1,"95":1,"156":1,"173":1,"204":1,"229":1,"231":1,"233":2,"241":1,"259":1,"271":1,"420":1,"513":1,"542":1,"543":1,"713":1,"714":1,"727":1,"760":1,"768":4,"888":1,"889":1,"891":1,"974":5,"976":1,"1332":2}}],["context矩阵的第i行实际上是由v的前i行加权平均的结果",{"2":{"71":1}}],["control",{"2":{"396":1,"429":1}}],["contrastive",{"2":{"156":1,"429":1,"727":1,"734":2,"740":1,"1360":1}}],["contraction",{"2":{"113":1}}],["continually",{"2":{"543":1}}],["continues",{"2":{"1762":1,"2087":1}}],["continue",{"2":{"139":1,"1627":1,"1631":2}}],["continuous",{"2":{"17":2,"281":1,"292":2,"507":1,"714":1}}],["contiguous",{"2":{"36":2,"201":1,"395":1,"398":2,"410":2,"472":2,"658":2,"723":1,"981":1,"986":2,"1087":4,"1216":1,"1218":1}}],["语文",{"2":{"1825":1,"1843":1}}],["语句实现条件编译",{"2":{"1985":1}}],["语句中初始化状态码",{"2":{"1922":1}}],["语句中初始化并查找元素",{"2":{"1922":1}}],["语句中直接初始化变量",{"2":{"1922":1}}],["语句中每个token对应一个向量",{"2":{"158":1}}],["语句初始化来简化插入",{"2":{"1933":1}}],["语句初始化",{"0":{"1922":1},"2":{"1920":1,"1933":1}}],["语句后跟要返回的值",{"2":{"1729":1}}],["语句可以嵌套在其他控制流语句中",{"2":{"1631":1}}],["语句可以嵌套在另一个",{"2":{"1631":1}}],["语句块中",{"2":{"1631":1}}],["语句块的执行",{"2":{"1631":1}}],["语句嵌套",{"2":{"1631":2}}],["语句进行优化",{"2":{"1630":1}}],["语句在控制流中的作用",{"2":{"1627":1}}],["语句跳出",{"2":{"1621":1}}],["语句类似",{"2":{"1620":1}}],["语句与",{"2":{"1620":1}}],["语句用于在给定条件为真的情况下",{"2":{"1620":1}}],["语句的简写形式",{"2":{"1619":1}}],["语句的核心是条件表达式",{"2":{"1619":1}}],["语句有多种形式",{"2":{"1619":1}}],["语句允许程序根据一个条件的真假来决定是否执行特定的代码块",{"2":{"1619":1}}],["语句",{"0":{"1619":1,"1620":1},"2":{"1491":1,"1619":1,"1620":1,"1630":1,"1631":3,"1649":1,"1729":3,"1905":1}}],["语料进行微调",{"2":{"1313":1}}],["语音音频",{"2":{"1472":1}}],["语音处理",{"2":{"878":1}}],["语音和视频等",{"2":{"250":1}}],["语义角色标注",{"2":{"906":1}}],["语义影响",{"0":{"755":1},"2":{"741":1}}],["语义搜索",{"2":{"696":1}}],["语义搜索能够让计算机更好的理解人类的语言和需求",{"2":{"678":1}}],["语义理解",{"2":{"696":1}}],["语义理解是自然语言的基础",{"2":{"678":1}}],["语义文本相似度",{"2":{"692":1}}],["语义分析是人工智能实现的基础",{"2":{"687":1}}],["语义相似性是基于分布假说的",{"2":{"685":1}}],["语义相似性",{"0":{"685":1},"2":{"682":2}}],["语义表示能力",{"2":{"676":1}}],["语义独立性和语义表达能力",{"2":{"567":1}}],["语义",{"2":{"512":1,"687":2}}],["语义中心",{"2":{"318":1}}],["语义对齐",{"2":{"287":1}}],["语义逻辑",{"2":{"12":1,"33":1}}],["语言相同",{"2":{"1728":1}}],["语言的",{"2":{"1728":1}}],["语言的增强版",{"2":{"1603":1}}],["语言中的",{"2":{"1728":1}}],["语言中的对应操作",{"2":{"1668":1}}],["语言中只有指针",{"2":{"1612":1}}],["语言没有引用",{"2":{"1612":1}}],["语言对之间可能存在共享知识可以用来处理小众语言之间的翻译",{"2":{"1317":1}}],["语言生成",{"2":{"906":1}}],["语言到token",{"2":{"740":1}}],["语言破坏了空间关系",{"2":{"689":1}}],["语言作为一个抽象符号",{"2":{"689":1}}],["语言特性",{"2":{"568":1}}],["语言无关",{"2":{"563":1}}],["语言规范化",{"2":{"552":1}}],["语言是人类特有的概念",{"2":{"545":1}}],["语言",{"2":{"513":1}}],["语言理解等的不同路径或组件",{"2":{"221":1}}],["语言理解等不同的组件",{"2":{"221":1}}],["语言模型可能面临数据稀疏性的问题",{"2":{"898":1}}],["语言模型",{"2":{"894":1,"906":1,"1316":1}}],["语言模型输出端共享embedding的重新探索",{"2":{"740":1}}],["语言模型之text",{"2":{"740":1}}],["语言模型就是一个压缩器",{"2":{"684":1}}],["语言模型在逐层为输入分配特征",{"2":{"482":1}}],["语言模型总是在抑制正确答案的输出",{"2":{"437":1}}],["语言模型存在一种普遍机制",{"2":{"437":1}}],["语言模型将文本看作是时间序列",{"2":{"238":1}}],["语言模型完成事实回忆任务使用到的若干重要机制",{"2":{"156":1}}],["语言模型的物理学",{"2":{"156":1}}],["语言模型需要学习的一个重要信息子集是简单关联",{"2":{"154":1}}],["语言模型中的参数数量决定了语言模型在训练期间学习和存储信息的能力",{"2":{"119":1}}],["语法上两者并没有本质区别",{"2":{"1728":1}}],["语法更简洁",{"2":{"1650":1}}],["语法结构",{"2":{"1619":1,"1620":1,"1621":1}}],["语法高亮",{"2":{"1605":1}}],["语法",{"2":{"709":1,"1491":1,"1612":1,"1641":1}}],["语法和概念等",{"2":{"123":1}}],["语法信息",{"2":{"20":1}}],["语法逻辑",{"2":{"4":1,"12":1,"33":1}}],["均对应着一个标签",{"2":{"1323":1}}],["均为线性链表示的随机变量序列",{"2":{"1322":1}}],["均为独立同分布的",{"2":{"1000":1}}],["均值都是0",{"2":{"1004":1}}],["均值分布",{"2":{"848":1}}],["均值的问题",{"2":{"839":1}}],["均值",{"2":{"839":1}}],["均值为零",{"2":{"838":1}}],["均值池",{"2":{"735":1}}],["均值和标准化是由输入决定的",{"2":{"313":1}}],["均匀分布",{"2":{"1007":1}}],["均匀注意力",{"2":{"93":1}}],["均匀划分到各注意头中来完成的",{"2":{"29":1}}],["均是",{"2":{"12":1}}],["均衡单一注意力机制可能产生的偏差",{"2":{"5":1}}],["那种不因别人的评价而影响自己",{"2":{"2054":1}}],["那真的是很难得啊",{"2":{"2054":1}}],["那也分为好多类",{"2":{"2054":1}}],["那基本都是",{"2":{"2054":1}}],["那更是比",{"2":{"2054":1}}],["那便自然而然能够说得通",{"2":{"2054":1}}],["那这时候就是",{"2":{"2054":1}}],["那这个标签序列就是模型的输出",{"2":{"1320":1}}],["那你就真正明白什么是",{"2":{"2054":1}}],["那你将永远不知道大脑是怎么工作的",{"2":{"506":1}}],["那我们不妨用简单的案例来让你感受感受",{"2":{"2121":1}}],["那我们到时候直接沠一个跑腿送过来就行",{"2":{"2051":1}}],["那我将无法取得任何的进展",{"2":{"506":1}}],["那是无法用文字来说清楚的",{"2":{"2054":1}}],["那是一件多么快乐的事情啊",{"2":{"2054":1}}],["那是由于内存本身的缺陷引起的",{"2":{"1477":1}}],["那是不是大模型直接生成向量的效果就是不理想呢",{"2":{"736":1}}],["那如何获得输入数字",{"2":{"1372":1}}],["那一维归一化",{"2":{"1339":1}}],["那一个是weight",{"2":{"1109":1}}],["那下一个位置就很有可能是",{"2":{"1324":1}}],["那什么是线性链crf呢",{"2":{"1322":1}}],["那视图的底层逻辑到底是什么呢",{"2":{"1079":1}}],["那dropblock为什么在卷积网络上可以有效果",{"2":{"1019":1}}],["那卷积层呢",{"2":{"1018":1}}],["那时relu激活函数还未兴起",{"2":{"1000":1}}],["那些表现优异的smt模型",{"2":{"908":1}}],["那些被跳过的层并不是真正的被跳过",{"2":{"301":1}}],["那不就是通过前面定义的score函数来比较吗",{"2":{"904":1}}],["那怎么去计算每一步的损失呢",{"2":{"899":1}}],["那它们就算不一样的词",{"2":{"576":1}}],["那样",{"2":{"2101":1}}],["那样就是",{"2":{"528":1}}],["那样模型有可能利用已经存在的未来词来辅助当前词的生成",{"2":{"525":1}}],["那数值的发散足以成为灾难",{"2":{"314":1}}],["那就要不断地去见习",{"2":{"2054":1}}],["那就说明ai领域将迎来新的历史性突破",{"2":{"280":1}}],["那就是要做到",{"2":{"2056":1}}],["那就是每一步最优",{"2":{"901":1}}],["那就是做一个到各个分类的映射",{"2":{"267":1}}],["那就是所有的归因加起来就是f",{"2":{"134":1}}],["那就意味着其难以压缩和加速",{"2":{"118":1}}],["那么最多就只能处理长度为512的句子",{"2":{"1337":1}}],["那么最大训练step数就太高了",{"2":{"1155":1}}],["那么问题来了",{"2":{"1320":1}}],["那么您可能无需再次签署",{"2":{"1197":1}}],["那么您需要负责调用",{"2":{"1122":1}}],["那么该模型可能存在着优化不稳定性的情况",{"2":{"1182":1}}],["那么修复不稳定性可能会得到更好的训练结果",{"2":{"1179":1}}],["那么需要对学习率",{"2":{"1174":1}}],["那么需要对学习率以及",{"2":{"1174":1}}],["那么只需要对基本学习率进行调整",{"2":{"1174":1}}],["那么只要ggg是准强连通的",{"2":{"94":1}}],["那么理想情况是在许多不同的调整轮次中逐渐增加训练运行的长度",{"2":{"1159":1}}],["那么max",{"2":{"1156":1}}],["那么训练时间越长",{"2":{"1155":1}}],["那么训练工作流可能存在瓶颈",{"2":{"1132":1}}],["那么以后可能很难改变它",{"2":{"1137":1}}],["那么直到项目成熟且容易权衡成本效益前",{"2":{"1134":1}}],["那么使用更大的batch",{"2":{"1133":1}}],["那么可以对其进行调优",{"2":{"1184":1}}],["那么可以免费获得性能提升",{"2":{"1121":1}}],["那么可能需要重新运行整个过程",{"2":{"1183":1}}],["那么可得到输出",{"2":{"1000":1}}],["那么作为梯度使用的值是任意的",{"2":{"1115":1}}],["那么模型层数就是目标超参数",{"2":{"1143":1}}],["那么模型更容易过拟合",{"2":{"1012":1}}],["那么模型参数在更新时可能会因为步长过大而跳出最优解的范围",{"2":{"400":1}}],["那么一个自然的思路就是让每张卡去算",{"2":{"974":1}}],["那么具体怎么做呢",{"2":{"934":1}}],["那么具有平移不变性的算法或函数的输出不会受输入变化的影响",{"2":{"320":1}}],["那么每步消耗的增加可能超过训练所需步数的减少",{"2":{"1134":1}}],["那么每一步资源消耗的增加可能被步骤数的减少所抵消",{"2":{"1134":1}}],["那么每一步就需要分出k×v个分支并逐一计算score",{"2":{"902":1}}],["那么每个参数设置不同的学习率",{"2":{"1041":1}}],["那么每个特征的归因可以认为是对该样本属于label=1的贡献",{"2":{"134":1}}],["那么很明显这样的输出必须依赖以前的输入",{"2":{"850":1}}],["那么query端就不再需要位置编码了",{"2":{"760":1}}],["那么为了回忆存储的知识",{"2":{"754":1}}],["那么为何transformer为何选取点积注意力而非用加法注意力",{"2":{"175":1}}],["那么嵌入矩阵的大小就是",{"2":{"700":1}}],["那么词嵌入矩阵",{"2":{"698":1}}],["那么词表扩增通常是有必要的",{"2":{"560":1}}],["那么我们就可以用降维的数据进行机器学习模型的训练和预测",{"2":{"1370":1}}],["那么我们不妨降低学习率",{"2":{"1184":1}}],["那么我们会将此图中的最佳点与没有权重衰减的baseline进行比较",{"2":{"1150":1}}],["那么我们通常更愿意使用额外的正则化再次尝试",{"2":{"1149":1}}],["那么我们只考虑使用该batch",{"2":{"1132":1}}],["那么我们应该如何初始化weight呢",{"2":{"1003":1}}],["那么我们将其改写为",{"2":{"579":1}}],["那么我们或许可以期待它对decoder",{"2":{"542":1}}],["那么我们可能应该只降低学习率",{"2":{"1184":1}}],["那么我们可能应该采用它作为新的baseline为以后的比较",{"2":{"1152":1}}],["那么我们可以更努力地尝试",{"2":{"1184":1}}],["那么我们可以把instance",{"2":{"337":1}}],["那么我们可不可以观察特征是怎么逐层变化的",{"2":{"482":1}}],["那么整个过程就可以通过下图来进行表示",{"2":{"537":1}}],["那么它右边的所有参数都必须有默认值",{"2":{"1708":1}}],["那么它就是一个目标超参数",{"2":{"1143":1}}],["那么它可以被称为图灵完备",{"2":{"504":1}}],["那么它们就不应该有过多的交互",{"2":{"1340":1}}],["那么它们之间的关系会被严格限制在一个固定的模式中",{"2":{"172":1}}],["那么它们各自的通道都会只打开一部分",{"2":{"168":1}}],["那么把lm",{"2":{"482":1}}],["那么是如何实现让模型关注序列内部之间的关系呢",{"2":{"442":1}}],["那么其表示的是",{"2":{"341":1}}],["那么参数几乎没有被更新",{"2":{"333":1}}],["那么对我们来说",{"2":{"1175":1}}],["那么对应的",{"2":{"722":1}}],["那么对应的参数都会有常数量级的更新量",{"2":{"333":1}}],["那么对中间每一层的这个流程进行破解",{"2":{"147":1}}],["那么在预测时如果输入超过",{"2":{"1341":1}}],["那么在尝试其他东西之前解决这些问题很重要",{"2":{"1178":1}}],["那么在未来的实验中",{"2":{"1143":1}}],["那么在前向传播过程中",{"2":{"992":1}}],["那么在多层传播后",{"2":{"403":1}}],["那么在反向传播过程中",{"2":{"403":1}}],["那么在初始化阶段",{"2":{"332":1}}],["那么在神经网络训练过程中会自然出现叠加现象",{"2":{"118":1}}],["那么残差中的",{"2":{"332":1}}],["那么残差连接的输出就是x",{"2":{"300":1}}],["那么平均值和方差会受到",{"2":{"326":1}}],["那么二者区别何在",{"2":{"312":1}}],["那么就初始化一个512×768的矩阵作为位置向量",{"2":{"1337":1}}],["那么就需要进行",{"2":{"1184":1}}],["那么就不如折中方案",{"2":{"411":1}}],["那么就给它补上一个梯度为常数的项",{"2":{"304":1}}],["那么就算后面36层网络什么都不做",{"2":{"299":1}}],["那么就可以利用单词分布不均衡的特点",{"2":{"185":1}}],["那么经过多层传播后",{"2":{"296":1}}],["那么这个序列的转移分数可按照如下方式计算",{"2":{"1324":1}}],["那么这个搜索空间就有可疑之处",{"2":{"1147":1}}],["那么这个神经元之后的梯度就永远是0了",{"2":{"840":1}}],["那么这个非叶子节点的requires",{"2":{"661":1,"1104":1}}],["那么这个错误的输出就会作为下一轮解码器的输入",{"2":{"405":1}}],["那么这个压缩过程就是有损压缩",{"2":{"252":1}}],["那么这些残差链接起到了怎样的作用呢",{"2":{"306":1}}],["那么这些信息是如何聚合到一个最终分布的呢",{"2":{"129":1}}],["那么这些特征组成了激活空间上的一组过完备基",{"2":{"118":1}}],["那么注意力均匀分布到所有token上",{"2":{"194":1}}],["那么注意力将聚焦到某一个token上",{"2":{"194":1}}],["那么新算出来各个ai",{"2":{"194":1}}],["那么softmax将产生一个接近",{"2":{"192":1}}],["那么如何进行分类呢",{"2":{"185":1}}],["那么",{"2":{"184":1,"301":1,"338":1,"393":1,"512":1,"560":1,"714":1,"974":1,"1183":1,"1184":1,"1601":1}}],["那么令牌的秩崩溃就会指数级发生",{"2":{"93":1}}],["那么切分也只是逻辑上的切分",{"2":{"29":1}}],["那么让我们再看看这个",{"2":{"12":1}}],["那干脆让那个头对应的可学习参数大些",{"2":{"10":1}}],["前任铺路后人走",{"0":{"2143":1,"2145":1},"1":{"2144":1}}],["前置形式通常比后置形式效率略高",{"2":{"1630":1}}],["前置形式",{"2":{"1630":1}}],["前置钩子",{"2":{"1227":1}}],["前置钩子函数",{"2":{"1226":1}}],["前加",{"2":{"1607":1,"1676":1}}],["前期成本",{"2":{"1134":1}}],["前期较小的时候",{"2":{"1044":1}}],["前端提示",{"2":{"986":1}}],["前端首先发送前缀作为提示",{"2":{"986":1}}],["前端解释器将完整的提示发送到runtime",{"2":{"986":1}}],["前端开发工程师负责与用户界面相关的工作",{"2":{"5":1}}],["前提是在更改batch",{"2":{"1133":1}}],["前提是该知识在预训练阶段被充分训练",{"2":{"147":1}}],["前提",{"2":{"944":1,"963":1,"1260":1,"1264":1}}],["前11层只是用相对位置编码",{"2":{"763":1}}],["前两步改造使得llm可以生成高质量的token级别的表征",{"2":{"734":1}}],["前两个阶段是保证通用性的基石",{"2":{"726":1}}],["前256个单字节token",{"2":{"592":1}}],["前文分析过",{"2":{"757":1}}],["前文简述了如何加载词表",{"2":{"557":1}}],["前文提到",{"2":{"541":1}}],["前文提到过",{"2":{"405":1}}],["前一层权重梯度计算",{"0":{"1394":1}}],["前一个任务被称为遮盖语言建模",{"2":{"1315":1}}],["前一个解码器层的输出",{"2":{"526":1}}],["前一时刻解码器的输出",{"2":{"526":1}}],["前",{"2":{"477":1,"1214":1,"1227":1}}],["前三个参数都是输入",{"2":{"344":1}}],["前三个线性层分别用于对q向量",{"2":{"23":1}}],["前向过程为",{"2":{"1392":1}}],["前向",{"2":{"1208":1,"1398":1}}],["前向执行完后调用",{"2":{"1208":1}}],["前向钩子函数展示",{"0":{"1212":1}}],["前向钩子函数是否带参数",{"2":{"1208":1}}],["前向钩子函数是否always",{"2":{"1208":1}}],["前向钩子函数",{"2":{"1208":1}}],["前向推导过程",{"0":{"1003":1}}],["前向时计算row",{"2":{"971":1}}],["前向语言模型从左到右对文本进行编码",{"2":{"717":1}}],["前向函数的参数如下",{"2":{"523":1}}],["前向计算时对数值进行归一化",{"2":{"320":1}}],["前向传播可以持续向前直到它产生一个标量",{"2":{"1438":1}}],["前向传播过程",{"0":{"1387":1},"1":{"1388":1,"1389":1}}],["前向传播计算",{"2":{"1215":1}}],["前向传播发生的两件事",{"2":{"1089":1}}],["前向传播中的空操作和反向传播中的",{"2":{"976":1}}],["前向传播中的",{"2":{"976":2}}],["前向传播+反向传播求导",{"2":{"840":1}}],["前向传播函数",{"2":{"701":1}}],["前向传播函数接受两个参数",{"2":{"522":1}}],["前向传播函数有四个参数",{"2":{"450":1,"532":1,"533":1}}],["前向传播角度",{"0":{"482":1}}],["前向传播的算法描述",{"2":{"216":1}}],["前向传播方法",{"2":{"113":1}}],["前向传播",{"0":{"147":1},"2":{"96":1,"1211":1,"1213":1,"1216":1,"1295":1}}],["前者瓶颈在于计算",{"2":{"420":1}}],["前者同时考虑了编码器的隐状态",{"2":{"285":1}}],["前者是对数据进行有差别的对待",{"2":{"276":1}}],["前半部分的卷积组负责处理前半部分的输入层",{"2":{"775":1}}],["前半部分的rnn只有输入",{"2":{"241":1}}],["前半部分可以叫做编码器",{"2":{"241":1}}],["前后两轮的输入只相差一个",{"2":{"239":1}}],["前言与背景知识",{"2":{"156":1}}],["前馈全连接层",{"2":{"448":1}}],["前馈网络",{"2":{"620":1}}],["前馈网络的key捕捉了输入的某种模式",{"2":{"126":1}}],["前馈网络可以分为两种主要类型",{"2":{"98":1}}],["前馈层占了模型大约三分之二的参数量",{"2":{"119":1}}],["前馈神经网络计算流程",{"0":{"1467":1}}],["前馈神经网络的概念",{"0":{"1457":1}}],["前馈神经网络的一种特例",{"2":{"772":1}}],["前馈神经网络的关系",{"0":{"772":1}}],["前馈神经网络",{"0":{"1451":1},"2":{"97":1,"517":1,"769":1}}],["前面说了知己难遇",{"2":{"2054":1}}],["前面一直说系统资源",{"2":{"1411":1}}],["前面画的几个图展示的预测过程",{"2":{"901":1}}],["前面所有的输入都对未来的输出产生了影响",{"2":{"860":1}}],["前面所有节点得到的信息都保存在上一步计算出来的中间隐状态ht中",{"2":{"249":1}}],["前面",{"2":{"860":1}}],["前面提及的解码策略有一种缺陷",{"2":{"727":1}}],["前面提及了多维向量",{"2":{"684":1}}],["前面提到过",{"2":{"596":1,"898":1}}],["前面提到的",{"2":{"242":1}}],["前面提到",{"2":{"12":1,"526":1}}],["前面已经讲过",{"2":{"1313":1}}],["前面已经简单介绍过注意力的分类",{"2":{"534":1}}],["前面已经介绍过",{"2":{"528":1}}],["前面已经分析了这种方案的弊端",{"2":{"267":1}}],["前面加载词表时我们提到",{"2":{"375":1}}],["前面加个1是为了和tgt的维度保持一致",{"2":{"74":1}}],["前面后加上",{"2":{"344":1}}],["前面是从表象来看",{"2":{"324":1}}],["前面对后面的影响越弱",{"2":{"255":1}}],["前面几本书的权重就高",{"2":{"169":1}}],["前面我们详细介绍了seq2seq的内部的结构",{"2":{"899":1}}],["前面我们从数据库角度来看到如何寻址获取讯息",{"2":{"166":1}}],["前面我们也提到",{"2":{"142":1}}],["前面主要从实体的角度调查",{"2":{"136":1}}],["前面的元素均不大于它",{"2":{"1752":1}}],["前面的层也许几乎没有得到更新",{"2":{"296":1}}],["前面的梯度信息无法有效地传递",{"2":{"255":1}}],["前面的词不应该注意后面的词",{"2":{"77":1}}],["前面的讨论已经知道",{"2":{"71":1}}],["前i个权重之和为1",{"2":{"71":1}}],["前缀来表示二进制字面量",{"2":{"1910":1}}],["前缀递增和递减",{"2":{"1635":1}}],["前缀树",{"2":{"986":1}}],["前缀解码器可以对前缀序列进行双向编码",{"2":{"541":1}}],["前缀解码器架构",{"2":{"541":1}}],["前缀解码器",{"2":{"541":1}}],["前缀和目标token之间的注意力",{"2":{"541":1}}],["前缀",{"2":{"59":1,"409":1}}],["以后要常相守了",{"2":{"2056":1}}],["以后还是随手关一关",{"2":{"2054":1}}],["以二进制模式写入文件",{"2":{"1820":1,"1838":1}}],["以二进制模式打开文件",{"2":{"1820":1,"1838":1}}],["以追加模式打开文件",{"2":{"1820":1,"1838":1}}],["以写入模式打开文件",{"2":{"1820":2,"1838":2}}],["以读取模式打开文件",{"2":{"1820":1,"1838":1}}],["以十进制显示整数",{"2":{"1817":1,"1835":1}}],["以十六进制显示整数",{"2":{"1817":1,"1835":1}}],["以八进制显示整数",{"2":{"1817":1,"1835":1}}],["以科学计数法表示浮点数",{"2":{"1817":1,"1835":1}}],["以固定点表示法显示浮点数",{"2":{"1817":1,"1835":1}}],["以固定大小的瓶颈代替交叉注意力",{"2":{"631":1}}],["以处理特殊的数据格式或设备",{"2":{"1810":1,"1828":1}}],["以处理当前上下文窗口大小",{"2":{"231":1}}],["以返回多个值",{"2":{"1805":1}}],["以重载",{"2":{"1789":1}}],["以打印地址",{"2":{"1704":1}}],["以空字符",{"2":{"1704":1}}],["以同类型的另一个对象作为参数的构造函数",{"2":{"1675":1}}],["以内除以",{"2":{"1625":1}}],["以内的偶数",{"2":{"1625":1}}],["以字节为单位",{"2":{"1607":1,"1630":1}}],["以人类可读格式显示文件大小",{"2":{"1509":1}}],["以取得最大化的预期利益",{"2":{"1472":1}}],["以取得更好的性能",{"2":{"1312":1}}],["以10",{"2":{"1363":1}}],["以更新权重来最小化损失函数",{"2":{"1439":1}}],["以更大的批次训练了更长的时间",{"2":{"1315":1}}],["以更好地适应由数据和损失函数指定的集合",{"2":{"508":1}}],["以生成一个面向学术",{"2":{"1313":1}}],["以有监督学习的方式对预训练模型参数进行微调",{"2":{"1312":1}}],["以有效地组合任务特定的",{"2":{"225":1}}],["以class",{"2":{"1227":1}}],["以cbow为例",{"2":{"714":1}}],["以捕获静态图",{"2":{"1214":1}}],["以捕捉单词之间的语义和句法关系",{"2":{"711":1}}],["以捕捉source端和target端词与词之间的依赖关系",{"2":{"649":1,"931":1}}],["以捕捉source端或target端自身的词与词之间的依赖关系",{"2":{"649":1,"931":1}}],["以一个恒定的速率进行训练",{"2":{"1183":1}}],["以一定概率随机屏蔽每一层中若干神经元",{"2":{"1017":1}}],["以支持离线分析",{"2":{"1164":1}}],["以实时监控其进度",{"2":{"1164":1}}],["以实现真实的交互效果",{"2":{"2009":1}}],["以实现良好的训练效果",{"2":{"1154":1}}],["以实现加速",{"2":{"977":1}}],["以实现对内存访问的细粒度控制",{"2":{"940":1,"962":1}}],["以实现对前缀token的双向关注和仅对生成的token的单向关注",{"2":{"541":1}}],["以实现高效计算",{"2":{"610":1}}],["以优化max",{"2":{"1155":1}}],["以使模型达到最佳效果",{"2":{"1155":1}}],["以使用尽可能大的调整预算进行最终的自动调整研究",{"2":{"1153":1}}],["以使损失函数达到最优或接近最优",{"2":{"1021":1}}],["以估计训练方差",{"2":{"1152":1}}],["以改进搜索空间和",{"2":{"1146":1}}],["以公平地比较目标超参数",{"2":{"1145":1}}],["以充分满足以下三个要求",{"2":{"1145":1}}],["以朝着实验目标取得进展",{"2":{"1144":1}}],["以平衡资源成本与科学价值",{"2":{"1142":1}}],["以gpu小时计",{"2":{"1134":1}}],["以帮助大众追求最佳方法",{"2":{"1127":1}}],["以帮助调节网络",{"2":{"866":1}}],["以呈现更清晰的原理",{"2":{"1127":1}}],["以计算梯度",{"2":{"1113":1}}],["以计算更高级别的权重",{"2":{"209":1}}],["以在有限的资源和耐心内获得最大的理解",{"2":{"1157":1}}],["以在不同设置之间进行公平比较目标超参数",{"2":{"1143":1}}],["以在非凸设定下效果更好",{"2":{"1048":1}}],["以在需要的时间和地点获取信息",{"2":{"260":1}}],["以控制生成文本的风格",{"2":{"1316":1}}],["以控制参数更新的步长",{"2":{"1027":1}}],["以控制下一句话表示采样的随机性和多样性水平",{"2":{"636":1}}],["以卷积为例",{"2":{"1003":1}}],["以保持网络中正向和反向数据流动",{"2":{"999":1}}],["以保持自回归属性",{"2":{"535":1}}],["以达到较好的性能",{"2":{"988":1}}],["以启用共享",{"2":{"986":1}}],["以允许两个聊天会话共享系统提示",{"2":{"986":1}}],["以允许针对gemm目标fp8张量核心",{"2":{"973":1}}],["以动态扩展或缩减sp组",{"2":{"977":1}}],["以满足ttft",{"2":{"977":1}}],["以满足实际需求",{"2":{"612":1}}],["以块对块的方式进行注意力和前馈计算来解释这一点",{"2":{"975":1}}],["以规避softmax和gemm之间的某些顺序依赖关系",{"2":{"973":1}}],["以防止损失函数产生偏向",{"2":{"1164":1}}],["以防止位置关注到后面的位置",{"2":{"915":1}}],["以防止模型访问未来的标记",{"2":{"464":1}}],["以概率1",{"2":{"897":1}}],["以概率p靠自己上一步的输入来预测",{"2":{"897":1}}],["以概率p随机将输入张量的某些元素置零",{"2":{"835":1}}],["以决定隐藏状态应携带哪些信息",{"2":{"868":1}}],["以进一步改进在新的gpu架构上的性能",{"2":{"973":1}}],["以进一步提高连接的稀疏性",{"2":{"841":1}}],["以进行归一化",{"2":{"338":1}}],["以找到最适合的激活函数",{"2":{"838":1}}],["以增加卷积神经网络的非线性表达能力",{"2":{"838":1}}],["以增强模型的表示能力",{"2":{"466":2}}],["以绝对位置编码的形式",{"2":{"767":1}}],["以t5",{"2":{"766":1}}],["以alibi为代表的绝对偏置编码",{"2":{"765":1}}],["以qtqtq",{"2":{"760":1}}],["以适应顺序试验中的多个这样的评估",{"2":{"1165":1}}],["以适应不同任意的距离",{"2":{"759":1,"1339":1}}],["以适应特定任务或应用的更有效且高效的技术路线",{"2":{"139":1}}],["以句子",{"2":{"744":1}}],["以表示该元素在序列中的具体位置",{"2":{"742":1}}],["以形成更全面的嵌入表示",{"2":{"739":1}}],["以前的双向嵌入模型通常使用均值池mean",{"2":{"735":1}}],["以确定哪种方法在特定的使用案例中效果更好",{"2":{"874":1}}],["以确定最适合特定应用的相似度",{"2":{"692":1}}],["以确保输入操作成功完成",{"2":{"1814":1,"1832":1}}],["以确保资源得到释放",{"2":{"1762":1}}],["以确保所有数组元素的析构函数都被调用",{"2":{"1647":1}}],["以确保这些共用资源是被互斥获得使用",{"2":{"1413":1}}],["以确保数据一致性",{"2":{"1407":1}}],["以确保它们在长时间运行中仍然适用",{"2":{"1157":1}}],["以确保编码后的文本符合特定模型的输入要求",{"2":{"555":1}}],["以确保每个输入token只能关注过去的token和自身",{"2":{"541":1}}],["以确保每个查询和键的点积在控制范围内",{"2":{"355":1}}],["以确保后续层接收到维度一致的输入",{"2":{"466":1}}],["以确保准确重建",{"2":{"213":1}}],["以确保任意两个位置之间总有一个路径",{"2":{"204":1}}],["以确保其能够传递准确的信息",{"2":{"140":1}}],["以推荐系统为例",{"2":{"692":1}}],["以sd",{"2":{"1363":4}}],["以sora为例",{"2":{"689":1}}],["以self",{"2":{"36":1}}],["以相似性进行举例",{"2":{"688":1}}],["以文本翻译为例",{"2":{"676":1}}],["以获得浮点数结果",{"2":{"1623":1}}],["以获得更高的执行效率",{"2":{"1287":1}}],["以获得更好的效果",{"2":{"1155":1}}],["以获得适当数量的批归一化统计样本",{"2":{"1168":1}}],["以获得",{"2":{"1137":1}}],["以获得概念序列",{"2":{"628":1}}],["以获得最稳定的模型输出",{"2":{"392":1}}],["以flops计",{"2":{"613":1}}],["以效率换取性能",{"2":{"612":1}}],["以列表形式表示",{"2":{"580":1}}],["以单个字符作为最小颗粒度",{"2":{"564":1}}],["以英文文本",{"2":{"564":1}}],["以减少未定义行为",{"2":{"1931":1}}],["以减少未定义行为的可能性",{"2":{"1931":1}}],["以减少手动管理的错误",{"2":{"1672":1}}],["以减少推断期间kv缓存的大小",{"2":{"971":1}}],["以减少非矩阵乘法flop的数量",{"2":{"968":1}}],["以减少模型需要处理的变量数量",{"2":{"552":1}}],["以减少长序列输入对显存大小的需求",{"2":{"420":1}}],["以机器翻译",{"2":{"545":1}}],["以机器翻译为例",{"2":{"158":1,"245":1}}],["以自回归方式进行推理",{"2":{"540":1}}],["以三个参数的形式接受其输入",{"2":{"535":1}}],["以降低计算资源的消耗",{"2":{"512":1}}],["以微分方程的概念来审视和解释神经网络是近年来兴起的一个新的研究方向",{"2":{"492":1}}],["以",{"0":{"1394":1},"2":{"418":1,"450":1,"460":1,"757":1,"764":1,"765":1,"1391":1,"1642":1}}],["以这个数据处理流程为基础和起点",{"2":{"386":1}}],["以预测和控制外界",{"2":{"363":1}}],["以最大限度地降低不确定性和内部熵",{"2":{"363":1}}],["以参数",{"2":{"347":2}}],["以期获得目标图片的风格",{"2":{"337":1}}],["以消除极端情况对模型的影响",{"2":{"334":1}}],["以免严重破坏预训练效果",{"2":{"333":1}}],["以促进每个层的同步优化",{"2":{"333":1}}],["以跨样本的方式开展归一化",{"2":{"322":1}}],["以批次将多个样本组织起来送入神经网络能够减少数据吞吐量",{"2":{"322":1}}],["以第3个位置为例",{"2":{"316":1}}],["以解决下一个句子预测任务",{"2":{"636":1}}],["以解决上一层传递的数值的分布问题",{"2":{"312":1}}],["以解决预训练模型存在的不足或不良行为",{"2":{"138":1}}],["以区别不同元素所携带的信息量",{"2":{"272":1}}],["以翻译为例",{"2":{"260":1}}],["以创建多个专家模块显著增加了需要训练的参数数量",{"2":{"222":1}}],["以融入周围环境",{"2":{"220":1}}],["以缩小线性化和基于",{"2":{"211":1}}],["以尽可能准确地反映两个标记之间的相对位置",{"2":{"204":1}}],["以便排序",{"2":{"1933":1}}],["以便返回更具体",{"2":{"1763":1}}],["以便编译器在编译调用处时能够看到函数体",{"2":{"1709":1}}],["以便与系统的编译器集成",{"2":{"1589":1}}],["以便计算梯度",{"2":{"1438":1}}],["以便未来可以轻松地集成更复杂的方法",{"2":{"1219":1}}],["以便在每个设备上进行并行计算",{"2":{"1214":1}}],["以便在后向传播中快速重新计算注意力",{"2":{"940":1,"959":1}}],["以便可以准确地测量模型的改进",{"2":{"1165":1}}],["以便很容易生成整个模型的预测",{"2":{"1165":1}}],["以便我们可以在训练结束时检查训练曲线",{"2":{"1164":1}}],["以便我们更新建议",{"2":{"1127":1}}],["以便于追溯模型检查点选择",{"2":{"1164":1}}],["以便公平的比较不同的目标超参数值",{"2":{"1144":1}}],["以便根据输入的类型",{"2":{"1083":1}}],["以便更精确地表达程序中的错误",{"2":{"1762":1}}],["以便更大的batch",{"2":{"986":1}}],["以便更好地传递信息和控制梯度",{"2":{"294":1}}],["以便模型可以更灵活地处理语言中的变化和新词",{"2":{"601":1}}],["以便能够捕获词汇之间的语义关系和特征信息",{"2":{"545":1}}],["以便生成后续的单词",{"2":{"453":1}}],["以便接下来生成更接近结果的输出",{"2":{"398":1}}],["以便读者可以更好的理解",{"2":{"363":1}}],["以便矩阵相乘",{"2":{"201":2}}],["以便每个注意头能够独立地处理它",{"2":{"28":1}}],["以上内容是旧版本",{"2":{"2038":1}}],["以上",{"2":{"968":1}}],["以上三步让llm2vec能够将任何大型语言模型转化为一个在各种nlp任务中都非常实用的文本理解和表示工具",{"2":{"734":1}}],["以上步骤对应下图中标号1~6",{"2":{"267":1}}],["以上图为例",{"2":{"253":1}}],["以上的权重比例",{"2":{"162":1}}],["以上说的是单一注意力头得到的矩阵zizi",{"2":{"71":1}}],["以编码器为例",{"2":{"161":1}}],["以提供确切的间隔来反映在给定历元应该调用哪个调度器",{"2":{"1247":1}}],["以提高可读性",{"2":{"1910":1}}],["以提高代码结构清晰度",{"2":{"1775":1}}],["以提高调整效率",{"2":{"1140":1}}],["以提高缓存命中率",{"2":{"985":1}}],["以提高自然语言处理系统的性能和效果",{"2":{"906":1}}],["以提高对真实概率分布的估计准确性",{"2":{"180":1}}],["以提高其性能",{"2":{"150":1}}],["以提取更丰富的语意信息",{"2":{"116":1}}],["以放大在启用",{"2":{"148":1,"485":1}}],["以蓝色表示",{"2":{"148":1,"485":1}}],["以绿色表示",{"2":{"148":1,"485":1}}],["以回答反事实或存在排印错误",{"2":{"140":1}}],["以及生活感悟",{"2":{"2109":1}}],["以及如何实现",{"2":{"2120":1}}],["以及如何利用它们构建更复杂的软件系统",{"2":{"1729":1}}],["以及如何使用数组和字符串来有效地组织和处理数据",{"2":{"1617":1}}],["以及函数的基本概念和使用方法",{"2":{"1729":1}}],["以及函数体过于庞大的函数",{"2":{"1709":1}}],["以及计算薪水",{"2":{"1657":1}}],["以及图片尺寸在512x512以下的样本",{"2":{"1363":1}}],["以及将模型的规模扩大",{"2":{"1316":1}}],["以及对应的label",{"2":{"1250":1}}],["以及对研究的简短描述",{"2":{"1167":1}}],["以及参数组中参数的参数id列表",{"2":{"1227":1}}],["以及post",{"2":{"1183":1}}],["以及在训练后期使用过高的学习率",{"2":{"1149":1}}],["以及在训练过程中会使用损失低精度量化计算",{"2":{"396":1}}],["以及我们在生产中重现最佳试验结果的能力",{"2":{"1149":1}}],["以及选择自动搜索算法",{"2":{"1144":1}}],["以及哪些超参数对其他变化相对不敏感",{"2":{"1140":1}}],["以及",{"2":{"1137":1,"1174":1,"1339":1,"1921":1}}],["以及很多困惑",{"2":{"1127":1}}],["以及特定于该块的前馈网络",{"2":{"975":1}}],["以及最终注意力权重和value矩阵乘时",{"2":{"757":1}}],["以及无监督对比学习三个部分",{"2":{"734":1}}],["以及开源社区都发布了大量的embedding模型来提供给用户使用",{"2":{"711":1}}],["以及开发者做了哪些准备",{"2":{"545":1}}],["以及把某些概念重新组合成新的结构化思维等",{"2":{"689":1}}],["以及把源文本和历史译文融合起来",{"2":{"536":1}}],["以及一个轻量级的局部解码器",{"2":{"614":1}}],["以及一个用于应用规范化",{"2":{"327":1}}],["以及由压缩启发式方法引入的偏见",{"2":{"610":1}}],["以及其它的子词分类算法",{"2":{"596":1}}],["以及其由此引发的一系列使用方法的不同",{"2":{"321":1}}],["以及合并之后对应的索引",{"2":{"591":1}}],["以及这些映射关系之间的组合和复合规则",{"2":{"505":1}}],["以及它与全局变量的关系",{"2":{"1649":1}}],["以及它们的合理范围",{"2":{"1153":1}}],["以及它们之间的因果关系",{"2":{"629":1}}],["以及它们之间的关系r",{"2":{"122":1}}],["以及它和transformer的对比",{"2":{"490":1}}],["以及梯度对模型更新的影响",{"2":{"485":1}}],["以及定位参数的存储位置",{"2":{"475":1}}],["以及标签平滑",{"2":{"429":1}}],["以及通过权重衰减控制权重的范数",{"2":{"351":1}}],["以及残差连接",{"2":{"344":1}}],["以及降低节点的线性依赖性",{"2":{"305":1}}],["以及跳层连接恢复网络表达能力的案例",{"2":{"305":1}}],["以及为什么尾递归可以被优化",{"2":{"1646":1}}],["以及为了从根本上解决这些问题",{"2":{"272":1}}],["以及为何如此运作",{"2":{"235":1}}],["以及值",{"2":{"154":1}}],["以及连接两者的关系",{"2":{"145":1}}],["以及机器遗忘",{"2":{"139":1}}],["以移除过时信息或者整合新的知识",{"2":{"138":1}}],["以下经验法则对应于研究中试验次数的不同",{"2":{"1174":1}}],["以下分类并非是正交的",{"2":{"474":1}}],["以下维度可以并行",{"2":{"417":1}}],["以下是一个完整的代码示例解释友元函数的特性",{"2":{"1774":1}}],["以下是一个名为",{"2":{"1696":1}}],["以下是一些最常用的函数",{"2":{"1575":1}}],["以下是一些常见的nlp任务",{"2":{"906":1}}],["以下是一些与零中心激活函数相关的考虑因素",{"2":{"838":1}}],["以下是一些深刻的解释",{"2":{"722":1}}],["以下是你在学习过程中可以覆盖的重要主题和内容",{"2":{"1490":1}}],["以下是作者的猜测",{"2":{"1158":1}}],["以下是初始状态下的所有子词",{"2":{"580":1}}],["以下是各种归一化求均值的方法",{"2":{"340":1}}],["以下是修改逻辑和历程",{"2":{"288":1}}],["以下是",{"2":{"216":1,"1999":1}}],["以下是其关键技术细节",{"2":{"153":1}}],["以下是他们的修剪策略示例",{"2":{"20":1}}],["以下图为例",{"2":{"135":1,"249":1}}],["以缓解修复模型的缺陷",{"2":{"123":1}}],["以纠正这些错误并提高准确性",{"2":{"121":1}}],["以哈佛代码为例",{"2":{"116":1}}],["以原始transformer结构的编码器为例",{"2":{"97":1}}],["以避免访问无效内存",{"2":{"1611":1}}],["以避免给训练工作流增加不必要的复杂性",{"2":{"1139":1}}],["以避免增加不必要的复杂度",{"2":{"1139":1}}],["以避免丢失模型的推理选择",{"2":{"739":1}}],["以避免",{"2":{"89":1}}],["以megatron",{"2":{"89":1}}],["以是确保解码器只能关注到它之前已经生成的词",{"2":{"81":1}}],["以此作为新序列再输入给解码器",{"2":{"529":1}}],["以此观察latent",{"2":{"475":1}}],["以此实现将德语句子翻译成英语的功能",{"2":{"370":1}}],["以此来让我们获得远高于unstable",{"2":{"1183":1}}],["以此来让模型有机会捕捉到长距离依赖和复杂的结构关系",{"2":{"247":1}}],["以此来产生新的token",{"2":{"575":1}}],["以此来提高网络的鲁棒性",{"2":{"396":1}}],["以此来降低拟合难度以及过拟合的风险",{"2":{"321":1}}],["以此来降低拟合难度和过拟合的风险",{"2":{"310":1}}],["以此来决定哪个元素会对目标元素造成多少影响",{"2":{"168":1}}],["以此来解释模型预测和输入特征之间的关系",{"2":{"134":1}}],["以此避免最后影响模型自身的效果",{"2":{"54":1}}],["以此为出发点",{"2":{"46":1}}],["以此类推",{"2":{"33":1,"189":1,"241":1,"249":1,"529":1,"532":1,"533":1,"575":1}}],["以至于看不见",{"2":{"18":1,"20":1}}],["以分治+融合的模式对数据进行加工",{"2":{"11":1}}],["以用于不同特定领域的任务",{"2":{"4":1,"12":1}}],["也希望我们能够互相成长",{"2":{"2108":1}}],["也没必要解释说哪个是内在的哪个是外在的",{"2":{"2054":1}}],["也没有返回类型",{"2":{"1676":1}}],["也没有bn层",{"2":{"1001":1}}],["也没有考虑深层语义关系",{"2":{"242":1}}],["也非常具有成就感",{"2":{"2010":1}}],["也支持迭代器",{"2":{"1713":1}}],["也一样同样逻辑的实现",{"2":{"1651":1}}],["也学习如何提取特征",{"2":{"1455":1}}],["也学会了一个双层双向的lstm网络结构",{"2":{"718":1}}],["也同时减少了所需的训练步数",{"2":{"1133":1}}],["也同时方便预测后续的token",{"2":{"528":1}}],["也采用了这个",{"2":{"1275":1}}],["也采用了",{"2":{"1059":1}}],["也对应了pytorch里面的kaiming初始化只要传卷积核的参数进去就行了",{"2":{"1003":1}}],["也很简单",{"2":{"934":1}}],["也避免出现梯度爆炸问题",{"2":{"848":1}}],["也即卷积核变成只有一个数字",{"2":{"774":1}}],["也即所有表征趋于一个向量",{"2":{"115":1}}],["也叫做纯虚类",{"2":{"1693":1}}],["也叫做卷积神经网络",{"2":{"769":1}}],["也叫作前馈神经网络",{"2":{"1457":1}}],["也叫element",{"2":{"829":1}}],["也叫silu",{"2":{"108":1}}],["也搞清楚了这项测试的精髓",{"2":{"768":1}}],["也建模了词之间的相对位置",{"2":{"751":1}}],["也影响着语义",{"2":{"744":1}}],["也善于奔跑",{"2":{"713":1}}],["也发现独热编码这个常见向量的众多问题",{"2":{"682":1}}],["也包括输出序列",{"2":{"536":1}}],["也包括encoder的输出",{"2":{"529":1}}],["也包括其输入",{"2":{"519":1}}],["也能让我们更加有信心相信搜索流程能够找到好的冗余超参数配置",{"2":{"1145":1}}],["也能考虑到较远位置的信息",{"2":{"746":1}}],["也能看到本单词后续的单词",{"2":{"717":1}}],["也能跃过3米以上至6米的高度或5米以上至13米的距离",{"2":{"713":1}}],["也能被算作字符对的一部分",{"2":{"579":1}}],["也能够达到与所有",{"2":{"504":1}}],["也能搜出来",{"2":{"169":1}}],["也无关",{"2":{"504":1}}],["也无法通过这五个元素之间的比较来确定",{"2":{"318":1}}],["也加入了tokenizer",{"2":{"454":1}}],["也被称做因果自注意力",{"2":{"439":1}}],["也被称为是感知机的功能层",{"2":{"1461":1}}],["也被称为自编码",{"2":{"1315":1}}],["也被称为",{"2":{"329":1}}],["也被称为因果自注意力",{"2":{"50":1}}],["也指解码器",{"2":{"417":1}}],["也降低",{"2":{"382":1}}],["也提出了独到见解",{"2":{"320":1}}],["也提升了",{"2":{"217":1}}],["也存在问题",{"2":{"316":1}}],["也大大提升了梯度下降算法的训练速度",{"2":{"298":1}}],["也起到了重要作用",{"2":{"283":1}}],["也梳理一下历史上的重要节点",{"2":{"280":1}}],["也为传递有价值的语义关联提供了渠道",{"2":{"746":1}}],["也为改进模型设计提供了重要理论指导",{"2":{"507":1}}],["也为prompt工程提供了坚实的理论基础",{"2":{"504":1}}],["也为模型训练提供了更多的统计信息",{"2":{"322":1}}],["也为机器学习领域开辟了新的视野",{"2":{"279":1}}],["也为研究人员提供了解释模型行为的机会",{"2":{"148":1,"484":1}}],["也要从",{"2":{"277":1}}],["也需要考虑出上下文中每一个元素应该考虑多少",{"2":{"260":1}}],["也称为字段或属性",{"2":{"1728":1}}],["也称为隐藏",{"2":{"1663":1}}],["也称为目标代码",{"2":{"1604":1}}],["也称为链接调度器",{"2":{"1231":1}}],["也称为xavier条件",{"2":{"999":1}}],["也称为切片",{"2":{"940":1,"959":1}}],["也称为深度学习模型图",{"2":{"785":1}}],["也称为l1范数",{"2":{"692":1}}],["也称为欧几里得向量",{"2":{"680":1}}],["也称为非因果解码器",{"2":{"541":1}}],["也称为glorot初始化",{"2":{"403":1}}],["也称为上下文向量或隐向量或隐状态",{"2":{"241":1}}],["也称为additive",{"2":{"175":1}}],["也请各位读者指出",{"2":{"235":1}}],["也从传统",{"2":{"216":1}}],["也有双向的",{"0":{"871":1}}],["也有人把其单独列为旋转位置编码",{"2":{"767":1}}],["也有人把其归结为相对位置编码",{"2":{"767":1}}],["也有的方案使用距离编码矩阵",{"2":{"745":1}}],["也有研究人员在考虑使用xk⊗pk",{"2":{"751":1}}],["也有研究人员认为",{"2":{"509":1}}],["也有研究表明在超球面上进行表示学习与更稳定的训练",{"2":{"351":1}}],["也有其它工作把神经元投影到unembedding",{"2":{"475":1}}],["也有一种说法叫做上下文并行",{"2":{"420":1}}],["也有可能发生较大变化",{"2":{"326":1}}],["也有从所有训练样本中获得的统计量来代替推理数据的均值和方差",{"2":{"316":1}}],["也有全局感受野",{"2":{"204":1}}],["也有把sequence",{"2":{"50":1}}],["也许是",{"2":{"1478":1}}],["也许这些超参数是最能保证转移的选择",{"2":{"1158":1}}],["也许可以通过压缩token的方式减少context长度",{"2":{"204":1}}],["也许读者会问",{"2":{"172":1}}],["也会因为内存的容量限制不能一直呆在内存中",{"2":{"1477":1}}],["也会跟着增大",{"2":{"698":1}}],["也会导致在解码的过程中对于某个字节不确定是来自某个character还是单独的character中从而导致歧义",{"2":{"608":1}}],["也会在生成输出时产生计算负担",{"2":{"562":1}}],["也会加入dropout",{"2":{"523":1}}],["也会给相似词b更多的关注",{"2":{"176":1}}],["也会做取模再还原的操作",{"2":{"109":1}}],["也减少了模型处理信息的灵活性",{"2":{"172":1}}],["也给出了知识擦除前后四种关系的缺失实体预测准确性",{"2":{"143":1}}],["也给出了不同注意力头的示例",{"2":{"18":1}}],["也不是为了体现我有多么多么厉害",{"2":{"2054":1}}],["也不是类对象的一部分",{"2":{"1772":1}}],["也不是像全息图一样在任何地方存储所有东西",{"2":{"129":1}}],["也不要忽略异常",{"2":{"1764":1}}],["也不要太自信",{"2":{"399":1}}],["也不要太小",{"2":{"346":1}}],["也不占用内存空间",{"2":{"1632":1}}],["也不存在跨层连接",{"2":{"1464":1}}],["也不具有远程衰减性",{"2":{"749":1}}],["也不以特定语言或模态生成输出",{"2":{"629":1}}],["也不需要进行比较",{"2":{"316":1}}],["也不至于比20层网络效果更差",{"2":{"299":1}}],["也不利于整个算法领域发展",{"2":{"291":1}}],["也不能注意的部分",{"2":{"77":1}}],["也类似于elmo等论文在nlp学界的发现",{"2":{"127":1}}],["也做了研究",{"2":{"127":1}}],["也保留了此处代码和模型结构",{"2":{"81":1}}],["也保证了训练时和预测时解码器运行的情况是一样的",{"2":{"71":1}}],["也使llm服务变得负担得起",{"2":{"980":1}}],["也使用概率统计的方式来预测每个单词作为独立单元出现的概率",{"2":{"601":1}}],["也使用broadcasting",{"2":{"79":1,"198":1}}],["也使得它比",{"2":{"291":1}}],["也使得模型对输入中的噪声和变化更加鲁棒",{"2":{"21":1}}],["也向我们展示了masked",{"2":{"71":1}}],["也正是因为计算的需要",{"2":{"34":1}}],["也可能是由外部因素导致的",{"2":{"1761":1}}],["也可能是某个隐藏层的输出",{"2":{"920":1}}],["也可能是稀疏向量",{"2":{"680":1}}],["也可能是一个单词的一部分",{"2":{"567":1}}],["也可能是一个表示空白字符",{"2":{"363":1,"548":1}}],["也可能稠密",{"2":{"676":1}}],["也可能减少",{"2":{"576":1}}],["也可能存在其适用范围的局限性",{"2":{"181":1}}],["也可能包含",{"2":{"88":1}}],["也可以显式获取枚举常量的值",{"2":{"1728":1}}],["也可以不是2维",{"2":{"1373":1}}],["也可以将超过512的位置向量随机初始化",{"2":{"1337":1}}],["也可以直接置为none",{"2":{"1214":1}}],["也可以返回一个新的state",{"2":{"1214":1}}],["也可以帮助我们优先考虑下一步采取什么行动",{"2":{"1149":2}}],["也可以在模块级别使用",{"2":{"1117":1}}],["也可以在向量中编码语义相似性",{"2":{"688":1}}],["也可以解决oom问题",{"2":{"976":1}}],["也可以对所有的隐状态做变换",{"2":{"888":1}}],["也可以称之为seq2seq模型",{"2":{"883":1}}],["也可以用迭代解决",{"2":{"1646":1}}],["也可以用来提取全局上下文信息",{"2":{"816":1}}],["也可以用不同的学习率调整每一层",{"2":{"402":1}}],["也可以这样认为",{"2":{"689":1}}],["也可以这样理解",{"2":{"598":1}}],["也可以通过对象访问",{"2":{"1639":1}}],["也可以通过相加两个向量或用一个数缩放一个向量来创建新的向量",{"2":{"689":1}}],["也可以通过bpe算法初始化",{"2":{"602":1}}],["也可以通过构建特定的跨度集合来提高对梯度矩阵的解释能力",{"2":{"485":1}}],["也可以从tokenizer角度来给出解释",{"2":{"595":1}}],["也可以从卷积的角度解释",{"2":{"101":1}}],["也可以看到",{"2":{"588":1}}],["也可以看出来cv和nlp领域使用bn和ln的不同区别",{"2":{"341":1}}],["也可以让模型表达更复杂的语言模式",{"2":{"561":1}}],["也可以达到几乎与所有",{"2":{"504":1}}],["也可以使transformer能够更好的应对提示的长度和复杂性",{"2":{"474":1}}],["也可以使用mask",{"2":{"78":1}}],["也可以更好地捕捉更复杂的关系",{"2":{"290":1}}],["也可以更加突出重要的权重",{"2":{"269":1}}],["也可以叫做基于记忆的方法",{"2":{"141":1}}],["也可以理解为引入非线性对向量进行筛选",{"2":{"99":1}}],["也可以灵活选择文字中的各种关系和特征",{"2":{"5":1}}],["也可采用小矩阵的方式进行计算",{"2":{"29":1}}],["也让参数计算更加高效",{"2":{"19":1}}],["也就意味着",{"2":{"1478":1}}],["也就不需要耗费大量空间来保存中间结果了",{"2":{"495":1}}],["也就有可能成功训练了",{"2":{"333":1}}],["也就导致词之间距离越远",{"2":{"255":1}}],["也就是局部最优",{"2":{"2115":1}}],["也就是第一个一维数组",{"2":{"1705":1}}],["也就是变量",{"2":{"1611":1}}],["也就是某一时刻不允许多个进程同时访问",{"2":{"1412":1}}],["也就是一种特殊的共享资源",{"2":{"1412":1}}],["也就是一个词向量的维度",{"2":{"343":1}}],["也就是后验",{"2":{"1377":1}}],["也就是所谓的",{"2":{"1313":1,"1478":1}}],["也就是所谓的梯度消失",{"2":{"188":1}}],["也就是预期的长时间训练",{"2":{"1157":1}}],["也就是混合位置编码",{"2":{"767":1}}],["也就是encoder所产出cls句向量的信息",{"2":{"727":1}}],["也就是embedding",{"2":{"457":1}}],["也就是深度学习中常说的词嵌入",{"2":{"676":1}}],["也就是i",{"2":{"665":1}}],["也就是stride不协调",{"2":{"658":1}}],["也就是通过mse回归来优化参数",{"2":{"632":1}}],["也就是通过起始符l来预测实际的第一个输出",{"2":{"528":1}}],["也就是下一个",{"2":{"628":1}}],["也就是对应到这里的token",{"2":{"626":1}}],["也就是对文字进行向量化",{"2":{"455":1,"698":1}}],["也就是用链式法则以网络每层的权重为变量计算损失函数的梯度",{"2":{"1439":1}}],["也就是用更短的词元去表示文本",{"2":{"561":1}}],["也就是用到了query",{"2":{"162":1}}],["也就是在解码期间参考源句子的上下文信息",{"2":{"525":1}}],["也就是无法并行",{"2":{"511":1}}],["也就是实现全文级别的推理",{"2":{"510":1}}],["也就是这次的输出会加到上次的输入后面",{"2":{"529":1}}],["也就是这次的输出",{"2":{"453":1}}],["也就是越靠后的层学习得越快",{"2":{"401":1}}],["也就是最大位移方向",{"2":{"2018":1}}],["也就是最小语义单位",{"2":{"363":1,"548":1}}],["也就是最后一个时间步的隐藏层输出",{"2":{"284":1}}],["也就是bn",{"2":{"337":1}}],["也就是norm",{"2":{"334":1}}],["也就是我们可以在一个",{"2":{"325":1}}],["也就是将某个词与同句中其他词",{"2":{"318":1}}],["也就是产生了遗忘问题",{"2":{"246":1}}],["也就是模型训练的并行度或参与计算的设备",{"2":{"201":1}}],["也就是dropblock的由来",{"2":{"1019":1}}],["也就是d",{"2":{"199":1}}],["也就是q",{"2":{"173":1}}],["也就是不论任何位置任何搭配",{"2":{"172":1}}],["也就是词之间进行互相操作",{"2":{"167":1}}],["也就是语境的影响",{"2":{"167":1,"259":1}}],["也就是可以区分不同",{"2":{"90":1}}],["也就是",{"2":{"89":1,"90":1,"386":1,"840":1,"1183":1}}],["也就是绿色部分为",{"2":{"89":1}}],["也就是每个batch都对参数进行更新",{"2":{"385":1}}],["也就是每个特征图",{"2":{"325":1}}],["也就是每个单词与句子中所有单词的关系",{"2":{"158":1}}],["也就是每个",{"2":{"89":1}}],["也就是多个z",{"2":{"36":1}}],["也就是token个数",{"2":{"23":1}}],["也就是单头注意力下",{"2":{"23":1}}],["也就是hidden",{"2":{"20":1}}],["也就是说通过统计学的方法",{"2":{"1456":1}}],["也就是说网络必须拥有一定的记忆能力",{"2":{"850":1}}],["也就是说输入序列和位置编码不再共享权值",{"2":{"760":1}}],["也就是说它们的维度上的数值",{"2":{"694":1}}],["也就是说它包含了在解码当前时刻时应该将注意力放在memory中哪些位置上的信息",{"2":{"537":1}}],["也就是说在纯粹的语义层面对基本推理过程进行建模",{"2":{"628":1}}],["也就是说",{"2":{"17":1,"28":1,"71":1,"172":1,"189":1,"230":4,"337":1,"409":1,"519":1,"690":1,"713":1,"879":1,"914":1,"1140":1,"1165":1,"1185":1,"1312":1,"1409":1,"1478":1,"1708":1}}],["也就越趋近于",{"2":{"17":1}}],["也是不对的",{"2":{"2054":1}}],["也是正确建立人生价值观和世界观的起点",{"2":{"2054":1}}],["也是node的实例",{"2":{"1110":1}}],["也是一个不可忽视的问题",{"2":{"746":1}}],["也是一种新的参数高效微调",{"2":{"224":1}}],["也是每个字的数学表达",{"2":{"709":1}}],["也是日常生活中最常用的度量方法",{"2":{"692":1}}],["也是实现人工智能必不可少的一环",{"2":{"678":1}}],["也是选取出现频数最高的字符对进行合并",{"2":{"607":1}}],["也是由多个解码器层组成",{"2":{"525":1}}],["也是编码层的大小",{"2":{"523":1}}],["也是上下文感知的",{"2":{"516":1}}],["也是寻找米田嵌入的过程",{"2":{"472":1}}],["也是本篇讲解的基础",{"2":{"431":1}}],["也是类似的需要在",{"2":{"344":1}}],["也是独立于",{"2":{"338":1}}],["也是因为一个句子中不同的token之间相关性极强",{"2":{"326":1}}],["也是最常见的归一化方案",{"2":{"312":1}}],["也是下一层网络模块的输入",{"2":{"300":1}}],["也是为了更好的融合前面多头注意力机制的输出内容",{"2":{"99":1}}],["也是为了让代表句子长度维度和词向量维度能够相邻",{"2":{"36":1}}],["也是collabhead方式的一种特例",{"2":{"19":1}}],["也是融合操作",{"2":{"16":1}}],["也是在模型训练阶段一同训练出来的权重矩阵",{"2":{"10":3}}],["对孩子的胃口和饼干大小进行排序",{"2":{"2153":1}}],["对孩子的胃口数组",{"2":{"2152":1}}],["对了",{"2":{"2054":1}}],["对面寝室都好久没充电费了",{"2":{"2054":1}}],["对博客部分做了重构",{"2":{"2038":1}}],["对你有什么启发",{"2":{"2031":1}}],["对表达式的求值顺序做了更严格的规定",{"2":{"1931":1}}],["对部分元素进行排序",{"2":{"1751":1}}],["对给定范围进行排序",{"2":{"1749":1}}],["对给定的上下文进行下一个单词或字符的预测",{"2":{"894":1,"906":1}}],["对范围内的每个元素执行给定的操作",{"2":{"1739":1}}],["对形参",{"2":{"1729":1}}],["对形参的任何修改都会直接影响到外部的实参",{"2":{"1729":1}}],["对房子的任何操作都会反映在两个名字上",{"2":{"1650":1}}],["对条件取反",{"2":{"1619":1}}],["对编程的思想和逻辑有了初步的认识",{"2":{"1601":1}}],["对编码器所有的向量",{"2":{"267":1}}],["对复杂任务来说",{"2":{"1466":1}}],["对手写数字的分类",{"2":{"1462":1}}],["对资料进行表征学习的算法",{"2":{"1455":1}}],["对资源消耗过大",{"2":{"242":1}}],["对相对位置进行了一个",{"2":{"1340":1}}],["对第三个标签的分数预测为",{"2":{"1323":1}}],["对第二个标签的分数预测为",{"2":{"1323":1}}],["对第一个标签的分数预测为",{"2":{"1323":1}}],["对one",{"2":{"1242":1}}],["对前向传播进行装饰",{"2":{"1214":1}}],["对前缀的支持不够好",{"2":{"600":1}}],["对该项目的贡献必须附有贡献者许可协议",{"2":{"1197":1}}],["对搜索空间以及应该从搜索空间中采样数量做出概括性陈述是非常困难的",{"2":{"1174":1}}],["对这块感兴趣的可以自己了解了解",{"2":{"1589":1}}],["对这三个要求的任何一个进行改进都需要增加试验次数",{"2":{"1145":1}}],["对这些分数进行softmax操作的目的是放大高分隐藏状态",{"2":{"267":1}}],["对这些事实或者知识的回答效果也会变好或者变差",{"2":{"143":1}}],["对这些神经元内的数值进行增强或者抑制",{"2":{"143":1}}],["对冗余超参数的搜索空间进行足够密集的采样",{"2":{"1145":1}}],["对您的模型的影响完全取决于您模型中使用的具体模块以及它们是否定义了任何训练模式特定的行为",{"2":{"1122":1}}],["对模块的所有参数",{"2":{"1117":1}}],["对模型的保存和加载有影响吗",{"2":{"1260":1}}],["对模型进行约束",{"2":{"399":1}}],["对模型性能有帮助的特征",{"2":{"118":1}}],["对用户不可知",{"2":{"1105":1}}],["对我们当前正在学习的地方做一个定位",{"2":{"1405":1,"1426":1}}],["对我们需要更新梯度的tensor",{"2":{"1098":1}}],["对我们用户是可见的",{"2":{"661":1,"1104":1}}],["对它的数据进行了更新",{"2":{"1098":1}}],["对z进行操作",{"2":{"1094":1}}],["对输出的更改会反映在",{"2":{"1087":1}}],["对输入",{"2":{"1087":1}}],["对输入噪声的敏感性",{"2":{"612":1}}],["对输入x进行自注意力操作",{"2":{"523":1}}],["对输入x进行层归一化",{"2":{"343":1}}],["对输入文字进行tokenize",{"2":{"455":1,"698":1}}],["对输入激活进行缩放的同时压缩极端值",{"2":{"359":1}}],["对输入隐藏状态进行归一化",{"2":{"355":1}}],["对输入进行scale",{"2":{"313":1}}],["对输入进行平滑处理",{"2":{"106":1}}],["对输入数据等进行中心化转换",{"2":{"298":1}}],["对外api接口",{"0":{"1087":1}}],["对外部记忆",{"2":{"512":1}}],["对子类进行包装",{"2":{"1083":1}}],["对rnn",{"2":{"1047":1}}],["对require",{"2":{"665":1}}],["对resnets与neural",{"2":{"497":1}}],["对梯度的调节太大",{"2":{"1045":1}}],["对梯度下降更加友好",{"2":{"329":1}}],["对称失效",{"2":{"992":1}}],["对称性可以提高网络的稳定性和泛化能力",{"2":{"838":1}}],["对称性",{"2":{"838":1}}],["对各个节点的权重",{"2":{"988":1}}],["对flashattention算法进行了调整",{"2":{"968":1}}],["对话系统",{"2":{"906":1}}],["对话生成等",{"2":{"906":2}}],["对话场景",{"2":{"722":1}}],["对候选项进行排序",{"2":{"904":1}}],["对激活函数的研究一直没有停止过",{"2":{"846":1}}],["对激活函数进行改进",{"2":{"105":1}}],["对负半轴引入软饱和以代替置",{"2":{"843":1}}],["对线性层的输出用",{"2":{"838":1}}],["对张量进行切片",{"2":{"832":1}}],["对那个维度分组",{"2":{"775":2}}],["对绝对位置编码公式做了改动",{"2":{"760":1}}],["对绝对位置不敏感",{"2":{"247":1}}],["对词嵌入和位置编码进行融合",{"2":{"751":1}}],["对词义的维度进行全面的拆分",{"2":{"712":1}}],["对llm做进一步的训练",{"2":{"734":1}}],["对llm中训练不足的token",{"2":{"562":1}}],["对概念序列进行处理",{"2":{"628":1}}],["对推理",{"2":{"627":1}}],["对拼写错误非常敏感",{"2":{"600":1}}],["对ids进行更新",{"2":{"592":1}}],["对item",{"2":{"209":1}}],["对哪两个进行合并",{"2":{"591":1}}],["对已经切为字符的语料",{"2":{"576":1}}],["对语言模型的性能的影响",{"2":{"561":1}}],["对过短的序列进行填充",{"2":{"555":1}}],["对过长的序列进行截断",{"2":{"555":1}}],["对预分词器",{"2":{"553":1}}],["对文本进行必要的清理工作",{"2":{"545":1}}],["对具有非因果可见性的输入来说",{"2":{"542":1}}],["对最终的输出x应用层归一化",{"2":{"522":1}}],["对注意力分数施加scaling",{"2":{"464":1}}],["对不同头使用不同的线性变换",{"2":{"462":1}}],["对不同的实体进行整合",{"2":{"99":1}}],["对数几率",{"2":{"451":1}}],["对src编码",{"2":{"450":1}}],["对scores的最后一维做归一化操作",{"2":{"199":1}}],["对填充符号进行掩码",{"2":{"450":1}}],["对tgt编码",{"2":{"450":1}}],["对transformer实现具有竞争力的性能至关重要",{"2":{"446":1}}],["对token",{"2":{"437":1}}],["对损失进行正则化",{"2":{"398":1}}],["对标签进行平滑和计算损失",{"2":{"398":1}}],["对mlp输出表示进行操作",{"2":{"396":1}}],["对自注意力的输出表示进行操作",{"2":{"396":1}}],["对自注意力机制下的秩崩溃进行了综合分析",{"2":{"91":1}}],["对源句子处理",{"2":{"384":2,"558":2}}],["对逻辑值进行重新缩放",{"2":{"357":1}}],["对隐藏状态应用层变换",{"2":{"354":1}}],["对存储在和中的嵌入向量进行归一化",{"2":{"352":1}}],["对c",{"2":{"341":1}}],["对cnn而言",{"2":{"337":1}}],["对nhw做归一化",{"2":{"807":1}}],["对n",{"2":{"341":1}}],["对同一个",{"2":{"326":1}}],["对每层l",{"2":{"1004":1}}],["对每个参数组的学习率进行衰减",{"2":{"1236":1}}],["对每个序列的得分",{"2":{"904":1}}],["对每个单词采用相同的线性变换",{"2":{"498":1}}],["对每个batch进行前向传播",{"2":{"385":1}}],["对每个样本的",{"2":{"337":1}}],["对每个样本的所有特征做归一化",{"2":{"322":1}}],["对每一维特征进行归一化",{"2":{"309":1}}],["对所有矩阵沿着其嵌入维度进行归一化",{"2":{"357":1}}],["对所有通道都施加一遍这个操作",{"2":{"315":1}}],["对所有h个头应用同样的mask",{"2":{"36":1}}],["对其进行解引用操作会导致程序崩溃或未定义的行为",{"2":{"1611":1}}],["对其求均值和方差时",{"2":{"315":1}}],["对其他层或者输出产生了巨大影响",{"2":{"309":1}}],["对x求导结果为1",{"2":{"304":1}}],["对xq和xk应用旋转位置编码",{"2":{"201":1}}],["对深层神经网络使用了跳层连接",{"2":{"298":1}}],["对比图",{"0":{"1056":1}}],["对比度",{"2":{"1015":1}}],["对比elmo",{"2":{"721":1}}],["对比openai",{"2":{"721":1}}],["对比以下",{"2":{"692":1}}],["对比",{"0":{"353":1,"620":1,"694":1,"873":1},"1":{"354":1,"355":1,"356":1},"2":{"293":1,"702":1,"806":1}}],["对内存的要求很高",{"2":{"279":1}}],["对理解query越重要",{"2":{"265":1}}],["对本身进行动态调整",{"2":{"261":1}}],["对齐要求",{"2":{"1635":1}}],["对齐机制",{"0":{"277":1}}],["对齐问题",{"2":{"256":1,"272":1,"284":1}}],["对齐问题和长依赖问题",{"2":{"244":1}}],["对齐",{"0":{"245":1},"2":{"245":1}}],["对齐及其认知机制的内在规律",{"2":{"121":1}}],["对分开",{"2":{"213":1}}],["对的能力",{"2":{"212":1}}],["对此做了对比实验",{"2":{"175":1}}],["对点积结果进行归一化",{"2":{"173":1}}],["对得分矩阵scores进行缩放",{"2":{"173":1}}],["对角向上的值一定是最大的",{"2":{"172":1}}],["对角线上的每个元素",{"2":{"542":1}}],["对角线上的",{"2":{"89":1}}],["对角线也是",{"2":{"70":1}}],["对结果进行排序",{"2":{"164":1}}],["对",{"2":{"151":1,"204":1,"230":1,"260":1,"265":1,"357":1,"396":1,"419":1,"727":1,"1085":3,"1316":1,"1339":1,"1612":1}}],["对象存储",{"2":{"1951":1}}],["对象文件",{"2":{"1917":1}}],["对象即将被销毁时被自动调用",{"2":{"1674":1}}],["对象时被自动调用",{"2":{"1674":1}}],["对象在这里会被销毁",{"2":{"1674":1}}],["对象和继承的知识",{"2":{"1657":1}}],["对象可以自动管理内存",{"2":{"1624":1}}],["对象可迭代",{"2":{"1083":1}}],["对象关系映射",{"2":{"1479":1}}],["对象将定义一个单独的参数组",{"2":{"1222":1}}],["对象使用除法操作符",{"2":{"1085":1}}],["对象进行操作",{"2":{"1713":1}}],["对象进行赋值操作时触发",{"2":{"1085":1}}],["对象进行真值测试",{"2":{"1085":1}}],["对象用作整数索引",{"2":{"1085":1}}],["对象所属的模块",{"2":{"1083":1}}],["对象所在的设备信息并返回",{"2":{"1083":1}}],["对象重新转换为具有指定维度大小的多维",{"2":{"1083":1}}],["对象中",{"2":{"1083":1}}],["对象转换为",{"2":{"1083":3}}],["对象的清理",{"0":{"1676":1}}],["对象的初始化",{"0":{"1675":1}}],["对象的定义",{"2":{"1624":1}}],["对象的可迭代对象",{"2":{"1222":2}}],["对象的元素或子集",{"2":{"1085":1}}],["对象的属性和方法列表",{"2":{"1083":1}}],["对象的哈希操作",{"2":{"1083":1}}],["对象的维度名称",{"2":{"1083":3}}],["对象的维度顺序以匹配指定的顺序",{"2":{"1083":1}}],["对象的维度命名",{"2":{"1083":1}}],["对象的维度",{"2":{"1083":1}}],["对象",{"2":{"122":1,"449":2,"1083":2,"1114":1,"1214":1,"1624":1,"1664":1,"1683":2,"1811":1,"1829":1,"1873":2,"1874":2,"1891":1}}],["对应向后兼容很重要",{"2":{"1208":1}}],["对应公式如下",{"2":{"842":1}}],["对应图像和导函数图像为",{"2":{"840":1}}],["对应图上圆形标号5",{"2":{"519":1}}],["对应图上圆形标号4",{"2":{"519":1}}],["对应图上圆形标号3",{"2":{"519":1}}],["对应图上圆形标号2",{"2":{"519":1}}],["对应图上圆形标号1",{"2":{"519":1}}],["对应图上的标签5",{"2":{"704":1}}],["对应图上的标签4",{"2":{"704":1}}],["对应图上的标签3",{"2":{"704":1}}],["对应图上的标签2",{"2":{"704":1}}],["对应图上的标签1",{"2":{"704":1}}],["对应图上的output",{"2":{"454":1}}],["对应图上的positional",{"2":{"454":2}}],["对应图上的",{"2":{"454":1}}],["对应图上的序号8",{"2":{"36":1}}],["对应图上的序号7",{"2":{"36":1}}],["对应图上的序号6",{"2":{"36":1}}],["对应图上的序号5",{"2":{"36":1}}],["对应图上的序号4",{"2":{"36":1}}],["对应图上的序号1",{"2":{"36":1}}],["对应图上的序号2",{"2":{"36":1}}],["对应图上序号9",{"2":{"538":1}}],["对应图上序号8",{"2":{"538":1}}],["对应图上序号7",{"2":{"538":1}}],["对应图上序号6",{"2":{"538":1}}],["对应图上序号5",{"2":{"538":1}}],["对应图上序号4",{"2":{"394":1,"538":1}}],["对应图上序号3",{"2":{"394":1,"538":1}}],["对应图上序号2",{"2":{"394":1,"538":1}}],["对应图上序号11",{"2":{"538":1}}],["对应图上序号10",{"2":{"538":1}}],["对应图上序号1",{"2":{"394":1,"538":1}}],["对应一个权重和bias",{"2":{"808":1}}],["对应经典的神经网络",{"2":{"769":1}}],["对应nn",{"2":{"703":2}}],["对应离散的输入数据",{"2":{"674":1}}],["对应序号9",{"2":{"537":1}}],["对应序号8",{"2":{"537":1}}],["对应序号7",{"2":{"537":1}}],["对应序号6",{"2":{"537":1}}],["对应序号4和5",{"2":{"537":1}}],["对应序号3",{"2":{"537":1}}],["对应序号2",{"2":{"537":1}}],["对应序号1",{"2":{"537":1}}],["对应论文中图1左侧encoder的部分",{"2":{"523":1}}],["对应论文原图中",{"2":{"343":1}}],["对应device会调用对应的fn算子",{"2":{"662":1}}],["对应d",{"2":{"523":1}}],["对应下面代码的tgt",{"2":{"528":1}}],["对应下面代码的memory",{"2":{"528":1}}],["对应下面的encoderlayer类",{"2":{"522":1}}],["对应下图标号2",{"2":{"502":1}}],["对应下图标号1",{"2":{"502":1}}],["对应下图标号6",{"2":{"230":1}}],["对应下图的紫色圈",{"2":{"436":1}}],["对应下图的红色圈",{"2":{"436":1}}],["对应下图的蓝色圈",{"2":{"436":1}}],["对应下图的绿色圈",{"2":{"436":1}}],["对应操作是max",{"2":{"519":1}}],["对应多粒子动态系统里的函数g",{"2":{"498":1}}],["对应上面多粒子动态系统里的函数f",{"2":{"498":1}}],["对应上图的数字标号9",{"2":{"449":1}}],["对应上图的数字标号7",{"2":{"449":1}}],["对应上图的数字标号5",{"2":{"449":1}}],["对应上图的数字标号4",{"2":{"449":1}}],["对应上图的数字标号3",{"2":{"449":1}}],["对应上图的数字标号2",{"2":{"449":1}}],["对应上图的数字标号10",{"2":{"449":1}}],["对应上图的数字标号1",{"2":{"449":1}}],["对应上图的编号9",{"2":{"449":1}}],["对应上图的编号7",{"2":{"449":1}}],["对应上图中从下到上的顺序",{"2":{"173":1}}],["对应了rnn的hidden",{"2":{"453":1}}],["对应了哈佛代码中的padding",{"2":{"84":1}}],["对应了哈佛代码中的sequence",{"2":{"84":1}}],["对应每个位置",{"2":{"444":1}}],["对应到我们当前任务就是输入文本序列",{"2":{"1323":1}}],["对应到具体数据构建",{"2":{"408":1}}],["对应到v则是",{"2":{"175":1}}],["对应bn的操作",{"2":{"343":1}}],["对应处理过的所有token",{"2":{"273":1}}],["对应元素相乘并求和",{"2":{"199":1}}],["对应代码是",{"2":{"330":1}}],["对应代码如下",{"2":{"199":1}}],["对应代码为",{"2":{"36":1}}],["对应代码为`transpose",{"2":{"36":1}}],["对应代码为`view",{"2":{"36":1}}],["对应代码为linear",{"2":{"36":1}}],["对应于神经元抑制",{"2":{"1460":1}}],["对应于神经元兴奋",{"2":{"1460":1}}],["对应于源序列中的一个词",{"2":{"161":1}}],["对应于每个头的一个矩阵",{"2":{"29":1}}],["对应着value",{"2":{"126":1}}],["对应记忆网络里键值对",{"2":{"126":1}}],["对应的另一个方向",{"2":{"2016":1}}],["对应的华氏温度为",{"2":{"1608":1}}],["对应的摄氏温度为",{"2":{"1608":3}}],["对应的tags",{"2":{"1330":1}}],["对应的数据类型",{"2":{"1214":1}}],["对应的导函数",{"2":{"839":1}}],["对应的嵌入",{"2":{"709":1}}],["对应的高维向量表示",{"2":{"700":1}}],["对应的整数转换回原始的词汇",{"2":{"431":1}}],["对应的特征向量映射为下一个待预测词的概率分布",{"2":{"431":1}}],["对应的逻辑图如下",{"2":{"427":1}}],["对应的值变为self",{"2":{"399":1}}],["对应的输出的计算",{"2":{"922":2}}],["对应的输出",{"2":{"144":1}}],["对应的属性",{"2":{"122":1}}],["对应的单个token的信息表征进行独立的非线性变换",{"2":{"101":1}}],["对应",{"2":{"90":1,"1672":1}}],["对应如下代码",{"2":{"529":1}}],["对应如下",{"2":{"38":1}}],["对应transformer架构图",{"2":{"38":1}}],["对多轮对话更友好",{"2":{"542":1}}],["对多个编码层的输出结果进行层归一化并返回最终的结果",{"2":{"532":1}}],["对多个head进行了分析",{"2":{"20":1}}],["对多头的输出进行压缩和融合来提升特征表征和泛化能力",{"2":{"10":1}}],["对于最大位移方向",{"2":{"2016":1}}],["对于最后一个token向量来说有一个分类任务",{"2":{"473":1}}],["对于最后一条样本",{"2":{"57":1}}],["对于光照",{"2":{"2009":1}}],["对于完全封装基类而不暴露任何接口的情况",{"2":{"1864":1}}],["对于派生类需要进一步继承扩展但不公开接口的情况",{"2":{"1864":1}}],["对于父类接口需要保留的情况",{"2":{"1864":1}}],["对于可以预见的错误",{"2":{"1764":1}}],["对于返回值类型为",{"2":{"1729":1}}],["对于赋值运算符",{"2":{"1712":1}}],["对于算术运算符",{"2":{"1712":1}}],["对于表示点的",{"2":{"1712":1}}],["对于编写更灵活",{"2":{"1644":1}}],["对于类",{"2":{"1638":1}}],["对于简单的函数",{"2":{"1632":1}}],["对于有符号数",{"2":{"1630":1}}],["对于无符号数相当于除以",{"2":{"1630":1}}],["对于数字",{"2":{"1914":2}}],["对于数值",{"2":{"1817":1,"1835":1}}],["对于数值类型",{"2":{"1623":1}}],["对于数学任务",{"2":{"224":1}}],["对于初学者来说",{"2":{"1611":1}}],["对于新手",{"2":{"1611":1}}],["对于高手",{"2":{"1611":1}}],["对于终生学习来说这些简单了解即可",{"2":{"1598":1}}],["对于终生学习来说",{"2":{"1598":1}}],["对于应用系统而言",{"2":{"1478":1}}],["对于相对位置编码来说",{"2":{"1338":1}}],["对于没有元数据的状态字典",{"2":{"1214":1}}],["对于没有经验的用户来说",{"2":{"1175":1}}],["对于你",{"2":{"1197":1}}],["对于使用例如100倍",{"2":{"1183":1}}],["对于早期和中期训练中不稳定情况都有好处",{"2":{"1180":1}}],["对于此工作量可能约为",{"2":{"1177":1}}],["对于此矩阵中的向量来说",{"2":{"172":1}}],["对于具有非常成熟建模和调整的工作流以及非常长且昂贵的生产训练运行的团队来说",{"2":{"1159":1}}],["对于余弦衰减",{"2":{"1159":1}}],["对于当前问题",{"2":{"1143":1}}],["对于给定的值x∈",{"2":{"1377":1}}],["对于给定的词语",{"2":{"1316":1}}],["对于给定的目标",{"2":{"1143":1}}],["对于给定的模型和优化器",{"2":{"1132":1}}],["对于给定任务或单个输入提示",{"2":{"225":1}}],["对于不会修改对象状态的运算符重载",{"2":{"1712":1}}],["对于不适合问题的问题或其他消息",{"2":{"1196":1}}],["对于不平衡的数据集",{"2":{"1165":1}}],["对于不可微分的函数的梯度计算",{"0":{"1115":1}}],["对于不同语言中字符可以采用不同长度的字节编码",{"2":{"607":1}}],["对于不同组应用不同的学习率",{"2":{"402":1}}],["对于不同信息量的元素给予不同的关注度",{"2":{"276":1}}],["对于不同的元素给予不同的关注度",{"2":{"256":1}}],["对于不同的注意力采取的整合方式是直接拼接",{"2":{"19":1}}],["对于矩阵",{"2":{"1082":1}}],["对于复数",{"2":{"1082":2}}],["对于训练深度神经网络模型而言",{"2":{"1042":1}}],["对于训练阶段",{"2":{"381":1}}],["对于短期上下文预填没有明显的开销",{"2":{"977":1}}],["对于这种训练式的绝对位置编码",{"2":{"1337":1}}],["对于这一切",{"2":{"1158":1}}],["对于这样长的上下文请求",{"2":{"977":1}}],["对于这三个训练好的网络",{"2":{"359":1}}],["对于那些确保行索引严格小于列索引的块",{"2":{"970":1}}],["对于所有可行的batch",{"2":{"1133":1}}],["对于所有列索引均大于行索引的块",{"2":{"970":1}}],["对于所提供的输入",{"2":{"405":1}}],["对于向量",{"2":{"943":1,"961":1}}],["对于向量a=",{"2":{"692":1}}],["对于卷积",{"2":{"772":1}}],["对于大规模系统和大型数据集",{"2":{"1025":1}}],["对于大序列长度约占一半的块",{"2":{"970":1}}],["对于大型图像处理有出色表现",{"2":{"769":1}}],["对于大部分query",{"2":{"20":1}}],["对于边缘位置窗口超出2k的单词",{"2":{"759":1}}],["对于裁剪我们做下说明",{"2":{"759":1}}],["对于模型来说",{"2":{"745":1}}],["对于如何获取序列顺序",{"2":{"744":1}}],["对于tm的学习",{"2":{"908":1}}],["对于t5使用",{"2":{"731":1}}],["对于transformer模型来说",{"2":{"1334":1}}],["对于transformer",{"2":{"474":1}}],["对于transformer中bn表现不好的原因做了一定的分析",{"2":{"316":1}}],["对于句子对",{"2":{"722":1}}],["对于句子来说",{"2":{"316":1}}],["对于从正则化线性模型派生的ebmedding",{"2":{"692":1}}],["对于很大的",{"2":{"647":1,"924":1}}],["对于vanilla",{"2":{"620":1}}],["对于gpt2",{"2":{"941":1,"960":1}}],["对于gpt",{"2":{"612":1}}],["对于噪声文本或字符丰富的语言",{"2":{"606":1}}],["对于处理数字方法有所不同",{"2":{"595":1}}],["对于stats的每个key",{"2":{"592":1}}],["对于英语来说",{"2":{"580":1}}],["对于英文",{"2":{"579":1,"606":1}}],["对于英文这种存在空格的语言天然就容易切分",{"2":{"565":1}}],["对于英文就是按照空格进行切分",{"2":{"553":1}}],["对于步骤3",{"2":{"515":1}}],["对于机器翻译",{"2":{"515":1}}],["对于几乎所有初始条件",{"2":{"507":1}}],["对于prompt工程师来说",{"2":{"504":1}}],["对于任何主机i",{"2":{"975":1}}],["对于任何可计算函数",{"2":{"504":1}}],["对于任意",{"2":{"178":1}}],["对于特定的输入",{"2":{"475":1}}],["对于交叉注意力来说",{"2":{"444":1}}],["对于掩码自注意力来说",{"2":{"443":1}}],["对于掩码ggg",{"2":{"93":1}}],["对于全局自注意力来说",{"2":{"442":1}}],["对于单词",{"2":{"700":1}}],["对于单词w",{"2":{"587":1}}],["对于单词an",{"2":{"409":1}}],["对于单个",{"2":{"89":1}}],["对于自回归语言模型而言",{"2":{"397":1}}],["对于自注意力",{"2":{"199":1}}],["对于中间隐藏层的神经元",{"2":{"396":1}}],["对于超球面上的任意两点和",{"2":{"354":1}}],["对于超长序列",{"2":{"210":1,"216":1}}],["对于cv",{"2":{"341":1}}],["对于cnn的特征图",{"2":{"325":1}}],["对于cnn方案",{"2":{"256":1}}],["对于",{"2":{"334":2,"341":1,"346":1,"676":1,"838":1,"1083":1,"1114":1,"1143":1,"1183":1,"1344":1,"1914":2}}],["对于架构图如下",{"2":{"330":1}}],["对于layernorm",{"2":{"327":1}}],["对于llm来说",{"2":{"222":1}}],["对于对nlp",{"2":{"326":1}}],["对于第三个单词",{"2":{"700":1}}],["对于第二种隐向量",{"2":{"416":1}}],["对于第一种隐向量",{"2":{"416":1}}],["对于第一个样本",{"2":{"57":1}}],["对于第",{"2":{"313":1}}],["对于归一化方案是否可以有效缓解ics",{"2":{"309":1}}],["对于零输出权重w=0",{"2":{"305":1}}],["对于结构化输入的问题",{"2":{"287":1}}],["对于记忆压缩问题",{"2":{"287":1}}],["对于序列进行",{"2":{"256":1,"272":1}}],["对于某些类型的超参数",{"2":{"1158":1}}],["对于某些复合操作也分解说明",{"2":{"462":1}}],["对于某些领域",{"2":{"235":1}}],["对于某些线性模型来说",{"2":{"692":1}}],["对于某些线性模型",{"2":{"233":1,"740":1}}],["对于语言理解任务",{"2":{"224":1}}],["对于长序列的数据和高维度的模型",{"2":{"203":1}}],["对于多层的作用",{"2":{"437":1}}],["对于多头注意力",{"2":{"199":1}}],["对于多个辅助记忆分片",{"2":{"143":1}}],["对于需要的模型",{"2":{"555":1}}],["对于需要聚焦于关键信息的场景更加合适",{"2":{"194":1}}],["对于需要全局信息的场景更加合适",{"2":{"194":1}}],["对于较大数值这部分",{"2":{"187":1}}],["对于两个词语",{"2":{"172":1}}],["对于两种掩码",{"2":{"77":1}}],["对于主要将信息编码为线性矩阵变换权重的密集深度神经网络来说",{"2":{"154":1}}],["对于未更改模型和提示",{"2":{"145":1}}],["对于上下文",{"2":{"137":1}}],["对于一维或多维数据处理都非常方便",{"2":{"1797":1}}],["对于一份文档而言",{"2":{"1185":1}}],["对于一些机器学习任务",{"2":{"1015":1}}],["对于一个目标超参数配置",{"2":{"1145":1}}],["对于一个由",{"2":{"1143":1}}],["对于一个序列",{"2":{"934":1}}],["对于一个多粒子动态系统",{"2":{"498":1}}],["对于一个关系和它的激活神经元",{"2":{"135":1}}],["对于一类通用的输入序列",{"2":{"94":1}}],["对于下图的回路来说",{"2":{"130":1}}],["对于每次上线",{"2":{"1139":1}}],["对于每一项研究",{"2":{"1149":1}}],["对于每一个head采用不同数值设置",{"2":{"765":1}}],["对于每一个词",{"2":{"679":1}}],["对于每一个查询语句q",{"2":{"168":1}}],["对于每一层",{"2":{"128":1,"717":1}}],["对于每个参数的梯度进行平方并相加后再取平方根的过程",{"2":{"1179":1}}],["对于每个类别只有少量样本的数据集",{"2":{"1165":1}}],["对于每个batch",{"2":{"1132":1}}],["对于每个请求",{"2":{"977":1}}],["对于每个提示",{"2":{"135":2}}],["对于每个头得到的q",{"2":{"9":1}}],["对于输入y的input",{"2":{"530":1}}],["对于输入x的input",{"2":{"520":1}}],["对于输入序列中的每个位置",{"2":{"419":1}}],["对于输入序列中的每个标记",{"2":{"352":1}}],["对于输入层的神经元",{"2":{"396":1}}],["对于输入为",{"2":{"341":1}}],["对于输入输出都是变长的序列",{"2":{"241":1}}],["对于输入",{"2":{"125":1}}],["对于强连通图",{"2":{"93":1}}],["对于推理阶段",{"2":{"381":1}}],["对于推理",{"2":{"83":1}}],["对于encoder来说",{"2":{"77":1}}],["对于解码器输出的每个词yi",{"2":{"267":1}}],["对于解码器来说",{"2":{"78":1,"284":1,"426":1}}],["对于解码器",{"2":{"72":1}}],["对于已经padding到同一长度的一个batch中的句子",{"2":{"62":1}}],["对于decoder来说",{"2":{"77":1}}],["对于decoder中间的多头注意力机制",{"2":{"39":1}}],["对于decoder最前面的掩码多头注意力机制",{"2":{"39":1}}],["对于参数矩阵",{"2":{"29":1}}],["输出当前余额",{"2":{"1873":1}}],["输出当前批次的均值和方差",{"2":{"1211":1}}],["输出p",{"2":{"1821":1,"1839":1}}],["输出m",{"2":{"1821":1,"1839":1}}],["输出每一行",{"2":{"1820":1,"1838":1}}],["输出文件流",{"2":{"1819":1,"1837":1}}],["输出字符",{"2":{"1816":1,"1834":1}}],["输出字符串流",{"2":{"1823":1,"1841":1}}],["输出字符串的前",{"2":{"1816":1,"1834":1}}],["输出字符串",{"2":{"1624":1}}],["输出字符串到控制台",{"2":{"1606":1}}],["输出指定数量的字符",{"2":{"1816":1,"1834":1}}],["输出指针的大小",{"2":{"1667":1}}],["输出一个字符",{"2":{"1816":1,"1834":1}}],["输出一个隐状态",{"2":{"249":1}}],["输出流用于向外部设备",{"2":{"1815":1,"1833":1}}],["输出流",{"0":{"1815":1,"1833":1},"1":{"1816":1,"1817":1,"1834":1,"1835":1}}],["输出流运算符重载",{"2":{"1712":1}}],["输出错误信息",{"2":{"1811":1,"1829":1}}],["输出到标准输出",{"2":{"1811":1,"1829":1}}],["输出结果",{"0":{"1785":1},"2":{"1774":1,"1779":1,"1788":1,"1789":1,"1791":1,"1797":1,"1805":1,"1806":1,"1807":1,"1933":1}}],["输出结果就是隐藏状态",{"2":{"868":1}}],["输出整个数组的大小",{"2":{"1667":1}}],["输出整理后的结果",{"2":{"1566":1}}],["输出数组",{"2":{"1623":1}}],["输出内容长度",{"2":{"1566":1}}],["输出示例",{"2":{"1513":1}}],["输出值范围内",{"2":{"1460":1}}],["输出最终的运行时均值和方差",{"2":{"1211":1}}],["输出最差也是恒等映射x",{"2":{"301":1}}],["输出张量",{"2":{"1208":1}}],["输出张量维度为",{"2":{"510":1}}],["输出长度进行抽样",{"2":{"980":1}}],["输出序列的累积损失越小",{"2":{"903":1}}],["输出序列长度m",{"2":{"883":1}}],["输出门确定了下一个隐藏状态应该是什么样子",{"2":{"869":1}}],["输出门决定下一个隐藏状态应该是什么样子",{"2":{"868":1}}],["输出门",{"0":{"868":1},"2":{"864":1}}],["输出是什么",{"2":{"2003":1,"2004":1,"2005":1,"2006":3,"2008":1}}],["输出是相应的单词嵌入",{"2":{"834":1}}],["输出是一个包含源句子信息的上下文表示",{"2":{"525":1}}],["输出也以同样的方式改变这一性质",{"2":{"772":1}}],["输出也是",{"2":{"528":1}}],["输出有时被称作特征映射或特征图",{"2":{"770":1}}],["输出则是对应的嵌入向量",{"2":{"702":1}}],["输出给解码器",{"2":{"453":1}}],["输出模块包括两部分",{"2":{"471":1}}],["输出模块",{"2":{"436":1,"449":1}}],["输出模型使用上下文向量c将各个部分组合成最终的高级特征向量^yy^",{"2":{"263":1}}],["输出模型",{"2":{"263":1}}],["输出不同",{"2":{"426":1}}],["输出为tensor",{"2":{"1094":1}}],["输出为一维的",{"2":{"1083":1}}],["输出为value的加权和",{"2":{"916":1}}],["输出为运行算法得到的subword词表",{"2":{"576":1}}],["输出为",{"2":{"330":1,"399":2,"428":1,"879":1}}],["输出为f",{"2":{"301":1}}],["输出如下",{"2":{"315":1}}],["输出key和value向量之间的相关性",{"2":{"268":1}}],["输出特征数为词表大小",{"2":{"201":1}}],["输出层将目标序列的编码表转换为单词概率和最终输出序列",{"2":{"427":2}}],["输出层一般不加dropout",{"2":{"396":1}}],["输出层附近的参数的梯度会较大",{"2":{"333":1}}],["输出层",{"2":{"201":1,"530":2,"1461":1}}],["输出层中预测词的概率",{"2":{"187":1}}],["输出的context",{"2":{"889":1}}],["输出的值介于",{"2":{"865":1}}],["输出的概念可以解码为其他语言或模态",{"2":{"629":1}}],["输出的embedding和位置编码",{"2":{"449":1}}],["输出的特征",{"2":{"201":1}}],["输出的数字往往具有较高的方差",{"2":{"189":1}}],["输出的方差往往较小",{"2":{"189":1}}],["输出的矩阵z与其输入的矩阵x的维度是一样的",{"2":{"35":1}}],["输出的矩阵大些",{"2":{"10":1}}],["输出就变成了f",{"2":{"304":1}}],["输出就可以被解释为概率分布",{"2":{"180":1}}],["输出就应该是",{"2":{"83":1}}],["输出",{"0":{"198":1,"2150":1},"2":{"157":1,"158":1,"475":1,"542":1,"576":1,"587":1,"702":1,"773":1,"832":3,"986":1,"1607":2,"1611":1,"1624":1,"1653":2,"1663":2,"1667":1,"1668":1,"1673":2,"1685":1,"1693":2,"1694":5,"1695":4,"1704":2,"1712":1,"1713":11,"1714":7,"1715":3,"1728":1,"1729":2,"1806":1,"1817":2,"1821":1,"1835":2,"1839":1,"1866":2,"1905":1,"1906":3,"1907":2,"1908":2,"1910":2,"1911":3,"1912":1,"1924":1,"1933":1,"2059":2,"2154":1}}],["输出时",{"2":{"145":1}}],["输出被放大",{"2":{"122":1}}],["输出维度为d",{"2":{"113":2}}],["输出维度可能是",{"2":{"99":1}}],["输出形状",{"2":{"113":1}}],["输入services",{"2":{"2093":1}}],["输入文件流",{"2":{"1819":1,"1837":1}}],["输入文本对应的embedding在transformer内部各层流通时会不断演变",{"2":{"437":1}}],["输入错误",{"2":{"1814":1,"1832":1}}],["输入失败",{"2":{"1814":1,"1832":1}}],["输入流对象",{"2":{"1814":1,"1832":1}}],["输入流的状态",{"0":{"1814":1,"1832":1}}],["输入流用于从外部设备",{"2":{"1812":1,"1830":1}}],["输入流",{"0":{"1812":1,"1830":1},"1":{"1813":1,"1814":1,"1831":1,"1832":1},"2":{"1826":1,"1844":1}}],["输入字符串流",{"2":{"1823":1,"1841":1}}],["输入字符串",{"2":{"1624":1}}],["输入管道性能受限的原因及干预措施与具体任务高度相关",{"2":{"1161":1}}],["输入标记可能比输出标记大10到100倍",{"2":{"977":1}}],["输入一段视频并判断它的类别等等",{"2":{"881":1}}],["输入一个句子判断其情感倾向",{"2":{"881":1}}],["输入一个中心词的contex后得到的就是中心词",{"2":{"714":1}}],["输入一个中心词的上下文后得到的就是中心词",{"2":{"714":1}}],["输入一个",{"2":{"510":1}}],["输入门决定了从当前步骤中添加哪些相关信息",{"2":{"869":1}}],["输入门",{"0":{"866":1},"2":{"864":1}}],["输入time的时候",{"2":{"860":1}}],["输入token代表查询部分",{"2":{"620":1}}],["输入token被送入一个称为token",{"2":{"620":1}}],["输入中元素的顺序保持不变",{"2":{"828":1}}],["输入中最外层0是什么",{"2":{"773":1}}],["输入中心词",{"2":{"714":1}}],["输入中文",{"2":{"161":1}}],["输入=output",{"2":{"698":2}}],["输入=input",{"2":{"698":2}}],["输入首先通过线性投影块来计算注意力块的输入",{"2":{"620":1}}],["输入首先经过embedding",{"2":{"446":1}}],["输入给解码器",{"2":{"529":1}}],["输入和输出序列必须要是等长的",{"2":{"879":1}}],["输入和输出token通过解码器以相同的方式进行处理",{"2":{"541":1}}],["输入和输出",{"0":{"518":1,"526":1},"2":{"698":1}}],["输入和输出两种语言中相同含义的词或者段落应该能对齐",{"2":{"284":1}}],["输入归因方法通常用于通过估计输入元素",{"2":{"478":1}}],["输入归因",{"0":{"478":1}}],["输入元素",{"2":{"476":1}}],["输入经过所有解码块处理后",{"2":{"472":1}}],["输入数据",{"2":{"522":1,"1202":1}}],["输入数据按照顺序被逐层处理",{"2":{"492":1}}],["输入数据最终来到了它的归宿",{"2":{"471":1}}],["输入数据与过去数据的偏差就越大",{"2":{"230":1}}],["输入数据与三个线性层分别相乘",{"2":{"26":1}}],["输入层用于接受外界输入信号",{"2":{"1461":1}}],["输入层初始化的时候直接为每个词随机生成一个n维的向量",{"2":{"714":1}}],["输入层",{"2":{"520":3,"530":3}}],["输入层中",{"2":{"520":1,"530":1}}],["输入层的作用就是把自然语言",{"2":{"460":1}}],["输入层与隐层节点的任何一个子集结合",{"2":{"28":1}}],["输入分类",{"0":{"453":1}}],["输入模块具体包括如下",{"2":{"454":1}}],["输入模块",{"0":{"454":1},"2":{"436":1,"449":1}}],["输入输出流",{"0":{"1673":1}}],["输入输出以及常用操作",{"2":{"1624":1}}],["输入输出",{"0":{"426":1,"704":1}}],["输入到注意力层",{"2":{"698":1}}],["输入到",{"2":{"420":1,"889":1}}],["输入时的dropout",{"2":{"394":1}}],["输入是隐变量",{"2":{"346":1}}],["输入参数转为",{"2":{"1226":1}}],["输入参数是",{"2":{"701":1}}],["输入参数x是输入张量",{"2":{"344":1}}],["输入参数为x",{"2":{"113":1}}],["输入为",{"2":{"341":1}}],["输入为一个包含n个token的句子",{"2":{"289":1}}],["输入矩阵x经过encoder之后",{"2":{"330":1}}],["输入特征的尺度会影响梯度下降算法的迭代步数以及梯度更新的难度",{"2":{"309":1}}],["输入特征数为词嵌入的维度",{"2":{"201":1}}],["输入结构化存在问题",{"2":{"287":1}}],["输入词和隐状态",{"2":{"249":1}}],["输入句子每个时间步的信息都包含在了这个上下文中",{"2":{"241":1}}],["输入长度为n",{"2":{"210":1}}],["输入x分别和权重wq",{"2":{"201":1}}],["输入x和σσ",{"2":{"108":1}}],["输入h被32个transformerblock逐个处理",{"2":{"201":1}}],["输入更多的token会导致注意力更加分散",{"2":{"181":1}}],["输入序列",{"2":{"1322":1}}],["输入序列是一个需要组装的玩具",{"2":{"524":1}}],["输入序列中每个位置的单词都按照各自单独的路径流入编码器",{"2":{"419":1}}],["输入序列中每个位置的单词都各自单独的路径流入编码器",{"2":{"415":1}}],["输入序列的长度就成了限制模型性能的瓶颈",{"2":{"891":1}}],["输入序列的长度",{"2":{"316":1}}],["输入序列的每一个元素xi∈rdxi∈rdx",{"2":{"161":1}}],["输入序列接下来会通过三个矩阵wk∈rd×dkwk∈rd×dk",{"2":{"161":1}}],["输入序列长度为l",{"2":{"161":1}}],["输入序列就是源语句或者目标语句",{"2":{"158":1}}],["输入序列首先遇到的是masked",{"2":{"71":1}}],["输入嵌入层的设计允许模型轻松地适应不同的语言或领域",{"2":{"696":1}}],["输入嵌入层是transformer框架中不可或缺的一部分",{"2":{"674":1}}],["输入嵌入",{"2":{"460":1}}],["输入嵌入被划分为更小的块并独立处理",{"2":{"153":1}}],["输入嵌入首先通过局部敏感哈希算法进行哈希处理",{"2":{"153":1}}],["输入在最终主语token中的决定性作用",{"2":{"145":1}}],["输入维度为d",{"2":{"113":2}}],["输入形状为",{"2":{"428":1}}],["输入形状",{"2":{"113":1}}],["输入的tensor",{"2":{"1106":1}}],["输入的视图",{"2":{"1087":1}}],["输入的索引",{"2":{"702":1}}],["输入的句子被打散变为一个个token的过程",{"2":{"584":1}}],["输入的维度一般是",{"2":{"518":1}}],["输入的维度是",{"2":{"30":1}}],["输入的embedding和位置编码",{"2":{"449":1}}],["输入的信息",{"2":{"437":1}}],["输入的特征数",{"2":{"201":1}}],["输入的",{"2":{"172":1}}],["输入的q",{"2":{"16":1}}],["输入会与",{"2":{"26":1}}],["输入依然是原始的q",{"2":{"16":1}}],["输入",{"0":{"25":1,"161":1,"198":1,"391":1,"452":1,"2149":1},"1":{"453":1,"454":1,"455":1,"456":1,"457":1,"458":1,"459":1,"460":1},"2":{"0":1,"74":1,"148":1,"157":2,"158":1,"173":1,"267":4,"427":1,"485":1,"542":1,"698":2,"758":4,"773":2,"922":1,"1283":1,"1340":6,"1438":1,"1673":1,"1712":1,"1729":1}}],["可否互换",{"2":{"2157":1}}],["可否让胃口最大的孩子吃相应最大",{"2":{"2157":1}}],["可省略",{"2":{"2018":1}}],["可执行文件源代码",{"2":{"1997":1}}],["可执行文件名",{"2":{"1917":1}}],["可根据",{"2":{"1808":1}}],["可维护性和可测试性",{"2":{"1729":1}}],["可维护的代码至关重要",{"2":{"1709":1}}],["可重载与不可重载的运算符",{"2":{"1712":1}}],["可设置为返回固定值或简单计算",{"2":{"1657":1}}],["可读性更高",{"2":{"1761":1}}],["可读性差",{"2":{"1632":1}}],["可读性和",{"2":{"1228":1}}],["可选",{"2":{"1608":1,"1729":2,"1933":3,"1999":1}}],["可选的掩码和dropout",{"2":{"944":1,"963":1}}],["可视模式",{"0":{"1551":1},"2":{"1541":1}}],["可视化",{"2":{"837":1,"849":1}}],["可视化这个深度学习模型",{"0":{"787":1}}],["可变性",{"2":{"1613":1}}],["可变数据目录",{"2":{"1506":1}}],["可变形卷积在kernel中加入偏移量offset",{"2":{"781":1}}],["可变形卷积",{"0":{"780":1},"1":{"781":1,"782":1}}],["可划分为两个阶段",{"2":{"1441":1}}],["可是这些数字不能是随随便便的数字吧",{"2":{"1372":1}}],["可直接设定中间过程",{"2":{"1244":1}}],["可有效解决鞍点问题",{"2":{"1243":1}}],["可通过register",{"2":{"1208":1}}],["可参考umut",{"2":{"1155":1}}],["可用内存减少",{"2":{"1671":1}}],["可用计算资源",{"2":{"1138":1}}],["可用硬件通常能能够支持一系列batch",{"2":{"1132":1}}],["可用硬件支持的最大batch",{"2":{"1131":1,"1133":1}}],["可调节的更新步长",{"2":{"1027":1}}],["可共享的部分包括",{"2":{"985":1}}],["可在相同功耗和芯片面积下实现双倍或四倍的吞吐量",{"2":{"973":1}}],["可在任何transformer架构中替换mha",{"2":{"43":1}}],["可增加序列长度",{"2":{"945":1,"965":1}}],["可恨之处",{"2":{"909":1}}],["可容许机器通过此模型发现及学习将一种语言的语句",{"2":{"885":1}}],["可化简为",{"2":{"844":1}}],["可知",{"2":{"839":2}}],["可微性",{"2":{"838":1}}],["可微分的",{"2":{"164":1}}],["可扩展性",{"2":{"1810":1,"1828":1}}],["可扩展性和灵活性",{"2":{"696":1}}],["可扩展处理单元",{"2":{"1214":1}}],["可扩展的内存查找表",{"2":{"153":1}}],["可计算性",{"2":{"689":1}}],["可区分性",{"2":{"689":1}}],["可训练",{"0":{"686":1}}],["可训练的位置编码",{"2":{"747":1}}],["可训练的记忆层会输出值的软组合",{"2":{"154":1}}],["可训练的记忆层类似于注意力机制",{"2":{"154":1}}],["可训练的",{"2":{"10":1}}],["可见",{"0":{"1426":1},"2":{"602":1,"994":1,"1426":1}}],["可解释性",{"2":{"568":1}}],["可解释性之积分梯度算法",{"2":{"156":1}}],["可迭代对象",{"2":{"385":1,"1225":1}}],["可学习方案的优点是可以根据任务的需要进行调整",{"2":{"749":1}}],["可学习且无约束的",{"2":{"747":1}}],["可学习的绝对位置偏置",{"2":{"757":1}}],["可学习的方法",{"2":{"612":1}}],["可学习的变度量矩阵的对角元素",{"2":{"352":1}}],["可学习的激活函数",{"2":{"155":1}}],["可学习",{"0":{"1337":1},"2":{"172":1}}],["可可",{"2":{"156":1}}],["可描述",{"2":{"127":1}}],["可能举例举的不好",{"2":{"2118":1}}],["可能大家不太理解我具体在说什么",{"2":{"2117":1}}],["可能大于实际长度",{"2":{"1713":1}}],["可能抛出异常",{"2":{"1762":1}}],["可能在不合适的情况下进行转换",{"2":{"1629":1}}],["可能在讨论贝叶斯优化时",{"2":{"1185":1}}],["可能返回",{"2":{"1611":1}}],["可能返回原始张量的视图",{"2":{"1083":1}}],["可能出现问题说明",{"2":{"1482":1}}],["可能出现的问题",{"0":{"1374":1}}],["可能你在解决某种模型需求",{"2":{"1175":1}}],["可能可以用大多数计划做到",{"2":{"1158":1}}],["可能转移",{"2":{"1158":1}}],["可能的补救措施包括增加batch",{"2":{"1149":1}}],["可能的变化",{"2":{"875":1}}],["可能很难知道是否搜索空间已经被足够密集地采样",{"2":{"1148":1}}],["可能包括多种不同类型的成本",{"2":{"1134":1}}],["可能都需要重复这些步骤",{"2":{"1132":1}}],["可能有重复值",{"2":{"1086":1}}],["可能有漏给出处的现象",{"2":{"235":1}}],["可能使得学习率在达到这样的凸结构前就变得太小了",{"2":{"1048":1}}],["可能陷入局部最小值",{"2":{"1026":1}}],["可能存在更复杂的依赖关系",{"2":{"1326":1}}],["可能存在各种kv",{"2":{"985":1}}],["可能存在部分",{"2":{"90":1}}],["可能耗时太久",{"2":{"904":1}}],["可能表现更好",{"2":{"838":1}}],["可能尺寸会变小",{"2":{"779":1}}],["可能增加训练和推理的时间",{"2":{"746":1}}],["可能忽略了输入的可变性",{"2":{"738":1}}],["可能忽略了词语之间的语义联系",{"2":{"696":1}}],["可能无法捕捉输入令牌的关键特征和所有信息",{"2":{"738":1}}],["可能无法很好地区分同一个词在不同语境下的含义",{"2":{"696":1}}],["可能提供更细致的比较",{"2":{"692":1}}],["可能不保留语义",{"2":{"676":1}}],["可能稀疏",{"2":{"676":1}}],["可能是",{"2":{"1714":1}}],["可能是为了自一致性提示",{"2":{"986":1}}],["可能是非线性的",{"2":{"689":1}}],["可能是完整的单词",{"2":{"567":1}}],["可能是增进也可能是噪声",{"2":{"20":1}}],["可能被分为单独的",{"2":{"560":1}}],["可能需要通过实验和调整来找到最适合特定模型和任务的词汇表大小",{"2":{"562":1}}],["可能需要更大的词汇量来覆盖其丰富的复合词形态",{"2":{"560":1}}],["可能需要根据实际情况来忽略掉一些输入",{"2":{"173":1}}],["可能导致的内存泄漏",{"2":{"1911":1}}],["可能导致数据丢失或程序崩溃",{"2":{"1683":1}}],["可能导致",{"2":{"1670":1}}],["可能导致未定义行为",{"2":{"1669":1,"1713":1}}],["可能导致栈空间不足",{"2":{"1648":1}}],["可能导致栈溢出",{"2":{"1646":1}}],["可能导致程序崩溃或数据损坏",{"2":{"1634":1}}],["可能导致输入给下游非线性函数比如激活函数的时候产生负面效果",{"2":{"313":1}}],["可能导致很多数据落入梯度饱和区",{"2":{"309":1}}],["可能性越大就应该赋予更大的权重",{"2":{"268":1}}],["可能和原始论文作者的思路或者与实际历史发展轨迹不尽相同",{"2":{"235":1}}],["可能通过多个中间节点",{"2":{"204":1}}],["可能也能工作得很好",{"2":{"172":1}}],["可能会使用",{"2":{"1866":1}}],["可能会降低代码的可读性",{"2":{"1615":1}}],["可能会出现一些不可预期的问题",{"2":{"1407":1}}],["可能会处理一些其他的内部机制",{"2":{"1214":1}}],["可能会解决一些学习率预热无法解决的问题",{"2":{"1180":1}}],["可能会看到明显的性能改进",{"2":{"1158":1}}],["可能会转移",{"2":{"1158":1}}],["可能会需要更多的训练步骤才能达到最优状态",{"2":{"1157":1}}],["可能会减少max",{"2":{"1155":1}}],["可能会以增大训练误差为代价",{"2":{"1011":1}}],["可能会给深度网络训练带来潜在的问题",{"2":{"841":1}}],["可能会引入冗余信息",{"2":{"739":1}}],["可能会稀释关键短语中的重要信息",{"2":{"735":1}}],["可能会得到难以解释且没有实际意义的结果",{"2":{"692":1}}],["可能会得到完全不同的观察结果",{"2":{"136":1}}],["可能会产生一些不太合理的子词或者说错误的切分",{"2":{"600":1}}],["可能会有多个析构函数被调用",{"2":{"1764":1}}],["可能会有不同的subword序列",{"2":{"595":1}}],["可能会有彼此交叉",{"2":{"474":1}}],["可能会带来一定程度上的性能损失",{"2":{"335":1}}],["可能会导致程序崩溃或产生不可预测的结果",{"2":{"1623":1}}],["可能会导致较差的输入管道性能",{"2":{"1161":1}}],["可能会导致高楼层较大的倾斜",{"2":{"309":1}}],["可能会导致内存溢出",{"2":{"231":1}}],["可能会更加清晰一点",{"2":{"165":1}}],["可能具备一定的好处",{"2":{"119":1}}],["可能截断",{"2":{"90":1}}],["可按需动态组合出多至hxh个注意力头",{"2":{"43":1}}],["可以像对待数组一样处理位",{"2":{"2062":1}}],["可以像访问普通数组元素一样",{"2":{"1624":1}}],["可以求出直线的斜率",{"2":{"2018":1}}],["可以生成非常逼真的图像",{"2":{"2009":1}}],["可以生成特定于编译器的项目文件",{"2":{"1963":1}}],["可以生成一个更小的词表",{"2":{"594":1}}],["可以统一处理多种字符串类型",{"2":{"1929":1}}],["可以增加一些注释",{"2":{"1918":1}}],["可以增加模型的非线性表示能力",{"2":{"517":1}}],["可以兼容左值和右值",{"2":{"1914":1}}],["可以创建并初始化新的变量",{"2":{"1907":1}}],["可以创建一个存储任意类型的数组",{"2":{"1700":1}}],["可以接受任意类型的参数",{"2":{"1906":1}}],["可以写出更安全",{"2":{"1901":1}}],["可以写在",{"2":{"1662":1}}],["可以简单计数",{"2":{"1868":1}}],["可以简化声明",{"2":{"1615":1}}],["可以简化代码并提高可读性",{"2":{"1615":1}}],["可以定义行为接口",{"2":{"1866":1}}],["可以定义常用的常量或代码片段",{"2":{"1632":1}}],["可以实现一些特殊的功能",{"2":{"1776":1}}],["可以实现任意图像大小的输入",{"2":{"816":1}}],["可以方便地访问类的私有成员",{"2":{"1776":1}}],["可以考虑使用其他方式",{"2":{"1764":1}}],["可以考虑使用map传递参数",{"2":{"1488":1}}],["可以捕获右值引用",{"2":{"1907":1}}],["可以捕获任何类型的异常",{"2":{"1762":1}}],["可以捕获输入序列的模式",{"2":{"125":1}}],["可以跨越多个函数调用栈来传递和处理错误",{"2":{"1761":1}}],["可以跨语言共用词表",{"2":{"608":1}}],["可以显式地为枚举常量指定整数值",{"2":{"1728":1}}],["可以显式地使用",{"2":{"1638":1}}],["可以根据需要自动扩展容量",{"2":{"1797":1}}],["可以根据需要自动调整大小",{"2":{"1719":1}}],["可以根据程序的需要动态地调整数组的大小",{"2":{"1714":1}}],["可以根据不同的比较规则进行排序",{"2":{"1645":1}}],["可以传入不同类型的参数",{"2":{"1701":1}}],["可以传参数",{"2":{"1208":1}}],["可以比较任意两种类型的值并返回较大的值",{"2":{"1698":1}}],["可以比较完全的掌握它的设计思路和实现",{"2":{"1479":1}}],["可以共享同一块内存",{"2":{"1695":1}}],["可以自定义",{"2":{"1810":1,"1828":1}}],["可以自动化编译过程",{"2":{"1918":1}}],["可以自动扩展或缩小以适应字符串的长度变化",{"2":{"1803":1}}],["可以自动释放内存",{"2":{"1695":1}}],["可以自由分配和回收的空地",{"2":{"1648":1}}],["可以选择不实现",{"2":{"1693":1}}],["可以选择添加或者不添加偏置",{"2":{"8":1}}],["可以没有函数体的虚函数",{"2":{"1693":1}}],["可以添加一些私有成员",{"2":{"1674":1}}],["可以添加其他训练信息",{"2":{"1266":1}}],["可以调用其中一个",{"2":{"1665":1}}],["可以访问父类的私有和受保护部分",{"2":{"1784":1}}],["可以访问私有和受保护成员",{"2":{"1773":1}}],["可以访问类的私有和保护成员",{"2":{"1712":1}}],["可以访问公有成员变量",{"2":{"1674":1}}],["可以访问",{"2":{"1655":1,"1853":2,"1857":1,"1861":1}}],["可以访问静态成员变量和调用静态成员函数",{"2":{"1639":1}}],["可以多层次继承",{"2":{"1654":1}}],["可以多尝试使用",{"2":{"1485":1}}],["可以判断它们在数组中的相对位置",{"2":{"1633":1}}],["可以返回一个值",{"2":{"1631":1}}],["可以无条件跳转到程序中的标记位置",{"2":{"1631":1}}],["可以无限地改善",{"2":{"1157":1}}],["可以相互嵌套",{"2":{"1631":1}}],["可以放在赋值运算符的左边",{"2":{"1629":1}}],["可以取地址",{"2":{"1629":1}}],["可以省略",{"2":{"1638":1,"1729":1}}],["可以省略数组大小",{"2":{"1634":1}}],["可以省略数组的大小",{"2":{"1623":1}}],["可以省略花括号",{"2":{"1619":1}}],["可以配合",{"2":{"1619":1}}],["可以为空指针",{"2":{"1612":1}}],["可以不初始化",{"2":{"1612":1}}],["可以指向字符数组的首地址",{"2":{"1715":1}}],["可以指向其他地址",{"2":{"1614":1}}],["可以指向任何类型",{"2":{"1611":1}}],["可以指出某些实现错误",{"2":{"1164":1}}],["可以安全使用",{"2":{"1611":1}}],["可以扩展正数的表示范围",{"2":{"1607":1}}],["可以获取整数",{"2":{"1607":1}}],["可以获得不同的模型",{"2":{"446":1}}],["可以存储任何类型的值",{"2":{"1928":1}}],["可以存储多种不同类型",{"2":{"1926":1}}],["可以存储多种不同类型的值",{"2":{"1926":1}}],["可以存储两种不同类型的数据对",{"2":{"1805":1}}],["可以存储各种类型的元素",{"2":{"1797":1}}],["可以存储更多位的小数",{"2":{"1607":1}}],["可以存储",{"2":{"1607":1}}],["可以存储的范围通常为",{"2":{"1607":1}}],["可以解决的问题也更加广泛",{"2":{"1465":1}}],["可以修改每个神经元输入节点的权重系数",{"2":{"1443":1}}],["可以得到包含相对位置信息的",{"2":{"1343":1}}],["可以加速您的pytorch代码",{"2":{"1293":1}}],["可以加速模型收敛",{"2":{"809":1}}],["可以加速模型训练过程",{"2":{"346":1}}],["可以清为0",{"2":{"1214":1}}],["可以改变这种行为",{"2":{"1211":1}}],["可以给我们的邮箱",{"2":{"1196":1}}],["可以给大模型提供具有事实更新的示范语句",{"2":{"141":1}}],["可以舒适地包含最佳观察试验周围的局部区域",{"2":{"1153":1}}],["可以启用无梯度模式",{"2":{"1120":1}}],["可以设置张量的",{"2":{"1116":1}}],["可以灵活地适应训练数据",{"2":{"1012":1}}],["可以模型学习的结果更好",{"2":{"908":1}}],["可以是centos7",{"2":{"2089":1}}],["可以是以下值之一",{"2":{"1821":1,"1839":1}}],["可以是正数或负数",{"2":{"1821":1,"1839":1}}],["可以是任何有效的",{"2":{"1729":1}}],["可以是任意长度",{"2":{"504":1}}],["可以是指针或智能指针",{"2":{"1690":1}}],["可以是基于检索的问答或阅读理解型问答",{"2":{"906":1}}],["可以利用预训练模型在大规模数据上学习到的语言分布信息",{"2":{"898":1}}],["可以利用门控线形单元",{"2":{"105":1}}],["可以处理不同的context",{"2":{"898":1}}],["可以处理不同维度的key与query",{"2":{"175":1}}],["可以直接使用",{"2":{"1905":1,"2068":1}}],["可以直接使用枚举常量来赋值和比较",{"2":{"1728":1}}],["可以直接使用枚举常量",{"2":{"1728":1}}],["可以直接使用预测时语言模型",{"2":{"895":1}}],["可以直接赋值或在构造函数中使用",{"2":{"1715":1}}],["可以直接操作数组中的元素",{"2":{"1715":1}}],["可以直接通过内存地址计算访问元素",{"2":{"1714":1}}],["可以直接访问",{"2":{"1665":1}}],["可以直接给别人使用吗",{"2":{"1260":1}}],["可以直接把向量化理解为一种数据格式转换的技术",{"2":{"676":1}}],["可以初步得出一个良好的激活函数常具备以下一些特点",{"2":{"848":1}}],["可以替代梯度",{"2":{"838":1}}],["可以替代bn",{"2":{"338":1}}],["可以进行比较和运算",{"2":{"1728":1}}],["可以进行pointwise",{"2":{"829":1}}],["可以进行相加",{"2":{"722":1,"740":1}}],["可以事先计算出来",{"2":{"765":1}}],["可以形成基于路由的嵌入erwerwe",{"2":{"739":1}}],["可以采用两种方式来聚合输出向量",{"2":{"735":1}}],["可以采用嵌套文档格式",{"2":{"369":1}}],["可以视为乘性的",{"2":{"1344":1}}],["可以视为是乘性位置编码的变体",{"2":{"1344":1}}],["可以视为输出词表的一个分布",{"2":{"128":1}}],["可以视作一种大规模的弱监督学习过程",{"2":{"726":1}}],["可以学习到更多数据中的细节和噪声",{"2":{"1012":1}}],["可以学习",{"2":{"682":1}}],["可以学习任何函数",{"2":{"154":1}}],["可以基于规则或统计方法生成",{"2":{"676":1}}],["可以基于key捕获的模式",{"2":{"126":1}}],["可以作为一种通用的网络结构",{"2":{"620":1}}],["可以作为长期",{"2":{"226":1}}],["可以最大限度地共享多种语言的词汇并实现更好的翻译质量",{"2":{"608":1}}],["可以较好的平衡词表大小和oov问题",{"2":{"600":1}}],["可以反映一个词内部的两个部分结合的紧密程度",{"2":{"598":1}}],["可以很直观的看出",{"2":{"587":1}}],["可以很好地保留词的边界信息和完整语义",{"2":{"565":1}}],["可以很好的将",{"2":{"17":1}}],["可以管窥",{"2":{"558":1}}],["可以打破transformer的位置不变性",{"2":{"542":1}}],["可以聚焦于源语言句子中的不同部分",{"2":{"536":1}}],["可以并行预测目标序列的所有单词",{"2":{"528":1}}],["可以综合考虑目标序列与源序列的内容",{"2":{"525":1}}],["可以驱动网络连接权重的连续更新以获得一个聪明的自适应的物理模型",{"2":{"506":1}}],["可以对元素进行任意操作",{"2":{"1914":1}}],["可以对模型的属性进行形式化验证",{"2":{"505":1}}],["可以对序列上下文信息以及不同范围元素间的局部依赖关系进行捕捉",{"2":{"248":1}}],["可以帮助实现模型的可信性",{"2":{"505":1}}],["可以帮助我们理解这些算法在底层是如何高效执行的",{"2":{"1602":1}}],["可以帮助我们在不同数学领域之间建立桥梁",{"2":{"505":1}}],["可以帮助我们对这些矩阵有更好的理解",{"2":{"463":1}}],["可以描述和比较不同数学结构之间的共性和相似性",{"2":{"505":1}}],["可以以一定的速率对neural",{"2":{"497":1}}],["可以与神经元形成三向连接",{"2":{"488":1}}],["可以更灵活的修改外部变量",{"2":{"1907":1}}],["可以更深入地探讨",{"2":{"1678":1}}],["可以更深入地了解编译过程",{"2":{"1605":1}}],["可以更深入地了解它们的运行机制",{"2":{"1602":1}}],["可以更深入地了解准确性改进",{"2":{"1165":1}}],["可以更容易地保持梯度的一致性",{"2":{"838":1}}],["可以更多的保留图像的背景信息",{"2":{"815":1}}],["可以更准确地区分不同位置的词语",{"2":{"749":1}}],["可以更准确的表达单词的真实含义",{"2":{"718":1}}],["可以更好地管理",{"2":{"1918":1}}],["可以更好地捕捉长距离依赖关系",{"2":{"635":1}}],["可以更好的将对其他单词的",{"2":{"274":1}}],["可以更新零样本的注意力权重",{"2":{"542":1}}],["可以更换组件",{"2":{"450":1}}],["可以在链表起点快速添加",{"2":{"1801":1}}],["可以在链表的任意位置高效地插入和删除元素",{"2":{"1799":1}}],["可以在头尾高效地插入",{"2":{"1800":1}}],["可以在程序的多个地方调用",{"2":{"1729":1}}],["可以在程序运行时根据需要分配不同大小的数组",{"2":{"1714":1}}],["可以在任意时刻申请和释放任意大小的内存块",{"2":{"1648":1}}],["可以在任何时候指向不同的变量",{"2":{"1612":1}}],["可以在任何语言或模态的输入上以零样本",{"2":{"629":1}}],["可以在声明数组的同时进行初始化",{"2":{"1623":1}}],["可以在计算梯度",{"2":{"1223":1}}],["可以在每次迭代中更改形状",{"2":{"1108":1}}],["可以在合理时间内完成训练",{"2":{"563":1}}],["可以在o",{"2":{"504":1,"511":1}}],["可以在",{"2":{"504":1,"1594":1}}],["可以在更有限的计算资源下完成复杂任务",{"2":{"474":1}}],["可以在解码阶段捕获当前词与已经解码的词之间的关联",{"2":{"443":1}}],["可以在一定程度上达到这种稀疏性理论分析效果",{"2":{"393":1}}],["可以确保最常见的词在token列表中表示为单个token",{"2":{"575":1}}],["可以确保解码器本次预测是基于正确基础上进行",{"2":{"407":1}}],["可以确定属性提取的三步如下",{"2":{"122":1}}],["可以保证对每次推理的监督训练都是从正确的输入出发",{"2":{"406":1}}],["可以保证无论维度dkdkd",{"2":{"187":1}}],["可以限制参数在更新过程中的取值范围",{"2":{"393":1}}],["可以避免许多常见的输入输出错误",{"2":{"1810":1,"1828":1}}],["可以避免定义结构体或类",{"2":{"1805":1}}],["可以避免不必要的内存拷贝",{"2":{"1729":1}}],["可以避免手动内存管理带来的问题",{"2":{"1713":1}}],["可以避免",{"2":{"346":1}}],["可以使这些函数或类可以访问当前类的私有属性和方法",{"2":{"1769":1}}],["可以使代码结构更清晰",{"2":{"1729":1}}],["可以使用以下成员函数检查这些状态",{"2":{"1814":1,"1832":1}}],["可以使用智能指针等技术来自动管理资源",{"2":{"1764":1}}],["可以使用联合体来存储不同类型的消息内容",{"2":{"1728":1}}],["可以使用联合体来查看同一块内存的不同类型解释",{"2":{"1728":1}}],["可以使用联合体来节省内存",{"2":{"1728":1}}],["可以使用迭代器或键访问并修改值",{"2":{"1725":1}}],["可以使用迭代器遍历",{"2":{"1719":1,"1720":1,"1721":1,"1722":1}}],["可以使用迭代器直接访问和修改元素",{"2":{"1719":1,"1720":1,"1721":1,"1722":1}}],["可以使用相同的函数名执行相似但针对不同数据类型的操作",{"2":{"1707":1}}],["可以使用虚继承",{"2":{"1662":1}}],["可以使用构造函数初始化列表为成员变量賦值",{"2":{"1657":1}}],["可以使用初始化列表进行初始化",{"2":{"1634":1}}],["可以使用关系运算符",{"2":{"1633":1}}],["可以使用简单的",{"2":{"1476":1}}],["可以使用像",{"2":{"1116":1}}],["可以使用矩阵来表示多个向量",{"2":{"680":1}}],["可以使用更少的token表示更多的数据",{"2":{"563":1}}],["可以使用hebbian学习规则更新神经元之间的权重",{"2":{"489":1}}],["可以使用自适应学习率算法来根据参数梯度的统计信息来调整学习率",{"2":{"400":1}}],["可以使用",{"2":{"354":1,"709":1,"1607":1,"1608":1,"1632":1,"1638":2,"1642":1,"1668":1,"1673":1,"1825":2,"1843":2,"1906":1,"1986":1}}],["可以使用一个参数来调用的函数",{"2":{"344":1}}],["可以使得前向传播的输入分布变得稳定",{"2":{"320":1}}],["可以认为这是一种one",{"2":{"456":1}}],["可以认为卷积提取的特征是一种非结构化的特征或者向量",{"2":{"338":1}}],["可以认为是句子中最大包含多少单词",{"2":{"23":1}}],["可以分为prelayernorm和postlayernorm",{"2":{"329":1}}],["可以用以下的贪心策略",{"2":{"2151":1}}],["可以用来生成索引序列",{"2":{"1912":1}}],["可以用",{"2":{"1083":1,"1820":1,"1838":1}}],["可以用于遍历字符串中的字符",{"2":{"1713":1}}],["可以用于传递数据",{"2":{"1706":1}}],["可以用于之后的分类任务",{"2":{"722":1}}],["可以用于计算点之间的关系",{"2":{"689":1}}],["可以用一个参数来调用",{"2":{"344":1}}],["可以用到很多任务上",{"2":{"280":1}}],["可以用0",{"2":{"170":1}}],["可以通过add",{"2":{"1979":1}}],["可以通过提供推导指引来定制推导过程",{"2":{"1925":1}}],["可以通过键值查找对应的值",{"2":{"1807":1}}],["可以通过下标快速访问任意元素",{"2":{"1800":1}}],["可以通过下标直接访问元素",{"2":{"1797":1}}],["可以通过继承",{"2":{"1762":1}}],["可以通过解引用操作符",{"2":{"1650":1}}],["可以通过类名直接调用",{"2":{"1649":1}}],["可以通过类名和作用域解析符",{"2":{"1639":2}}],["可以通过给定的名称将缓冲区作为属性进行访问",{"2":{"1211":1}}],["可以通过引入非线性变换",{"2":{"838":1}}],["可以通过计算高维向量之间的距离来表示两个单词含义的相近程度",{"2":{"687":1}}],["可以通过分析数据集的特点",{"2":{"560":1}}],["可以通过",{"2":{"502":1,"1113":1,"1614":1,"1772":1,"1911":1}}],["可以通过拓展隐状态的长度",{"2":{"256":1}}],["可以通过对mhsa的隐藏状态",{"2":{"144":1}}],["可以明显看出",{"2":{"217":1}}],["可以提前优化模型的计算图",{"2":{"1288":1}}],["可以提升其在ner任务上的效果",{"2":{"761":1}}],["可以提高程序的健壮性和可维护性",{"2":{"1765":1}}],["可以提高复杂的大型语言模型",{"2":{"985":1}}],["可以提高llm在下游任务中的表现",{"2":{"221":1}}],["可以提高训练和推理的速度",{"2":{"101":1}}],["可以提高训练的稳定性",{"2":{"8":1}}],["可以提出来约掉",{"2":{"191":1}}],["可以参见论文脚注",{"2":{"187":1}}],["可以参考",{"2":{"90":1}}],["可以表示如下",{"2":{"1342":1,"1343":1}}],["可以表示成下面的式子",{"2":{"1342":1}}],["可以表示一个二维位置",{"2":{"680":1}}],["可以表示为下图",{"2":{"184":1}}],["可以表示遵循该模式的token分布",{"2":{"125":1}}],["可以理解为给定句子y",{"2":{"908":1}}],["可以理解为",{"2":{"473":1}}],["可以理解为某个单词的特征",{"2":{"265":1}}],["可以理解为某个单词像其它单词发出询问",{"2":{"265":1}}],["可以理解为输出y是在value之间根据key",{"2":{"173":1,"271":1}}],["可以理解为输出是在value之间根据key",{"2":{"168":1}}],["可以理解为n个head一起并行计算",{"2":{"36":1}}],["可以推断",{"2":{"167":1}}],["可以说",{"2":{"162":1}}],["可以控制该函数在不同区间形状不同",{"2":{"155":1}}],["可以擦除部分知识",{"2":{"143":1}}],["可以有多个模板参数",{"2":{"1699":1}}],["可以有多个重载的构造函数",{"2":{"1675":1}}],["可以有三级",{"2":{"1611":1}}],["可以有效地平衡词典大小和编码步骤数",{"2":{"594":1}}],["可以有效地提取出数据的关键特征",{"2":{"137":1}}],["可以有效缓解这个问题",{"2":{"402":1}}],["可以有效缓解这些这些问题",{"2":{"296":1}}],["可以有不同形式和定义",{"2":{"3":1}}],["可以被程序中的任何函数访问",{"2":{"1649":1}}],["可以被",{"2":{"1607":1}}],["可以被看作是在一个多维空间中的一种方向",{"2":{"137":1}}],["可以被认为是",{"2":{"115":1}}],["可以将其分解成以下模块",{"2":{"1916":1}}],["可以将",{"2":{"1810":1,"1828":1}}],["可以将复杂的程序逻辑分解成更小的",{"2":{"1729":1}}],["可以将函数作为参数传递给其他函数",{"2":{"1706":1}}],["可以将数组名赋值给相同数据类型的指针变量",{"2":{"1704":1}}],["可以将数组想象成一系列连续的存储单元",{"2":{"1623":1}}],["可以将任意两种相同类型的数值相加",{"2":{"1699":1}}],["可以将计算的过程分解成小部分",{"2":{"1564":1}}],["可以将块视为页面",{"2":{"982":1}}],["可以将这部分代码封装成一个函数",{"2":{"1729":1}}],["可以将这些丰富的语言表示能力引入到模型中",{"2":{"898":1}}],["可以将这个变化比喻成模型应用了",{"2":{"122":1}}],["可以将这个过程比作模型形成了类似于",{"2":{"122":1}}],["可以将这个过程想象为一个时间线",{"2":{"59":1}}],["可以将直接映射成可学习的标量",{"2":{"762":1}}],["可以将位置信息渗透进特征向量",{"2":{"746":1}}],["可以将自然语言",{"2":{"676":1}}],["可以将扩散lcm建模视为自动回归模型",{"2":{"633":1}}],["可以将相加后得到的向量拉回到原来向量的附近",{"2":{"326":1}}],["可以将记忆更新如下图标号1",{"2":{"230":1}}],["可以将svd看作是一名外科医生",{"2":{"221":1}}],["可以将传统的二次计算复杂度转化为线性复杂度",{"2":{"210":1}}],["可以将ϵϵ",{"2":{"93":1}}],["可以尽可能降低计算负载不均衡问题",{"2":{"90":1}}],["可以减少不必要的计算",{"2":{"1288":1}}],["可以减少",{"2":{"90":1}}],["可以包含需要传递的任何信息",{"2":{"1563":1}}],["可以包含",{"2":{"89":1}}],["可以互相看到",{"2":{"81":1}}],["可以",{"2":{"74":1}}],["可以把引用想象成一个人的",{"2":{"1612":1}}],["可以把相对位置编码分为以下两种流派",{"2":{"766":1}}],["可以把独热向量看作是word",{"2":{"694":1}}],["可以把",{"2":{"294":1}}],["可以把隐藏状态ht看作是信息循环的载体",{"2":{"249":1}}],["可以把高维数据表示为字典中元素的线性组合",{"2":{"137":1}}],["可以把上三角的值赋值为负无穷",{"2":{"70":1}}],["可以把dcmha近似理解为",{"2":{"43":1}}],["可以上三角的值全为",{"2":{"70":1}}],["可以让代码更专注于变量的含义",{"2":{"1615":1}}],["可以让代码更简洁",{"2":{"1615":1}}],["可以让我们更直观地理解neural",{"2":{"496":1}}],["可以让编码器同时考虑输入序列中每个单词的前后上下文信息",{"2":{"278":1}}],["可以让序列中每个词都和序列中其他词关联起来",{"2":{"261":1}}],["可以让两个序列互相交换信息",{"2":{"261":1}}],["可以让训练更快收敛",{"2":{"57":1}}],["可以让大家对多头自注意力有直观的感受",{"2":{"5":1}}],["可以看做一台服务器或者pc机",{"2":{"1563":1}}],["可以看作从概率密度函数所在的函数空间到实数域r的一个函数f",{"2":{"1377":1}}],["可以看作是",{"2":{"1603":1}}],["可以看作是bpe的变种",{"2":{"597":1}}],["可以看作是从一个rdrd",{"2":{"499":1}}],["可以看作是在学习一组基函数",{"2":{"116":1}}],["可以看作是由每个输入位置",{"2":{"99":1,"419":1}}],["可以看下源码对应的计算",{"2":{"1003":1}}],["可以看成是wordpiece算法在执行过程中进行反向操作",{"2":{"601":1}}],["可以看到随着n的增大",{"2":{"1336":1}}],["可以看到两个文件",{"2":{"370":1}}],["可以看到其主要分为两步",{"2":{"364":1}}],["可以看到",{"2":{"35":1,"85":1,"150":1,"170":1,"284":1,"344":1,"407":1,"449":1,"559":1,"702":1,"1344":1}}],["可以看出来是如何处理独热编码的",{"2":{"700":1}}],["可以看出来相互操作",{"2":{"169":1}}],["可以看出",{"2":{"18":1,"170":1,"582":1}}],["可以发现",{"2":{"20":1}}],["把几何图元转换为像素点以显示在屏幕上",{"2":{"2009":1}}],["把几万个单词嵌入到这个空间里绰绰有余",{"2":{"698":1}}],["把最后一层看作线性预测器",{"2":{"1464":1}}],["把最终结果返回",{"2":{"1250":1}}],["把最高值去掉一些",{"2":{"399":1}}],["把parameters初始化为0是不可以的",{"2":{"992":1}}],["把所有的输入序列都编码成一个统一的语义向量context",{"2":{"886":1}}],["把绝对位置编码替换为相对位置编码",{"2":{"760":1}}],["把相对位置信息i",{"2":{"759":1}}],["把相对位置信息植入到transformer架构的每一层的注意力机制中",{"2":{"757":1}}],["把可训练的潜在数组",{"2":{"735":1}}],["把decoder",{"2":{"735":1}}],["把它前后n个单词叫做它的上下文",{"2":{"713":1}}],["把语料中所有词的集合构建成一个词典",{"2":{"681":1}}],["把计算统一为各种不同的",{"2":{"624":1}}],["把字节变成整型数",{"2":{"592":1}}],["把词表中的w进行合并",{"2":{"582":1}}],["把词嵌入拆分成h组q",{"2":{"23":1}}],["把数据压缩成",{"2":{"575":1}}],["把文本拆分成单词",{"2":{"553":1}}],["把预分词器得到的单词送进分词模型或者依据词汇表进行分词",{"2":{"554":1}}],["把预分词器得到的单词送进分词模型进行分词",{"2":{"545":1}}],["把预测出的第一个单词",{"2":{"445":1}}],["把序号2看作是k和v",{"2":{"537":1}}],["把序号1看作是q",{"2":{"537":1}}],["把sublayerconnection的实现方式叫做pre",{"2":{"523":1}}],["把src中非pad的部分置为true",{"2":{"66":1,"382":1}}],["把处理输入部分也涵盖进来",{"2":{"519":1}}],["把input",{"2":{"455":1,"698":1}}],["把copy",{"2":{"449":1}}],["把输入设置为自然语言句子",{"2":{"445":1,"454":1}}],["把中文",{"2":{"445":1}}],["把堆叠的结果称为编码器或解码器",{"2":{"436":1}}],["把用户的输入文本",{"2":{"431":1}}],["把列表堆叠在一起",{"2":{"384":2}}],["把接收size",{"2":{"344":1}}],["把一个单词嵌入到一个高维欧式空间中",{"2":{"714":1}}],["把一句话按照一定规则来分成一个个词",{"2":{"363":1}}],["把一本",{"2":{"340":1}}],["把一页书中所有字加起来",{"2":{"340":1}}],["把一部分不重要的复杂信息过滤掉",{"2":{"310":1}}],["把子层的输入记作x",{"2":{"300":1}}],["把注意力机制用到了文本表示学习中",{"2":{"289":1}}],["把感知域",{"2":{"274":1}}],["把历史上下文存储到一个隐藏状态",{"2":{"273":1}}],["把解码器前一次的输出",{"2":{"267":1}}],["把目标系列认为是query",{"2":{"267":1}}],["把长序列的信息压缩到一个卷积窗口内部",{"2":{"247":1}}],["把隐状态看成对输入信息的一种编码的话",{"2":{"241":1}}],["把ffn从",{"2":{"232":1}}],["把靠底层的layer换成了local",{"2":{"210":1}}],["把源序列转换为源隐状态",{"2":{"200":1}}],["把得分变成概率",{"2":{"200":3}}],["把l个qiqiq",{"2":{"161":1}}],["把重叠的部分当作锚点",{"2":{"143":1}}],["把每个",{"2":{"674":1}}],["把每个token对应到一个input",{"2":{"698":1}}],["把每个token",{"2":{"455":1}}],["把每个token的上下文信息加工成最终需要的的语义空间向量",{"2":{"101":1}}],["把每一个单词的向量维度从词典大小降维到较小的维度",{"2":{"682":1}}],["把每一本书的所有字加起来",{"2":{"340":1}}],["把每一层的中间输出解码",{"2":{"130":1}}],["把这些位置的值加上一个非常大的负数",{"2":{"651":1,"933":1}}],["把这些书按页码一一对应地加起来",{"2":{"340":1}}],["把这些hthth",{"2":{"267":1}}],["把这些样本并行输入到模型",{"2":{"57":1}}],["把这个矩阵作用在每一个序列上",{"2":{"70":1,"934":1}}],["把对应的注意力强制置零",{"2":{"70":1,"409":1}}],["把整个句子",{"2":{"58":1}}],["把多余的直接舍弃",{"2":{"376":1,"933":1}}],["把多余的单词直接舍弃",{"2":{"53":1}}],["把多个头的输出拼接起来",{"2":{"36":1}}],["把8个head的64维向量拼接成一个512的向量",{"2":{"36":1}}],["把投影输出拆分成多头",{"2":{"36":1}}],["把分拆最后一个维度到",{"2":{"29":1}}],["把",{"2":{"10":1,"59":1,"69":1,"122":1,"334":1,"407":1,"934":1}}],["综述分享",{"2":{"740":1}}],["综述",{"2":{"292":1}}],["综述和观点",{"2":{"156":1}}],["综合标号5和6",{"2":{"498":1}}],["综合在一起就是用提纲挈领",{"2":{"276":1}}],["综合三种注意力之后",{"2":{"204":1}}],["综合他们八个人的意见",{"2":{"13":1}}],["综合多个头可以让模型就能够更全面地理解输入数据",{"2":{"9":1}}],["综上所述",{"2":{"101":1,"276":1,"279":1,"495":1,"561":1,"838":1,"998":1,"1139":1}}],["综上",{"2":{"10":1}}],["自研引擎架构优化",{"2":{"1936":1}}],["自减",{"2":{"1630":3}}],["自增",{"2":{"1630":1}}],["自增自减运算符重载",{"2":{"1712":1}}],["自增自减运算符",{"2":{"1630":1}}],["自己想想自己的生活哈哈哈",{"2":{"2140":1}}],["自己吓唬自己吗",{"2":{"2056":1}}],["自己提前查阅相关环境安装资料进行环境配置",{"2":{"1561":1}}],["自己run一遍",{"2":{"668":1}}],["自变量是q的密度函数",{"2":{"1377":1}}],["自带的",{"0":{"1201":1},"1":{"1202":1,"1203":1}}],["自定义目标",{"0":{"1994":1}}],["自定义一个立方体类",{"2":{"1791":1}}],["自定义二维向量类",{"2":{"1788":1}}],["自定义立方体类",{"0":{"1791":1,"1792":1},"2":{"1768":2}}],["自定义数据类型的魅力",{"0":{"1728":1}}],["自定义数据类型与函数",{"0":{"1727":1},"1":{"1728":1,"1729":1}}],["自定义拷贝构造函数",{"2":{"1694":1}}],["自定义类型名",{"2":{"1700":1}}],["自定义类型转换",{"0":{"1685":1}}],["自定义类",{"0":{"1653":1},"2":{"1797":1}}],["自定义异常类型的好处是什么",{"2":{"1763":1}}],["自定义异常类型通常通过公有继承",{"2":{"1763":1}}],["自定义异常类型",{"0":{"1763":1},"2":{"1762":1,"1763":1}}],["自定义异常",{"2":{"1491":1}}],["自定义操作",{"2":{"1213":1}}],["自定义的模型应该继承自基类",{"2":{"1206":1}}],["自定义自己的反向传播函数",{"0":{"1100":1}}],["自定义函数都可以",{"2":{"1099":1}}],["自定义在反序列化过程中恢复对象状态的行为",{"2":{"1083":1}}],["自定义对象在序列化和反序列化过程中的行为",{"2":{"1083":1}}],["自定义对象在深拷贝",{"2":{"1083":1}}],["自一致性中的问题",{"2":{"985":1}}],["自注意机制运算过程",{"0":{"922":1}}],["自注意的思想",{"0":{"921":1}}],["自注意力会分析输入序列中token之间的关系",{"2":{"676":1}}],["自注意力组件允许输入token之间相互交互",{"2":{"620":1}}],["自注意力或ffn还是放在encoderlayer",{"2":{"523":1}}],["自注意力负责基于其全部输入向量来建模每个输出均可以借鉴的隐向量",{"2":{"515":1}}],["自注意力网络的输出会朝着一个rank",{"2":{"446":1}}],["自注意力将query",{"2":{"442":1}}],["自注意力的意思就是关注于序列内部关系的注意力机制",{"2":{"442":1}}],["自注意力的q",{"2":{"161":1}}],["自注意力块",{"0":{"355":1},"2":{"293":1}}],["自注意力可以注意到整个句子的所有单词",{"2":{"274":1}}],["自注意力层是一种和循环层和卷积层等效的计算单元",{"2":{"267":1}}],["自注意力是可以并行化计算的",{"2":{"160":1}}],["自注意力并非transformer首创",{"2":{"160":1}}],["自注意力",{"0":{"157":1,"418":1},"1":{"158":1,"159":1,"160":1,"161":1,"162":1,"163":1,"164":1,"165":1,"166":1,"167":1,"168":1,"169":1,"170":1,"171":1,"172":1,"173":1,"174":1,"175":1,"176":1,"177":1,"178":1,"179":1,"180":1,"181":1,"182":1,"183":1,"184":1,"185":1,"186":1,"187":1,"188":1,"189":1,"190":1,"191":1,"192":1,"193":1,"194":1,"195":1,"196":1,"197":1,"198":1,"199":1,"200":1,"201":1,"202":1,"203":1,"204":1,"205":1,"206":1,"207":1,"208":1,"209":1,"210":1,"211":1,"212":1,"213":1,"214":1,"215":1,"216":1,"217":1,"218":1,"219":1,"220":1,"221":1,"222":1,"223":1,"224":1,"225":1,"226":1,"227":1,"228":1,"229":1,"230":1,"231":1,"232":1,"233":1},"2":{"157":1}}],["自注意力动态可以拥有从1到满秩的任意秩的平衡点",{"2":{"94":1}}],["自注意力机制就是句子中每个单词看看其它单词对自己的影响力有多大",{"2":{"519":1}}],["自注意力机制只能提炼解构本序列的关联性特征",{"2":{"515":1}}],["自注意力机制本身具有二次复杂度",{"2":{"512":1}}],["自注意力机制和layer",{"2":{"499":1}}],["自注意力机制是互相作用的粒子系统中的非线性耦合机制",{"2":{"499":1}}],["自注意力机制是通过经验度量实现粒子之间的非线性耦合",{"2":{"499":1}}],["自注意力机制是llm架构中唯一计算序列中词元间关系的地方",{"2":{"462":1}}],["自注意力机制是一种动态的",{"2":{"170":1}}],["自注意力机制使模型能够专注于输入序列的相关部分",{"2":{"462":1}}],["自注意力机制使用一个列表",{"2":{"273":1}}],["自注意力机制使用的是tgt",{"2":{"82":1}}],["自注意力机制利用输入元素两两之间的相关性作为权重",{"2":{"418":1}}],["自注意力机制给这个计算机配置了类似消息传递的架构",{"2":{"294":1}}],["自注意力机制",{"2":{"287":1}}],["自注意力机制摒弃了",{"2":{"274":1}}],["自注意力机制可以增大信息含量",{"2":{"273":1}}],["自注意力机制的复杂度是o",{"2":{"511":1}}],["自注意力机制的输入是n",{"2":{"463":1}}],["自注意力机制的并行化",{"2":{"417":1}}],["自注意力机制的信息传播损失小",{"2":{"274":1}}],["自注意力机制的目的是为当前单词创建抽象的",{"2":{"261":1}}],["自注意力机制的计算量是非常大的",{"2":{"203":1}}],["自注意力机制的核心是重构词向量",{"2":{"166":1}}],["自注意力机制中",{"2":{"274":1}}],["自注意力机制中的key和value相当于注意力机制中的编码器隐藏状态",{"2":{"261":1}}],["自注意力机制中的query相当于注意力机制中的解码器隐藏状态",{"2":{"261":1}}],["自注意力机制中的qkv思想",{"2":{"164":1}}],["自注意力机制中这种q和k一问一答的形式",{"2":{"164":1}}],["自注意力机制中第一步就是用token来生成查询向量",{"2":{"162":1}}],["自注意力机制会接受计算查询",{"2":{"158":1}}],["自注意力机制会计算序列中每个向量与序列中其他向量的关系",{"2":{"158":1}}],["自注意力机制模块会接到全连接网络",{"2":{"10":1}}],["自身具有置换不变性",{"2":{"742":1}}],["自监督训练得到预训练模型",{"2":{"670":1}}],["自动ip分配的可以不用操作",{"0":{"2095":1}}],["自动处理依赖关系",{"2":{"1964":1}}],["自动处理内存分配和回收",{"2":{"1602":1}}],["自动驾驶系统",{"2":{"1941":1}}],["自动编译项目",{"2":{"1917":1}}],["自动类型推导",{"0":{"1877":1},"1":{"1878":1,"1879":1,"1880":1}}],["自动类型推断",{"0":{"1615":1}}],["自动类型推断等",{"2":{"1603":1}}],["自动去重和排序",{"2":{"1806":1}}],["自动排序",{"2":{"1806":1,"1807":1}}],["自动匹配最合适的重载版本",{"2":{"1707":1}}],["自动生成侧边栏",{"2":{"2043":1}}],["自动生成相应的函数实例",{"2":{"1699":1}}],["自动生成这些标准化的工具",{"2":{"1589":1}}],["自动进行模板实例化",{"2":{"1699":1}}],["自动释放内存",{"2":{"1695":2}}],["自动管理内存",{"2":{"1715":1}}],["自动管理",{"2":{"1648":1}}],["自动化推导规则",{"2":{"1917":1}}],["自动化脚本",{"2":{"1602":1}}],["自动化常用的绘图",{"0":{"1151":1}}],["自动内存管理",{"2":{"1602":1,"1713":1,"1714":1,"1892":1}}],["自动格式化选中的代码块",{"2":{"1560":1}}],["自动缩进",{"0":{"1559":1},"2":{"1559":1}}],["自动求导的缓冲区释放和重用使其非常高效",{"2":{"1123":1}}],["自动微分同时执行请求的计算",{"2":{"1113":1}}],["自动微分将这个图表示为一组函数对象",{"2":{"1113":1}}],["自动微分在执行操作",{"2":{"1113":1}}],["自动微分是一种反向自动微分系统",{"2":{"1113":1}}],["自动微分如何编码历史记录",{"0":{"1113":1}}],["自动微分机制",{"0":{"661":1,"1104":1,"1112":1},"1":{"1113":1,"1114":1,"1115":1,"1116":1,"1117":1,"1118":1,"1119":1,"1120":1,"1121":1,"1122":1,"1123":1}}],["自动转换到全精度",{"2":{"1086":1}}],["自动推导的",{"2":{"661":1,"1104":1}}],["自由度",{"2":{"305":1}}],["自回归语言模型而言",{"2":{"612":1}}],["自回归推理有两个问题",{"2":{"405":1}}],["自回归模式是",{"2":{"406":1}}],["自回归模式有几个弊端",{"2":{"239":1}}],["自回归模型本时刻的输入是上一时刻自己输出的值",{"2":{"407":1}}],["自回归模型在预测时会把过去观测到的信息总结起来记作hthth",{"2":{"240":1}}],["自回归模型的核心思想是利用一个变量的历史值来预测其未来的值",{"2":{"239":1}}],["自回归模型",{"0":{"239":1}}],["自回归",{"2":{"239":1}}],["自适应算法很难击败预算是其两倍的quasi",{"2":{"1175":1}}],["自适应黑盒优化算法可能会因为一些不幸的早期试验而忽略了搜索空间的中间部分",{"2":{"1175":1}}],["自适应学习率的概念",{"0":{"1041":1}}],["自适应的学习应该关注的重点位置",{"2":{"260":1}}],["自适应",{"2":{"222":1}}],["自适应模型提供了更灵活和高效的方法",{"2":{"222":1}}],["自适应性",{"0":{"220":1,"225":1},"2":{"157":2}}],["自然会明了其中的真正用意",{"2":{"2054":1}}],["自然会问有没有其他方法",{"2":{"1041":1}}],["自然会被忽略",{"2":{"260":1}}],["自然也就解决了多义词的问题了",{"2":{"717":1}}],["自然也会有所改变",{"2":{"194":1}}],["自然语言处理",{"2":{"376":1,"878":1,"906":1}}],["自然而然地识别出各种语言特征",{"2":{"9":1}}],["自主性提示",{"2":{"163":1}}],["自我纠正和道德适应的能力",{"2":{"140":1}}],["如屏幕",{"2":{"1815":1,"1833":1}}],["如键盘",{"2":{"1812":1,"1830":1}}],["如检查某个元素是否存在",{"2":{"1806":1}}],["如返回值",{"2":{"1764":1}}],["如文件",{"2":{"1810":1,"1828":1}}],["如文件不存在",{"2":{"1761":1}}],["如文本摘要",{"2":{"906":1}}],["如文本",{"2":{"250":1}}],["如除零错误",{"2":{"1761":1}}],["如查找",{"2":{"1734":1}}],["如链表",{"2":{"1728":1}}],["如追加",{"2":{"1713":1}}],["如种类和名字",{"2":{"1690":1}}],["如猫",{"2":{"1689":1}}],["如动态分配的内存",{"2":{"1676":1}}],["如智能指针",{"2":{"1603":1}}],["如mpi",{"2":{"1576":1}}],["如megabyte",{"2":{"613":1}}],["如广播",{"2":{"1573":1}}],["如天气预测",{"2":{"1569":1}}],["如消息传递接口mpi",{"2":{"1568":1}}],["如磁盘",{"2":{"1477":1}}],["如内存中的对象",{"2":{"1477":1}}],["如cpu",{"2":{"1411":1}}],["如数字",{"2":{"1563":1}}],["如数据竞争",{"2":{"1407":1}}],["如数学符号",{"2":{"613":1}}],["如论文中所述",{"2":{"1241":1}}],["如学习率和权重衰减",{"2":{"1227":1}}],["如共轭梯度",{"2":{"1223":1}}],["如跟踪计算图",{"2":{"1214":1}}],["如不用计算梯度之类的",{"2":{"1164":1}}],["如100个epoch评估一次",{"2":{"1162":1}}],["如减少磁盘空间占用",{"2":{"1161":1}}],["如无梯度模式和推断模式",{"2":{"1119":1}}],["如有必要",{"2":{"1108":1,"1146":1}}],["如有侵权",{"2":{"47":1,"95":1,"156":1,"233":1,"361":1,"387":1,"429":1,"513":1,"543":1,"638":1,"740":1,"768":1}}],["如神经网络",{"2":{"1014":1}}],["如线性回归和逻辑回归可以使用简单",{"2":{"1011":1}}],["如线性注意力和闪光注意力",{"2":{"152":1}}],["如并行采样和beam",{"2":{"983":1}}],["如hopper中的fp8和blackwell中的fp4",{"2":{"973":1}}],["如词性标注",{"2":{"906":1}}],["如关系抽取",{"2":{"906":1}}],["如人名",{"2":{"906":1}}],["如情感分类",{"2":{"906":1}}],["如机器翻译",{"2":{"906":1}}],["如机器翻译问题",{"2":{"891":1}}],["如机器翻译中",{"2":{"883":1}}],["如聊天机器人",{"2":{"885":1}}],["如输入一段文字判别它所属的类别",{"2":{"881":1}}],["如黑色和绿色区域",{"2":{"861":1}}],["如橙色区域",{"2":{"861":1}}],["如完形填空",{"2":{"850":1}}],["如一个图像",{"2":{"850":1}}],["如relu",{"2":{"838":1}}],["如reshape",{"2":{"785":1}}],["如boost",{"2":{"1961":1}}],["如bert和t5",{"2":{"729":1}}],["如bert和roberta",{"2":{"711":1}}],["如b是否是a的下文",{"2":{"722":1}}],["如潜在语义分析",{"2":{"711":1}}],["如语义文本相似性",{"2":{"711":1}}],["如同义词",{"2":{"709":1}}],["如同人类阅读一样",{"2":{"565":1}}],["如掩码语言模型或下一个词预测",{"2":{"709":1}}],["如自注意力层和前馈网络层",{"2":{"709":1}}],["如自注意力机制和前馈网络",{"2":{"674":1}}],["如产品推荐",{"2":{"696":1}}],["如距离",{"2":{"692":1}}],["如项目之间的相似性",{"2":{"691":1}}],["如欧几里得距离",{"2":{"689":1}}],["如句法和语义",{"2":{"689":1}}],["如单词或符号",{"2":{"674":1}}],["如jpeg图像压缩",{"2":{"637":1}}],["如归纳总结",{"2":{"629":1}}],["如领域",{"2":{"612":1}}],["如拼写检查",{"2":{"611":1}}],["如lstm和gru",{"2":{"711":1}}],["如llama",{"2":{"611":2}}],["如layer",{"2":{"294":1}}],["如日语和中文",{"2":{"606":1}}],["如上面",{"2":{"1712":1}}],["如上面的",{"2":{"1645":1}}],["如上所述",{"2":{"562":1,"754":1,"944":1,"963":1}}],["如上图中h0和x1分别有一个箭头连接",{"2":{"878":1}}],["如上图",{"2":{"775":1}}],["如上图右侧",{"2":{"635":1}}],["如上图左",{"2":{"634":1}}],["如上图标号2所示",{"2":{"739":1}}],["如上图标号3所示",{"2":{"739":1}}],["如上图标号3",{"2":{"145":1}}],["如上图标号1",{"2":{"145":1}}],["如上图所示",{"2":{"139":1,"860":1,"920":1}}],["如德语",{"2":{"560":1}}],["如空格和标点",{"2":{"553":1}}],["如对话",{"2":{"542":1}}],["如对象",{"2":{"505":1}}],["如等变性和对称性",{"2":{"505":1}}],["如计算机科学",{"2":{"505":1}}],["如分词",{"2":{"431":1}}],["如维基百科和bigquery",{"2":{"367":1}}],["如std",{"2":{"1914":1}}],["如self",{"2":{"344":1}}],["如softmax",{"2":{"327":1}}],["如图像处理的多步操作等",{"2":{"1578":1}}],["如图中10000步处展示的那样",{"2":{"1182":1}}],["如图3所示",{"2":{"623":1}}],["如图2所示",{"2":{"360":1}}],["如图概述softmax",{"2":{"327":1}}],["如图",{"2":{"320":4,"1611":1}}],["如图所示",{"2":{"41":1,"540":1,"888":1,"975":1}}],["如今已经向采用",{"2":{"311":1}}],["如基于",{"2":{"213":1}}],["如反向传播",{"2":{"155":1}}],["如下文所述",{"2":{"1143":1}}],["如下",{"2":{"191":1,"731":1}}],["如下图的架构",{"2":{"931":1}}],["如下图典型rnn",{"2":{"878":1}}],["如下图为膨胀率",{"2":{"778":1}}],["如下图为",{"2":{"776":1}}],["如下图",{"2":{"301":1,"631":1,"776":1}}],["如下图标号3",{"2":{"230":1}}],["如下图中右上角部分所示",{"2":{"209":1}}],["如下图右下角部分所示",{"2":{"209":1}}],["如下图所示",{"2":{"90":1,"144":1,"302":1,"311":1,"349":1,"393":1,"394":1,"419":1,"623":1,"632":1,"722":1,"759":1,"774":1,"860":1,"901":1,"926":1,"985":1,"995":1,"1337":1,"1465":1}}],["如下图左所示",{"2":{"89":1}}],["如下所示",{"2":{"18":1,"175":1,"943":1,"961":1}}],["如前文所述",{"2":{"57":1}}],["如前所述",{"2":{"29":1,"32":1}}],["如负无穷",{"2":{"50":1}}],["如",{"2":{"50":1,"62":1,"63":1,"130":1,"204":1,"212":1,"249":1,"314":1,"542":1,"555":1,"567":1,"584":1,"587":1,"692":1,"808":1,"1589":2,"1602":1,"1607":1,"1622":1,"1631":1,"1670":1,"1690":1,"1728":1,"1729":1,"1758":4,"1814":1,"1821":2,"1832":1,"1839":2,"1866":1,"1933":1}}],["如何安排时间让你完成尽可能多的任务",{"2":{"2133":1}}],["如何分配时间",{"0":{"2132":1},"1":{"2133":1,"2134":1,"2135":1}}],["如何用最少的硬币凑齐",{"2":{"2137":1}}],["如何用尽量少的钱去尽量多的地方",{"2":{"2129":1}}],["如何用moe进行embedding的获取",{"2":{"740":1}}],["如何组成最大的四位数",{"2":{"2123":1}}],["如何组合在一起",{"2":{"858":1}}],["如何理解ta呢",{"2":{"2054":1}}],["如何理解线性代数中的点积",{"2":{"233":1}}],["如何进行统一管理",{"2":{"2031":1}}],["如何进行保存张量的打包",{"2":{"1114":1}}],["如何确定下一个点是在右边还是右上角",{"2":{"2022":1}}],["如何创建你的代码模块",{"2":{"1729":1}}],["如何将函数指针作为参数传递",{"2":{"1645":1}}],["如何将模型安全的交付",{"2":{"1268":1}}],["如何使用该站点",{"2":{"2035":1}}],["如何使用",{"2":{"1729":1,"2062":1}}],["如何使用头文件",{"2":{"1628":1}}],["如何使用torch",{"0":{"1220":1},"1":{"1221":1,"1222":1,"1223":1}}],["如何判断一个年份是否为闰年",{"2":{"1619":1}}],["如何判断两个tensor",{"2":{"1079":1}}],["如何解决异或问题",{"0":{"1463":1}}],["如何解决这个问题呢",{"2":{"895":1}}],["如何解决这个问题并改进",{"2":{"891":1}}],["如何对学习率进行预热",{"0":{"1183":1}}],["如何计算它是一个悬而未决的问题",{"2":{"1133":1}}],["如何backward",{"0":{"1101":1}}],["如何找到两个向量间的相关性",{"2":{"921":1}}],["如何做到集二者之所长呢",{"2":{"767":1}}],["如何做注意力计算",{"2":{"262":1}}],["如何实现一个可拆分可解释的方向感知位置编码",{"2":{"761":1}}],["如何通过相对距离控制自注意力矩阵不同位置的偏置大小",{"2":{"758":1,"766":1}}],["如何通过句子中的其它词来推断",{"2":{"167":1}}],["如何刻画序列不同位置间的相对距离",{"2":{"758":1,"766":1}}],["如何获取序列顺序",{"2":{"757":1}}],["如何在transformer中加上相对位置信息",{"2":{"757":1}}],["如何在位置编码层面实现",{"2":{"746":1}}],["如何看待瘦身成功版bert",{"2":{"740":1}}],["如何快速提高大模型的向量表征效果能力",{"2":{"740":1}}],["如何更好地将生成模型预测下一个token的方式",{"2":{"736":1}}],["如何更好地multi",{"2":{"726":1}}],["如何构成",{"2":{"689":1}}],["如何选择",{"0":{"568":1}}],["如何设计一个兼顾通用且高效推理的tokenizer是非常重要的事情",{"2":{"563":1}}],["如何设置不当会失去统计意义",{"2":{"316":1}}],["如何调试和缓解优化失败",{"0":{"1178":1},"1":{"1179":1,"1180":1,"1181":1,"1182":1,"1183":1,"1184":1}}],["如何调用",{"0":{"451":1}}],["如何调整不同头之间的权重比例",{"2":{"10":1}}],["如何评价微软亚研院提出的把",{"2":{"361":1}}],["如何把编码器产生的信息更有效的传递给解码器的问题",{"2":{"284":1}}],["如何把大量的token压缩到一个隐藏状态中",{"2":{"256":1}}],["如何压缩",{"0":{"242":1},"2":{"242":1}}],["如何有效和高效地利用这些模型",{"2":{"1318":1}}],["如何有效且高效地将神经记忆融合到深度学习架构中",{"2":{"231":1}}],["如何有机融合",{"2":{"10":1}}],["如何才能对",{"2":{"167":1,"259":1}}],["如何降低方差",{"0":{"193":1},"2":{"157":1}}],["如何过滤",{"2":{"135":1}}],["如何应用于注意力",{"0":{"78":1},"2":{"49":1}}],["如果当前饼干能满足当前孩子",{"2":{"2153":1}}],["如果当前饼干可以满足孩子的胃口",{"2":{"2152":1}}],["如果当前的内存空间不足以容纳新的字符串",{"2":{"1713":1}}],["如果觉得枯燥无趣的话",{"2":{"2140":1}}],["如果总是先处理不紧急的任务",{"2":{"2134":1}}],["如果换a壳的话会花多少多少",{"2":{"2051":1}}],["如果已存在",{"2":{"1933":1}}],["如果lambda表达式满足constexpr函数的要求",{"2":{"1924":1}}],["如果用户不希望派生类公开基类的某些属性",{"2":{"1867":1}}],["如果文件存在",{"2":{"1820":2,"1838":2}}],["如果文件不存在",{"2":{"374":1}}],["如果到达文件末尾",{"2":{"1814":1,"1832":1}}],["如果流处于正常状态",{"2":{"1814":1,"1832":1}}],["如果类",{"2":{"1786":1}}],["如果类中有指针成员变量",{"2":{"1694":1}}],["如果类中没有显式定义构造函数",{"2":{"1675":1}}],["如果类中定义的成员被封装到python的普通数据类型中",{"2":{"1210":1}}],["如果取消注释",{"2":{"1784":1}}],["如果取款金额超过余额",{"2":{"1766":1}}],["如果滥用友元",{"2":{"1776":1}}],["如果存款金额为负数",{"2":{"1766":1}}],["如果存在多个值",{"2":{"1115":1}}],["如果存在",{"2":{"1025":1}}],["如果析构函数中可能发生错误",{"2":{"1764":1}}],["如果注释掉",{"2":{"1762":1}}],["如果注意力机制时用于分类模型",{"2":{"267":1}}],["如果沿着调用栈一直找不到匹配的",{"2":{"1762":1}}],["如果找到匹配的",{"2":{"1762":1}}],["如果找不到",{"2":{"1628":1}}],["如果被调用的函数没有参数",{"2":{"1729":1}}],["如果返回值类型不是",{"2":{"1729":1}}],["如果返回了一个",{"2":{"1227":1}}],["如果代码被组织成多个函数",{"2":{"1729":1}}],["如果程序中有多处需要执行相同的操作",{"2":{"1729":1}}],["如果未找到则返回",{"2":{"1724":1,"1725":1}}],["如果未提供步数",{"2":{"402":1}}],["如果转换不成功会返回",{"2":{"1683":1}}],["如果管理不当容易出现内存泄漏等问题",{"2":{"1648":1}}],["如果元素是对象的话",{"2":{"1647":1}}],["如果递归深度过大",{"2":{"1646":1}}],["如果部分初始化",{"2":{"1634":1}}],["如果函数有多个",{"2":{"1905":1}}],["如果函数有返回值类型",{"2":{"1631":1}}],["如果函数不需要接收任何参数",{"2":{"1729":1}}],["如果函数不接受任何参数",{"2":{"1607":1}}],["如果函数不返回任何值",{"2":{"1607":1,"1729":1}}],["如果初始化时提供了所有元素的初始值",{"2":{"1623":1}}],["如果初始化时提供的元素少于数组的大小",{"2":{"1623":1}}],["如果初始时所有token对的余弦相似度都是非负的",{"2":{"94":1}}],["如果为假",{"2":{"1621":1}}],["如果为真",{"2":{"1621":1}}],["如果条件为假",{"2":{"1620":1,"1621":1}}],["如果条件为真",{"2":{"1620":1,"1621":1,"1630":1}}],["如果参数比较少",{"2":{"1485":1}}],["如果参数过多",{"2":{"1485":1}}],["如果传播的误差来自多个神经元",{"2":{"1443":1}}],["如果传递了start",{"2":{"828":1}}],["如果生成过程中在坐标中取的点接近蓝色区域",{"2":{"1373":1}}],["如果保留了原有数据的主要信息",{"2":{"1370":1}}],["如果数据维度的输入",{"2":{"1370":1}}],["如果strict为true",{"2":{"1214":1}}],["如果special",{"2":{"557":1}}],["如果派生于",{"2":{"1210":2}}],["如果针对每个batch",{"2":{"1186":1}}],["如果梯度范数",{"2":{"1184":1}}],["如果梯度过小",{"2":{"333":1}}],["如果稳定训练需要较长的warmup",{"2":{"1183":1}}],["如果该图展示的训练损失在稳步下降后突然上升",{"2":{"1182":1}}],["如果该函数不是确定性映射",{"2":{"1115":1}}],["如果该函数未定义",{"2":{"1115":1}}],["如果该函数被定义",{"2":{"1115":1}}],["如果该函数是凹函数",{"2":{"1115":1}}],["如果该函数是凸函数",{"2":{"1115":1}}],["如果该函数可微分",{"2":{"1115":1}}],["如果怀疑模型受到早期训练不稳定的影响",{"2":{"1179":1}}],["如果基于低差异序列的quasi",{"2":{"1176":1}}],["如果quasi",{"2":{"1175":1}}],["如果q和k使用相同的权重矩阵",{"2":{"172":1}}],["如果优化工具作为服务运行",{"2":{"1175":1}}],["如果使用",{"2":{"1647":1}}],["如果使用复杂的贝叶斯优化软件",{"2":{"1175":1}}],["如果使用掩码",{"2":{"199":1}}],["如果试验次数在25次以上",{"2":{"1174":1}}],["如果试验次数在10到25次之间",{"2":{"1174":1}}],["如果这种类型的验证错误敏感计划可以完全自动化",{"2":{"1173":1}}],["如果这个时候继续以这样的方式更新参数",{"2":{"1036":1}}],["如果这个情况发生了",{"2":{"840":1}}],["如果发生不可恢复的错误",{"2":{"1814":1,"1832":1}}],["如果发生可恢复的错误",{"2":{"1814":1,"1832":1}}],["如果发生这种情况",{"2":{"1173":1}}],["如果发现问题",{"2":{"1146":1}}],["如果发现mask是0",{"2":{"67":1}}],["如果训练误差",{"2":{"1157":1}}],["如果训练过程以某种方式改进",{"2":{"1155":1}}],["如果训练过程中",{"2":{"1155":1}}],["如果训练吞吐量到某个batch",{"2":{"1132":1}}],["如果搜索空间包含大量发散点",{"2":{"1153":1}}],["如果任何最佳试验出现过拟合问题",{"2":{"1149":1}}],["如果所有试验对于大于某个阈值的学习率都是不可行的",{"2":{"1147":1}}],["如果所有令牌都从其中一个半球开始",{"2":{"507":1}}],["如果最好的step总是出现在训练过程的最后的25",{"2":{"1155":1}}],["如果最佳step总是出现在训练过程的前10",{"2":{"1155":1}}],["如果最佳点聚集在搜索空间的边缘",{"2":{"1147":1}}],["如果最终增加了训练时间",{"2":{"1133":1}}],["如果研究的最佳点在一维或多维搜索空间的边界附近",{"2":{"1146":1}}],["如果一项研究中的所有学习率都太小",{"2":{"1156":1}}],["如果一开始选择了一个不必要的大训练步数",{"2":{"1137":1}}],["如果一个参数有默认值",{"2":{"1708":1}}],["如果一个模型在训练时只使用了",{"2":{"1341":1}}],["如果一个新的超参数点",{"2":{"1152":1}}],["如果一个节点的引用计数器为零",{"2":{"986":1}}],["如果一个函数满足输入改变",{"2":{"772":1}}],["如果一个问题和我熟知的物理毫无联系",{"2":{"506":1}}],["如果一个系统具备条件分支",{"2":{"504":1}}],["如果一个",{"2":{"89":1,"1924":1}}],["如果大batch",{"2":{"1134":1}}],["如果大于max",{"2":{"65":1}}],["如果增加batch",{"2":{"1134":1}}],["如果增加",{"2":{"1134":1}}],["如果与上述情况不符",{"2":{"1132":1}}],["如果与接近",{"2":{"867":1}}],["如果batch",{"2":{"1132":1}}],["如果您已经签署过一次",{"2":{"1197":1}}],["如果您以后需要切换到不同的batch",{"2":{"1135":1}}],["如果您的模型依赖于诸如",{"2":{"1122":1}}],["如果您无法避免在您的情况下使用这样的张量",{"2":{"1121":1}}],["如果它能够适用于您的用例",{"2":{"1121":1}}],["如果它觉得某个头重要",{"2":{"10":1}}],["如果在析构函数中抛出异常且未被捕获",{"2":{"1764":1}}],["如果在蓝色和黑色交界处",{"2":{"1373":1}}],["如果在连续",{"2":{"1245":1}}],["如果在优化模型时遇到困难",{"2":{"1178":1}}],["如果在一次研究中",{"2":{"1174":1}}],["如果在模型中引入数据增强或dropout等正则化方法",{"2":{"1155":1}}],["如果在启用推断模式后遇到错误",{"2":{"1121":1}}],["如果在需要梯度的张量上在",{"2":{"1115":1}}],["如果在nlp领域",{"2":{"323":1}}],["如果权值的初始值过大",{"2":{"991":2}}],["如果权重由标准正态分布初始化",{"2":{"403":1}}],["如果权重初始化过大",{"2":{"403":1}}],["如果计算时间超过了传输key",{"2":{"975":1}}],["如果学习过crf或者hmm",{"2":{"908":1}}],["如果学习率大于",{"2":{"1179":1}}],["如果学习率过小",{"2":{"1157":1}}],["如果学习率过低",{"2":{"400":1}}],["如果学习率设置得过高",{"2":{"400":1}}],["如果等每一个分支都遇到end才停的话",{"2":{"904":1}}],["如果可以使用一条线将数据集分为两个类别",{"2":{"1462":1}}],["如果可以使用某种手段直接将中间层的结果求解出来",{"2":{"496":1}}],["如果可以运行的试验次数大于25次",{"2":{"1174":1}}],["如果可以完美地拟合整个训练集",{"2":{"1156":1}}],["如果可以的话",{"2":{"1129":1}}],["如果可能的话",{"2":{"819":1}}],["如果从搜索空间中采样的最佳点靠近其边界",{"2":{"1147":1}}],["如果从xlnet论文公式角度看是替换掉",{"2":{"760":1}}],["如果从集成学习角度来看",{"2":{"393":1}}],["如果上下文不同",{"2":{"715":1}}],["如果监督数据较少",{"2":{"706":1}}],["如果设置了该值",{"2":{"702":1}}],["如果设置了8个头",{"2":{"173":1}}],["如果让你设计一个工程的上数据结构来表示tensor",{"2":{"1077":1}}],["如果让",{"2":{"698":1}}],["如果两者相等",{"2":{"698":1}}],["如果两个向量的夹角为90度",{"2":{"176":1,"692":1}}],["如果直接使用余弦相似度",{"2":{"692":1}}],["如果直接对原始的embedding做自注意力操作",{"2":{"172":1}}],["如果维度过高",{"2":{"684":1}}],["如果下一个字节的熵大",{"2":{"613":1}}],["如果仅预测代码中的空白字符就可能会浪费一个transformer步骤",{"2":{"613":1}}],["如果能对不同的subword进行训练的话",{"2":{"595":1}}],["如果相邻子词间没有中止符",{"2":{"588":1}}],["如果遍历完字典后",{"2":{"587":1}}],["如果句子中仍然有子字符串没被替换但所有subword都已迭代完毕",{"2":{"584":1}}],["如果句子较短",{"2":{"453":1}}],["如果算法看到token",{"2":{"579":1}}],["如果词嵌入维度很大时会导致点积的结果也很大",{"2":{"701":1}}],["如果词表中可以查到对应的token",{"2":{"563":1}}],["如果词汇表只包括",{"2":{"560":1}}],["如果特定领域包含大量专业术语和行话",{"2":{"560":1}}],["如果index为1",{"2":{"557":1}}],["如果index为0",{"2":{"557":1}}],["如果你看见代码部分比较头疼",{"2":{"2146":1}}],["如果你想获取相关通知",{"2":{"1196":1}}],["如果你喜欢这本手册",{"2":{"1196":1}}],["如果你要用gpu训练模型",{"2":{"796":1}}],["如果你有两个相互竞争的观点来解释同一现象",{"2":{"542":1}}],["如果你不能用数学的语言去描述大脑",{"2":{"506":1}}],["如果想模仿原论文的行为",{"2":{"1242":1}}],["如果想了解两个单词或句子在语义上的接近程度",{"2":{"692":1}}],["如果想把玩具组装好",{"2":{"524":1}}],["如果想从根本上解决问题",{"2":{"256":1}}],["如果考虑到限制最大长度",{"2":{"520":1,"530":1}}],["如果考虑前面n个单词",{"2":{"242":1}}],["如果逐字翻译",{"2":{"516":1}}],["如果将batch",{"2":{"1134":1}}],["如果将深度学习模型视为范畴",{"2":{"505":1}}],["如果将token",{"2":{"446":1}}],["如果第二步预测之后",{"2":{"407":1}}],["如果编码器在某一轮预测错了",{"2":{"405":1}}],["如果前面层没有学习好",{"2":{"401":1}}],["如果达到了accum",{"2":{"385":1}}],["如果小于max",{"2":{"384":2}}],["如果此函数在执行过程中没有找到可以加载的模型",{"2":{"372":1}}],["如果哪位读者知道出处",{"2":{"339":1}}],["如果处理不当",{"2":{"327":1}}],["如果每个椭圆离得特别远会发生什么",{"2":{"1374":1}}],["如果每层的输出都加上一个x的时候",{"2":{"304":1}}],["如果每一层的梯度都稍微增大一点",{"2":{"296":1}}],["如果恒等映射是最优的",{"2":{"301":1}}],["如果20层的浅层网络的性能已经足够好",{"2":{"299":1}}],["如果模型中尚未包含残差连接和归一化",{"2":{"1180":1}}],["如果模型具有较大的容量",{"2":{"1012":1}}],["如果模型超过这个层数",{"2":{"296":1}}],["如果模型无法处理长距离依赖",{"2":{"246":1}}],["如果对于前面的介绍你还是不能够理解到",{"2":{"2121":1}}],["如果对原图进行dropout正则化",{"2":{"1019":1}}],["如果对后两个维度做标准化",{"2":{"326":1}}],["如果对应到seq2seq",{"2":{"268":1}}],["如果对全连接层以及",{"2":{"8":1}}],["如果非要将自注意力机制和之前的注意力机制进行对比的话",{"2":{"261":1}}],["如果关键信息出现在序列头部",{"2":{"253":1}}],["如果几百字的散文",{"2":{"252":1}}],["如果n数目太大",{"2":{"242":1}}],["如果有效",{"2":{"1695":1}}],["如果有任何不正确的地方",{"2":{"1196":1}}],["如果有无限的计算资源",{"2":{"1143":1}}],["如果有限定max",{"2":{"557":1}}],["如果有遗漏的精品文献",{"2":{"235":1}}],["如果有多个key都与query高度相似",{"2":{"168":1}}],["如果理解有误",{"2":{"235":1}}],["如果原作者发现",{"2":{"235":1}}],["如果把",{"2":{"340":1}}],["如果把token看作是instruction",{"2":{"231":1}}],["如果把llama的结构中gatedmlp换成标准的mlp",{"2":{"147":1}}],["如果提供了dropout",{"2":{"199":1}}],["如果掩码张量处为0",{"2":{"199":1}}],["如果熵为logn",{"2":{"194":1}}],["如果熵为0",{"2":{"194":1}}],["如果xixix",{"2":{"191":1}}],["如果只指定部分常量的值",{"2":{"1728":1}}],["如果只有一个attention",{"2":{"928":1}}],["如果只有一个值为",{"2":{"188":1}}],["如果只设置了一个头",{"2":{"173":1}}],["如果",{"2":{"188":1,"332":1,"1398":1,"1607":1,"1619":1,"1630":2,"1761":2,"1764":1,"1781":2,"1926":1,"1928":1,"2018":2}}],["如果不能满足",{"2":{"2152":1}}],["如果不为空则使用",{"2":{"1694":1}}],["如果不可能完美地拟合训练集",{"2":{"1158":1}}],["如果不是",{"2":{"1146":1}}],["如果不运行或者编译完整的训练程序",{"2":{"1132":1}}],["如果不进行warmup",{"2":{"401":1}}],["如果不选择这个维度",{"2":{"337":1}}],["如果不使用softmax",{"2":{"180":1}}],["如果不加残差和ffn",{"2":{"115":1}}],["如果没有抛出异常",{"2":{"1762":1}}],["如果没有为带有默认值的参数提供实参",{"2":{"1708":1}}],["如果没有找到",{"2":{"1698":1}}],["如果没有将基类函数声明为虚函数",{"2":{"1688":1}}],["如果没有定义",{"2":{"1685":1}}],["如果没有进行动态类型转换可能会导致未定义行为",{"2":{"1683":1}}],["如果没有合适的同步机制",{"2":{"1407":1}}],["如果没有贝叶斯优化和其他高级黑盒优化方法方面的专业知识",{"2":{"1175":1}}],["如果没有",{"2":{"1132":1}}],["如果没有预热阶段",{"2":{"333":1}}],["如果没有softmax的话",{"2":{"180":1}}],["如果没有softmax",{"2":{"180":2}}],["如果没有激活函数",{"2":{"102":1}}],["如果目标超参数是",{"2":{"1149":1}}],["如果目标超参数包括正则化参数",{"2":{"1149":1}}],["如果目标语言数据存在",{"2":{"380":1}}],["如果目标",{"2":{"145":1}}],["如果给定查询在之前的编辑范围内",{"2":{"143":1}}],["如果f",{"2":{"134":1,"301":1}}],["如果ffn变小",{"2":{"118":1}}],["如果值向量表示的是在词空间的分布",{"2":{"129":1}}],["如果其在训练数据中的频率是稀疏的",{"2":{"118":1}}],["如果调节过大",{"2":{"100":1}}],["如果中间比率调得过小",{"2":{"100":1}}],["如果输入seq",{"2":{"665":1}}],["如果输入数据的均值发生了变化",{"2":{"320":1}}],["如果输入是正数",{"2":{"99":1}}],["如果输入是负数",{"2":{"99":1}}],["如果输入维度是",{"2":{"99":1}}],["如果输入序列中的某个位置是填充词",{"2":{"63":1}}],["如果采用",{"2":{"89":1}}],["如果我们认为输入信息与位置信息应该是独立",{"2":{"1340":1}}],["如果我们正在编译",{"2":{"1227":1}}],["如果我们需要进行极其激进的梯度截断来处理我们的不稳定问题",{"2":{"1184":1}}],["如果我们发现自己使用了非常激进的截断",{"2":{"1184":1}}],["如果我们发现全损失梯度的",{"2":{"1179":1}}],["如果我们尝试梯度截断并且不稳定问题仍然存在",{"2":{"1184":1}}],["如果我们一开始的max",{"2":{"1183":1}}],["如果我们使用自适应随机搜索来调参",{"2":{"1175":1}}],["如果我们使用大小为2的窗口",{"2":{"714":1}}],["如果我们不关心分析时间",{"2":{"1159":1}}],["如果我们不问这些问题",{"2":{"1146":1}}],["如果我们能以某种方式延长训练时间或提高训练效率",{"2":{"1154":1}}],["如果我们提出正确的问题",{"2":{"1146":1}}],["如果我们决定根据这个实验将",{"2":{"1143":1}}],["如果我们的计算资源只允许少量试验并行运行",{"2":{"1175":1}}],["如果我们的实验目标涉及在两个或多个不同的优化器之间进行公平比较",{"2":{"1143":1}}],["如果我们的目标是确定是否要加入权重衰减",{"2":{"1150":1}}],["如果我们的目标是从",{"2":{"1144":1}}],["如果我们的目标是",{"2":{"1143":1}}],["如果我们要公平对比不同深度的模型",{"2":{"1143":1}}],["如果我们试图一次添加多个特征或回答多个问题",{"2":{"1141":1}}],["如果我们想最大化我们的最终效果",{"2":{"1140":1}}],["如果我们想限制",{"2":{"78":1}}],["如果我们相信方向敏感度在某种程度是轴对齐的",{"2":{"1041":1}}],["如果我们跟踪一些额外的统计信息",{"2":{"944":1,"963":1}}],["如果我们把",{"2":{"713":1}}],["如果我们把提示词看作是给注意力权重引入梯度",{"2":{"542":1}}],["如果我们希望某一个途径或者方法可以精确的表征人类概念",{"2":{"689":1}}],["如果我们在右侧进行填充",{"2":{"378":1}}],["如果我们将搜索范围朝着这个方向扩大",{"2":{"1147":1}}],["如果我们将子层的操作表示为f",{"2":{"300":1}}],["如果我们将每个特征视为一个神经元对应的向量",{"2":{"118":1}}],["如果我们从关系的角度来处理相同的知识",{"2":{"136":1}}],["如果我们去看",{"2":{"84":1}}],["如果我们有",{"2":{"34":1}}],["如果是计科或者编程相关人士",{"2":{"2146":1}}],["如果是奇数就+2",{"2":{"1634":1}}],["如果是偶数就+1",{"2":{"1634":1}}],["如果是线性计划",{"2":{"1159":1}}],["如果是这样",{"2":{"1149":3}}],["如果是自己写的算子",{"2":{"668":1}}],["如果是",{"2":{"587":1}}],["如果是rnn方案",{"2":{"267":1}}],["如果是训练过程",{"2":{"168":1}}],["如果是多头注意力",{"2":{"71":1}}],["如果是浮点矩阵",{"2":{"70":1}}],["如果是布尔矩阵",{"2":{"70":1}}],["如果是scaled",{"2":{"9":1}}],["如果processed",{"2":{"65":1,"384":2}}],["然后换衣服",{"2":{"2097":1}}],["然后重新启动就好啦",{"2":{"2094":1}}],["然后重复2~5步骤",{"2":{"519":1}}],["然后添加我们创建的这些管理员",{"2":{"2070":1}}],["然后查看manage",{"2":{"2070":1}}],["然后去可能的地方一个个找",{"2":{"2100":1}}],["然后去审视自己",{"2":{"2054":1}}],["然后去噪的方式来生成下一个句子的嵌入",{"2":{"634":1}}],["然后给了我们一个方案",{"2":{"2051":1}}],["然后给你",{"2":{"1729":1}}],["然后拿到电脑",{"2":{"2051":1}}],["然后带入直线的一般式方程",{"2":{"2022":1}}],["然后求和",{"2":{"1883":1}}],["然后编译成员函数",{"2":{"1653":1}}],["然后自增",{"2":{"1630":1}}],["然后返回修改后的值",{"2":{"1630":1}}],["然后链接在一起",{"2":{"1628":1}}],["然后根据用户的选择来决定是否继续",{"2":{"1620":1}}],["然后根据需求将其离线开发并增强到基础llm中",{"2":{"222":1}}],["然后声明一个指向",{"2":{"1611":1}}],["然后看看里面住着谁",{"2":{"1611":1}}],["然后看损失",{"2":{"410":1}}],["然后按",{"2":{"1559":1}}],["然后传播到每一层的隐藏单元",{"2":{"1438":1}}],["然后经过神经网络",{"2":{"1371":1}}],["然后经过softmax转换为词典的概率分布",{"2":{"147":1,"482":1}}],["然后训练一段时间后发现",{"2":{"1344":1}}],["然后论文中提出了一个满足上述关系的",{"2":{"1342":1}}],["然后继续微调",{"2":{"1337":1}}],["然后要求",{"2":{"1317":1}}],["然后运用",{"2":{"1315":1}}],["然后让模型进行重构来进行预训练",{"2":{"1315":1}}],["然后微调它",{"2":{"1313":1}}],["然后可以重复地执行编译后的图形",{"2":{"1287":1}}],["然后扩展到大量训练步骤将导致在过小的步骤上进行大部分训练",{"2":{"1158":1}}],["然后创建另一项研究",{"2":{"1144":1}}],["然后发送剩余的提示",{"2":{"986":1}}],["然后相对于这些块计算注意力输出",{"2":{"942":1,"959":1}}],["然后相加",{"2":{"692":1}}],["然后选择概率或得分最高的k个候选项",{"2":{"904":1}}],["然后综合这k×v个score的结果",{"2":{"902":1}}],["然后输入给下一步",{"2":{"901":1}}],["然后输出为",{"2":{"428":1}}],["然后一步步将输出序列解码出来",{"2":{"889":1}}],["然后每个group内做归一化",{"2":{"810":1}}],["然后每次加入下一个真值",{"2":{"426":1}}],["然后每次加入上一次的输出",{"2":{"426":1}}],["然后与位置嵌入向量相加",{"2":{"745":1}}],["然后与输出嵌入矩阵相乘",{"2":{"482":1}}],["然后取第二个",{"2":{"731":1}}],["然后concat到一起",{"2":{"722":1}}],["然后cnn中逐通道卷积最后沿着通道求和做特征融合",{"2":{"12":1}}],["然后到词嵌入",{"2":{"711":1}}],["然后促进各种自然语言处理",{"2":{"711":1}}],["然后模型会将这些向量作为输入",{"2":{"700":1}}],["然后加上positional",{"2":{"698":1}}],["然后加权求和把每一个输入元素xixix",{"2":{"418":1}}],["然后dataset根据batch",{"2":{"665":1}}],["然后dataloader再一批次一批次的提取吗",{"2":{"665":1}}],["然后dataloader再一批次的提取吗",{"2":{"665":1}}],["然后使用一个一层transformer的解码器再进行重构",{"2":{"727":1}}],["然后使用",{"2":{"623":1,"1524":1}}],["然后使用logistic",{"2":{"184":1}}],["然后基于熵来划分patch",{"2":{"613":1}}],["然后基于xt=p",{"2":{"240":1}}],["然后迭代到最短的token",{"2":{"587":1}}],["然后从剩下的",{"2":{"2125":1}}],["然后从字符级别开始",{"2":{"576":1}}],["然后从每个key地址都会取出value",{"2":{"164":1}}],["然后依据词表",{"2":{"554":1}}],["然后依据说明书的描述找到最相似的零件",{"2":{"524":1}}],["然后跟词典中的词语进行匹配",{"2":{"550":1}}],["然后这个整数向量列表会被转换为更具表达能力的向量形式",{"2":{"545":1}}],["然后这些专家模块可以根据任务需求在运行时动态组合",{"2":{"222":1}}],["然后排序之后就可以得到词汇表",{"2":{"545":1}}],["然后我们沿水平位置排列的key",{"2":{"975":1}}],["然后我们再看交叉注意力是如何在编码器和解码器之间起作用的",{"2":{"537":1}}],["然后我们将这些块分配到不同的头上",{"2":{"9":1}}],["然后保存在layers这个列表中",{"2":{"532":1}}],["然后转换成一组向量",{"2":{"528":1}}],["然后转换成一组向量进行后续处理",{"2":{"528":1}}],["然后向量r作为上一层的输入",{"2":{"519":1}}],["然后向右移动一位构建标签",{"2":{"408":1}}],["然后旁边写了个",{"2":{"517":1}}],["然后才能生成向量",{"2":{"547":1}}],["然后才能被transformer处理",{"2":{"473":1}}],["然后才开始实施softmax操作",{"2":{"67":1}}],["然后会判断该年份是否为闰年并输出结果",{"2":{"1729":1}}],["然后会施加dropout操作来随机丢弃一些元素",{"2":{"464":1}}],["然后会把",{"2":{"453":1}}],["然后会调用",{"2":{"344":1}}],["然后embedding送入重复的blocks中",{"2":{"446":1}}],["然后input",{"2":{"445":1}}],["然后第二次`ys`为",{"2":{"428":1}}],["然后以各种说法说那些风险啊什么的",{"2":{"2051":1}}],["然后以字节为颗粒度进行聚合",{"2":{"607":1}}],["然后以并行的方式一次性的输出完整的目标序列",{"2":{"411":1}}],["然后以向量内积的指数形式建模键",{"2":{"125":1}}],["然后计算这两个特征向量集合的均值",{"2":{"1361":1}}],["然后计算它们之间的余弦相似度",{"2":{"1360":1}}],["然后计算结果也需要存下来",{"2":{"396":1}}],["然后计算得到hthth",{"2":{"285":1}}],["然后调用",{"2":{"423":1}}],["然后调用run",{"2":{"385":1}}],["然后调用subsequent",{"2":{"74":1,"382":1}}],["然后需要把out和tgt",{"2":{"381":1}}],["然后填充词对应的掩码和未来词汇相关的掩码会做与操作",{"2":{"380":1}}],["然后构建词表",{"2":{"374":1}}],["然后进行层标准化",{"2":{"915":1}}],["然后进行加权求和",{"2":{"739":1}}],["然后进行编码",{"2":{"727":1}}],["然后进行关联",{"2":{"676":1}}],["然后进行多任务微调的模型性能最好",{"2":{"542":1}}],["然后进行处理",{"2":{"453":1}}],["然后进行进一步处理以获得预训练语料库",{"2":{"367":1}}],["然后进行softmax",{"2":{"128":1}}],["然后用",{"2":{"1969":1}}],["然后用编码器对每个句子进行编码",{"2":{"628":1}}],["然后用其对输入特征进行缩放",{"2":{"346":1}}],["然后用每一行的每一个元素减去这行的均值",{"2":{"319":1}}],["然后是和vanilla",{"2":{"620":1}}],["然后是",{"2":{"344":1}}],["然后是attention的代码如下",{"2":{"201":1}}],["然后吃了一个香蕉",{"2":{"277":1,"516":1}}],["然后做残差连接和层归一化",{"2":{"523":1}}],["然后做了进一步的分析",{"2":{"302":1}}],["然后做一个概率值到目标词表的映射",{"2":{"267":1}}],["然后做softmax操作做归一化",{"2":{"170":1}}],["然后把每个token映射到一个整数上",{"2":{"549":1}}],["然后把word",{"2":{"515":1}}],["然后把隐状态解码称输出logits",{"2":{"451":1}}],["然后把矩阵作为一个批量一次性输入给解码器",{"2":{"408":1}}],["然后把输入的batch发给模型",{"2":{"384":1,"558":1}}],["然后把这个隐向量传递给解码器进行解码",{"2":{"267":1}}],["然后把处理后的processed",{"2":{"65":1}}],["然后对矩阵进行转置操作",{"2":{"1082":1}}],["然后对每个",{"2":{"1344":1}}],["然后对每个动作维度分别应用离散余弦变换",{"2":{"637":1}}],["然后对每个对输出序列的每个元素都计算损失即可",{"2":{"408":1}}],["然后对损失计算梯度",{"2":{"398":1}}],["然后对不同类别的数据进行数据分布调整",{"2":{"368":1}}],["然后对句中的每个词做归一化操作",{"2":{"318":1}}],["然后对重点区域进行特殊关注",{"2":{"257":1}}],["然后对这些指数值进行归一化处理来转换为概率",{"2":{"180":1}}],["然后逐步在较低的抽象层次上添加细节",{"2":{"627":1}}],["然后逐步对这个隐状态进行解码",{"2":{"245":1}}],["然后逐渐切换到注意力机制",{"2":{"235":1}}],["然后应用特定于任务的调整来生成最佳结果",{"2":{"225":1}}],["然后应用到原始的多头注意力计算中",{"2":{"19":1}}],["然后动态调整组件的组合",{"2":{"224":1}}],["然后偶尔看一下目录对照一下整体",{"2":{"217":1}}],["然后后面的layer用去掉了分母的",{"2":{"210":1}}],["然后归一化",{"2":{"180":1}}],["然后注意力机制会对点积结果进行一个softmax操作",{"2":{"168":1}}],["然后由这三者构建了注意力机制",{"2":{"163":1}}],["然后在需要的地方调用",{"2":{"1729":1}}],["然后在编译时根据实际需要",{"2":{"1698":1}}],["然后在laion5b的高分辨率数据集以512x512尺寸训练194",{"2":{"1363":1}}],["然后在没有任何先验知识的情况下开始训练",{"2":{"1313":1}}],["然后在更新后的位置再求梯度",{"2":{"1034":1}}],["然后在每个词的嵌入xkxkx",{"2":{"748":1}}],["然后在通过查表可以单独查看每个元素的",{"2":{"709":1}}],["然后在训练过程中",{"2":{"709":1}}],["然后在训练过程中进行更新",{"2":{"708":1}}],["然后在这些离散单元上建模",{"2":{"636":1}}],["然后在前向传播时候由encoderlayer",{"2":{"523":1}}],["然后在multi",{"2":{"445":1}}],["然后在最左边放上一个表示开始的token",{"2":{"407":1}}],["然后在除以均方根的基础上再点乘一个参数张量",{"2":{"346":1}}],["然后在全部编码序列基础上计算一个上下文向量",{"2":{"284":1}}],["然后在",{"2":{"145":1}}],["然后遗忘旧的知识",{"2":{"143":1}}],["然后观察其预测结果",{"2":{"130":1}}],["然后该token和",{"2":{"128":1}}],["然后尽量保证每个",{"2":{"90":1}}],["然后",{"2":{"90":1,"94":2,"122":1,"135":1,"137":1,"145":1,"153":1,"213":1,"320":1,"359":1,"510":1,"540":1,"614":1,"620":1,"628":1,"635":1,"637":1,"866":1,"867":1,"868":1,"1086":1,"1221":1,"1317":1,"1645":1}}],["然后通过激活函数",{"2":{"1459":1}}],["然后通过查看其他所有单词",{"2":{"734":1}}],["然后通过上下文信息对mask的单词进行预测",{"2":{"721":1}}],["然后通过残差连接与输入token合并",{"2":{"622":1}}],["然后通过拼接各个头的输出并进行投影获得最终结果",{"2":{"503":1}}],["然后通过刻花来在已经干了的坯体上刻画出各种精美的花纹或者图案",{"2":{"437":1}}],["然后通过all",{"2":{"420":1}}],["然后通过最小化损失来逐步优化模型参数",{"2":{"397":1}}],["然后通过两个单独的线性投影产生两个中间向量和",{"2":{"356":1}}],["然后通过加权求和取得最后的阅读量",{"2":{"169":1}}],["然后通过加权求和把这些词加起来",{"2":{"167":1}}],["然后通过观察得到的top",{"2":{"147":1}}],["然后通过将这m个神经元设置为",{"2":{"143":1}}],["然后通过修改这些特定区域来进行知识编辑",{"2":{"142":1}}],["然后通过",{"2":{"89":1}}],["然后同步预测每个位置上的token",{"2":{"57":1}}],["然后只计算这些非填充token的损失函数",{"2":{"55":1}}],["然后再去拿",{"2":{"2101":1}}],["然后再去字典中查询",{"2":{"473":1}}],["然后再根据结果决定是否继续循环时",{"2":{"1620":1}}],["然后再从最大学习率退火到远低于初始学习率的最小学习率",{"2":{"1242":1}}],["然后再设置为",{"2":{"1120":1}}],["然后再由解码器",{"2":{"886":1}}],["然后再作为下一个层的输入",{"2":{"838":1}}],["然后再以求和的方式做坐标综合",{"2":{"722":1}}],["然后再映射到隐层空间",{"2":{"698":1}}],["然后再把source端的得到的self",{"2":{"649":1,"931":1}}],["然后再在这些离散单元的基础上建模",{"2":{"636":1}}],["然后再在所得的分类中进行softmax计算",{"2":{"185":1}}],["然后再统计并选出频数最高的前n个词组成词表",{"2":{"565":1}}],["然后再将",{"2":{"529":1}}],["然后再做归一化",{"2":{"341":1}}],["然后再用目标风格图片对应",{"2":{"337":1}}],["然后再用q左乘它",{"2":{"180":1}}],["然后再预测具体是哪个词",{"2":{"185":1}}],["然后再和tgt",{"2":{"74":1}}],["然后再迭代计算向量中每一个值对应的softmax值",{"2":{"54":1,"179":1}}],["然后再使用一个线性变换",{"2":{"36":1}}],["然后将字符串的内容复制到新分配的内存中",{"2":{"1647":1}}],["然后将模拟后的位置处梯度替换动量方法中的当前位置梯度",{"2":{"1034":1}}],["然后将结果转回实数域",{"2":{"1345":1}}],["然后将结果写回到hbm",{"2":{"944":1,"963":1}}],["然后将结果组合起来",{"2":{"944":1,"963":1}}],["然后将结果作为一个特征向量输出到下一层",{"2":{"816":1}}],["然后将结果传给子层",{"2":{"344":1}}],["然后将它们组合在一起",{"2":{"1916":1}}],["然后将它们相加",{"2":{"764":1}}],["然后将它们转换为前缀解码器以加速收敛",{"2":{"541":1}}],["然后将最常见的字节对合并成一个token",{"2":{"581":1}}],["然后将token映射到数字",{"2":{"563":1}}],["然后将和",{"2":{"529":1}}],["然后将x作为下一层的输入",{"2":{"522":1}}],["然后将问题映射到类似的问题上",{"2":{"505":1}}],["然后将当前输出文本附加到之前输入上变成新的输入",{"2":{"443":1}}],["然后将其投影为查询",{"2":{"355":1}}],["然后将句子中的所有词都聚集在这个中心的周围",{"2":{"318":1}}],["然后将这两部分信息进行整合",{"2":{"717":1}}],["然后将这两个softmax的差值作为最终注意力分数",{"2":{"502":1}}],["然后将这个向量输入到一个softmax层中",{"2":{"242":1}}],["然后将这些分解代入",{"2":{"213":1}}],["然后将注意力计算分为块内计算",{"2":{"216":1}}],["然后将嵌入的部分聚合到分片中",{"2":{"154":1}}],["然后将n次编辑应用到k个分片中",{"2":{"143":1}}],["然后将",{"2":{"36":1}}],["然后利用mntp",{"2":{"734":1}}],["然后利用一个编码器",{"2":{"727":1}}],["然后利用一个可以学习的权重",{"2":{"175":1}}],["然后利用额外的网络结构计算得到~hth~t",{"2":{"285":1}}],["然后利用注意力权重对ｖ进行加权求和",{"2":{"169":1}}],["然后利用for循环将",{"2":{"36":1}}],["然后利于for循环一个一个计算头",{"2":{"29":1}}],["然后过两个矩阵乘就可以了",{"2":{"24":1}}],["然后提取不同子空间的信息",{"2":{"13":1}}],["然后也会接着一个特征融合过程",{"2":{"12":1}}],["然而也不会有相聚甚远的情况",{"2":{"1375":1}}],["然而有些模型则可能需要4万次以上的训练",{"2":{"1183":1}}],["然而一些常见的批归一化实现不同步这些ema",{"2":{"1168":1}}],["然而缺失了修正因子",{"2":{"1059":1}}],["然而这是有弊端的",{"2":{"993":1}}],["然而这样整个模式就是串行化过程",{"2":{"405":1}}],["然而现在每次遍历都需要保存一次",{"2":{"967":1}}],["然而现在词表还是太大",{"2":{"582":1}}],["然而我们遇到的大部分问题序列都是不等长的",{"2":{"883":1}}],["然而其论文迄今为止在",{"2":{"844":1}}],["然而传统的sigmoid",{"2":{"841":1}}],["然而casual",{"2":{"732":1}}],["然而近期随着推理期计算等新事物的实现",{"2":{"626":1}}],["然而过大的词表也有问题",{"2":{"562":1}}],["然而某些大模型中也的确依然使用dropout",{"2":{"396":1}}],["然而但是实验结果显示这种做法并没有比浅层网络",{"2":{"301":1}}],["然而上述注意力方案中依然有rnn",{"2":{"291":1}}],["然而记忆几乎一直是神经网络中令人头疼的现象",{"2":{"230":1}}],["然而中间层的注意力模式变得更加复杂",{"2":{"204":1}}],["然而经过mask处理",{"2":{"71":1}}],["然而实际上",{"2":{"54":1}}],["然而",{"2":{"10":1,"26":1,"41":1,"91":2,"93":1,"103":1,"116":2,"121":1,"138":1,"141":1,"142":1,"148":1,"151":1,"176":1,"222":1,"227":1,"241":1,"246":1,"284":1,"311":1,"327":1,"334":1,"359":1,"360":1,"401":1,"488":1,"512":1,"540":2,"541":1,"542":1,"561":1,"595":1,"606":2,"612":1,"613":2,"692":1,"711":2,"735":1,"838":1,"937":1,"946":1,"953":1,"966":1,"975":1,"977":2,"980":1,"1026":1,"1042":1,"1143":1,"1146":1,"1149":1,"1154":1,"1165":1,"1175":2,"1326":1,"1460":1,"2096":1}}],["相信通过以上简单的几个例子",{"2":{"2140":1}}],["相信也会遇到那个跨越时空的的知己",{"2":{"2054":1}}],["相信大家也能够理解我想说什么了",{"2":{"2054":1}}],["相信大家已经掌握了",{"2":{"1601":1}}],["相信大家一直都有个疑问",{"2":{"162":1}}],["相结合",{"2":{"1312":1}}],["相邻元素在空间上共享语义信息",{"2":{"1019":1}}],["相邻token",{"2":{"576":1}}],["相应的资源也被释放",{"2":{"1891":1}}],["相应的value",{"2":{"126":1}}],["相应地",{"2":{"976":1}}],["相近",{"2":{"689":1}}],["相近的向量在颜色空间内位置相近",{"2":{"687":1}}],["相近的token分到一个",{"2":{"204":1}}],["相当的性能",{"2":{"621":1}}],["相当于乘以",{"2":{"1630":1}}],["相当于",{"2":{"1548":1,"1612":1}}],["相当于阻止了其跨度太大",{"2":{"1034":1}}],["相当于是将卷积核转换为稀疏矩阵后",{"2":{"779":1}}],["相当于是对",{"2":{"694":1}}],["相当于增强局域性的作用",{"2":{"765":1}}],["相当于两个",{"2":{"765":1}}],["相当于两个掩码只要有任意一种情况需要被遮蔽",{"2":{"72":1}}],["相当于取出embedding矩阵的第四行",{"2":{"700":1}}],["相当于顺序运行了两个函数",{"2":{"533":1}}],["相当于context与rnn单元各个时间步的",{"2":{"510":1}}],["相当于rnn中关联encoder与decoder的注意力context计算",{"2":{"510":1}}],["相当于rnn中各个时间步的",{"2":{"510":1}}],["相当于通过建立一个可微的关联结构来编排一堆transformer模块",{"2":{"508":1}}],["相当于将输出整体右移一位",{"2":{"408":1}}],["相当于在结构上使用更少的",{"2":{"396":1}}],["相当于给数据增加噪声",{"2":{"396":1}}],["相当于从原始的网络中采样得到一个子网络",{"2":{"393":1}}],["相当于把最里面一维变成了一个数",{"2":{"315":1}}],["相当于一个完整的隐层",{"2":{"175":1}}],["相当于门控",{"2":{"20":1}}],["相当于对不同的head抽取不同维度的矩阵信息",{"2":{"19":1}}],["相当于默认了每个头或者说每个子空间的重要性是一样的",{"2":{"10":1}}],["相互连接",{"2":{"785":1}}],["相互贡献就越小",{"2":{"765":1}}],["相互任意交互",{"2":{"620":1}}],["相互操作",{"0":{"167":1},"2":{"157":1}}],["相较于机器语言更容易理解",{"2":{"1603":1}}],["相较于hf",{"2":{"980":1}}],["相较于t5压缩后学习一个标量刻画相对位置",{"2":{"764":1}}],["相较于高层键值来说",{"2":{"127":1}}],["相较而言",{"2":{"498":1}}],["相匹配",{"2":{"489":1,"611":1}}],["相符的概率分布",{"2":{"398":1}}],["相同数据类型元素的集合",{"2":{"1634":1}}],["相同点",{"0":{"1469":1}}],["相同的",{"2":{"1114":1}}],["相同的数据指针创建一个",{"2":{"1087":1}}],["相同的核心计算",{"2":{"487":1}}],["相同语义但不同表述方式的文本可以被映射到同一个位置",{"2":{"710":1}}],["相同语义的不同表达都可以算作是多次出现",{"2":{"147":1}}],["相同",{"2":{"343":1,"1653":1,"1705":1}}],["相差较小",{"2":{"334":1}}],["相对较慢",{"2":{"1648":1}}],["相对较大",{"2":{"1648":1}}],["相对快速",{"2":{"1137":1}}],["相对应的是",{"2":{"1119":1}}],["相对参考位置的位置偏置",{"2":{"757":1}}],["相对语序比绝对语序对语义的影响更加关键",{"2":{"755":1}}],["相对定位的缺乏可能会阻碍模型理解语言结构的细微差别的能力",{"2":{"746":1}}],["相对距离越大",{"2":{"765":1}}],["相对距离",{"2":{"745":2}}],["相对位置并没有完整建模每个输入的位置信息",{"2":{"1338":1}}],["相对位置权重矩阵aijaija",{"2":{"759":1}}],["相对位置信息的术语",{"2":{"745":1}}],["相对位置信息主要是在模型网络层做文章",{"2":{"745":1}}],["相对位置信息",{"2":{"744":1,"757":1,"760":1}}],["相对位置编码是由绝对位置编码启发而来",{"2":{"1339":1}}],["相对位置编码起源于google的论文",{"2":{"1339":1}}],["相对位置编码起源于论文",{"2":{"759":1}}],["相对位置编码的目标是序列元素的边或者距离",{"2":{"759":1}}],["相对位置编码的引入",{"2":{"758":1}}],["相对位置编码的优点是",{"2":{"746":1}}],["相对位置编码会根据矩阵元素的下标",{"2":{"757":1}}],["相对位置编码并不是将位置编码直接加到词嵌入上",{"2":{"745":1}}],["相对位置编码并没有对每个输入的位置信息做完整建模",{"2":{"745":1}}],["相对位置编码考虑的则是进行attention计算时的query",{"2":{"745":1}}],["相对位置编码作用于自注意力机制",{"2":{"744":1}}],["相对位置编码则侧重于考虑元素之间的距离信息",{"2":{"742":1}}],["相对位置编码",{"0":{"752":1,"1338":1},"1":{"753":1,"754":1,"755":1,"756":1,"757":1,"758":1,"759":1,"760":1,"761":1,"762":1,"763":1,"764":1,"765":1,"766":1,"767":1,"1339":1,"1340":1},"2":{"741":1,"744":1}}],["相对容易预测",{"2":{"613":1}}],["相对",{"2":{"276":1,"795":1}}],["相对于y0",{"2":{"1377":1}}],["相对于bgd需要存储整个训练数据集的梯度以及sgd需要存储单个样本的梯度",{"2":{"1027":1}}],["相对于gpt",{"2":{"940":1,"962":1}}],["相对于当前内容的位置偏差",{"2":{"758":1}}],["相对于二次",{"2":{"273":1}}],["相对于",{"0":{"972":1},"2":{"217":1,"1821":1,"1839":1}}],["相对于知识编辑关注知识的存储区域",{"2":{"130":1}}],["相似的文字在欧式空间中相邻",{"2":{"714":1}}],["相似的模式",{"2":{"714":1}}],["相似性是通过为相似的词分配大的数字",{"2":{"691":1}}],["相似性",{"0":{"691":1}}],["相似度甚至不是唯一的",{"2":{"692":1}}],["相似度甚至不唯一",{"2":{"233":1,"740":1}}],["相似度越高",{"2":{"692":1}}],["相似度计算",{"0":{"692":1}}],["相似度计算函数是通过矩阵形式来计算",{"2":{"268":1}}],["相似或者相关的数据",{"2":{"164":1}}],["相加后的",{"2":{"326":1}}],["相加",{"2":{"175":3}}],["相乘",{"2":{"175":4,"394":1}}],["相乘之后得到的新query还是512",{"2":{"36":1}}],["相反",{"2":{"154":1,"204":1,"260":1,"301":1,"305":1,"692":1,"986":1,"990":1,"1127":1}}],["相比sd",{"2":{"1363":1}}],["相比原来的text",{"2":{"1363":1}}],["相比较其他深度",{"2":{"769":1}}],["相比其他两种偏置形式更难优化",{"2":{"765":1}}],["相比其他进一步训练的方案",{"2":{"734":1}}],["相比从头开始训练",{"2":{"620":1}}],["相比ascii只能覆盖英文中字符",{"2":{"607":1}}],["相比上次",{"2":{"582":1}}],["相比pre",{"2":{"333":1}}],["相比之下",{"2":{"222":1,"326":1,"607":1,"696":1,"946":1,"966":1,"1143":1,"1175":1}}],["相比于adagrad",{"2":{"1048":1}}],["相比于bgd",{"2":{"1027":1}}],["相比于非零中心的激活函数",{"2":{"838":1}}],["相比于",{"2":{"501":1,"1228":1,"1713":1}}],["相比于加性注意力减少了计算量",{"2":{"160":1}}],["相比于sigmoid和tanh函数",{"2":{"104":1}}],["相比于顺序处理的",{"2":{"34":1}}],["相比主要的区别就是最后一个",{"2":{"90":1}}],["相比",{"2":{"90":1,"183":1,"553":1,"843":1,"1175":1}}],["相关概念",{"0":{"1452":1},"1":{"1453":1,"1454":1,"1455":1,"1456":1,"1457":1}}],["相关性指的是调整结果与最终长时间运行之间的相似性",{"2":{"1157":1}}],["相关性和彻底性",{"2":{"1157":1}}],["相关性越高的书",{"2":{"169":1}}],["相关软件库",{"0":{"793":1},"1":{"794":1,"795":1,"796":1}}],["相关的常量值",{"2":{"1728":1}}],["相关的工具",{"2":{"1589":1}}],["相关的部分",{"2":{"536":1}}],["相关的token敏感的qk矩阵",{"2":{"122":1}}],["相关知识",{"0":{"297":1,"2090":1},"1":{"298":1,"299":1},"2":{"293":1}}],["相关路线",{"0":{"139":1},"2":{"96":1}}],["相关路由分数公式如下图标号1",{"2":{"42":1}}],["相关问题",{"2":{"89":1}}],["相关",{"0":{"805":1,"842":1},"2":{"3":1}}],["小孩胃口数组",{"2":{"2152":1}}],["小时的任务",{"2":{"2135":3}}],["小时可用",{"2":{"2135":1}}],["小时",{"2":{"2135":4}}],["小角处有点磕碰",{"2":{"2051":1}}],["小更新",{"0":{"2043":1,"2046":1,"2049":1}}],["小的计算函数等",{"2":{"1709":1}}],["小猫",{"2":{"1675":1}}],["小狗的年龄是",{"2":{"1674":1}}],["小狗的名字是",{"2":{"1674":1}}],["小狗",{"2":{"1674":2}}],["小于等于",{"2":{"1619":1,"1635":1}}],["小于",{"2":{"1619":1,"1635":1}}],["小于16时",{"2":{"807":1}}],["小",{"2":{"1607":1}}],["小数部分会被直接舍弃",{"2":{"1607":1}}],["小明",{"2":{"1485":1,"1729":3}}],["小心让日子把你们给混了",{"2":{"2056":1}}],["小心将来日子混了你",{"2":{"2056":1}}],["小心",{"0":{"1097":1}}],["小但一致的梯度",{"2":{"1028":1}}],["小批量梯度下降",{"2":{"1027":1}}],["小冬瓜aigc",{"2":{"740":1}}],["小智",{"2":{"513":1}}],["小狸愚",{"2":{"233":1}}],["小embedding",{"2":{"10":1,"28":1}}],["小结",{"0":{"30":1,"85":1,"120":1,"170":1,"195":1,"271":1,"291":1,"335":1,"386":1,"412":1,"446":1,"585":1,"687":1,"693":1,"767":1},"1":{"694":1,"695":1,"696":1},"2":{"0":1,"49":1,"96":1,"157":2,"293":1,"741":1,"1488":1}}],["8位",{"2":{"1607":1}}],["88109∗0",{"2":{"1393":2}}],["88109",{"2":{"1393":4}}],["891090×",{"2":{"1394":2}}],["891090",{"2":{"1389":5,"1393":9,"1394":7}}],["891090out",{"2":{"1389":1}}],["8gf",{"2":{"1308":2}}],["86sj",{"2":{"1302":1}}],["8bit",{"2":{"1075":1}}],["8倍的加速",{"2":{"970":1}}],["8us",{"2":{"935":1,"951":1}}],["8的编码方式将任意字符转化为长度1到4个字节",{"2":{"607":1}}],["8编码可以在不同语言之间具有一定互通性",{"2":{"608":1}}],["8编码是一个变长的编码",{"2":{"607":1}}],["8编码创建的本身就是为了通用的将世界上不同的语言字符尽可能全部用一套编码进行编号",{"2":{"607":1}}],["81",{"2":{"1247":1}}],["8101",{"2":{"833":1}}],["8185",{"2":{"558":1,"679":1}}],["8165",{"2":{"315":4}}],["8311",{"2":{"557":1}}],["83transformer",{"2":{"429":1}}],["8a",{"2":{"429":1}}],["84",{"2":{"429":1}}],["8d",{"2":{"173":1}}],["8dk=dmodel",{"2":{"173":1}}],["8ℎ",{"2":{"109":1}}],["8k+",{"2":{"768":1}}],["8k",{"2":{"89":1}}],["800mf",{"2":{"1308":2}}],["80",{"2":{"83":1,"399":1,"424":1,"429":1,"1210":2,"1231":1,"1236":3,"1623":2,"1680":1,"1813":2,"1825":3,"1831":2,"1843":3,"1948":1}}],["8=64",{"2":{"23":1}}],["8",{"0":{"288":1,"672":1,"735":1,"763":1,"776":1,"824":1,"825":1,"826":1,"827":1,"828":1,"846":1,"876":1,"910":1,"932":1,"933":1,"934":1,"966":1,"976":1,"1008":1,"1055":1,"1056":1,"1057":1,"1084":1,"1085":1,"1086":1,"1087":1,"1099":1,"1111":1,"1240":1,"1472":1,"1498":1,"1529":1,"1558":1,"1579":1,"1674":1,"1695":1,"1725":1,"1826":1,"1844":1,"1912":1,"1928":1,"1995":1,"1996":1,"2154":1},"1":{"825":1,"826":1,"827":1,"828":1,"933":1,"934":1,"1056":1,"1057":1,"1085":1,"1086":1,"1087":1,"1530":1,"1559":1,"1560":1,"1996":1},"2":{"10":2,"12":1,"20":1,"36":1,"83":1,"89":1,"119":1,"173":1,"217":1,"224":1,"315":1,"372":1,"399":2,"424":1,"428":1,"450":2,"453":2,"472":1,"529":1,"557":1,"591":4,"592":3,"608":1,"740":1,"741":1,"816":1,"820":1,"828":1,"832":3,"986":3,"1078":1,"1183":1,"1215":1,"1393":1,"1394":1,"1395":1,"1474":2,"1481":2,"1607":4,"1611":1,"1635":1,"1653":2,"1667":2,"1668":1,"1725":4,"1736":1,"1737":1,"1751":1,"1752":1,"1779":1,"1906":1,"1933":1,"2006":1,"2139":1}}],["后进先出",{"2":{"1648":1}}],["后置形式",{"2":{"1630":1}}],["后置钩子函数",{"2":{"1226":1,"1227":1}}],["后缀递增和递减",{"2":{"1635":1}}],["后缀",{"2":{"1607":1}}],["后会自动生成一套与",{"2":{"1589":1}}],["后跟查找的文本",{"2":{"1553":1}}],["后端学习框架的学习指南",{"2":{"1490":1}}],["后端学习框架",{"0":{"1490":1},"1":{"1491":1,"1492":1,"1493":1,"1494":1,"1495":1,"1496":1,"1497":1,"1498":1,"1499":1,"1500":1,"1501":1}}],["后端工程师负责服务器逻辑和数据库管理",{"2":{"5":1}}],["后一个点是前一个点的横坐标加一或者横纵坐标都加一",{"2":{"2022":1}}],["后一个任务被称为下句预测",{"2":{"1315":1}}],["后一个词的计算都会用到前面所有词的输出结果",{"2":{"249":1}}],["后执行后处理操作",{"2":{"1227":1}}],["后钩子",{"2":{"1227":1}}],["后再相加",{"2":{"1087":1}}],["后期较大的时候",{"2":{"1044":1}}],["后期使用该方案时需要使用一个合并",{"2":{"576":1}}],["后来跟一个室友说了一下",{"2":{"2054":1}}],["后来",{"2":{"907":1}}],["后来又出现了基于",{"2":{"861":1}}],["后来又出现了从右到左",{"2":{"595":1}}],["后的输入向量",{"2":{"890":1}}],["后的形状可以随便写吗",{"2":{"827":1}}],["后的矩阵实际上并不是有机地融合",{"2":{"10":1}}],["后逐会减少到趋于一个稳定值",{"2":{"576":1}}],["后处理主要包括",{"2":{"555":1}}],["后处理阶段是对编码后的文本进行一些额外的处理步骤",{"2":{"555":1}}],["后处理",{"0":{"555":1},"2":{"545":1}}],["后与期望目标比较",{"2":{"484":1}}],["后文会详细解读",{"2":{"382":1}}],["后面会有具体的经典算法问题",{"2":{"2140":1}}],["后面会让你越来越忙碌",{"2":{"2134":1}}],["后面大家慢慢会有更深刻的认识",{"2":{"2120":1}}],["后面依旧会遇到同样的问题",{"2":{"2051":1}}],["后面跟着要重载的运算符",{"2":{"1712":1}}],["后面添加",{"2":{"1704":1}}],["后面我们会讲分布式环境下的并行计算",{"2":{"1566":1}}],["后面我们会介绍如何将椭圆尽可能堆叠",{"2":{"1374":1}}],["后面详讲",{"2":{"1407":1}}],["后面每一步都是分出来2×3=6支",{"2":{"902":1}}],["后面2层加入绝对位置信息",{"2":{"763":1}}],["后面章节会详细分析",{"2":{"451":2}}],["后面两个pad被置为false",{"2":{"450":1}}],["后面有文章详细讲解",{"2":{"286":1}}],["后面的元素均不小于它",{"2":{"1752":1}}],["后面的代码块只有一行语句时",{"2":{"1619":1}}],["后面的apple作为我们将要预测的单词",{"2":{"409":1}}],["后面的单词",{"2":{"409":1}}],["后面的时刻天然就依赖于前面时刻的输出",{"2":{"248":1}}],["后面的权重都为0",{"2":{"71":1}}],["后面的部分都被mask掉了",{"2":{"71":1}}],["后者只考虑了编码器当前步的隐状态",{"2":{"285":1}}],["后半部分的卷积组负责处理后半部分的输入层",{"2":{"775":1}}],["后半部分的rnn只有输出",{"2":{"241":1}}],["后半部分可以叫做解码器",{"2":{"241":1}}],["后训练仍然非常消耗资源",{"2":{"222":1}}],["后训练试图通过一次全面的训练来优化模型",{"2":{"222":1}}],["后对角线上的注意力一定是本行最大",{"2":{"172":1}}],["后",{"2":{"163":1,"517":1,"714":2,"899":1,"1108":1}}],["后续常量的值仍然会依次递增",{"2":{"1728":1}}],["后续常量的值依次递增",{"2":{"1728":1}}],["后续不再使用已释放的",{"2":{"1672":1}}],["后续版本不断引入新特性和改进",{"2":{"1603":1}}],["后续补充",{"0":{"1416":1,"1420":1},"1":{"1417":1,"1418":1,"1419":1,"1421":1,"1422":1,"1423":1,"1424":1}}],["后续这个矩阵就像词表一样使用",{"2":{"749":1}}],["后续自己补hf的那些",{"2":{"669":1}}],["后续迭代",{"0":{"584":1}}],["后续会提到",{"2":{"2117":1}}],["后续会经过generator等模块把高维向量转换回人类可以理解的低维向量",{"2":{"526":1}}],["后续会根据这些标记成为下一个单词的可能性对其进行采样",{"2":{"473":1}}],["后续解码器层的输入有两个",{"2":{"526":1}}],["后续章节会对掩码做详细阐释",{"2":{"525":1}}],["后续章节会对q",{"2":{"265":1}}],["后续文章会进行详述",{"2":{"523":1}}],["后续篇幅会对encoderlayer和decoderlayer进行详细介绍",{"2":{"461":1}}],["后续各种类的构造函数中会调用copy",{"2":{"449":1}}],["后续的rpe都只加到k上去",{"2":{"760":1}}],["后续的任务都以这个分词词典来切词和编码",{"2":{"576":1}}],["后续的输出依赖于前面的输出词",{"2":{"443":1}}],["后续的词",{"2":{"307":1}}],["后续简称为哈佛源码",{"2":{"432":1}}],["后续将对模型结构和执行流程进行逐步细化",{"2":{"431":1}}],["后续时间步会输入迄今为止生成的整个输出序列",{"2":{"427":1}}],["后续随着学生的进步",{"2":{"411":1}}],["后续我们依然称之为word",{"2":{"460":1}}],["后续我们将没有做softmax归一化之前的对齐系数称为注意力分数",{"2":{"267":1}}],["后续我们用",{"2":{"265":1}}],["后续在图上也只展示train数据集相关的代码",{"2":{"364":1}}],["后续在multiheadattention的forward",{"2":{"199":1}}],["后续在处理mask时",{"2":{"66":1}}],["后续没有用到",{"2":{"198":1}}],["后续编码器层的qkv由上一个编码器层的输出经过线性变化而来",{"2":{"161":1}}],["后续再乘v的时候",{"2":{"70":1}}],["它都是最高的",{"2":{"2115":1}}],["它其实是我们每个人生活中的一部分",{"2":{"2108":1}}],["它其实也是一种思维方式",{"2":{"2103":1}}],["它帮助我们管理编译过程",{"2":{"1963":1}}],["它类似于指针",{"2":{"1927":1}}],["它自动对元素进行排序",{"2":{"1806":1}}],["它自然适用于短期和长期上下文",{"2":{"977":1}}],["它经常用于函数的返回类型",{"2":{"1805":1}}],["它专门用于操作以字符构成的字符串",{"2":{"1803":1}}],["它执行特定的任务",{"2":{"1729":1}}],["它封装了动态内存管理",{"2":{"1714":1}}],["它拥有自己的状态和行为",{"2":{"1678":1}}],["它没有参数",{"2":{"1676":1}}],["它没有其他head中的信息",{"2":{"41":1}}],["它和被引用的变量指向同一块内存空间",{"2":{"1650":1}}],["它接收一个整数类型的参数",{"2":{"1729":1}}],["它接收一个整数数组和一个回调函数",{"2":{"1645":1}}],["它接受一个参数并打印它",{"2":{"1883":1}}],["它接受一个可链接的学习率调度器列表",{"2":{"1246":1}}],["它接受四个函数",{"2":{"450":1}}],["它所指向的地址不能改变",{"2":{"1614":1}}],["它并非新的变量",{"2":{"1612":1}}],["它提供了一种更安全",{"2":{"1612":1}}],["它提出并综合了三个新想法",{"2":{"973":1}}],["它访问指针所指向的内存地址中的值",{"2":{"1611":1}}],["它用来存储内存地址",{"2":{"1611":1}}],["它返回变量在内存中的起始位置",{"2":{"1611":1}}],["它返回两个列表",{"2":{"1214":1}}],["它既保留了",{"2":{"1603":1}}],["它广泛应用于科学计算",{"2":{"1564":1}}],["它模拟了人脑神经元之间的相互连接和信息传递方式",{"2":{"1455":1}}],["它避免了手动提取特征的麻烦",{"2":{"1455":1}}],["它负责将字节码转化为机器码并执行",{"2":{"1435":1}}],["它负责处理整个输入序列",{"2":{"442":1}}],["它存在的目的是有效的防止竞争条件又能保证最大化使用共享数据",{"2":{"1412":1}}],["它仅仅是在attention矩阵的基础上加一个可训练的偏置项而已",{"2":{"1340":1}}],["它指向一个数组",{"2":{"1705":1}}],["它指向的内存已经无效",{"2":{"1647":1}}],["它指示了当前的输入",{"2":{"1323":1}}],["它指出nlp的大多数任务可能都只需要相对位置信息",{"2":{"763":1}}],["它适用于具有固定结构和无需动态控制流的模型",{"2":{"1287":1}}],["它以一种动态的方式表示模型的计算过程",{"2":{"1287":1}}],["它以一种静态的方式表示模型的结构和计算逻辑",{"2":{"1287":1}}],["它通常有以下列",{"2":{"1167":1}}],["它通过跟踪模型的输入和输出",{"2":{"1292":1}}],["它通过利用循环神经网络",{"2":{"885":1}}],["它通过每次归一化操作逐渐削弱恒等分支的影响",{"2":{"335":1}}],["它通过计算神经网络相对于输入的梯度来衡量输入的",{"2":{"228":1}}],["它通过限制字典元素的数量和它们的线性组合的稀疏性",{"2":{"137":1}}],["它明显比上面的东西弱",{"2":{"1158":1}}],["它被编译器自动传递给成员函数",{"2":{"1638":1}}],["它被特别强调是因为我们在调整超参数方面遇到了困难",{"2":{"1127":1}}],["它被归零的概率会升高",{"2":{"844":1}}],["它支持任何计算图的梯度自动计算",{"2":{"1105":1}}],["它也是我们日常生活的助手",{"2":{"2107":1}}],["它也是一种有序容器",{"2":{"1807":1}}],["它也可能是",{"2":{"1924":1}}],["它也可能是constexpr的",{"2":{"1924":1}}],["它也可以用正确的第一个词",{"2":{"406":1}}],["它也有自己的内存地址",{"2":{"1611":1}}],["它也许最好被看作结合rmsprop",{"2":{"1059":1}}],["它有潜力收敛到全局最优解",{"2":{"1025":1}}],["它不是字符串内容的一部分",{"2":{"1704":1}}],["它不是指输出到屏幕的内容的大小",{"2":{"1678":1}}],["它不是随机将隐含层节点的输出清0",{"2":{"1018":1}}],["它不会改变向量的模长",{"2":{"1343":1}}],["它不仅理解了每个单词的含义",{"2":{"629":1}}],["它不仅计算这些组合的频率",{"2":{"598":1}}],["它不仅能够识别输入数据中的模式和关系",{"2":{"144":1}}],["它从数据中获得信息",{"2":{"1009":1}}],["它淘汰来自第二个聊天会话的所有节点",{"2":{"986":1}}],["它作为一种空间高效的替代方案",{"2":{"986":1}}],["它同时将key",{"2":{"975":1}}],["它常用于编程语言中表示变量的初始化或赋值操作",{"2":{"943":1,"961":1}}],["它常用于多类分类问题中",{"2":{"847":1}}],["它关联单个序列的不同位置以计算序列的表示",{"2":{"911":1}}],["它关注于对象和态射之间的映射关系",{"2":{"505":1}}],["它把可能在较大范围内变化的输入值挤压到",{"2":{"1460":1}}],["它把那些需要大量人力的工作都吃掉了",{"2":{"909":1}}],["它把句子看作一个概念单元",{"2":{"628":1}}],["它决定了运算的优先级",{"2":{"1705":1}}],["它决定了在每次参数更新中应用的梯度下降的步长",{"2":{"1229":1}}],["它决定了要丢弃哪些信息以及要添加哪些新信息",{"2":{"874":1}}],["它决定了中间层相对于整个网络的大小",{"2":{"100":1}}],["它只是一个非正式的描述语",{"2":{"1155":1}}],["它只需存储每个小批量样本的梯度",{"2":{"1027":1}}],["它只有两个门",{"2":{"874":1}}],["它只在第一次执行到其定义语句时被初始化一次",{"2":{"1649":1}}],["它只在每个流水线阶段的边界需要跨节点通信",{"2":{"977":1}}],["它只在",{"2":{"337":1}}],["它由多个神经网络层",{"2":{"785":1}}],["它由两个线性变换组成",{"2":{"735":1}}],["它可能在某些特定的底层编程或模板编程中会用到",{"2":{"1704":1}}],["它可能会在注意力中带来不必要的随机性",{"2":{"764":1}}],["它可以指向一个",{"2":{"1929":1}}],["它可以指向任何类型的变量的内存地址",{"2":{"1611":1}}],["它可以访问另一个类的私有和受保护成员",{"2":{"1778":1}}],["它可以存储一个",{"2":{"1611":1}}],["它可以带来一定的远程衰减性",{"2":{"1344":1}}],["它可以通过两种方式使用",{"2":{"1223":1}}],["它可以在较少的内存访问次数下计算精确的注意力",{"2":{"940":1,"959":1}}],["它可以在保持模型通用能力的同时",{"2":{"560":1}}],["它可以是为了适应新的任务或结构",{"2":{"938":1,"954":1}}],["它可以建模依赖关系而不考虑其在输入或输出序列中的距离",{"2":{"911":1}}],["它可以扩展到任意长的序列",{"2":{"762":1}}],["它可以保留每个路由权重捕获的不同信息",{"2":{"739":1}}],["它可以形象化地表示为带箭头的线段",{"2":{"680":1}}],["它可以表现为仅编码器",{"2":{"539":1}}],["它可以尝试从已经学到的范畴中找到类似的类比",{"2":{"505":1}}],["它可以将代码分割成独立的可重用模块",{"2":{"1729":1}}],["它可以将给定的",{"2":{"473":1}}],["它可以将词向量序列映射到一个固定长度的向量表示",{"2":{"242":1}}],["它可以当成函数调用",{"2":{"344":1}}],["它一共有13层",{"2":{"763":1}}],["它基于目标单词预测其上下文",{"2":{"714":1}}],["它基于损失函数l",{"2":{"230":1}}],["它基于损失函数之上的",{"2":{"230":2}}],["它更像是一个循环",{"2":{"709":1}}],["它更关注上下文中的关系",{"2":{"130":1}}],["它描述了向量的集合在一定维度和基下的分布和操作",{"2":{"689":1}}],["它需要新的数学或者新的语言",{"2":{"689":1}}],["它才能明白你想做什么",{"2":{"678":1}}],["它蕴含着更多的信息",{"2":{"676":1}}],["它理解了",{"2":{"629":1}}],["它非常适合沿参数轴",{"2":{"623":1}}],["它独立于输入和输出通道维度运行",{"2":{"621":1}}],["它比",{"2":{"1624":1}}],["它比字符词汇表更紧凑",{"2":{"606":1}}],["它比纯整数更适合深度学习",{"2":{"457":1}}],["它表示当前位置的标签",{"2":{"1324":1}}],["它表示subword是词后缀",{"2":{"579":1}}],["它表示模型需要",{"2":{"162":1}}],["它直接影响模型的性能",{"2":{"559":1}}],["它应该具有以下的签名",{"2":{"1227":2}}],["它应该具有以下签名",{"2":{"1227":4}}],["它应该足够大",{"2":{"1165":1}}],["它应该仍然适用于encoder",{"2":{"542":1}}],["它应用激活函数",{"2":{"1086":1}}],["它应用广泛",{"2":{"103":1}}],["它能加快搜索",{"2":{"1175":1}}],["它能加快学习过程",{"2":{"542":1}}],["它能学习一个模型分布pθpθp",{"2":{"633":1}}],["它能够生成篇幅较长且语义连贯的文本",{"2":{"1316":1}}],["它能够在给定搜索空间内生成",{"2":{"1176":1}}],["它能够更快地朝着梯度下降的方向移动",{"2":{"1032":1}}],["它能够根据你的语义理解你想做什么",{"2":{"678":1}}],["它能够学习记忆历史上下文",{"2":{"228":1}}],["它能够动态地调整自身的权重以在陌生的环境中不断学习",{"2":{"220":1}}],["它能够通过并行运行多个具有独特视角的注意力头来同时处理数据",{"2":{"1":1}}],["它使系统能够更多地将序列进行批处理",{"2":{"982":1}}],["它使用编码序列的输出作为交叉注意力模块的输入",{"2":{"540":1}}],["它使得数学家和研究者能够在广泛的数学领域中发现新的见解和联系",{"2":{"505":1}}],["它说明了解码器是如何利用编码器的信息来确定最合适的输出的",{"2":{"536":1}}],["它包含管理输出流所需的各种成员变量",{"2":{"1678":1}}],["它包含两个条目",{"2":{"1227":1}}],["它包含对模块参数和缓冲区的引用",{"2":{"1214":1}}],["它包含了1亿对文本",{"2":{"725":1}}],["它包含当前生成的词与已生成词之间的上下文依赖关系",{"2":{"525":1}}],["它包括两个模块",{"2":{"727":1}}],["它包括多头注意力机制和多层感知器层",{"2":{"461":1}}],["它包括warmup",{"2":{"402":1}}],["它继承了nn",{"2":{"522":1}}],["它对搜索空间的统一探索使得对结果以及它们可能对搜索空间提出的建议的推理变得更容易",{"2":{"1175":1}}],["它对具有更多正则化的配置更偏爱",{"2":{"1149":1}}],["它对某些值进行掩盖",{"2":{"650":1,"932":1}}],["它对每个位置的词向量独立地进行变换",{"2":{"517":1}}],["它对生成当前token的输出有贡献",{"2":{"162":1}}],["它相当于人为设计的一种索引",{"2":{"512":1}}],["它代表了所有可能的命题结构",{"2":{"510":1}}],["它首先被离散化成一个个单词token的集合",{"2":{"510":1}}],["它本质上服从正则系综分布",{"2":{"506":1}}],["它完全从另外一个角度考虑如何以连续的方式借助神经网络对数据建模",{"2":{"493":1}}],["它允许在同一块内存空间存储不同类型的数据",{"2":{"1728":1}}],["它允许你像处理文件或标准输入输出一样处理字符串",{"2":{"1822":1,"1840":1}}],["它允许你为一组整型常量赋予有意义的名称",{"2":{"1728":1}}],["它允许你将不同类型的数据组合在一起",{"2":{"1728":1}}],["它允许我们将数据",{"2":{"1674":1}}],["它允许我们根据不同的条件执行不同的代码块",{"2":{"1618":1}}],["它允许对子图进行细粒度的梯度计算排除",{"2":{"1117":1}}],["它允许token与其他token进行通信",{"2":{"461":1}}],["它允许每个标记关注序列中的其他标记",{"2":{"355":1}}],["它逐个生成文本",{"2":{"443":1}}],["它赋予模型洞察句子中每个单词与其它单词间错综复杂关系的超能力",{"2":{"438":1}}],["它遍历一个",{"2":{"423":1}}],["它下次的输入都是本次输出对应的真值",{"2":{"407":1}}],["它确实保留了归一化层在非线性压缩极端值方面的效果",{"2":{"360":1}}],["它为模型提供非线性以增强表达能力",{"2":{"334":1}}],["它反而是好处",{"2":{"333":1}}],["它还是可以得到有效更新的",{"2":{"333":1}}],["它还不至于削弱到小于随机误差的地步",{"2":{"333":1}}],["它还引入了varlen",{"2":{"217":1}}],["它在类的任意位置都可声明",{"2":{"1775":1}}],["它在",{"2":{"1674":1}}],["它在创建",{"2":{"1674":1}}],["它在提出时横扫了整个",{"2":{"1315":1}}],["它在两种算法中的作用相似",{"2":{"1143":1}}],["它在前向传播和反向传播中都起作用",{"2":{"1117":1}}],["它在每次参数更新时使用一小批次的样本来计算梯度和更新参数",{"2":{"1027":1}}],["它在gpt",{"2":{"603":1}}],["它在slm中得到了广泛的应用",{"2":{"311":1}}],["它在背景向量中的意义和它作为查询词时候代表的意义不同",{"2":{"288":1}}],["它之前的所有神经层的参数变化会导致本层的输入分布发生较大的改变",{"2":{"309":1}}],["它之后的词汇",{"2":{"59":1}}],["它会按照预定的流程进行处理",{"2":{"1729":1}}],["它会自动重新分配更大的内存空间",{"2":{"1714":1}}],["它会提示你选择或配置编译器",{"2":{"1605":1}}],["它会检查函数中的各种操作符和模块",{"2":{"1291":1}}],["它会引入计算延迟",{"2":{"975":1}}],["它会带来计算和存储的双重负担",{"2":{"682":1}}],["它会对其输入进行转换以调整传递的信息",{"2":{"496":1}}],["它会计算该单词与其他单词之间的注意力分数",{"2":{"268":1}}],["它会让模型在处理长文本时复杂度成n的平方的增加",{"2":{"210":1}}],["它输出一个预测",{"2":{"263":1}}],["它输出的是一个方阵",{"2":{"74":1}}],["它就在我们身边",{"2":{"2108":1}}],["它就是一种解决问题的规则和步骤",{"2":{"2096":1}}],["它就是记忆模块在测试时学习的目标",{"2":{"230":3}}],["它就像一个初始化于该碗状结构的adagrad",{"2":{"1048":1}}],["它就会知道它更可能是",{"2":{"579":1}}],["它就可以模拟任意计算过程",{"2":{"504":1}}],["它就不用费劲计算这个输出了",{"2":{"58":1}}],["它衡量传入数据的意外",{"2":{"230":1}}],["它衡量最近过去的意外程度",{"2":{"230":1}}],["它限制了模型的泛化能力",{"2":{"230":1}}],["它编码了序列中每个元素相对于所有其他元素的关系",{"2":{"209":1}}],["它选择重用预训练模型中的原始位置索引及其嵌入",{"2":{"204":1}}],["它将输入值映射为输出值",{"2":{"1460":1}}],["它将参数组合成一个多张量",{"2":{"1228":1}}],["它将被用于加载到优化器中",{"2":{"1227":1}}],["它将结果使用",{"2":{"1086":1}}],["它将是一个副本",{"2":{"819":1}}],["它将每个离散的输入索引",{"2":{"702":1}}],["它将前一个句子的sonar嵌入作为输入",{"2":{"632":1}}],["它将全局",{"2":{"614":1}}],["它将一系列潜在输入",{"2":{"614":1}}],["它将停止生成",{"2":{"529":1}}],["它将原本的输入按照sequence",{"2":{"420":1}}],["它将词嵌入层",{"2":{"201":1}}],["它将信息从主语位置",{"2":{"130":1}}],["它实质做的是一个soft版本的argmax操作",{"2":{"180":1}}],["它要面对一个更大的集合",{"2":{"176":1}}],["它是很多成功人士用来处理繁琐事务的秘诀",{"2":{"2104":1}}],["它是指通过分析",{"2":{"2103":1}}],["它是指多进程存在时必须互斥访问的资源",{"2":{"1412":1}}],["它是一种动态数组",{"2":{"1797":1}}],["它是一个特殊的非打印字符",{"2":{"1704":1}}],["它是一个紧凑的表示",{"2":{"224":1}}],["它是一个线性变换后面接门控机制的结构",{"2":{"105":1}}],["它是唯一可以生效的",{"2":{"1119":1}}],["它是最陡下降方向",{"2":{"1115":1}}],["它是深度学习从业者经常采用的优化方法之一",{"2":{"1047":1}}],["它是由",{"2":{"999":1}}],["它是网络的记忆单元",{"2":{"855":1}}],["它是在",{"2":{"762":1}}],["它是对解码器的输入序列执行类似全局自注意力层的工作",{"2":{"443":1}}],["它是",{"2":{"338":1,"1433":1}}],["它是否像实体知识一样存储在",{"2":{"136":1}}],["它的引用计数自动减1",{"2":{"1891":1}}],["它的主要作用是在创建类的对象时自动初始化对象的状态",{"2":{"1675":1}}],["它的作用域被限制在声明它的源文件中",{"2":{"1649":1}}],["它的作用如下",{"2":{"674":1}}],["它的生命周期贯穿整个程序执行期间",{"2":{"1649":1}}],["它的一个重要特点是以空字符",{"2":{"1624":1}}],["它的特点是先判断条件",{"2":{"1620":1}}],["它的配置和select标签差不多",{"2":{"1486":1,"1487":1,"1488":1}}],["它的灵活性更大",{"2":{"1338":1}}],["它的存在能够帮助我们显示地去建模标签之间的转移关系",{"2":{"1324":1}}],["它的值是",{"2":{"1143":1}}],["它的核心思想是",{"2":{"999":1,"2121":1}}],["它的吞吐量比huggingface",{"2":{"980":1}}],["它的思路类似于把",{"2":{"974":1}}],["它的思路是",{"2":{"411":1}}],["它的平均值会削弱这个信息",{"2":{"928":1}}],["它的意思就是在两种语言a和b之间",{"2":{"908":1}}],["它的意思是时刻",{"2":{"74":1}}],["它的导数在",{"2":{"838":1}}],["它的人工神经元可以响应一部分覆盖范围内的周围单元",{"2":{"769":1}}],["它的向量表示都是相同的",{"2":{"715":1}}],["它的流行源于人们相信它捕获了嵌入向量之间的方向对齐",{"2":{"692":1}}],["它的处理原则是",{"2":{"567":1}}],["它的forward",{"2":{"522":1}}],["它的信息传递速度远胜卷积层和循环层",{"2":{"511":1}}],["它的潜在假设是",{"2":{"475":1}}],["它的工作原理是对输入的各个特征进行归一化",{"2":{"468":1}}],["它的index在词典中对应的词就是预测结果",{"2":{"428":1}}],["它的目的就是阻碍模型过度学习",{"2":{"393":1}}],["它的shape是",{"2":{"346":1}}],["它的80层注意力层里",{"2":{"214":1}}],["它的公式为",{"2":{"104":1}}],["它们各自具有独特的功能和用途",{"2":{"1798":1}}],["它们各自发挥优势并避免各自的弱点",{"2":{"155":1}}],["它们允许我们遍历容器中的元素",{"2":{"1718":1}}],["它们允许输出缩放到任意尺度",{"2":{"360":1}}],["它们会自动管理内存并提供边界检查",{"2":{"1670":1}}],["它们会找到一种平衡点",{"2":{"709":1}}],["它们指向同一块内存空间",{"2":{"1650":1}}],["它们指向相同的内存地址",{"2":{"1612":1}}],["它们在定义时被创建",{"2":{"1649":1}}],["它们在程序开始执行时被创建",{"2":{"1649":1}}],["它们如何帮助管理动态分配的内存",{"2":{"1647":1}}],["它们提供了以下几种关键工具",{"2":{"1589":1}}],["它们采用不同的预训练目标在不同的数据集上进行训练",{"2":{"1312":1}}],["它们将被用作默认值",{"2":{"1222":1}}],["它们将所有数字拆分为单个数字",{"2":{"595":1}}],["它们通常具有足够的相关性",{"2":{"1158":1}}],["它们通常不会以任何方式影响前向传递或梯度的计算成本",{"2":{"1143":1}}],["它们通过缓慢升高然后降低钙反应来产生信号",{"2":{"488":1}}],["它们很少是目标超参数",{"2":{"1143":1}}],["它们比具有动量的",{"2":{"1130":1}}],["它们彼此交换kv",{"2":{"976":1}}],["它们需要更小的kv缓存",{"2":{"956":1}}],["它们需要至少ggg的直径那么多的层数才能实现完整的sequence",{"2":{"93":1}}],["它们就不应该有过多的交互",{"2":{"762":1}}],["它们就聚在一起",{"2":{"685":1}}],["它们表现出直观的权重分配模式",{"2":{"757":1}}],["它们表示其他知识",{"2":{"135":1}}],["它们是函数内部使用的占位符",{"2":{"1729":1}}],["它们是当前研究中非常活跃的领域",{"2":{"1175":1}}],["它们是可替换的",{"2":{"714":1}}],["它们是由正则化隐式控制的",{"2":{"692":1}}],["它们只能移动直角",{"2":{"692":1}}],["它们不是任何先验分布的参数",{"0":{"1185":1}}],["它们不仅是模型理解文本语言的基础",{"2":{"545":1}}],["它们不如样条准确",{"2":{"155":1}}],["它们都支持一个共同的接口",{"2":{"1866":1}}],["它们都包含在",{"2":{"1811":1,"1829":1}}],["它们都继承自",{"2":{"1762":1}}],["它们都可以包含数据成员和成员函数",{"2":{"1728":1}}],["它们都应该能在同一个空间内做计算",{"2":{"689":1}}],["它们都会先做自己的业务逻辑",{"2":{"523":1}}],["它们都源于",{"2":{"212":1}}],["它们最终收敛到共识均衡",{"2":{"507":1}}],["它们构成了输入的嵌入矩阵",{"2":{"457":1}}],["它们结合在一起又可以作为一个新的积木块",{"2":{"449":2}}],["它们也可以在不同的处理单元上并行执行",{"2":{"417":1}}],["它们可能只是4个函数",{"2":{"676":1}}],["它们可能会根据自身的设计目标和优化方向选择最适合的分词工具",{"2":{"568":1}}],["它们可能会产生大量开销",{"2":{"327":1}}],["它们可以被投影",{"2":{"689":1}}],["它们可以被用作短期记忆模块",{"2":{"231":1}}],["它们可以在不同的处理单元上并行执行",{"2":{"417":1}}],["它们可以更准确地回忆和使用这些信息",{"2":{"154":1}}],["它们的返回类型必须可以推导为同一类型",{"2":{"1905":1}}],["它们的作用是",{"2":{"1589":1}}],["它们的梯度将作为计算需要",{"2":{"1117":1}}],["它们的点积计算结果是1∗1+1∗1=2",{"2":{"692":1}}],["它们的形状与xx",{"2":{"343":1}}],["它们的目的都是把一个向量序列映射成另一个向量序列",{"2":{"267":1}}],["它们的信息一定会在某一步计算之中汇聚",{"2":{"249":1}}],["它们",{"2":{"246":8,"247":3}}],["它们之间的关系通过后续的变换来捕捉",{"2":{"173":1}}],["它们能够赋予",{"2":{"140":1}}],["它们关注这些主语token并将它们移动到残差流的末尾位置",{"2":{"122":1}}],["它们学习到的一些特征我们能够理解",{"2":{"9":1}}],["头文件和源文件",{"2":{"1916":1}}],["头文件中",{"2":{"1715":1,"1811":1,"1817":1,"1819":1,"1823":1,"1829":1,"1835":1,"1837":1,"1841":1}}],["头文件内容",{"2":{"1628":2}}],["头文件是以",{"2":{"1628":1}}],["头文件是什么",{"2":{"1628":1}}],["头文件的原理与使用",{"0":{"1628":1}}],["头文件的内容复制到当前文件中",{"2":{"1604":1}}],["头文件提供了一个功能强大的",{"2":{"1624":1}}],["头文件",{"2":{"1606":1,"1624":1,"1695":1,"1916":1}}],["头之间的差距",{"2":{"20":1}}],["头与层级的关系",{"2":{"20":1}}],["头与头之间存在较多的通用信息",{"2":{"19":1}}],["头太多太少都会变差",{"2":{"20":1}}],["头的数量增多会导致各个子空间变小",{"2":{"20":1}}],["头数越多",{"2":{"20":1}}],["头数越少",{"2":{"20":1}}],["头数目",{"2":{"20":1}}],["头",{"2":{"9":1,"130":1}}],["角度等概念",{"2":{"692":1}}],["角度做出了分析",{"2":{"490":1}}],["角度进行的探索",{"2":{"474":1}}],["角度",{"2":{"9":1}}],["0变1",{"2":{"2063":1}}],["0δy=y​1​​−y​0​​",{"2":{"2018":1}}],["0δx=x​1​​−x​0​​",{"2":{"2018":1}}],["0b0010",{"2":{"2060":1}}],["0b0001",{"2":{"2060":1}}],["0b0101",{"2":{"2060":1}}],["0b1101",{"2":{"1910":1}}],["0b10101010",{"2":{"1910":1}}],["0b",{"2":{"1910":2}}],["0=0",{"2":{"1393":1}}],["0的基础上放开了限制",{"2":{"1363":1}}],["0～7",{"2":{"1340":1}}],["0f​",{"2":{"2021":1}}],["0f",{"2":{"1215":2,"1607":1}}],["07",{"2":{"1404":1,"2042":1,"2046":1}}],["073889",{"2":{"1396":3}}],["078064=0",{"2":{"1395":3}}],["078064",{"2":{"1393":3}}],["0722",{"2":{"1098":1}}],["07909",{"2":{"574":1}}],["0或1",{"2":{"1087":1}}],["081",{"2":{"1246":1}}],["08",{"2":{"1087":2,"1404":1,"2043":1}}],["08821",{"2":{"638":1}}],["0e",{"2":{"1003":1}}],["062",{"2":{"1779":1}}],["06",{"2":{"1404":1}}],["06和45",{"2":{"739":1}}],["0635",{"2":{"702":1}}],["06146",{"2":{"429":1}}],["0~n",{"2":{"698":1}}],["0~v",{"2":{"698":1}}],["0~255",{"2":{"592":1}}],["0|>",{"2":{"571":1}}],["00",{"2":{"1323":1}}],["005",{"2":{"1235":1,"1236":1}}],["0011",{"2":{"1121":1}}],["001",{"2":{"1049":1,"1092":1,"1099":1,"1240":2,"1243":1,"1266":1,"1267":1,"1398":1,"2059":1}}],["008",{"2":{"638":2}}],["000步数",{"2":{"1363":4}}],["000步",{"2":{"1363":2}}],["00075",{"2":{"1240":1}}],["000",{"2":{"572":2,"1910":7}}],["000个单词或token",{"2":{"559":1}}],["0006",{"2":{"558":1,"679":1}}],["00050",{"2":{"1240":1}}],["0005",{"2":{"558":1,"679":1,"1235":1,"1236":1}}],["0004",{"2":{"558":1,"679":1}}],["0003",{"2":{"558":1,"679":1}}],["00025",{"2":{"1240":1}}],["0002",{"2":{"558":1,"679":1}}],["0001101000",{"2":{"1603":1}}],["0001",{"2":{"558":1,"679":1,"1218":1,"1221":1}}],["00000100",{"2":{"2059":1}}],["00000000",{"2":{"2059":1}}],["00000001",{"2":{"2059":2}}],["0000001111",{"2":{"722":1}}],["0000",{"2":{"315":8,"399":2,"558":1,"679":1,"702":6}}],["0098",{"2":{"399":3}}],["05灵敏度改进听起来很令人兴奋",{"2":{"1165":1}}],["0507",{"2":{"843":3}}],["05201",{"2":{"840":1}}],["05258",{"2":{"513":1}}],["05987",{"2":{"429":1}}],["05407",{"2":{"429":1}}],["05",{"2":{"399":23,"1087":2,"1235":2,"1236":2,"1237":2,"1238":2,"1404":1,"1874":1,"2048":3,"2049":1}}],["0表示不做平滑处理",{"2":{"399":1}}],["049019",{"2":{"1396":3}}],["04375",{"2":{"1238":1}}],["0403",{"2":{"1098":1}}],["0405",{"2":{"1098":1}}],["044715x​3​​",{"2":{"844":1}}],["044715x3",{"2":{"844":1}}],["044715",{"2":{"844":1}}],["04",{"2":{"399":5,"432":1,"1404":1,"1599":1}}],["04658",{"2":{"233":1}}],["022971",{"2":{"1396":3}}],["025",{"2":{"1237":4,"1238":1}}],["02",{"0":{"1480":1,"1617":1},"1":{"1481":1,"1482":1,"1618":1,"1619":1,"1620":1,"1621":1,"1622":1,"1623":1,"1624":1,"1625":1},"2":{"361":2,"700":1,"713":1,"1102":2,"1323":1,"1398":1,"1404":1,"1599":1,"2046":1}}],["03m",{"2":{"1363":1}}],["0375",{"2":{"1238":1}}],["03762v7",{"2":{"432":1}}],["03200中建议来实现移位的",{"2":{"1176":1}}],["03125",{"2":{"1238":1}}],["0316",{"0":{"663":1},"1":{"664":1,"665":1,"666":1,"667":1,"668":1,"669":1,"670":1,"671":1}}],["0315",{"0":{"663":1},"1":{"664":1,"665":1,"666":1,"667":1,"668":1,"669":1,"670":1,"671":1}}],["0309",{"0":{"657":1},"1":{"658":1,"659":1,"660":1,"661":1,"662":1},"2":{"702":1}}],["030866",{"2":{"1394":3}}],["0308",{"0":{"657":1},"1":{"658":1,"659":1,"660":1,"661":1,"662":1}}],["0301",{"2":{"656":1}}],["0302",{"0":{"656":1},"2":{"656":1}}],["0305",{"2":{"315":1}}],["03",{"0":{"1483":1,"2047":1},"1":{"1484":1,"1485":1,"1486":1,"1487":1,"1488":1,"1489":1,"2048":1,"2049":1},"2":{"233":1,"432":1,"1404":1,"1599":1,"1603":1,"2048":3,"2049":5}}],["0之外所有项都接近于0",{"2":{"191":1}}],["094534",{"2":{"1395":3}}],["097049∗0",{"2":{"1393":2}}],["097049",{"2":{"1393":4}}],["09",{"2":{"169":2,"1246":1,"1404":1,"2043":2}}],["0全部变为true",{"2":{"74":1}}],["0其实是做了一个三角阵的反转",{"2":{"74":1}}],["0101",{"2":{"2062":1}}],["0100",{"2":{"315":1}}],["0139069",{"2":{"1395":3}}],["011204",{"2":{"1394":3}}],["01−0",{"2":{"1389":2,"1393":4,"1394":2}}],["01255",{"2":{"361":1}}],["01286",{"2":{"156":1}}],["0151",{"2":{"315":2}}],["01有点",{"2":{"233":1}}],["01中lightning",{"2":{"233":1}}],["01是基于线性注意力机制",{"2":{"215":1}}],["01是第一个依赖线性注意力机制的大规模部署的模型",{"2":{"214":1}}],["01在注意力机制层面做了大胆的创新",{"2":{"214":1}}],["01",{"0":{"214":1,"1475":1,"1600":1},"1":{"215":1,"216":1,"217":1,"1476":1,"1477":1,"1478":1,"1479":1,"1601":1,"1602":1,"1603":1,"1604":1,"1605":1,"1606":1,"1607":1,"1608":1},"2":{"42":1,"157":1,"569":1,"735":1,"994":4,"995":3,"1045":1,"1102":1,"1202":1,"1205":1,"1221":1,"1231":2,"1241":1,"1242":1,"1296":1,"1303":2,"1323":1,"1386":1,"1389":1,"1393":2,"1394":1,"1398":3,"1440":1,"1599":1,"2086":1}}],["0",{"0":{"431":1,"432":1,"838":1,"850":1,"1219":1,"1628":1},"1":{"839":1,"840":1},"2":{"23":1,"36":4,"38":1,"39":1,"47":1,"65":2,"66":1,"67":1,"70":46,"74":2,"76":4,"82":2,"83":8,"84":8,"89":2,"99":7,"104":3,"113":1,"167":8,"169":10,"170":7,"178":1,"188":2,"191":1,"192":6,"199":1,"224":8,"267":9,"309":1,"314":2,"315":14,"334":3,"344":7,"361":1,"364":1,"372":1,"380":1,"382":2,"383":2,"384":5,"385":10,"394":1,"395":3,"396":1,"399":65,"401":1,"402":9,"422":1,"423":5,"424":7,"428":2,"450":2,"451":1,"472":1,"503":2,"519":3,"523":2,"529":2,"533":2,"557":1,"558":3,"560":1,"572":2,"590":3,"592":3,"595":1,"613":1,"681":2,"694":6,"698":1,"700":6,"702":17,"723":2,"807":3,"808":1,"809":1,"810":2,"819":1,"821":1,"822":1,"825":1,"833":10,"834":3,"838":3,"839":7,"840":11,"841":3,"842":3,"843":1,"845":1,"847":1,"865":2,"866":2,"867":1,"933":2,"976":1,"994":7,"995":6,"1000":3,"1003":4,"1004":4,"1006":6,"1029":1,"1045":1,"1049":2,"1072":1,"1075":2,"1076":3,"1086":1,"1087":34,"1092":6,"1094":1,"1097":2,"1098":23,"1099":6,"1102":18,"1110":1,"1115":2,"1116":9,"1177":1,"1178":1,"1180":1,"1189":2,"1190":2,"1191":3,"1192":4,"1193":4,"1195":1,"1202":2,"1205":5,"1211":8,"1215":12,"1216":4,"1217":3,"1218":4,"1222":1,"1226":2,"1227":1,"1233":1,"1234":1,"1235":4,"1236":4,"1237":7,"1238":7,"1240":9,"1243":3,"1244":2,"1246":6,"1247":6,"1250":2,"1254":4,"1257":2,"1266":1,"1273":1,"1283":4,"1295":6,"1298":2,"1303":3,"1306":3,"1307":2,"1308":4,"1309":1,"1323":3,"1328":3,"1329":1,"1330":4,"1331":3,"1332":1,"1345":4,"1350":2,"1360":3,"1363":5,"1386":2,"1388":5,"1389":12,"1393":14,"1394":8,"1395":16,"1396":39,"1398":32,"1440":1,"1460":3,"1474":1,"1481":5,"1513":1,"1547":1,"1590":1,"1594":5,"1606":2,"1607":4,"1608":5,"1616":2,"1619":1,"1620":3,"1621":4,"1623":11,"1624":7,"1625":5,"1629":1,"1633":3,"1634":7,"1639":2,"1640":1,"1641":1,"1645":1,"1646":2,"1647":3,"1648":1,"1649":1,"1650":1,"1651":2,"1653":1,"1654":1,"1655":1,"1656":1,"1659":1,"1660":1,"1661":1,"1662":1,"1663":1,"1665":1,"1667":3,"1668":4,"1670":2,"1671":2,"1672":1,"1673":2,"1674":1,"1675":2,"1676":1,"1677":3,"1680":2,"1683":4,"1684":2,"1685":2,"1687":2,"1688":2,"1691":2,"1693":7,"1694":2,"1695":6,"1696":1,"1698":1,"1699":1,"1700":4,"1701":1,"1704":20,"1705":8,"1706":4,"1707":1,"1708":2,"1709":1,"1710":5,"1712":6,"1713":10,"1714":3,"1715":6,"1718":1,"1719":11,"1720":4,"1721":4,"1722":4,"1723":1,"1724":3,"1725":4,"1728":6,"1729":14,"1736":2,"1737":2,"1738":2,"1739":1,"1741":1,"1742":1,"1743":1,"1744":1,"1746":2,"1747":1,"1749":1,"1750":1,"1751":1,"1752":1,"1754":1,"1755":1,"1756":1,"1761":2,"1762":3,"1763":1,"1772":2,"1774":7,"1778":1,"1779":3,"1784":1,"1788":3,"1789":7,"1791":4,"1792":7,"1797":3,"1799":2,"1800":2,"1801":2,"1802":1,"1803":1,"1805":1,"1806":1,"1807":1,"1811":2,"1813":1,"1814":1,"1816":1,"1817":1,"1820":2,"1821":3,"1824":1,"1825":2,"1829":2,"1831":1,"1832":1,"1834":1,"1835":1,"1838":2,"1839":3,"1842":1,"1843":2,"1849":1,"1853":1,"1857":1,"1861":1,"1866":1,"1867":1,"1868":2,"1869":1,"1874":7,"1883":2,"1887":7,"1891":2,"1897":1,"1902":5,"1905":1,"1906":1,"1907":1,"1908":1,"1909":1,"1910":1,"1911":1,"1912":1,"1914":3,"1921":1,"1922":3,"1923":2,"1924":1,"1925":1,"1926":1,"1927":2,"1928":1,"1929":3,"1930":1,"1933":2,"1972":1,"1999":3,"2003":1,"2004":2,"2005":1,"2006":6,"2007":1,"2008":2,"2014":1,"2018":11,"2019":2,"2038":1,"2059":6,"2060":2,"2061":2,"2062":3,"2063":1,"2086":2,"2153":2}}],["0^v",{"2":{"9":1}}],["0^k",{"2":{"9":1}}],["0^q",{"2":{"9":1}}],["0x7ffee3a1b9dc",{"2":{"1611":1}}],["0xff",{"0":{"47":1,"95":1,"156":1,"233":1,"292":1,"361":1,"387":1,"429":1,"513":1,"543":1,"638":1,"740":1,"768":1},"2":{"0":1,"49":1,"96":1,"157":1,"293":1,"741":1}}],["0x07",{"0":{"345":1,"510":1},"1":{"346":1,"347":1,"348":1,"349":1,"350":1,"351":1,"352":1,"353":1,"354":1,"355":1,"356":1,"357":1,"358":1,"359":1,"360":1,"511":1,"512":1},"2":{"293":1}}],["0x06",{"0":{"342":1,"474":1,"609":1},"1":{"343":1,"344":1,"475":1,"476":1,"477":1,"478":1,"479":1,"480":1,"481":1,"482":1,"483":1,"484":1,"485":1,"486":1,"487":1,"488":1,"489":1,"490":1,"491":1,"492":1,"493":1,"494":1,"495":1,"496":1,"497":1,"498":1,"499":1,"500":1,"501":1,"502":1,"503":1,"504":1,"505":1,"506":1,"507":1,"508":1,"509":1,"610":1,"611":1,"612":1,"613":1,"614":1,"615":1,"616":1,"617":1,"618":1,"619":1,"620":1,"621":1,"622":1,"623":1,"624":1,"625":1,"626":1,"627":1,"628":1,"629":1,"630":1,"631":1,"632":1,"633":1,"634":1,"635":1,"636":1,"637":1},"2":{"293":1}}],["0x05",{"0":{"86":1,"149":1,"336":1,"471":1,"596":1},"1":{"87":1,"88":1,"89":1,"90":1,"91":1,"92":1,"93":1,"94":1,"150":1,"151":1,"152":1,"153":1,"154":1,"155":1,"337":1,"338":1,"339":1,"340":1,"341":1,"472":1,"473":1,"597":1,"598":1,"599":1,"600":1,"601":1,"602":1,"603":1,"604":1,"605":1,"606":1,"607":1,"608":1},"2":{"49":1,"96":1,"293":1}}],["0x04",{"0":{"40":1,"77":1,"121":1,"202":1,"280":1,"317":1,"375":1,"461":1,"539":1,"574":1},"1":{"41":1,"42":1,"43":1,"44":1,"45":1,"46":1,"78":1,"79":1,"80":1,"81":1,"82":1,"83":1,"84":1,"85":1,"122":1,"123":1,"124":1,"125":1,"126":1,"127":1,"128":1,"129":1,"130":1,"131":1,"132":1,"133":1,"134":1,"135":1,"136":1,"137":1,"138":1,"139":1,"140":1,"141":1,"142":1,"143":1,"144":1,"145":1,"146":1,"147":1,"148":1,"203":1,"204":1,"205":1,"206":1,"207":1,"208":1,"209":1,"210":1,"211":1,"212":1,"213":1,"214":1,"215":1,"216":1,"217":1,"218":1,"219":1,"220":1,"221":1,"222":1,"223":1,"224":1,"225":1,"226":1,"227":1,"228":1,"229":1,"230":1,"231":1,"232":1,"281":1,"282":1,"283":1,"284":1,"285":1,"286":1,"287":1,"288":1,"289":1,"290":1,"291":1,"318":1,"319":1,"320":1,"321":1,"322":1,"323":1,"324":1,"325":1,"326":1,"327":1,"328":1,"329":1,"330":1,"331":1,"332":1,"333":1,"334":1,"335":1,"376":1,"377":1,"378":1,"379":1,"380":1,"381":1,"382":1,"383":1,"384":1,"385":1,"386":1,"462":1,"463":1,"464":1,"465":1,"466":1,"467":1,"468":1,"469":1,"470":1,"540":1,"541":1,"542":1,"575":1,"576":1,"577":1,"578":1,"579":1,"580":1,"581":1,"582":1,"583":1,"584":1,"585":1,"586":1,"587":1,"588":1,"589":1,"590":1,"591":1,"592":1,"593":1,"594":1,"595":1},"2":{"0":1,"49":1,"96":1,"157":1,"293":1}}],["0x03",{"0":{"22":1,"68":1,"115":1,"196":1,"257":1,"312":1,"371":1,"452":1,"534":1,"563":1,"710":1,"752":1},"1":{"23":1,"24":1,"25":1,"26":1,"27":1,"28":1,"29":1,"30":1,"31":1,"32":1,"33":1,"34":1,"35":1,"36":1,"37":1,"38":1,"39":1,"69":1,"70":1,"71":1,"72":1,"73":1,"74":1,"75":1,"76":1,"116":1,"117":1,"118":1,"119":1,"120":1,"197":1,"198":1,"199":1,"200":1,"201":1,"258":1,"259":1,"260":1,"261":1,"262":1,"263":1,"264":1,"265":1,"266":1,"267":1,"268":1,"269":1,"270":1,"271":1,"272":1,"273":1,"274":1,"275":1,"276":1,"277":1,"278":1,"279":1,"313":1,"314":1,"315":1,"316":1,"372":1,"373":1,"374":1,"453":1,"454":1,"455":1,"456":1,"457":1,"458":1,"459":1,"460":1,"535":1,"536":1,"537":1,"538":1,"564":1,"565":1,"566":1,"567":1,"568":1,"569":1,"570":1,"571":1,"572":1,"573":1,"711":1,"712":1,"713":1,"714":1,"715":1,"716":1,"717":1,"718":1,"719":1,"720":1,"721":1,"722":1,"723":1,"724":1,"725":1,"726":1,"727":1,"728":1,"729":1,"730":1,"731":1,"732":1,"733":1,"734":1,"735":1,"736":1,"737":1,"738":1,"739":1,"753":1,"754":1,"755":1,"756":1,"757":1,"758":1,"759":1,"760":1,"761":1,"762":1,"763":1,"764":1,"765":1,"766":1,"767":1},"2":{"0":1,"49":1,"96":1,"157":1,"293":1,"741":1}}],["0x02",{"0":{"6":1,"60":1,"112":1,"171":1,"243":1,"308":1,"365":1,"425":1,"447":1,"524":1,"556":1,"697":1,"747":1},"1":{"7":1,"8":1,"9":1,"10":1,"11":1,"12":1,"13":1,"14":1,"15":1,"16":1,"17":1,"18":1,"19":1,"20":1,"21":1,"61":1,"62":1,"63":1,"64":1,"65":1,"66":1,"67":1,"113":1,"114":1,"172":1,"173":1,"174":1,"175":1,"176":1,"177":1,"178":1,"179":1,"180":1,"181":1,"182":1,"183":1,"184":1,"185":1,"186":1,"187":1,"188":1,"189":1,"190":1,"191":1,"192":1,"193":1,"194":1,"195":1,"244":1,"245":1,"246":1,"247":1,"248":1,"249":1,"250":1,"251":1,"252":1,"253":1,"254":1,"255":1,"256":1,"309":1,"310":1,"311":1,"366":1,"367":1,"368":1,"369":1,"370":1,"426":1,"427":1,"428":1,"448":1,"449":1,"450":1,"451":1,"525":1,"526":1,"527":1,"528":1,"529":1,"530":1,"531":1,"532":1,"533":1,"557":1,"558":1,"559":1,"560":1,"561":1,"562":1,"698":1,"699":1,"700":1,"701":1,"702":1,"703":1,"704":1,"705":1,"706":1,"707":1,"708":1,"709":1,"748":1,"749":1,"750":1,"751":1},"2":{"0":1,"49":1,"96":1,"157":1,"293":1,"741":1}}],["0x01",{"0":{"2":1,"51":1,"98":1,"159":1,"236":1,"295":1,"364":1,"390":1,"433":1,"516":1,"546":1,"675":1,"743":1},"1":{"3":1,"4":1,"5":1,"52":1,"53":1,"54":1,"55":1,"56":1,"57":1,"58":1,"59":1,"99":1,"100":1,"101":1,"102":1,"103":1,"104":1,"105":1,"106":1,"107":1,"108":1,"109":1,"110":1,"111":1,"160":1,"161":1,"162":1,"163":1,"164":1,"165":1,"166":1,"167":1,"168":1,"169":1,"170":1,"237":1,"238":1,"239":1,"240":1,"241":1,"242":1,"296":1,"297":1,"298":1,"299":1,"300":1,"301":1,"302":1,"303":1,"304":1,"305":1,"306":1,"307":1,"391":1,"392":1,"393":1,"394":1,"395":1,"396":1,"397":1,"398":1,"399":1,"400":1,"401":1,"402":1,"403":1,"404":1,"405":1,"406":1,"407":1,"408":1,"409":1,"410":1,"411":1,"412":1,"413":1,"414":1,"415":1,"416":1,"417":1,"418":1,"419":1,"420":1,"421":1,"422":1,"423":1,"424":1,"434":1,"435":1,"436":1,"437":1,"438":1,"439":1,"440":1,"441":1,"442":1,"443":1,"444":1,"445":1,"446":1,"517":1,"518":1,"519":1,"520":1,"521":1,"522":1,"523":1,"547":1,"548":1,"549":1,"550":1,"551":1,"552":1,"553":1,"554":1,"555":1,"676":1,"677":1,"678":1,"679":1,"680":1,"681":1,"682":1,"683":1,"684":1,"685":1,"686":1,"687":1,"688":1,"689":1,"690":1,"691":1,"692":1,"693":1,"694":1,"695":1,"696":1,"744":1,"745":1,"746":1},"2":{"0":1,"49":1,"96":1,"157":1,"293":1,"741":1}}],["0x0000021a26983df0>",{"2":{"558":1,"679":1}}],["0x00",{"0":{"1":1,"50":1,"97":1,"158":1,"235":1,"294":1,"363":1,"389":1,"430":1,"515":1,"545":1,"674":1,"742":1},"1":{"431":1,"432":1},"2":{"0":1,"49":1,"96":1,"157":1,"293":1,"741":1}}],["的饼干",{"2":{"2157":1}}],["的思维方式",{"2":{"2104":1}}],["的思想来理解",{"2":{"1645":1}}],["的思想",{"0":{"920":1},"2":{"204":1,"462":1,"634":1,"2121":1}}],["的使用",{"0":{"2062":1}}],["的使用方法",{"2":{"1647":1}}],["的途中当你正真有",{"2":{"2054":1}}],["的孔子这里说的很重要",{"2":{"2054":1}}],["的生命周期内有效",{"2":{"1929":1}}],["的捕获必须是常量表达式",{"2":{"1924":1}}],["的接口",{"2":{"1867":1}}],["的三种继承方式",{"2":{"1846":1}}],["的三个",{"2":{"722":1,"740":1}}],["的常用方法和格式化输出",{"2":{"1826":1,"1844":1}}],["的常见的输出",{"2":{"325":1}}],["的偏移量",{"2":{"1821":1,"1839":1}}],["的偏置是预设好",{"2":{"765":1}}],["的偏置是可训练参数",{"2":{"765":1}}],["的友元",{"2":{"1781":3,"1786":2}}],["的友元类",{"2":{"1778":1,"1781":2}}],["的修改会影响到实参",{"2":{"1729":1}}],["的异常处理机制是一种强大的错误处理工具",{"2":{"1765":1}}],["的异常输入",{"2":{"411":1}}],["的异同",{"2":{"1728":1}}],["的默认成员访问权限是",{"2":{"1728":3}}],["的成员",{"2":{"1728":1}}],["的成员赋值",{"2":{"1728":1}}],["的年龄",{"2":{"1725":1}}],["的元素",{"2":{"1720":2,"1721":1,"1724":3,"1725":3}}],["的整型数组",{"2":{"1716":1}}],["的容量不足以容纳新元素时",{"2":{"1714":1}}],["的迭代器",{"2":{"1713":1}}],["的子串初始化",{"2":{"1713":1}}],["的子类",{"2":{"1087":1,"1786":1}}],["的强大特性",{"2":{"1709":1}}],["的建议性",{"2":{"1709":1}}],["的代码直接替换到这里",{"2":{"1709":2}}],["的矩形面积",{"2":{"1708":1}}],["的矩阵作为位置向量",{"2":{"749":1}}],["的矩阵",{"2":{"36":2,"74":1,"83":1,"315":1,"341":1,"460":1,"713":1}}],["的首地址",{"2":{"1705":1}}],["的首选激活函数",{"2":{"103":1}}],["的必要性",{"2":{"1705":1}}],["的字符串",{"2":{"1713":1}}],["的字符",{"2":{"1704":1}}],["的字向量必然几乎是一样的",{"2":{"714":1}}],["的真正意义在于认识到",{"2":{"1678":1}}],["的各种重载",{"2":{"1678":1}}],["的各个",{"2":{"326":1,"337":1}}],["的深入理解",{"0":{"1678":1}}],["的深度有",{"2":{"334":1}}],["的析构函数会被调用",{"2":{"1676":1}}],["的类型也被推导为",{"2":{"1897":1}}],["的类型为",{"2":{"1897":1}}],["的类型可以看作是指向",{"2":{"1705":1}}],["的类",{"2":{"1674":1,"1678":1,"1696":1}}],["的连续内存",{"2":{"1668":1}}],["的连接构建关系",{"2":{"689":1}}],["的用法",{"2":{"1666":2,"1678":1,"1826":1,"1844":1}}],["的用时",{"2":{"1651":1}}],["的继承方式",{"2":{"1664":1}}],["的函数",{"2":{"1657":1,"1729":2}}],["的函数并进行截断的做法",{"2":{"1340":1}}],["的倍数时会进行填充",{"2":{"1653":1}}],["的请求时",{"2":{"1647":1}}],["的格式打印日期",{"2":{"1642":1}}],["的幂次方",{"2":{"1630":2}}],["的幂来尝试",{"2":{"1132":1}}],["的截断行为",{"2":{"1630":1}}],["的阶乘是",{"2":{"1646":1}}],["的阶乘计算完成",{"2":{"1646":1}}],["的阶乘为",{"2":{"1621":1}}],["的阶乘",{"2":{"1621":1,"1646":1}}],["的逻辑判断",{"2":{"1619":1}}],["的任何操作都会直接影响到",{"2":{"1612":1}}],["的任意一个子节点",{"2":{"184":1}}],["的引用",{"2":{"1612":1}}],["的地址",{"2":{"1611":2,"1648":4,"1704":1,"1705":2}}],["的地址赋给",{"2":{"1611":1}}],["的地方就是我们要进行处理的地方",{"2":{"62":1}}],["的权衡",{"2":{"1611":1}}],["的权重是一个共享的线性变换矩阵",{"2":{"702":1}}],["的权重就显然要略低",{"2":{"169":1}}],["的精髓",{"2":{"1610":1}}],["的正数范围是",{"2":{"1607":1}}],["的正态分布",{"2":{"994":1,"995":1}}],["的个位数字",{"2":{"1607":1}}],["的集成开发环境",{"2":{"1729":1}}],["的集成使得管理复杂的",{"2":{"1605":1}}],["的集合",{"2":{"1208":1}}],["的过程",{"2":{"1604":1}}],["的过程还是一样",{"2":{"9":1}}],["的解释执行不同",{"2":{"1604":1}}],["的解码过程中",{"2":{"82":1}}],["的早期稳定版本",{"2":{"1603":1}}],["的高效性",{"2":{"1603":1}}],["的高效计算",{"0":{"1344":1}}],["的发展历程",{"2":{"1603":1}}],["的发展历程和它所运行的平台",{"2":{"1603":1}}],["的发展与平台",{"0":{"1603":1}}],["的发生分数",{"2":{"1329":1}}],["的底层",{"2":{"1602":1}}],["的简单项目",{"2":{"1966":1}}],["的简单神经网络",{"2":{"320":1}}],["的简洁语法和丰富的库使其在这些领域非常受欢迎",{"2":{"1602":1}}],["的学习能够锻炼我们对内存管理",{"2":{"1602":1}}],["的学习能力",{"2":{"1014":1}}],["的学习之旅",{"2":{"1601":1}}],["的世界",{"0":{"1601":1}}],["的启航",{"0":{"1600":1},"1":{"1601":1,"1602":1,"1603":1,"1604":1,"1605":1,"1606":1,"1607":1,"1608":1}}],["的和为",{"2":{"1620":1,"1621":1}}],["的和",{"2":{"1594":1,"1620":1,"1621":1}}],["的总和",{"2":{"1594":1}}],["的具体路径或链接细节",{"2":{"1589":1}}],["的目录",{"2":{"1509":1}}],["的目标是细化每个",{"2":{"461":1}}],["的目标词来",{"2":{"58":1}}],["的主目录",{"2":{"1506":1}}],["的主要区别",{"2":{"1728":1}}],["的主要作用是将输入序列编码成一个上下文向量",{"2":{"888":1}}],["的主要部分",{"2":{"613":1}}],["的主要差别在于使用差分注意力替换了传统的",{"2":{"501":1}}],["的主要原因是post",{"2":{"333":1}}],["的主要功能是通过内存检索预计算的向量表示来替代传统的矩阵乘法",{"2":{"153":1}}],["的程序片段",{"2":{"1413":1}}],["的程度不一定一样",{"2":{"172":1}}],["的label",{"2":{"1386":1}}],["的负方向进行函数空间的梯度下降",{"2":{"1377":1}}],["的负半轴为软饱和区",{"2":{"843":1}}],["的负半轴下功夫改造",{"2":{"842":1}}],["的变化或偏离程度",{"2":{"1377":1}}],["的变换矩阵",{"2":{"761":1}}],["的规律呢",{"2":{"1372":1}}],["的规模一致",{"2":{"701":1}}],["的两倍",{"2":{"1607":1}}],["的两个",{"2":{"1364":1}}],["的两种类型",{"0":{"642":1}}],["的选择上",{"2":{"1344":1}}],["的形式如下",{"2":{"1342":1}}],["的形状为",{"2":{"1004":1}}],["的形状如何推导",{"2":{"775":1}}],["的形状有时是三维张量",{"2":{"34":1}}],["的是",{"2":{"1340":1}}],["的是两个单词的embedding处理部分",{"2":{"519":1}}],["的单词",{"2":{"1330":1}}],["的索引减去",{"2":{"1633":1}}],["的索引",{"2":{"1330":1,"1633":1}}],["的路径在前一个位置的标签索引",{"2":{"1330":1}}],["的产生",{"2":{"1322":1}}],["的产生过程有关",{"2":{"707":1}}],["的条件下",{"2":{"1322":1}}],["的条件概率分布",{"2":{"1322":1}}],["的条件概率",{"2":{"125":1}}],["的能力",{"2":{"1316":1}}],["的语料",{"2":{"1315":1}}],["的迁移学习方法",{"2":{"1312":1}}],["的东西",{"2":{"1275":1}}],["的保存和加载",{"0":{"1255":1}}],["的余弦退火学习率",{"2":{"1244":1}}],["的浅拷贝",{"2":{"1227":1}}],["的浅层结构",{"2":{"715":1}}],["的转换构造函数",{"2":{"1685":3}}],["的转换",{"2":{"1227":1}}],["的前一半",{"2":{"2056":1}}],["的前置钩子函数",{"2":{"1214":1,"1227":1}}],["的前生今世",{"2":{"156":1,"280":1,"292":1}}],["的将所有参数和缓冲区转换为",{"2":{"1214":4}}],["的缓冲区",{"2":{"1208":1}}],["的更新剪裁为",{"2":{"1184":1}}],["的更新量是正比于梯度的",{"2":{"333":1}}],["的验证误差的概率约为",{"2":{"1177":1}}],["的实际效果等价于在编译命令中手动添加包含路径",{"2":{"1589":1}}],["的实验来在最佳超参数点上获得最终模型",{"2":{"1157":1}}],["的实现通常经过优化",{"2":{"1715":1}}],["的实现如下",{"2":{"346":1}}],["的实现",{"2":{"36":1,"83":1,"429":1}}],["的初步猜测",{"2":{"1156":1}}],["的初始值",{"0":{"1156":1}}],["的初始化",{"2":{"1109":1,"1227":1}}],["的初始化范围",{"2":{"347":1}}],["的搜索空间的探索",{"2":{"1153":1}}],["的试验",{"2":{"1147":1}}],["的超参数应该如何调整",{"0":{"1174":1},"2":{"1130":2}}],["的其他重要问题",{"2":{"1127":1}}],["的运行统计信息",{"2":{"1122":1}}],["的运算完全相同",{"2":{"36":1}}],["的叶张量的中间结果",{"2":{"1117":1}}],["的叶张量才会将梯度累积到其",{"2":{"1117":1}}],["的工作原理和记录操作的方式",{"2":{"1112":1}}],["的工作流程",{"0":{"893":1},"1":{"894":1,"895":1,"896":1,"897":1,"898":1}}],["的别名",{"2":{"1087":3,"1214":1,"1912":1}}],["的value",{"2":{"1086":1}}],["的pre",{"2":{"1214":1}}],["的production",{"2":{"1157":2}}],["的prompt",{"2":{"485":1}}],["的pytorch",{"2":{"1086":1}}],["的共内存和非共内存的通信模式不同",{"2":{"1568":1}}],["的共轭转置视图",{"2":{"1082":1}}],["的共复杂度类之间的关系图",{"2":{"480":1}}],["的topk",{"2":{"1330":1}}],["的token都能看到句子内所有token",{"2":{"731":1}}],["的token在这些大模型上普遍存在",{"2":{"562":1}}],["的token会导致模型产生异常输出",{"2":{"562":1}}],["的token",{"2":{"128":1}}],["的tensor",{"2":{"1073":1}}],["的估计",{"2":{"1059":1}}],["的标准输入输出流",{"2":{"1666":1}}],["的标准形式",{"2":{"1049":1}}],["的标量",{"2":{"762":1,"1340":1}}],["的缩写",{"2":{"1025":1}}],["的丢掉有效特征",{"2":{"1019":1}}],["的节点",{"2":{"986":3}}],["的了解",{"2":{"974":1}}],["的特点",{"2":{"1800":1}}],["的特定硬件单元",{"2":{"973":1}}],["的特征l",{"2":{"415":1}}],["的特征映射",{"2":{"213":1}}],["的特征",{"2":{"137":1}}],["的改进",{"0":{"972":1}}],["的不足之处",{"0":{"967":1}}],["的压缩潜在向量",{"2":{"957":1}}],["的压缩算法",{"2":{"637":1}}],["的列进行耦合",{"2":{"943":1,"961":1}}],["的列表",{"2":{"557":1,"592":1}}],["的flops数量和hbm访问数量",{"2":{"941":1,"960":1}}],["的feature",{"2":{"341":1}}],["的dropout操作",{"2":{"941":1,"960":1}}],["的掩码操作或应用于",{"2":{"941":1,"960":1}}],["的自回归语言建模和来自",{"2":{"1315":1}}],["的自回归属性",{"2":{"977":1}}],["的自动推理机制",{"0":{"1093":1}}],["的自注意力层",{"2":{"937":1,"953":1}}],["的自然语言提示",{"2":{"145":1}}],["的query",{"2":{"926":1}}],["的获取写成矩阵形式",{"2":{"923":1}}],["的获取过程",{"2":{"923":1}}],["的速度更快",{"2":{"921":1}}],["的已知输出",{"2":{"915":1}}],["的损失函数计算",{"0":{"1325":1},"1":{"1326":1}}],["的损失函数",{"0":{"899":1}}],["的短期记忆问题",{"2":{"861":1}}],["的短语表示主要在神经网络的较低层捕捉短语级别的信息",{"2":{"437":1}}],["的神经网络",{"2":{"850":1}}],["的神经元从网络中删除",{"2":{"1017":1}}],["的神经元内有大量可解释的结构",{"2":{"477":1}}],["的神经元中减去",{"2":{"485":1}}],["的神经元中",{"2":{"148":1}}],["的神经元",{"2":{"135":1}}],["的研究人员使用自动搜索",{"2":{"845":1}}],["的研究旨在提出一个开创性的解决方案来实现这一愿景并解决这些",{"2":{"222":1}}],["的降低",{"2":{"844":1}}],["的激活信息规整为零",{"2":{"844":1}}],["的情况",{"2":{"840":1,"842":1,"970":1,"1164":1}}],["的情况下进行网络学习",{"2":{"495":1}}],["的情况下",{"2":{"230":1,"1120":1,"1315":1}}],["的情况下都会把最大的权值分配给左边或者右边的一个词",{"2":{"20":1}}],["的提出正是为了解决这一问题",{"2":{"838":1}}],["的逆运算",{"2":{"825":1}}],["的区别",{"0":{"1203":1},"2":{"808":1,"1704":1,"1711":1}}],["的index",{"2":{"808":1,"1330":1}}],["的空洞卷积",{"2":{"778":1}}],["的空间内部",{"2":{"499":1}}],["的卷积核作纵向扫描计算",{"2":{"777":1}}],["的卷积核作横向扫描计算",{"2":{"777":1}}],["的卷积核分解如下图",{"2":{"777":1}}],["的卷积核来提取特征",{"2":{"116":1}}],["的卷积操作",{"2":{"776":1}}],["的卷积",{"2":{"776":1}}],["的性能",{"2":{"1315":1}}],["的性能相差相近",{"2":{"647":1,"924":1}}],["的性质",{"2":{"772":1}}],["的光滑局部注意力技巧",{"2":{"765":1}}],["的值传递给它",{"2":{"1729":1}}],["的值不再有效",{"2":{"1728":1}}],["的值是",{"2":{"1728":1,"1729":1}}],["的值是很多个",{"2":{"1004":1}}],["的值被赋值",{"2":{"1630":1}}],["的值为",{"2":{"1630":1,"1633":1,"1673":1,"1705":1}}],["的值变成了",{"2":{"1612":1}}],["的值输出到控制台",{"2":{"1607":1}}],["的值",{"2":{"1242":1,"1594":1,"1611":4,"1614":3,"1620":1,"1630":2,"1684":1,"1705":3}}],["的值相乘",{"2":{"867":1}}],["的值表示要保留",{"2":{"865":1}}],["的值表示要遗忘",{"2":{"865":1}}],["的值都有所不同",{"2":{"765":1}}],["的值比较小的时候",{"2":{"647":1,"924":1}}],["的距离越远",{"2":{"765":1}}],["的可迭代对象",{"2":{"1225":1}}],["的可用性分析",{"2":{"740":1}}],["的可能性",{"2":{"525":1}}],["的同时",{"2":{"714":1}}],["的上下文就是",{"2":{"714":1}}],["的一个别名",{"2":{"1612":1}}],["的一个线性变换",{"2":{"117":1}}],["的一些基本特性",{"2":{"1607":1}}],["的一些描述",{"2":{"713":1}}],["的一种转移",{"2":{"1324":1}}],["的一部分",{"2":{"1211":1}}],["的一步上结束",{"2":{"1149":1}}],["的一般取值为0",{"2":{"1029":1}}],["的一项要求是已经生成的单词的位置编码",{"2":{"746":1}}],["的细节",{"2":{"702":1}}],["的平方根可以将方差恢复到1",{"2":{"701":1}}],["的平方根可以使得点积的范围更加合理",{"2":{"701":1}}],["的二维矩阵",{"2":{"700":1,"709":1}}],["的二进制向量来唯一表示某个单词",{"2":{"681":1}}],["的检索方法通过词向量来表示文本",{"2":{"696":1}}],["的量化值",{"2":{"689":1}}],["的有序列表",{"2":{"680":1}}],["的有效方法",{"2":{"477":1}}],["的处理和任务执行奠定了基础",{"2":{"674":1}}],["的那些会保存",{"2":{"668":1}}],["的基础",{"2":{"1603":1}}],["的基线架构是base",{"2":{"632":1}}],["的基本语法和常用技巧",{"2":{"1918":1}}],["的基本用法",{"2":{"1666":1,"1678":1}}],["的基本使用方法",{"2":{"1648":1,"1997":1}}],["的基本数据类型",{"2":{"1610":1}}],["的基本数据类型及其特性是编写有效和高效",{"2":{"1607":1}}],["的基本架构",{"2":{"629":1}}],["的基本实现",{"2":{"122":1}}],["的质的飞跃",{"2":{"627":1}}],["的模板类",{"2":{"1726":1}}],["的模式不能在训练时使用吗",{"2":{"895":1}}],["的模式",{"2":{"626":1,"895":1}}],["的模型",{"2":{"334":1,"335":1}}],["的观念",{"2":{"624":1}}],["的交互",{"2":{"624":1}}],["的灵活性",{"2":{"616":1}}],["的轻量级模型",{"2":{"614":2}}],["的stride",{"2":{"1086":1}}],["的storage",{"2":{"1086":1}}],["的sota",{"2":{"911":1}}],["的subword",{"2":{"602":1}}],["的size",{"2":{"185":1}}],["的词嵌入",{"2":{"715":1}}],["的词库则是从大到小变化",{"2":{"601":1}}],["的词表参数由于词表尺寸小于预测的最优值而处于次优状态",{"2":{"561":1}}],["的词表参数和预测最优词表参数的关系",{"2":{"561":1}}],["的词表大小对模型性能的影响",{"2":{"561":1}}],["的相对距离来惩罚",{"2":{"765":1}}],["的相对大小",{"2":{"176":1}}],["的相似程度的一种方式",{"2":{"691":1}}],["的相同点是",{"2":{"601":1}}],["的编码向量的第",{"2":{"1336":1}}],["的编码过程是将单词分割成词表中的token的过程",{"2":{"587":1}}],["的编码更重要",{"2":{"277":1}}],["的意义",{"2":{"1602":1,"1611":1}}],["的意义的",{"2":{"689":1}}],["的意义在于",{"2":{"579":1}}],["的意思",{"2":{"10":1,"576":1}}],["的频率为5",{"2":{"579":1}}],["的最低版本",{"2":{"1971":1}}],["的最大学习率进行稳定训练",{"2":{"1178":1}}],["的最大问题是如何调整学习率衰减计划",{"2":{"1157":1}}],["的最大区别是",{"2":{"601":1}}],["的最佳权重衰减值的isolation图",{"2":{"1150":1}}],["的最佳值时",{"2":{"1130":1}}],["的最优词表大小应该是至少",{"2":{"561":1}}],["的最终输出可以理解为激活值的加权和",{"2":{"126":1}}],["的数组的指针",{"2":{"1705":1}}],["的数组",{"2":{"1623":1,"1705":1}}],["的数值",{"2":{"701":1}}],["的数量",{"2":{"621":1}}],["的数目降到最低",{"2":{"567":1}}],["的数字每个数都有唯一的",{"2":{"595":1}}],["的数字表示",{"2":{"595":1}}],["的数字",{"2":{"560":1}}],["的数据部分置为0",{"2":{"841":1}}],["的数据导数值很小",{"2":{"839":1}}],["的数据",{"2":{"423":1}}],["的数据来计算一阶统计量和二阶统计量",{"2":{"316":1}}],["的机制",{"2":{"553":1}}],["的春天还是冬天",{"2":{"543":1}}],["的推断结果是明确的",{"2":{"1615":1}}],["的推断过程时",{"2":{"425":1}}],["的推理过程中加快生成速度并降低计算成本",{"2":{"542":1}}],["的描述是形象的",{"2":{"536":1}}],["的理解",{"2":{"536":1,"2053":1}}],["的翻译时",{"2":{"536":1}}],["的拼接",{"2":{"510":1,"1624":1}}],["的几个特定情况下的结果",{"2":{"507":1}}],["的几个分析可以为我们揭示部分机理",{"2":{"306":1}}],["的指向不能修改",{"2":{"1614":1}}],["的指针变量",{"2":{"1611":1}}],["的指针",{"2":{"1611":3,"1705":3}}],["的指导下能够计算出",{"2":{"504":1}}],["的指数缩放",{"2":{"212":1}}],["的映射",{"2":{"499":1}}],["的角度提出了对transformer架构的新理解",{"2":{"498":1}}],["的离散邻接法",{"2":{"497":1}}],["的科学家意识到",{"2":{"488":1}}],["的脑细胞组成的生物网络可以执行与",{"2":{"487":1}}],["的grad的颜色与前向传播输入相同",{"2":{"485":1}}],["的跨度",{"2":{"485":2}}],["的案例",{"2":{"479":1}}],["的范数来解决",{"2":{"478":1}}],["的范畴了",{"2":{"334":1}}],["的原则",{"2":{"1671":1}}],["的原理图如下",{"2":{"892":1}}],["的原始大小512",{"2":{"466":1}}],["的原因之一",{"2":{"108":1}}],["的word",{"2":{"463":1}}],["的返回结果",{"2":{"449":2}}],["的大小减小",{"2":{"937":1,"953":1}}],["的大小动态的控制两个注意图之间的权衡程度",{"2":{"502":1}}],["的大小",{"2":{"448":1,"700":1,"1705":1}}],["的层",{"2":{"437":1}}],["的attention结果",{"2":{"419":1}}],["的ai模型",{"2":{"233":1}}],["的序号是2",{"2":{"379":1}}],["的序号是1",{"2":{"379":1}}],["的扩展",{"2":{"370":1,"1052":1}}],["的逐元素操作",{"2":{"358":1,"359":1}}],["的步骤包括",{"2":{"357":1}}],["的技巧它变成一个参数的函数",{"2":{"344":1}}],["的部分",{"2":{"343":1}}],["的部分经过softmax处理都变成了0",{"2":{"71":1}}],["的例子",{"2":{"341":1}}],["的例子中",{"2":{"316":1,"1705":1}}],["的误差会迅速增加",{"2":{"338":1}}],["的折中",{"2":{"338":1}}],["的效果已经不是",{"2":{"334":1}}],["的梯度定义为",{"2":{"1440":1}}],["的梯度会被计算",{"2":{"1116":1}}],["的梯度会计算",{"2":{"661":1,"1104":1}}],["的梯度计算",{"2":{"1116":1,"1393":1}}],["的梯度",{"0":{"1099":1,"1106":1,"1446":1,"1447":1,"1448":1},"2":{"1099":1,"1227":1,"1393":1,"1394":1}}],["的梯度很小",{"2":{"994":1}}],["的梯度近似相等",{"2":{"402":1}}],["的梯度敏感空间来加速训练",{"2":{"334":1}}],["的梯度基本上也能得以保留",{"2":{"304":1}}],["的残差虽然被严重削弱",{"2":{"333":1}}],["的优化算法",{"2":{"861":1}}],["的优化路径从超球面上对应其输入嵌入向量的点开始",{"2":{"352":1}}],["的优点",{"2":{"332":1}}],["的优势",{"2":{"273":1,"1714":1}}],["的非线性主要存在于尺度缩放操作当中",{"2":{"320":1}}],["的非线性进行了数学证明",{"2":{"320":1}}],["的万能近似分类能力",{"2":{"320":1}}],["的所有",{"2":{"318":1,"323":1,"326":1}}],["的均值和标准差",{"2":{"337":1}}],["的均值和方差会影响到最终生成图像的风格",{"2":{"337":1}}],["的均值和方差",{"2":{"322":1}}],["的均值",{"2":{"315":1}}],["的操作步骤和打开模式",{"2":{"1826":1,"1844":1}}],["的操作才会在反向图中记录",{"2":{"1117":1}}],["的操作拆分至",{"2":{"765":1}}],["的操作",{"2":{"312":1,"316":1,"340":1,"745":1,"970":1}}],["的操作和",{"2":{"277":1}}],["的分词方法",{"2":{"595":1}}],["的分布在",{"2":{"1004":1}}],["的分布",{"2":{"310":1}}],["的分量",{"2":{"122":1}}],["的核心算法往往涉及大量的计算",{"2":{"1602":1}}],["的核心创新是",{"2":{"621":1}}],["的核心操作是对网络隐藏层状态的导数进行参数化",{"2":{"496":1}}],["的核心思想是引入一个",{"2":{"300":1}}],["的核心在于其",{"2":{"153":1}}],["的概念进行输入输出操作",{"2":{"1673":1}}],["的概念表示在训练过程中通过逐渐减小学习率来改变模型的行为和性能",{"2":{"1242":1}}],["的概念",{"2":{"284":1,"878":1,"1079":1,"1478":1,"1647":1,"1693":1}}],["的概率随机drop掉text",{"2":{"1363":1}}],["的概率比p",{"2":{"598":1}}],["的概率分布",{"2":{"473":1}}],["的概率都应该是1",{"2":{"398":1}}],["的概率应该是1",{"2":{"398":1}}],["的概率",{"2":{"83":1}}],["的私人邮件",{"2":{"284":1}}],["的开篇之作",{"2":{"281":1}}],["的局部假设",{"2":{"274":1}}],["的味道了",{"2":{"233":1}}],["的第二个字符修改为",{"2":{"1624":1}}],["的第三个元素赋值为",{"2":{"1623":1}}],["的第三个变体",{"2":{"231":1}}],["的第",{"2":{"1110":1}}],["的第几个输出",{"2":{"1110":1}}],["的第一个元素",{"2":{"1623":1,"1704":1}}],["的第一个",{"2":{"1364":1}}],["的第一个字节比预测",{"2":{"613":1}}],["的第一个变体",{"2":{"231":1}}],["的第一步可以通过开发专门的专家模块来实现",{"2":{"222":1}}],["的架构设计如上图",{"2":{"231":1}}],["的架构设计如上图所示",{"2":{"231":1}}],["的事件",{"2":{"228":1,"230":1}}],["的公式",{"2":{"217":1}}],["的公式为ffn",{"2":{"125":1}}],["的每一个维度",{"2":{"746":1}}],["的每个时间步中",{"2":{"889":1}}],["的每个维度上",{"2":{"766":1}}],["的每个索引都有独立的嵌入向量",{"2":{"702":1}}],["的每个词与目标端",{"2":{"649":1,"931":1}}],["的每个词进行处理",{"2":{"241":1}}],["的每",{"2":{"217":1}}],["的状态进行加权平均",{"2":{"284":1}}],["的状态",{"2":{"214":1,"1227":1}}],["的极性将它们分开",{"2":{"213":1}}],["的内容替换字符串流的当前内容",{"2":{"1824":1,"1842":1}}],["的内容",{"2":{"1704":2}}],["的内存管理",{"2":{"1713":1}}],["的内存地址",{"2":{"1611":2}}],["的内存要求",{"2":{"1317":1}}],["的内存开销",{"2":{"983":1}}],["的内存",{"2":{"941":1,"960":1,"981":1,"1132":1,"1668":1,"1694":1}}],["的内存查找表设计支持动态扩展",{"2":{"153":1}}],["的内积中",{"2":{"213":1}}],["的表示",{"2":{"461":1}}],["的表示形式",{"2":{"155":1}}],["的表达能力减弱和判别力降低",{"2":{"212":1}}],["的全局transformer参数",{"2":{"611":1}}],["的全连接层比其余层使用更多的叠加",{"2":{"477":1}}],["的全连接矩阵",{"2":{"210":1}}],["的全0张量和mask进行水平拼接",{"2":{"201":1}}],["的头数",{"2":{"201":3}}],["的头进行分类",{"2":{"20":1}}],["的张量",{"2":{"201":1,"1117":2}}],["的加权平均",{"2":{"198":1}}],["的加权和",{"2":{"63":1}}],["的向量",{"2":{"192":1,"341":1,"759":1,"1339":1}}],["的向量表示",{"2":{"145":1,"700":1}}],["的话",{"2":{"188":1,"1151":1,"1276":1}}],["的训练损失曲线显示不稳定",{"2":{"1179":1}}],["的训练损失曲线",{"2":{"1179":1}}],["的训练可能并不充分",{"2":{"176":1}}],["的训练步骤",{"2":{"83":1}}],["的存在",{"2":{"176":1,"326":1}}],["的带有词关联性的表示为",{"2":{"170":1}}],["的准确含义",{"2":{"167":1,"259":1}}],["的点积attention性能好",{"2":{"647":1,"924":1}}],["的点积",{"2":{"158":1}}],["的潜力",{"2":{"155":1}}],["的作用",{"0":{"1369":1},"1":{"1370":1,"1371":1,"1372":1,"1373":1,"1374":1,"1375":1}}],["的作用就是其可以隔离开不同的单词",{"2":{"588":1}}],["的作用就是把原始数据集中的语料按照一定的规则分开",{"2":{"549":1}}],["的作用是增加一个维度",{"2":{"380":1}}],["的作用是增加一维度",{"2":{"66":1}}],["的作用是",{"2":{"145":1}}],["的作者称这些无关的上下文为注意力噪音",{"2":{"500":1}}],["的作者认为mlps是当今神经网络的基础构建模块",{"2":{"155":1}}],["的示意图",{"2":{"153":1}}],["的组合",{"2":{"148":1,"155":1,"485":1}}],["的反向传播过程",{"2":{"148":1,"484":1}}],["的线性注意力机制解决了因果模型在计算单向注意力时",{"2":{"216":1}}],["的线性注意力模型无法保持与原始",{"2":{"212":1}}],["的线性scaling",{"2":{"147":1}}],["的线性投影",{"2":{"36":1}}],["的隐层空间",{"2":{"698":1}}],["的隐状态转换为词汇概率来展示模型在生成过程中的表现",{"2":{"482":1}}],["的隐状态转换为词汇概率来展示了模型在生成过程中的表现",{"2":{"147":1}}],["的隐式函数",{"2":{"122":1}}],["的对比",{"0":{"1602":1}}],["的对",{"2":{"145":1}}],["的新特性",{"2":{"1932":1}}],["的新理解",{"2":{"314":1}}],["的新型",{"2":{"151":1}}],["的新属性",{"2":{"145":1}}],["的新架构",{"2":{"42":1}}],["的属性全解",{"0":{"1082":1}}],["的属性",{"2":{"145":1}}],["的文件",{"2":{"1532":1,"1543":1}}],["的文本",{"2":{"145":1,"1341":1}}],["的文档",{"2":{"84":1}}],["的键值存储",{"2":{"145":1}}],["的回答",{"2":{"141":1}}],["的知识编辑主要分为以下几类",{"2":{"141":1}}],["的知识编辑技术的基本框架",{"2":{"140":1}}],["的知识编辑必须能实现以下三个基本功能",{"2":{"140":1}}],["的知识编辑领域应运而生",{"2":{"121":1}}],["的功能",{"2":{"139":1}}],["的系统中相互连接",{"2":{"137":1}}],["的贡献都是一致的",{"2":{"245":1}}],["的贡献",{"2":{"134":1}}],["的归因是一个向量af",{"2":{"134":1}}],["的差值",{"2":{"134":1}}],["的key向量",{"2":{"320":2}}],["的key",{"2":{"126":1}}],["的结果会比较大",{"2":{"1678":1}}],["的结果符号与被除数相同",{"2":{"1630":1}}],["的结果为",{"2":{"1607":1}}],["的结果",{"2":{"504":1,"1137":1,"1607":1}}],["的结果是",{"2":{"399":1,"1607":1,"1729":1}}],["的结果如下",{"2":{"74":1}}],["的结构体",{"2":{"1728":1}}],["的结构和功能的数学模型或计算模型",{"2":{"1456":1}}],["的结构进行了一些修改",{"2":{"204":1}}],["的结构",{"2":{"125":1}}],["的行为",{"2":{"1083":12}}],["的行为机制",{"2":{"123":1}}],["的行为将逐渐接近",{"2":{"107":1}}],["的参数将使用学习率",{"2":{"1222":1}}],["的参数将使用默认学习率",{"2":{"1222":1}}],["的参数都是对术语的一种滥用",{"2":{"1185":1}}],["的参数矩阵相乘",{"2":{"517":1}}],["的参数",{"2":{"122":1,"1117":1}}],["的复杂架构中有效地检索",{"2":{"121":1}}],["的出色表现一定程度上要归功于其海量参数中存储的丰富信息",{"2":{"121":1}}],["的计算了",{"2":{"1442":1}}],["的计算结果",{"2":{"1344":1}}],["的计算结果得到的权重",{"2":{"198":1}}],["的计算和内存可以在输出序列之间共享使用",{"2":{"983":1}}],["的计算",{"0":{"1331":1},"2":{"616":1,"903":1,"974":1}}],["的计算方式中发挥作用",{"2":{"488":1}}],["的计算方式更加稳定",{"2":{"346":1}}],["的计算与",{"2":{"338":1}}],["的计算说明",{"2":{"210":1}}],["的计算公式为",{"2":{"178":1,"2018":1}}],["的计算效率",{"2":{"152":1}}],["的计算相当于是对",{"2":{"117":1}}],["的计算可通过对一个的矩阵操作来实现",{"2":{"29":1}}],["的稀疏性可以提高学习的精度",{"2":{"841":1}}],["的稀疏性给卷积神经网络的训练带来了巨大的成功",{"2":{"841":1}}],["的稀疏性",{"2":{"111":1,"1344":1}}],["的问题输入",{"2":{"140":1}}],["的问题",{"2":{"104":1,"626":1}}],["的mntp任务",{"2":{"732":1}}],["的mean和std",{"2":{"341":3}}],["的mlp",{"2":{"101":1}}],["的mask",{"2":{"79":1,"198":1}}],["的影响力必然是最大的",{"2":{"260":1}}],["的影响更敏感",{"2":{"127":1}}],["的影响",{"2":{"91":1,"326":1,"382":1,"1141":1}}],["的占比",{"2":{"90":1}}],["的方向去努力",{"2":{"512":1}}],["的方向引导",{"2":{"122":1}}],["的方差",{"2":{"1004":1,"1007":1}}],["的方差为",{"2":{"701":1,"1003":1}}],["的方差是σ2σ2σ^2",{"2":{"332":1}}],["的方差是",{"2":{"332":2}}],["的方差也是同理",{"2":{"315":1}}],["的方式在大量生语料上进行训练",{"2":{"1312":1}}],["的方式去训练",{"2":{"405":1}}],["的方式对数据进行加工",{"2":{"276":1}}],["的方式",{"2":{"90":1,"735":1}}],["的方法通常会导致较慢的训练进度",{"2":{"1154":1}}],["的方法能够理解词汇之间的关系",{"2":{"696":1}}],["的方法代表了语言建模的重大转变",{"2":{"610":1}}],["的方法",{"2":{"41":1,"148":1,"739":1}}],["的预测",{"2":{"145":1,"277":1}}],["的预测序列来构造",{"2":{"57":1}}],["的预训练过程",{"2":{"1317":1}}],["的预训练目标和架构进行调整以进一步提高性能",{"2":{"1315":1}}],["的预训练模型进行改造",{"2":{"735":1}}],["的预训练",{"2":{"89":1}}],["的长度各不相同",{"2":{"89":1}}],["的长度分别为",{"2":{"89":1}}],["的长度",{"2":{"87":1,"1624":1,"1704":2}}],["的样本标准差",{"2":{"343":1}}],["的样本均值",{"2":{"343":1}}],["的样本被填充了",{"2":{"87":1}}],["的样本后面会被使用",{"2":{"87":1}}],["的样本可能会出现在同一个",{"2":{"87":1}}],["的样本和一个",{"2":{"87":1}}],["的位置实际上对应的是",{"2":{"1340":1}}],["的位置编码",{"0":{"1337":1}}],["的位置与其他单词一样对待是否是一个合理的设计",{"2":{"764":1}}],["的位置",{"2":{"84":1,"1200":1,"1330":1,"1705":1}}],["的中间结果",{"2":{"83":1}}],["的输入是词嵌入向量",{"2":{"1342":1}}],["的输入参数",{"0":{"1225":1}}],["的输入不同",{"2":{"533":1}}],["的输入和输出维度进行随机",{"2":{"396":1}}],["的输入",{"2":{"74":1,"330":1,"343":1,"504":1,"855":1,"886":1,"1120":1}}],["的输出作为后一个时刻",{"2":{"886":1}}],["的输出作为最终答案",{"2":{"141":1}}],["的输出中保留哪些重要信息",{"2":{"866":1}}],["的输出将决定从",{"2":{"866":1}}],["的输出相乘",{"2":{"866":1,"868":1}}],["的输出也产生了影响",{"2":{"860":1}}],["的输出是0",{"2":{"301":1}}],["的输出维度",{"2":{"201":1}}],["的输出维度是",{"2":{"173":1}}],["的输出进行处理",{"2":{"144":1}}],["的输出与",{"2":{"144":1,"866":1,"868":1}}],["的输出冲突",{"2":{"141":1}}],["的输出序列",{"2":{"84":1}}],["的输出",{"2":{"38":1,"343":1,"533":1,"1114":1}}],["的时间里完成所有元素间信息的传递",{"2":{"511":1}}],["的时间演化",{"2":{"507":1}}],["的时间和空间复杂性",{"2":{"210":1}}],["的时间复杂度",{"2":{"184":1}}],["的时刻",{"2":{"59":1,"934":1}}],["的时候搬迁的是",{"2":{"662":1}}],["的时候由",{"2":{"344":1}}],["的时候",{"2":{"58":1,"277":1,"333":1,"2054":1}}],["的信息传播到整个输入的最后一个",{"2":{"122":1}}],["的信息",{"2":{"58":2,"122":1,"162":1,"230":1,"247":1,"261":2,"536":1}}],["的关键组件",{"2":{"355":1}}],["的关系",{"2":{"50":1,"1147":1}}],["的关注点并非预设",{"2":{"9":1}}],["的cbow的训练机制",{"2":{"50":1}}],["的动态组合和头自身的动态门控",{"2":{"46":1}}],["的注意力层则只能访问输入中给定词语之前的词语",{"2":{"1317":1}}],["的注意力层都可以访问初始输入句子中的所有单词",{"2":{"1317":1}}],["的注意力运算",{"2":{"512":1}}],["的注意力",{"2":{"212":1}}],["的注意力之间的差距",{"2":{"211":1}}],["的注意力结果",{"2":{"99":1}}],["的注意力关系",{"2":{"50":1}}],["的注意力分数矩阵",{"2":{"50":1}}],["的注意力计算过程中可以提高性能",{"2":{"41":1}}],["的注意力的综合",{"2":{"5":1}}],["的比较",{"2":{"40":1}}],["的维度顺序或物理布局",{"2":{"1083":1}}],["的维度进行dropout",{"2":{"396":1}}],["的维度",{"2":{"315":1,"341":1,"1086":1,"1087":1}}],["的维度倒置",{"2":{"101":1}}],["的维度是",{"2":{"31":1,"83":1,"173":1}}],["的维度大小",{"2":{"10":1,"16":1}}],["的独立一部分",{"2":{"29":1}}],["的",{"0":{"1087":1},"2":{"17":2,"70":1,"78":2,"84":1,"89":2,"90":1,"122":1,"137":1,"145":1,"148":1,"172":2,"204":1,"216":1,"217":1,"221":1,"334":1,"338":1,"347":1,"378":1,"420":1,"485":1,"498":1,"515":2,"533":1,"538":1,"541":1,"561":1,"579":1,"595":1,"608":1,"624":1,"660":1,"683":2,"762":1,"772":1,"888":1,"889":1,"935":2,"938":1,"943":2,"951":2,"954":1,"961":2,"971":1,"974":1,"995":1,"1078":2,"1083":1,"1087":1,"1106":1,"1113":1,"1130":1,"1161":2,"1208":1,"1210":1,"1211":1,"1214":2,"1226":1,"1227":2,"1315":2,"1316":1,"1330":1,"1340":1,"1343":1,"1344":1,"1364":2,"1386":1,"1438":1,"1476":1,"1605":1,"1611":1,"1724":1,"1728":1,"1857":1,"1874":1,"1906":1,"1907":1,"1924":1,"1963":1,"2054":1}}],["当bitset的大小超过unsigned",{"2":{"2062":1}}],["当真正领悟到",{"2":{"2054":1}}],["当公共的基类通过不同路径被多次继承时",{"2":{"1869":1}}],["当派生类只想使用基类的实现细节",{"2":{"1860":1}}],["当派生类希望进一步封装",{"2":{"1856":1}}],["当派生类想向外界公开基类的接口时",{"2":{"1852":1}}],["当程序出现错误时",{"2":{"1729":1}}],["当给联合体的一个成员赋值时",{"2":{"1728":1}}],["当给定一个",{"2":{"702":1}}],["当编译器遇到内联函数的调用时",{"2":{"1709":1}}],["当编写优化器时",{"2":{"1120":1}}],["当引用计数为",{"2":{"1695":1}}],["当所有可用内存都被耗尽时",{"2":{"1671":1}}],["当所有条件都为假时执行的代码块",{"2":{"1619":1}}],["当不同的父类拥有相同名称的成员",{"2":{"1660":1}}],["当向一块堆内存中写入的数据超过了其分配的大小",{"2":{"1648":1}}],["当函数调用层级过深",{"2":{"1648":1}}],["当你逛超市时",{"2":{"2101":1}}],["当你",{"2":{"2054":1}}],["当你理解",{"2":{"2054":1}}],["当你对",{"2":{"1713":1}}],["当你调用一个函数时",{"2":{"1648":1}}],["当你需要先执行一次操作",{"2":{"1620":1}}],["当系统没有足够的内存来满足",{"2":{"1647":1}}],["当一个耗时的操作",{"2":{"1645":1}}],["当一个人看图片时",{"2":{"257":1}}],["当将一个低精度类型的值赋给高精度类型的变量时",{"2":{"1629":1}}],["当条件表达式为真时执行的代码块",{"2":{"1621":1}}],["当条件1为假且条件2为真时执行的代码块",{"2":{"1619":1}}],["当条件1为真时执行的代码块",{"2":{"1619":1}}],["当条件为真时重复执行的代码",{"2":{"1620":1}}],["当条件为真时执行的代码块",{"2":{"1619":2}}],["当条件为假时执行的代码块",{"2":{"1619":1}}],["当两个整数进行除法运算时",{"2":{"1607":1}}],["当手册使用",{"2":{"1584":1}}],["当计算完每个神经元的误差信号后",{"2":{"1443":1}}],["当计算",{"2":{"1442":1}}],["当有线程进入临界区段时",{"2":{"1413":1}}],["当clip",{"2":{"1360":1}}],["当度量指标停止改善时",{"2":{"1245":1}}],["当您只想改变单个选项",{"2":{"1222":1}}],["当您需要执行不应由自动求导记录的操作",{"2":{"1120":1}}],["当修改batch",{"2":{"1186":1}}],["当出现较大或离群的梯度问题时",{"2":{"1184":1}}],["当学习率过大时",{"2":{"1179":1}}],["当对6个试验进行抽样时",{"2":{"1177":1}}],["当评估集不能被",{"2":{"1164":1}}],["当工作模式受计算限制时",{"2":{"1154":1}}],["当决定是否对我们的模型或训练程序进行改变或采用新的超参数配置时",{"2":{"1152":1}}],["当第一次生成此类图表花费的努力越多",{"2":{"1151":1}}],["当验证误差在训练期间的某个时刻开始增加时",{"2":{"1149":1}}],["当研究中很大一部分点是不可行时",{"2":{"1146":1}}],["当需要在不同的时间存储不同类型的数据",{"2":{"1728":1}}],["当需要在成员函数中引用当前对象时",{"2":{"1638":1}}],["当需要区分成员变量和局部变量同名的情况时",{"2":{"1638":1}}],["当需要处理一组相同类型的数据时",{"2":{"1623":1}}],["当需要指定每个层的学习率时",{"2":{"1222":1}}],["当需要进行大量的调优实验时",{"2":{"1134":1}}],["当需要一个多维的数字形态时",{"2":{"679":1}}],["当加速器内存未饱和时",{"2":{"1132":1}}],["当训练集中的错误分类为0时",{"2":{"1155":1}}],["当训练损失为log",{"2":{"1155":1}}],["当训练不受计算限制时",{"2":{"1154":1}}],["当训练不受计算限制时如何决定该训练多久",{"0":{"1155":1},"1":{"1156":1},"2":{"1125":1}}],["当训练为不受计算限制时",{"2":{"1154":1}}],["当训练为受计算限制时",{"2":{"1154":1}}],["当训练受计算限制时如何决定该训练多久",{"0":{"1157":1},"1":{"1158":1,"1159":1},"2":{"1125":1}}],["当训练数据集的规模相对较小时",{"2":{"1012":1}}],["当训练数据和测试数据分布存在差异时",{"2":{"316":1}}],["当进行不需要在反向图中记录的计算",{"2":{"1121":1}}],["当应用于模块时",{"2":{"1117":1}}],["当应用于非凸函数训练神经网络时",{"2":{"1048":1}}],["当定义一个自定义的",{"2":{"1114":1}}],["当调用函数时",{"2":{"1708":1}}],["当调用",{"2":{"1083":1}}],["当刚好下降到山谷附近时",{"2":{"1036":1}}],["当参数初始化很小时",{"2":{"994":1}}],["当足够多的等待请求运行时",{"2":{"986":1}}],["当softmax在得分矩阵的一个块上执行时",{"2":{"973":1}}],["当生成下一个时",{"2":{"912":1}}],["当要翻译的句子较长时",{"2":{"891":1}}],["当做每一步的输入",{"2":{"889":1}}],["当激活函数是单调的时候",{"2":{"838":1}}],["当时主要为了解决",{"2":{"775":1}}],["当神经元激活一个又一个位置的时候",{"2":{"754":1}}],["当神经网络无法学习到残差f",{"2":{"301":1}}],["当神经网络能够学习到残差f",{"2":{"301":1}}],["当last",{"2":{"1233":1,"1234":1}}],["当llm遇上embedding",{"2":{"740":1}}],["当lcms处理",{"2":{"629":1}}],["当实际使用word",{"2":{"717":1}}],["当d",{"2":{"701":1}}],["当dkdkd",{"2":{"189":1}}],["当输入特征的维度较高时",{"2":{"1012":1}}],["当输入不是序列而输出为序列的情况怎么处理",{"2":{"882":1}}],["当输入经过",{"2":{"700":1}}],["当输入中每个token的embedding堆叠在一起时",{"2":{"457":1}}],["当查表操作得到了这个全连接层的参数之后",{"2":{"694":1}}],["当查询和键相似时给予更高的权重",{"2":{"175":1}}],["当场完成任务的",{"2":{"683":1}}],["当想要通过添加新参数来增量增加模型规模时",{"2":{"620":1}}],["当作一个词",{"2":{"576":1}}],["当词表大小达到一定程度之后",{"2":{"561":1}}],["当词之间距离过长时",{"2":{"256":1}}],["当某个单词出现的频率大于min",{"2":{"557":1}}],["当某单词获取其它单词信息时",{"2":{"274":1}}],["当序列长度",{"2":{"511":1}}],["当token的初始位置位于椭球某个半球的内部时",{"2":{"507":1}}],["当注意力矩阵为时不变",{"2":{"507":1}}],["当模型在代表生产环境的离线训练",{"2":{"1163":1}}],["当模型在生产环境中提供预测时收集指标",{"2":{"1163":1}}],["当模型架构或数据发生变化时",{"2":{"1155":1}}],["当模型接收到一个问题时",{"2":{"505":1}}],["当模型需要决定给予序列中某个单词以多大的",{"2":{"268":1}}],["当随着网络的层数不断加深",{"2":{"494":1}}],["当embedding到达最后一层时",{"2":{"437":1}}],["当eieie",{"2":{"326":1}}],["当掌握了大量的比较杂的知识之后",{"2":{"402":1}}],["当x到100时",{"2":{"399":1}}],["当在",{"2":{"1762":1}}],["当在类中定义成员时",{"2":{"1210":1}}],["当在残差块之间进行层归一化时",{"2":{"333":1}}],["当在各个head中计算注意力时也没有信息交换",{"2":{"41":1}}],["当使用jit",{"2":{"1291":1}}],["当使用打乱后的训练",{"2":{"1164":1}}],["当使用最大隐藏层数的最佳试验表现出过拟合问题",{"2":{"1149":1}}],["当使用transformer网络进行推断时",{"2":{"327":1}}],["当使用反向传播更新",{"2":{"148":1,"485":1}}],["当样本数很少时",{"2":{"316":1}}],["当反向传播时",{"2":{"304":1}}],["当网络深度超过",{"2":{"302":1}}],["当highway",{"2":{"301":1}}],["当层数增加到一定程度之后",{"2":{"296":1}}],["当翻译到",{"2":{"277":1}}],["当下预测单词应该更加关注源文本中其对应单词的信息",{"2":{"268":1}}],["当产生新输出时",{"2":{"267":1}}],["当梯度在相同方向上持续增加时",{"2":{"1031":1}}],["当梯度爆炸时",{"2":{"255":1}}],["当梯度消失时",{"2":{"255":1}}],["当把",{"2":{"245":1}}],["当面临冗长且信息密集的输入序列时",{"2":{"244":1}}],["当数组作为函数参数传递时",{"2":{"1667":1}}],["当数据集具有离散或二进制属性时",{"2":{"692":1}}],["当数量级相差到一定程度",{"2":{"191":1}}],["当数字较小时",{"2":{"191":1}}],["当数字较大时",{"2":{"191":1}}],["当qqq和klklk",{"2":{"189":1}}],["当我们面对很多任务时",{"2":{"2106":1}}],["当我们面对复杂问题时",{"2":{"2104":1}}],["当我们使用",{"2":{"1704":1}}],["当我们使用前馈神经网络",{"2":{"1438":1}}],["当我们调用一个函数时",{"2":{"1650":1}}],["当我们不再需要这块内存时",{"2":{"1647":1}}],["当我们声明一个变量时",{"2":{"1647":1}}],["当我们在交互式环境中输出对象或使用",{"2":{"1227":1}}],["当我们只训练",{"2":{"1157":1}}],["当我们的训练时间越长",{"2":{"1157":1}}],["当我们试图得出超出超参数空间中单个点水平的结论时",{"2":{"1152":1}}],["当我们试图改进我们的模型时",{"2":{"1152":1}}],["当我们试图找到各种其他超参数",{"2":{"1130":1}}],["当我们正在考虑为一个连续的超参数来绘制isolation图时",{"2":{"1150":1}}],["当我们判断将一个冗余超参数转换为固定超参数所带来的限制少于调优它所需的计算资源时",{"2":{"1143":1}}],["当我们判断意图的时候",{"2":{"860":1}}],["当我们更新我们的最佳配置时",{"2":{"1139":1}}],["当我们选择均值为0",{"2":{"994":1,"995":1}}],["当我们考虑另一对向量c=",{"2":{"692":1}}],["当我们说llm的提示是图灵完备的",{"2":{"504":1}}],["当我们谈论大型语言模型",{"2":{"425":1}}],["当我们对高维向量进行点积运算时",{"2":{"189":1}}],["当我们对低维向量进行点积时",{"2":{"189":1}}],["当我们将两个矩阵相乘",{"2":{"189":1}}],["当我们问模型",{"2":{"122":1}}],["当这些大数值经过softmax函数时",{"2":{"187":1}}],["当这些技术结合在一起时",{"2":{"140":1}}],["当维度dkdkd",{"2":{"187":1}}],["当用一个标量来表示一个词时",{"2":{"679":1}}],["当用于分类器时",{"2":{"183":1}}],["当用",{"2":{"145":1}}],["当然我依旧觉得这句话很有道理哈",{"2":{"2097":1}}],["当然我们也要看到",{"2":{"291":1}}],["当然我们也可以反向思考",{"2":{"55":1}}],["当然这只是我个人的看法",{"2":{"2054":1}}],["当然就是最佳的翻译",{"2":{"908":1}}],["当然没有",{"2":{"908":1}}],["当然是可以的",{"2":{"895":1}}],["当然不同的硬件使用的驱动程序也不一样",{"2":{"794":1}}],["当然也很有效",{"2":{"765":1}}],["当然也有其它的区分方式",{"2":{"742":1}}],["当然用户也可以手动添加这些token",{"2":{"563":1}}],["当然其设置会依据大模型的特点进行调整",{"2":{"396":1}}],["当然只是有可能",{"2":{"333":1}}],["当然",{"2":{"137":1,"273":1,"280":1,"334":1,"449":1,"1015":1,"1337":1}}],["当大模型的训练参数达到一定规模时",{"2":{"119":1}}],["当ββ",{"2":{"108":1}}],["当β=0β=0",{"2":{"108":1}}],["当仅使用layernorm时",{"2":{"94":1}}],["当上下文长度增加时",{"2":{"87":1}}],["当解码器生成标记时",{"2":{"529":1}}],["当解码第一个字的时候",{"2":{"70":1}}],["当解出第二个字的时候",{"2":{"70":1}}],["当前位置",{"2":{"1821":1,"1839":1}}],["当前没有指向任何有效的",{"2":{"1611":1}}],["当前进程计算的部分和",{"2":{"1594":1}}],["当前seq",{"2":{"1329":1}}],["当前单词发射分数",{"2":{"1328":1}}],["当前单词转移分数",{"2":{"1328":1}}],["当前单词只受到其前面单词的影响",{"2":{"239":1}}],["当前公布的模型具有",{"2":{"1316":1}}],["当前状态",{"2":{"1221":1}}],["当前状态下的梯度向量越大",{"2":{"1179":1}}],["当前优化器比其他优化器使用更少的内存",{"2":{"1143":1}}],["当前优化器的训练曲线更容易理解",{"2":{"1143":1}}],["当前系统无法有效地处理kv",{"2":{"985":1}}],["当前kv",{"0":{"985":1}}],["当前项的内容总来源于前一步的输出",{"2":{"885":1}}],["当前大多数",{"2":{"561":1}}],["当前词与已生成词的关系",{"2":{"525":1}}],["当前",{"2":{"333":1}}],["当前问题",{"0":{"256":1}}],["当前轮输出token",{"2":{"239":1}}],["当前的通讯域",{"2":{"1590":1}}],["当前的状态能够汇总所有lstm看到的token",{"2":{"287":1}}],["当前的位置就无法看到后面的词信息了",{"2":{"70":1}}],["当前的transformer注意力机制只是注重事物的单独方面",{"2":{"3":1}}],["当前token",{"2":{"58":1,"158":1}}],["当前及前面的",{"2":{"50":1}}],["当多头注意力机制计算完成后",{"2":{"36":1}}],["当",{"2":{"17":2,"255":1,"334":1,"396":1,"647":2,"698":2,"778":1,"843":1,"845":2,"924":2,"1235":1,"1236":1,"1237":1,"1238":1,"1239":1,"1240":1,"1619":1,"1647":1,"1648":1,"1714":1,"1762":1,"2054":1}}],["当语义逐渐复杂后",{"2":{"9":1}}],["owing",{"2":{"2081":1}}],["own",{"2":{"122":1}}],["owned",{"2":{"122":2}}],["ok",{"2":{"1853":2,"1857":2,"1861":2,"1867":1,"1922":1}}],["oct",{"2":{"1817":2,"1835":2}}],["occurrence",{"2":{"1756":2}}],["occurrences",{"2":{"590":1,"592":2}}],["o2",{"2":{"1386":2,"1389":2,"1393":1,"1394":12,"1398":4}}],["oi=∑jai",{"2":{"1339":2}}],["oio",{"2":{"944":2}}],["o+to",{"2":{"1324":1}}],["o+feedforward",{"2":{"330":2}}],["observe",{"2":{"1254":1}}],["obj2",{"2":{"1694":8}}],["obj1",{"2":{"1694":9}}],["obj",{"2":{"1083":2,"1604":1,"1639":3,"1640":3,"1641":1,"1653":3,"1770":1,"1931":1}}],["object>",{"2":{"1485":3}}],["objects",{"2":{"688":1,"1476":1,"1917":4}}],["objective",{"2":{"542":1,"1363":1}}],["object",{"2":{"380":1,"557":1,"558":1,"679":1,"1254":3,"1478":1,"1479":1,"2079":1}}],["o延迟",{"2":{"1161":1}}],["oor",{"2":{"1330":6}}],["oom",{"2":{"976":1}}],["oov",{"2":{"565":1}}],["o​​+t​o",{"2":{"1324":1}}],["o​i​​=​j​∑​​a​i",{"2":{"1339":1}}],["o​i​​=∑​j​​a​i",{"2":{"1339":1}}],["o​i​​",{"2":{"944":2}}],["o​t​r​​​​",{"2":{"944":1}}],["o​t​​=w​oh​​s​t​​",{"2":{"856":1}}],["o​t​​",{"2":{"855":1}}],["o​1​​",{"2":{"944":1}}],["o1",{"2":{"944":1,"1386":2,"1389":5,"1392":9,"1393":19,"1394":12,"1398":6}}],["o=",{"2":{"1396":9}}],["o=pv∈r​n×d​​",{"2":{"941":1,"960":1}}],["o=pv∈rn×d",{"2":{"941":1,"960":1}}],["o=layernorm",{"2":{"330":4}}],["oh",{"2":{"856":1}}],["o负担",{"2":{"665":1}}],["o的时候",{"2":{"665":1}}],["ojojo",{"2":{"614":2}}],["oldfunction",{"2":{"1909":3}}],["older",{"2":{"577":1}}],["old",{"2":{"577":1,"623":2,"1476":1,"1520":2,"1554":7,"1909":1}}],["omrimallis",{"2":{"513":1}}],["odd",{"2":{"1737":1}}],["ode进行正则化",{"2":{"497":1}}],["ode的解之间的距离",{"2":{"497":1}}],["ode的离散化",{"2":{"496":1}}],["odes之间的联系进行进一步的深入研究",{"2":{"497":1}}],["ode框架可以使用伴随状态方法在不存储激活值",{"2":{"495":1}}],["ode来可以直接通过方程求解器来计算网络梯度",{"2":{"495":1}}],["ode",{"2":{"494":2,"496":3,"498":1}}],["ode视角",{"0":{"492":1},"1":{"493":1,"494":1,"495":1,"496":1,"497":1}}],["odot",{"2":{"343":1}}],["otro",{"2":{"944":1}}],["ot=wohsto",{"2":{"856":1}}],["oto",{"2":{"855":1}}],["other不再拥有这些资源",{"2":{"1887":1}}],["other的资源",{"2":{"1887":1}}],["otherwise",{"2":{"591":1,"592":1}}],["other",{"0":{"1432":1},"2":{"450":1,"703":1,"820":1,"1083":12,"1085":61,"1087":160,"1111":2,"1694":2,"1712":3,"1887":18}}],["otimes⊗",{"2":{"1344":1}}],["otimes",{"2":{"109":1}}],["otimes表示逐元素乘法",{"2":{"109":1}}],["ostringstream",{"2":{"1823":1,"1826":1,"1841":1,"1844":1}}],["ostream",{"2":{"1678":4,"1712":4,"1789":4}}],["os通过parallels",{"2":{"1582":1}}],["os",{"0":{"1431":1},"2":{"373":2,"422":2,"1250":2,"1253":1,"1712":4,"1789":4}}],["o沿着其嵌入维度进行归一化",{"2":{"355":1}}],["oxaa55h",{"2":{"233":1}}],["opc",{"2":{"1957":1}}],["opt",{"2":{"1237":1,"1238":1,"1240":1,"1246":2,"1247":3,"1299":2}}],["options",{"0":{"1222":1},"2":{"395":1,"1984":2}}],["optional>",{"2":{"1927":2}}],["optionally",{"2":{"590":1}}],["optional",{"0":{"1927":1},"2":{"76":1,"114":1,"201":2,"384":1,"557":2,"558":1,"702":5,"723":4,"1082":6,"1086":10,"1087":188,"1208":5,"1214":14,"1227":4,"1308":1,"1330":1,"1920":1,"1927":3,"1932":1}}],["optimaai",{"2":{"740":1}}],["optimal",{"2":{"638":1}}],["optimizemigration",{"2":{"2070":1}}],["optimizes",{"2":{"1208":1}}],["optimize",{"2":{"1067":1,"1227":1,"2086":1}}],["optimizer参数是正在使用的优化器实例",{"2":{"1227":1}}],["optimizerposthook",{"2":{"1227":1}}],["optimizerprehook",{"2":{"1227":1}}],["optimizer=",{"2":{"1143":4,"1144":2}}],["optimizer=optimizer",{"2":{"83":1,"402":1,"423":1,"424":1}}],["optimizers",{"2":{"542":1,"543":1,"1404":1}}],["optimizer是dummyoptimizer",{"2":{"385":1}}],["optimizer",{"0":{"1021":1,"1066":1,"1225":1,"1226":1,"1227":1},"2":{"83":2,"364":1,"385":5,"399":2,"423":2,"424":2,"1039":3,"1143":1,"1202":3,"1205":10,"1214":1,"1215":8,"1218":3,"1221":2,"1223":6,"1224":1,"1226":7,"1227":19,"1231":7,"1233":2,"1234":1,"1235":2,"1236":2,"1237":1,"1238":1,"1239":3,"1240":1,"1241":1,"1242":3,"1243":3,"1244":4,"1245":1,"1246":1,"1247":1,"1266":3,"1267":3,"1295":3,"1296":2,"1404":1,"2086":6}}],["optimization",{"2":{"314":2,"513":1,"1046":1,"1059":1,"1067":3}}],["optim",{"0":{"666":1,"1219":1,"1220":1,"1224":1,"1225":1,"1226":1,"1227":1,"1230":1},"1":{"1221":1,"1222":1,"1223":1,"1225":1,"1226":1,"1227":1,"1231":1,"1232":1,"1233":1,"1234":1,"1235":1,"1236":1,"1237":1,"1238":1,"1239":1,"1240":1,"1241":1,"1242":1,"1243":1,"1244":1,"1245":1,"1246":1,"1247":1,"1248":1},"2":{"83":1,"402":1,"423":1,"424":1,"429":1,"667":1,"1039":1,"1202":1,"1205":6,"1215":6,"1218":3,"1219":2,"1221":3,"1224":1,"1227":4,"1230":2,"1231":3,"1239":4,"1241":2,"1242":2,"1243":4,"1245":1,"1257":2,"1266":1,"1267":1,"1278":2,"1296":1,"2086":3}}],["op",{"0":{"660":1,"1097":1},"2":{"382":1}}],["operator=",{"2":{"1887":2}}],["operator+",{"2":{"1712":2,"1788":2,"1789":2}}],["operator运算符",{"2":{"1712":1}}],["operator",{"2":{"1611":2,"1678":1,"1695":2,"1712":3,"1789":2,"1813":1,"1816":1,"1820":2,"1824":2,"1831":1,"1834":1,"1838":2,"1842":2}}],["operators",{"2":{"1404":1}}],["operation",{"2":{"591":1,"1706":2,"1914":2}}],["operations",{"0":{"1123":1},"2":{"160":1,"1086":1,"1113":2,"1123":1}}],["operating",{"0":{"1431":1},"2":{"370":1}}],["opengl",{"2":{"1937":1,"2009":1}}],["openfile",{"2":{"1902":3}}],["opening",{"2":{"1821":1,"1825":2,"1839":1,"1843":2,"1933":1}}],["openssh",{"2":{"1583":1}}],["opensession",{"2":{"1481":1}}],["openmpi",{"2":{"1585":1}}],["openmp",{"2":{"1569":1}}],["opencv",{"2":{"1250":1}}],["openai",{"2":{"688":1,"721":1,"844":1,"1316":2}}],["openllmai",{"2":{"638":1}}],["openllm",{"2":{"638":2}}],["open",{"2":{"557":1,"591":3,"1253":1,"1561":1,"1569":1,"1589":2,"1761":2,"1763":1,"1820":4,"1821":1,"1825":2,"1838":4,"1839":1,"1843":2,"1902":5,"1933":1,"2073":1}}],["openwebtext和pile",{"2":{"367":1}}],["our",{"2":{"160":1,"428":2}}],["outfile",{"2":{"1821":3,"1839":3,"1930":3}}],["outo1",{"2":{"1393":1}}],["outo1=11+e−neto1out",{"2":{"1393":1}}],["outo1=11+e−neto1=11+e−2",{"2":{"1389":1}}],["outo1out",{"2":{"1392":1,"1393":1}}],["outo2=0",{"2":{"1389":1}}],["out​o1​",{"2":{"1393":1}}],["out​o1​​=​1+e​−net​o1​​​​​​1​​",{"2":{"1393":1}}],["out​o1​​=​1+e​−net​o1​​​​​​1​​=​1+e​−2",{"2":{"1389":1}}],["out​o1​​",{"2":{"1392":1,"1393":1}}],["out​o2​​=0",{"2":{"1389":1}}],["out​h1​​",{"2":{"1392":1}}],["out​h1​​=​1+e​−net​h1​​​​​​1​​=​1+e​−2",{"2":{"1388":1}}],["out​h3​​=0",{"2":{"1388":1}}],["out​h2​​=0",{"2":{"1388":1}}],["outh1out",{"2":{"1392":1}}],["outh1=11+e−neth1=11+e−2",{"2":{"1388":1}}],["outh3=0",{"2":{"1388":1}}],["outh2=0",{"2":{"1388":1}}],["outdim",{"2":{"1087":1}}],["outer",{"2":{"975":1,"1087":1,"1345":1,"2003":1}}],["out=target",{"2":{"381":1}}],["out=∑fout",{"2":{"270":2}}],["outside",{"2":{"370":1,"557":1}}],["out",{"2":{"74":1,"83":4,"181":1,"201":2,"233":1,"270":2,"381":2,"382":1,"385":3,"399":2,"410":2,"411":1,"423":1,"428":2,"472":2,"503":2,"529":2,"565":1,"1000":1,"1006":1,"1087":7,"1101":7,"1102":4,"1217":15,"1218":2,"1330":1,"1345":5,"1389":3,"1392":6,"1393":12,"1394":21,"1398":15,"1436":1,"1481":1,"1485":1,"1486":1,"1487":1,"1488":1,"1594":2,"1684":1,"1713":4,"1762":1,"1820":2,"1838":2,"1902":1,"2075":1,"2087":1}}],["outputfile",{"2":{"1820":6,"1825":4,"1838":6,"1843":4}}],["outputting",{"2":{"1284":1}}],["output都是向量",{"2":{"916":1}}],["outputs相关的embedding模块包括output",{"2":{"457":1}}],["outputs实际上是解码器之前输出的拼接",{"2":{"453":1}}],["outputs",{"2":{"453":1,"698":3,"983":1,"1244":2,"1273":4,"1298":3,"1386":1,"2086":2}}],["output存储了这些tokens的信息",{"2":{"147":1}}],["output=false",{"2":{"8":3,"114":2,"201":3}}],["output",{"0":{"471":1,"1448":1,"1815":1,"1833":1},"1":{"472":1,"473":1,"1816":1,"1817":1,"1834":1,"1835":1},"2":{"8":1,"10":1,"130":1,"201":8,"235":1,"270":1,"315":6,"326":1,"343":1,"346":2,"428":1,"543":1,"698":1,"700":5,"775":1,"801":1,"802":5,"804":2,"807":1,"808":5,"809":1,"810":1,"814":1,"815":1,"816":6,"835":1,"839":2,"840":2,"841":1,"842":3,"843":2,"844":1,"845":1,"846":1,"847":1,"868":1,"916":1,"967":1,"969":1,"1082":2,"1086":1,"1087":3,"1092":2,"1095":1,"1096":4,"1098":8,"1099":2,"1100":3,"1106":1,"1110":2,"1202":2,"1205":8,"1211":1,"1212":4,"1213":5,"1215":10,"1216":13,"1218":32,"1223":4,"1231":4,"1254":27,"1257":2,"1259":3,"1262":1,"1263":3,"1267":3,"1270":3,"1283":7,"1295":3,"1296":3,"1297":3,"1298":1,"1332":1,"1345":1,"1398":1,"1810":1,"1820":2,"1825":1,"1828":1,"1838":2,"1843":1,"1849":1,"1853":1,"1857":1,"1861":1,"1993":3,"1994":1}}],["overflow",{"2":{"1646":1,"1648":3}}],["overfitting",{"2":{"396":1,"429":1}}],["override",{"2":{"1085":2,"1685":2,"1688":2,"1691":6,"1693":2,"1866":2}}],["overload",{"2":{"1085":34,"1086":2,"1087":347,"1214":5,"1227":2}}],["overlap",{"2":{"305":1}}],["overlapped",{"2":{"41":1}}],["overlapping",{"2":{"41":1}}],["overconfidence",{"2":{"437":1}}],["overconfidence机制",{"2":{"122":1}}],["overview",{"2":{"387":1,"1067":1,"1088":1,"1089":1}}],["overwrites",{"2":{"384":1}}],["over",{"2":{"197":1,"204":1,"233":2,"326":1,"1329":2,"1330":1}}],["once",{"2":{"1628":1}}],["onnection",{"2":{"1459":1}}],["onnxruntime",{"2":{"1273":2,"1298":1}}],["onnx",{"0":{"1298":1},"2":{"836":1,"930":1,"1215":2,"1272":2,"1273":4,"1274":3,"1283":2,"1298":8}}],["online",{"2":{"1046":1,"2083":1}}],["only结构",{"2":{"543":1}}],["only主要指causal",{"2":{"542":1}}],["only的大模型在诸多nlp任务上表现出色",{"2":{"730":1}}],["only的llm为什么需要位置编码",{"2":{"543":1}}],["only的架构",{"2":{"542":2,"543":1}}],["only的结构",{"2":{"396":1}}],["only架构主要使用mistral和llama",{"2":{"729":1}}],["only架构中",{"2":{"542":1}}],["only架构的attention矩阵一定是满秩的",{"2":{"542":1}}],["only架构",{"2":{"541":1}}],["only模型改造成文本表征模型的无监督方法",{"2":{"734":1}}],["only模型详解",{"2":{"543":1}}],["only模型产生更直接的效果",{"2":{"542":1}}],["only模型也可以细分为causal",{"2":{"541":1}}],["only模型",{"2":{"541":1}}],["onlyinfo",{"2":{"233":2,"292":1,"513":2}}],["only",{"0":{"539":1,"541":1},"1":{"540":1,"541":1,"542":1},"2":{"89":1,"201":2,"395":1,"501":1,"539":2,"540":3,"541":3,"542":11,"591":2,"732":1,"734":2,"735":2,"740":1,"977":1,"1082":1,"1083":1,"1328":2}}],["onecyclelr",{"0":{"1242":1},"2":{"1242":1}}],["one更新来更新",{"2":{"145":1}}],["onesided",{"2":{"1087":1}}],["ones",{"2":{"74":1,"83":2,"84":1,"343":1,"346":2,"382":1,"424":1,"428":1,"472":1,"529":1,"734":1,"834":1,"1070":1,"1071":1,"1075":1,"1087":3,"1095":2,"1096":1,"1098":3,"1101":3,"1102":2,"1211":1,"1216":1,"1255":1,"1345":1}}],["ones方法向矩阵中添加1元素",{"2":{"74":1}}],["one",{"0":{"634":1},"2":{"47":1,"160":1,"192":1,"222":1,"458":1,"473":1,"591":2,"676":1,"681":1,"688":1,"700":8,"736":1,"904":1,"935":1,"951":1,"1086":1,"1242":3,"1254":1,"1350":7,"1737":1,"1738":1,"2086":1}}],["on",{"2":{"36":1,"47":1,"76":1,"91":1,"95":1,"150":1,"156":2,"235":1,"263":1,"292":2,"320":2,"350":1,"361":3,"370":1,"393":1,"429":2,"499":1,"513":3,"557":1,"560":1,"638":1,"740":1,"768":1,"1036":1,"1086":1,"1254":2,"1284":6,"1300":1,"1308":3,"1594":1,"2077":1}}],["o",{"2":{"10":2,"145":4,"180":2,"210":5,"212":2,"216":4,"330":6,"504":8,"564":1,"579":3,"580":1,"582":8,"583":7,"585":1,"595":1,"804":1,"899":1,"927":2,"941":5,"942":1,"944":5,"959":1,"960":5,"1102":4,"1132":1,"1317":3,"1320":4,"1324":6,"1339":2,"1396":1,"1398":17,"1541":1,"1545":2,"1589":2,"1594":1,"1604":1,"1624":1,"1713":4,"1715":1,"1916":11,"1917":33,"2155":3}}],["orm",{"2":{"1492":1}}],["ormqr",{"2":{"1087":1}}],["origin",{"2":{"1821":4,"1839":4}}],["original",{"2":{"1350":6}}],["orig",{"2":{"1350":1}}],["oriented",{"2":{"233":1,"740":3}}],["ort",{"2":{"1087":1,"1298":9}}],["ordereddict",{"2":{"557":2,"1226":6}}],["ordered",{"2":{"557":3}}],["order",{"2":{"557":1,"751":1,"1083":1,"1087":1,"1883":1}}],["ordinary",{"0":{"497":1},"2":{"493":1,"497":1,"498":1,"513":1}}],["orange",{"2":{"407":1}}],["organization",{"0":{"1430":1},"2":{"1320":4,"1324":5}}],["orgqr",{"2":{"1087":1}}],["org",{"2":{"156":1,"233":2,"361":2,"370":1,"387":5,"429":5,"432":1,"513":1,"543":1,"638":1,"713":1,"740":2,"768":1,"840":1,"1176":1,"1254":1,"1301":1,"1302":1,"1476":1,"1481":8,"1566":1}}],["or",{"0":{"1432":1},"2":{"8":1,"235":1,"277":1,"385":2,"557":3,"572":1,"606":1,"764":1,"1085":3,"1087":7,"1102":1,"1227":4,"1254":2,"1303":1,"1304":1,"1308":2,"1322":1,"1332":1,"1462":1,"1619":1,"1635":1,"1927":2,"2077":2}}],["ofstream",{"2":{"1819":1,"1820":3,"1821":2,"1825":1,"1826":1,"1837":1,"1838":3,"1839":2,"1843":1,"1844":1,"1930":1}}],["offering",{"2":{"2079":1}}],["off",{"2":{"1284":1}}],["offsets",{"2":{"1086":1}}],["offset",{"2":{"503":2,"781":1,"1087":11,"1821":3,"1839":3}}],["official",{"0":{"2085":1},"2":{"130":4,"2085":1}}],["of=args",{"2":{"201":1}}],["ofhead",{"2":{"47":1}}],["of",{"0":{"1736":1,"1737":1,"1738":1,"2074":1,"2079":1,"2080":1},"1":{"2075":1,"2076":1,"2077":1,"2078":1,"2079":1,"2081":1,"2082":1,"2083":1},"2":{"8":1,"18":1,"19":1,"20":2,"23":1,"38":1,"39":1,"42":3,"47":3,"76":3,"82":1,"89":1,"90":1,"91":1,"95":4,"114":4,"122":2,"130":4,"141":1,"143":1,"147":1,"156":7,"160":3,"165":1,"175":1,"181":1,"201":3,"233":5,"235":1,"241":1,"260":1,"320":2,"334":1,"343":1,"344":1,"361":3,"380":1,"384":1,"393":2,"399":1,"411":1,"422":1,"428":1,"429":3,"437":3,"449":1,"480":1,"490":1,"498":1,"499":2,"503":2,"504":1,"507":1,"508":1,"513":12,"522":1,"523":1,"533":1,"557":3,"558":2,"565":1,"571":6,"572":5,"573":2,"574":1,"590":6,"591":7,"592":5,"621":1,"625":2,"638":3,"688":3,"692":2,"698":1,"700":7,"713":1,"714":1,"736":1,"737":1,"740":8,"747":1,"764":1,"768":5,"814":3,"815":3,"816":3,"827":1,"834":4,"840":1,"899":1,"935":1,"938":1,"945":1,"951":1,"954":1,"965":1,"983":1,"985":1,"1036":1,"1067":1,"1082":2,"1086":3,"1088":1,"1089":1,"1215":2,"1242":1,"1244":1,"1254":7,"1284":1,"1300":1,"1303":1,"1307":2,"1308":4,"1329":6,"1330":4,"1340":1,"1404":1,"1590":1,"1607":3,"1611":1,"1665":1,"1684":1,"1698":2,"1699":2,"1713":6,"1715":1,"1732":3,"1736":2,"1737":2,"1738":3,"1756":2,"1758":1,"1762":1,"1772":1,"1774":4,"1778":1,"1814":1,"1832":1,"1867":1,"1883":1,"1897":2,"1933":1,"2062":1,"2073":2,"2075":1,"2077":3,"2079":2,"2083":1,"2086":4}}],["rbegin",{"2":{"1713":2}}],["rbrace",{"2":{"1343":1}}],["rvalue",{"2":{"1629":1,"2007":1}}],["rvi",{"2":{"759":1}}],["rhel",{"2":{"1539":1}}],["rho",{"2":{"1191":2}}],["r1",{"0":{"1402":1}}],["r​θ​d​​",{"2":{"1343":1}}],["r​θ",{"2":{"1343":2,"1344":1}}],["r​i",{"2":{"1339":6}}],["rθdr",{"2":{"1343":1}}],["rθ",{"2":{"1343":2,"1344":1}}],["rng",{"2":{"1169":1}}],["rnn就无法直接解决",{"2":{"883":1}}],["rnn引入了隐状态h",{"2":{"878":1}}],["rnn扩展到多层构成循环神经网络",{"0":{"859":1}}],["rnn可扩展到双向的情况",{"0":{"858":1}}],["rnn可以灵活地处理不同长度的输入序列",{"2":{"250":1}}],["rnn可以通过隐状态得到任意两个词的依赖",{"2":{"249":1}}],["rnn可以预测无限长句子",{"2":{"248":1}}],["rnn具体计算公式为",{"0":{"856":1}}],["rnn由于其内部复杂的状态更新",{"2":{"512":1}}],["rnn更是在发挥巨大的作用",{"2":{"291":1}}],["rnn和cnn一直没有放弃努力",{"2":{"291":1}}],["rnn只要考虑之前的隐状态和当前输入",{"2":{"279":1}}],["rnn受限于固定大小的隐状态",{"2":{"273":1}}],["rnn需要大量内存来维持长序列的隐状态",{"2":{"255":1}}],["rnn需要对序列内容进行逐步处理",{"2":{"254":1}}],["rnn这种串行计算在本质上是一种递归",{"2":{"254":1}}],["rnn都会将序列中之前的所有信息压缩到一个固定长度的隐向量",{"2":{"251":1}}],["rnn在处理序列时采用了权重共享的策略",{"2":{"250":1}}],["rnn任意步的隐状态都包含了当前时间步之前所有时间步的几乎所有信息",{"2":{"250":1}}],["rnn天然地适合处理具有时间序列或序列结构的数据",{"2":{"250":1}}],["rnn的输入是",{"2":{"879":1}}],["rnn的网络结构特点导致难以训练",{"2":{"255":1}}],["rnn的特点会带来表达能力的缺失",{"2":{"252":1}}],["rnn的缺点同样鲜明",{"2":{"251":1}}],["rnn的优点如下",{"2":{"250":1}}],["rnn的独到之处在于引入了",{"2":{"248":1}}],["rnn是个偏序结构",{"2":{"252":1}}],["rnn是一种连接主义模型",{"2":{"248":1}}],["rnn是时序结构",{"2":{"248":1}}],["rnn方案因为无法弥补长距离依赖问题会导致梯度消失和梯度爆炸问题",{"2":{"274":1}}],["rnn方案中",{"2":{"273":1}}],["rnn方案",{"0":{"248":1},"1":{"249":1,"250":1,"251":1,"252":1,"253":1,"254":1,"255":1}}],["rnn或者transformer来实现编码器和解码器",{"2":{"242":1}}],["rnns之所以称为循环神经网路",{"2":{"851":1}}],["rnns",{"2":{"210":1,"233":1,"235":1}}],["rnn捕捉词与词之间关系需要把句子从头看到尾",{"2":{"160":1}}],["rnn",{"0":{"282":1,"850":1,"852":1,"853":1,"857":1,"860":1,"878":1,"879":1,"880":1,"881":1,"882":1},"1":{"853":1,"854":1,"855":1,"856":1,"857":1,"858":1,"859":1,"880":1,"881":1,"882":1},"2":{"34":1,"235":1,"237":1,"242":1,"255":2,"282":1,"284":1,"291":1,"292":2,"334":1,"511":2,"512":1,"808":1,"850":1,"861":5,"862":1,"911":1,"1472":1}}],["rfind",{"2":{"1713":2}}],["rfc",{"2":{"1121":1}}],["rfloordiv",{"2":{"1083":1,"1085":1}}],["rfloatdiv",{"2":{"1083":1}}],["rcond",{"2":{"1087":1}}],["rctm",{"0":{"281":1}}],["rxor",{"2":{"1085":1}}],["rrshift",{"2":{"1083":1}}],["rrelu",{"2":{"842":4}}],["rpc框架",{"2":{"1951":1}}],["rpow",{"2":{"1083":1,"1085":1}}],["rpes倾向于直接修改注意力机制来融合相对位置信息",{"2":{"745":1}}],["rpe通常在每一层都重复出现",{"2":{"745":1}}],["rpe",{"2":{"742":1,"756":4}}],["rm",{"2":{"1509":1}}],["rmdir",{"2":{"1509":2}}],["rmul",{"2":{"1085":1}}],["rmatmul",{"2":{"1083":1}}],["rmod",{"2":{"1083":1}}],["rms指的是均方根",{"2":{"1047":1}}],["rmsprop",{"0":{"1047":1,"1191":1},"1":{"1048":1,"1049":1,"1050":1},"2":{"1047":2,"1048":2,"1049":1,"1052":1,"1057":1,"1059":2}}],["rmsprop等优化算法都可以根据梯度的历史信息来动态调整学习率",{"2":{"400":1}}],["rms",{"0":{"812":1},"2":{"346":1,"1052":3}}],["rmsnorm的作者认为ln取得成功重要的是缩放不变性",{"2":{"346":1}}],["rmsnorm的方向转变了",{"2":{"311":1}}],["rmsnorm有助于调整和稳定神经网络每一层的值",{"2":{"311":1}}],["rmsnorm",{"0":{"346":1},"2":{"201":3,"293":1,"311":1,"320":2,"346":8,"351":1,"354":1,"355":1,"356":1,"501":1,"503":1}}],["rtos内核开发",{"2":{"1941":1}}],["rtol",{"2":{"1087":3}}],["rtruediv",{"2":{"1083":1,"1085":1}}],["rt",{"2":{"761":1}}],["rt−srt−sr",{"2":{"760":1}}],["rwx",{"2":{"1512":1}}],["rwk",{"2":{"760":1}}],["rw嵌入强调输入的不同主题",{"2":{"739":1}}],["rw和hs嵌入的聚类结果显示出中等的重叠",{"2":{"739":1}}],["rw",{"2":{"739":1,"760":1,"1513":1}}],["rgb",{"2":{"687":2,"1283":1}}],["r2l",{"2":{"595":1}}],["rdma",{"2":{"1952":1}}],["rdbuf",{"2":{"1824":1,"1842":1}}],["rdiv",{"2":{"1083":2}}],["rd",{"2":{"499":2}}],["rdrdr^d",{"2":{"485":1}}],["rdrdr^d映射到rdmrdmr^",{"2":{"485":1}}],["rshift",{"2":{"1085":3}}],["rsub",{"2":{"1083":1,"1085":1}}],["rs",{"2":{"976":10}}],["rsa",{"2":{"420":1,"1594":3}}],["rsqrt",{"2":{"346":2,"1087":2}}],["rsqrt表示对输入进行开根号求倒数",{"2":{"346":1}}],["rutherford",{"2":{"638":1}}],["rules",{"2":{"580":1}}],["ruled",{"2":{"361":1}}],["runserver",{"2":{"2070":2}}],["runs",{"2":{"1283":2,"2086":1}}],["runtime期间",{"2":{"985":1}}],["runtime",{"2":{"572":1,"986":1,"1435":1,"1762":5,"1763":4,"1902":1}}],["running是直接用上一个状态的输出",{"2":{"406":1}}],["running",{"2":{"406":1,"543":1,"807":12,"894":1,"895":1,"1211":14,"1280":1,"1295":3}}],["run",{"2":{"83":2,"364":1,"381":1,"385":3,"399":3,"423":2,"424":2,"428":2,"538":1,"1092":1,"1215":2,"1217":1,"1273":4,"1274":1,"1280":1,"1296":1,"1298":2,"1299":1,"1303":1,"1304":1,"1308":1,"1332":1,"1398":1,"2066":1,"2075":1,"2077":1}}],["r^b",{"2":{"943":2,"961":2}}],["r^v",{"2":{"759":1}}],["r^d",{"2":{"499":2}}],["r^d的流映射",{"2":{"499":1}}],["r^d到另一个rdrd",{"2":{"499":1}}],["r^",{"2":{"289":1,"941":2,"942":2,"943":1,"944":4,"946":1,"957":3,"959":2,"960":2,"961":1,"966":1}}],["rlshift",{"2":{"1083":1}}],["rl",{"2":{"224":1}}],["rich",{"0":{"2079":1}}],["rico",{"2":{"361":1,"638":1}}],["rit",{"2":{"1713":3}}],["rij",{"2":{"766":2}}],["ri",{"2":{"759":1,"1339":6}}],["ringattention",{"0":{"974":1},"1":{"975":1},"2":{"974":1}}],["ring",{"2":{"217":1,"974":2,"975":1}}],["right操作",{"2":{"526":1}}],["right的目的是将序列整体右移一位",{"2":{"453":1}}],["rightarrow",{"2":{"330":3,"344":5,"765":1}}],["right",{"2":{"82":1,"106":2,"210":1,"407":1,"408":1,"409":1,"453":1,"528":1,"529":1,"533":1,"943":1,"961":1,"1003":1,"1004":1,"1087":4,"1180":1,"1184":2,"1343":1,"1817":1,"1835":1}}],["r",{"2":{"93":3,"122":1,"145":7,"161":7,"558":1,"571":4,"591":1,"760":2,"766":1,"768":1,"927":4,"941":3,"944":19,"960":3,"1078":1,"1227":2,"1243":1,"1301":1,"1315":1,"1339":3,"1343":3,"1509":1,"1512":1,"1513":2,"1520":1,"1550":1,"1693":2,"1712":4,"1779":2,"1891":2,"1908":3,"2093":1}}],["ray",{"2":{"2009":1}}],["raft",{"2":{"1952":1}}],["ra",{"2":{"1612":8}}],["rabbitmq",{"2":{"1498":1}}],["ram",{"2":{"1411":1}}],["rather",{"2":{"1283":1}}],["ratio",{"2":{"1254":1}}],["rates",{"2":{"1241":1,"1242":1}}],["rate来说",{"2":{"1183":1}}],["rate的峰值学习率",{"2":{"1183":1}}],["rate这样的数值",{"2":{"1183":1}}],["rate要大一个数量级",{"2":{"1183":1}}],["rate",{"0":{"667":1,"1229":1,"1232":1},"2":{"83":1,"402":2,"423":1,"424":1,"1023":1,"1054":1,"1143":3,"1144":2,"1183":6,"1215":2,"1218":2,"1221":1,"1229":1,"1239":1,"1243":1,"1303":2,"1308":2,"1332":1,"1874":2}}],["ravel",{"2":{"1087":1}}],["radius",{"2":{"1678":2,"1693":4,"1779":5}}],["radix",{"2":{"985":1,"986":3}}],["radixattention",{"0":{"984":1},"1":{"985":1,"986":1},"2":{"985":2,"986":1}}],["rad2deg",{"2":{"1087":2}}],["radd",{"2":{"1085":1}}],["raii",{"2":{"1671":1,"1764":2}}],["rain",{"2":{"765":1}}],["raise",{"2":{"572":1,"591":3,"1226":1}}],["raising",{"2":{"402":1,"572":1,"1083":1}}],["rag检索有救了",{"2":{"740":1}}],["raw",{"2":{"592":2,"658":1,"1078":8,"2079":1}}],["rangle",{"2":{"1342":1}}],["range=0",{"2":{"700":1}}],["range",{"0":{"1756":1,"1898":1},"2":{"83":2,"160":1,"201":1,"364":1,"383":1,"399":3,"423":1,"424":1,"428":2,"472":1,"522":1,"529":1,"571":1,"572":1,"591":2,"592":4,"700":3,"1087":1,"1098":3,"1099":1,"1102":1,"1202":1,"1205":2,"1211":1,"1215":1,"1216":1,"1217":1,"1218":3,"1231":3,"1233":1,"1234":1,"1235":1,"1236":1,"1237":1,"1238":1,"1239":1,"1240":1,"1241":2,"1242":1,"1243":1,"1244":2,"1245":1,"1246":1,"1247":1,"1254":1,"1279":1,"1284":1,"1295":1,"1328":1,"1329":1,"1330":3,"1398":3,"1440":1,"1684":1,"1713":5,"1732":1,"1756":5,"1762":1,"1897":1,"2086":1}}],["rand",{"2":{"1085":1,"1093":3,"1205":1,"1207":1,"1215":1,"1216":1,"1259":1,"1262":1,"1263":1,"1267":1,"1282":1,"1296":1}}],["randint`",{"2":{"1254":1}}],["randint",{"2":{"383":1,"1070":1,"1218":3,"1250":1,"1254":2,"1295":1,"2086":1}}],["randn",{"2":{"326":2,"801":1,"802":2,"804":1,"805":6,"807":1,"808":1,"809":1,"810":1,"814":1,"815":1,"816":3,"820":2,"821":1,"822":1,"825":1,"826":2,"829":1,"833":4,"835":1,"839":2,"840":2,"841":1,"842":3,"843":2,"844":1,"845":1,"846":1,"847":1,"1070":1,"1076":1,"1082":1,"1083":1,"1086":2,"1092":3,"1095":3,"1096":2,"1097":1,"1098":6,"1114":2,"1211":1,"1212":1,"1213":2,"1250":1,"1269":1,"1270":1,"1272":1,"1283":1,"1295":1,"1297":2,"1298":2,"1299":2,"2086":1}}],["randomcrop",{"2":{"1254":4}}],["random",{"0":{"1175":1,"1176":1,"1177":1,"1319":1},"1":{"1320":1,"1321":1,"1322":1,"1323":1,"1324":1,"1325":1,"1326":1},"2":{"1087":3,"1144":1,"1150":1,"1152":1,"1153":1,"1175":9,"1176":1,"1215":1,"1254":6,"1279":4}}],["randomized",{"2":{"842":1}}],["randomly",{"2":{"428":1,"1254":3}}],["randompacking",{"2":{"90":1}}],["randomsampling",{"2":{"90":2}}],["ranks=mergeable",{"2":{"571":1}}],["ranks",{"2":{"571":3}}],["rank=gpu",{"2":{"423":1}}],["ranked",{"2":{"128":1}}],["rank",{"2":{"115":1,"724":1,"1306":1,"1307":2,"1308":5,"1575":2,"1590":1,"1594":7}}],["rare",{"2":{"20":1,"574":1,"638":3}}],["rohan",{"2":{"1194":1}}],["roy",{"2":{"1194":1}}],["round",{"2":{"1087":4,"1157":2}}],["rounding",{"2":{"1087":6,"1330":1}}],["rotray",{"2":{"1341":1}}],["rot90",{"2":{"1087":1}}],["rotary",{"2":{"201":1,"503":2,"767":1,"768":1,"1341":2,"1345":2}}],["roll",{"2":{"1087":1}}],["roles",{"2":{"2070":2}}],["role",{"2":{"47":1,"91":1,"95":1,"320":1,"906":1}}],["ror",{"2":{"1085":1}}],["roformer",{"2":{"768":1,"1341":1}}],["robust",{"2":{"638":2}}],["robin3d",{"2":{"638":2}}],["roberta中",{"2":{"698":1}}],["roberta",{"2":{"569":1,"844":1,"1315":5}}],["roofline",{"2":{"429":1}}],["root=",{"2":{"1253":1}}],["root",{"2":{"346":1,"361":2,"812":2,"1047":2,"1254":1,"1481":1,"1506":2}}],["row",{"2":{"201":1,"941":1,"960":1,"1087":1,"1284":1,"1329":1,"1634":1,"1705":10,"1797":2}}],["rowparallellinear",{"2":{"8":1,"114":1,"201":1}}],["rope外推的缩放法则",{"2":{"768":1}}],["rope",{"0":{"1342":1,"1344":1},"2":{"201":1,"355":1,"767":1,"768":3,"1341":2,"1342":1,"1343":2,"1344":6}}],["rome",{"0":{"145":1},"2":{"96":1,"156":1}}],["rossixyz",{"2":{"47":1,"95":1,"156":1,"233":1,"292":1,"361":1,"387":1,"429":1,"513":1,"543":1,"638":1,"740":1,"768":1}}],["reverse",{"2":{"1713":2}}],["reversed",{"2":{"1083":1}}],["reinterpret",{"2":{"1629":1,"1820":1,"1838":1}}],["reinforcement",{"2":{"1472":1,"2081":1}}],["reinforce",{"2":{"1083":1}}],["rezero",{"2":{"1180":1}}],["reqires",{"2":{"1106":1}}],["requres",{"2":{"1082":1}}],["requestexception",{"2":{"1566":1}}],["request功能来进行代码审核",{"2":{"1198":1}}],["request",{"2":{"982":1,"983":1,"1196":1,"1198":1}}],["requests",{"2":{"659":1,"1566":3}}],["requirements",{"0":{"1301":1},"2":{"1301":1}}],["require",{"0":{"1093":1},"2":{"1086":1,"1093":2,"1106":1,"1117":1,"1120":1}}],["requiredpermission",{"2":{"2060":2}}],["required",{"0":{"1971":1},"2":{"160":1,"1254":1,"1966":1,"1971":1,"1977":1,"1980":1,"1982":1,"1991":1,"1999":1}}],["requires",{"0":{"1117":1},"2":{"119":1,"383":2,"394":1,"661":5,"1082":2,"1087":11,"1092":2,"1093":3,"1094":1,"1095":9,"1096":3,"1097":1,"1098":8,"1101":1,"1102":2,"1104":5,"1106":3,"1107":2,"1110":1,"1114":2,"1116":5,"1117":8,"1118":1,"1119":2,"1120":1,"1214":2}}],["remain",{"2":{"2087":1}}],["remainder",{"2":{"1087":4,"1607":3,"1625":2}}],["remote",{"2":{"1583":1}}],["removablehandle",{"2":{"1214":5,"1227":6}}],["removed",{"2":{"1930":1}}],["remove",{"2":{"1212":1,"1213":1,"1214":4,"1720":2,"1721":1,"1930":5,"2048":1,"2070":1}}],["removing",{"2":{"151":1}}],["rend",{"2":{"1713":2}}],["render",{"2":{"591":4}}],["renorm",{"2":{"1087":2}}],["rename",{"2":{"1083":5,"1087":2,"1930":1}}],["reward",{"2":{"1083":1}}],["rewritten",{"2":{"731":1}}],["rewrite",{"2":{"731":1}}],["reuse",{"2":{"985":2}}],["reuse意味着具有相同前缀的不同提示可以共享中间kv",{"2":{"985":1}}],["re|",{"2":{"571":1}}],["redix",{"2":{"986":1}}],["redisattention",{"0":{"986":1}}],["reddit",{"2":{"768":1}}],["red",{"2":{"557":1,"1537":1}}],["reducelronplateau",{"0":{"1245":1},"2":{"1230":1,"1245":1}}],["reduce",{"0":{"833":1},"2":{"813":1,"976":2,"1083":3,"1087":12,"1573":1,"1575":1,"1594":2}}],["reduction=",{"2":{"398":1,"399":1,"1215":1}}],["reduction",{"2":{"364":1}}],["reducing",{"2":{"361":2}}],["rejean",{"2":{"429":1}}],["re",{"2":{"348":1,"361":1,"591":1,"724":1,"1329":1,"1342":1,"2075":1}}],["reasoning",{"2":{"627":1}}],["readme",{"2":{"2043":1}}],["readcontent",{"2":{"1902":3}}],["readfile",{"2":{"1761":4}}],["ready=torch",{"2":{"1284":1}}],["ready=trace",{"2":{"1284":1}}],["ready",{"2":{"1284":1}}],["readline",{"2":{"591":4}}],["read",{"2":{"591":5,"950":1,"987":1,"1250":2,"1253":1,"1820":1,"1821":1,"1838":1,"1839":1,"1867":4}}],["reading",{"2":{"292":2}}],["reading在交叉注意力的基础上提出了self",{"2":{"287":1}}],["realloc",{"2":{"1668":2}}],["really",{"2":{"47":1,"692":1,"740":1,"2078":1}}],["realy",{"0":{"1380":1}}],["real",{"2":{"688":2,"1082":1,"1345":2,"1712":8,"2073":1}}],["realformer沿用了post",{"2":{"349":1}}],["realformer相较于前面提到的两种结构",{"2":{"349":1}}],["realformer",{"0":{"349":1},"2":{"293":1,"349":2,"361":1}}],["regnet",{"2":{"1308":15}}],["regiter",{"2":{"1099":1}}],["register",{"2":{"723":2,"1083":2,"1098":1,"1099":2,"1208":9,"1211":5,"1212":1,"1213":1,"1214":13,"1227":6}}],["regular",{"2":{"1086":1}}],["regularization",{"2":{"399":1,"601":1,"638":1,"1064":1,"1404":1}}],["regularizer",{"2":{"396":1,"429":1}}],["regularized",{"2":{"156":1}}],["regression判断在哈夫曼树中走左子树还是右子树",{"2":{"184":1}}],["regressive",{"2":{"122":1,"156":1,"1312":1,"1316":1}}],["retain",{"2":{"1083":1,"1087":1,"1096":2,"1098":2,"1099":2,"1101":3,"1102":2}}],["retains",{"2":{"1082":1,"1110":1}}],["retromae",{"0":{"727":1},"2":{"740":3}}],["retrieval",{"2":{"724":1,"740":3}}],["retrieved",{"2":{"543":1}}],["retrieving",{"2":{"230":1}}],["rethinking",{"2":{"143":1,"233":1,"316":1,"326":1,"361":2,"429":1,"560":1,"616":1,"638":2,"751":1,"764":1,"768":2}}],["returns",{"2":{"76":1,"557":1,"558":1,"572":1,"573":1,"700":1,"1082":2,"1083":1}}],["return",{"2":{"36":1,"38":1,"39":1,"67":1,"74":2,"76":1,"79":1,"82":4,"83":2,"84":2,"110":1,"113":1,"114":1,"119":1,"199":1,"201":3,"343":3,"344":2,"346":3,"372":1,"373":1,"374":1,"375":4,"380":1,"382":1,"383":1,"384":1,"385":1,"394":4,"395":5,"398":1,"399":3,"402":1,"410":1,"449":1,"450":3,"472":1,"503":1,"522":2,"523":1,"529":3,"532":1,"533":1,"557":5,"558":1,"572":1,"573":1,"590":3,"591":1,"592":4,"700":1,"701":1,"702":1,"703":3,"723":1,"807":1,"808":2,"809":1,"810":1,"933":1,"1083":4,"1087":31,"1099":1,"1100":2,"1205":2,"1211":1,"1212":1,"1213":1,"1215":2,"1216":4,"1217":3,"1218":5,"1223":1,"1250":4,"1254":4,"1257":1,"1273":2,"1274":1,"1295":3,"1299":2,"1328":1,"1329":1,"1330":2,"1331":1,"1345":2,"1350":1,"1398":3,"1440":3,"1481":1,"1590":1,"1594":1,"1606":1,"1607":1,"1608":1,"1616":1,"1619":1,"1620":1,"1621":2,"1623":1,"1625":2,"1631":1,"1633":1,"1634":1,"1639":1,"1640":2,"1641":1,"1645":3,"1646":5,"1647":1,"1648":2,"1649":1,"1650":1,"1653":1,"1654":1,"1655":1,"1659":1,"1660":1,"1661":1,"1662":1,"1663":1,"1665":1,"1667":1,"1668":1,"1670":1,"1671":1,"1672":1,"1673":2,"1674":1,"1675":1,"1676":1,"1677":2,"1680":1,"1683":4,"1684":2,"1685":4,"1687":6,"1688":2,"1691":1,"1693":4,"1694":2,"1695":4,"1698":2,"1699":2,"1700":2,"1701":1,"1704":1,"1705":1,"1706":4,"1707":4,"1708":2,"1709":3,"1710":4,"1712":4,"1713":4,"1714":1,"1715":2,"1718":1,"1719":6,"1720":4,"1721":4,"1722":4,"1723":1,"1724":3,"1725":4,"1726":3,"1728":3,"1729":12,"1736":2,"1737":2,"1738":2,"1739":1,"1741":2,"1742":1,"1743":1,"1744":1,"1746":1,"1747":1,"1749":1,"1750":2,"1751":1,"1752":1,"1754":1,"1755":1,"1756":1,"1761":3,"1762":2,"1763":2,"1772":1,"1774":2,"1778":1,"1779":2,"1784":1,"1788":2,"1789":3,"1791":1,"1797":1,"1799":1,"1800":1,"1801":1,"1802":1,"1803":1,"1805":1,"1806":1,"1807":1,"1811":1,"1813":1,"1814":2,"1816":1,"1817":1,"1820":2,"1821":2,"1824":1,"1825":3,"1829":1,"1831":1,"1832":2,"1834":1,"1835":1,"1838":2,"1839":2,"1842":1,"1843":3,"1849":1,"1853":1,"1857":1,"1861":1,"1866":1,"1867":1,"1868":3,"1869":1,"1874":1,"1883":4,"1887":4,"1891":2,"1897":1,"1902":5,"1905":5,"1906":4,"1907":5,"1908":2,"1909":1,"1910":1,"1911":1,"1912":1,"1914":1,"1916":2,"1921":4,"1922":1,"1923":1,"1924":3,"1925":1,"1926":1,"1927":5,"1928":1,"1929":1,"1930":1,"1931":1,"1933":5,"1999":6,"2003":1,"2004":1,"2005":1,"2006":5,"2007":1,"2008":1,"2059":5,"2060":2,"2061":2,"2062":3,"2063":2,"2086":1,"2153":1}}],["ref",{"2":{"1214":1,"2005":2}}],["refine",{"2":{"1083":1,"1087":1}}],["refinement",{"2":{"209":5}}],["refining",{"0":{"135":1},"2":{"96":1,"133":1}}],["reffnement",{"2":{"209":1}}],["reformer",{"2":{"204":2,"513":1}}],["reference",{"0":{"1612":1},"2":{"131":1,"1650":1,"1729":2,"2007":1}}],["related",{"2":{"573":1,"2079":1}}],["relative",{"2":{"742":1,"759":1,"768":1,"1339":1}}],["relating",{"2":{"490":1}}],["relationship",{"2":{"1479":1}}],["relationships",{"2":{"688":1,"764":1}}],["relation",{"2":{"130":2,"136":1,"156":2}}],["relational",{"2":{"122":1}}],["rel",{"2":{"503":3}}],["relying",{"2":{"235":1}}],["relu还是统治着深度学习的激活函数",{"2":{"846":1}}],["relu的输出不是zero",{"2":{"840":1}}],["relu不会对数据做幅度压缩",{"2":{"840":1}}],["relu神经元坏死了",{"2":{"840":1}}],["relu在训练的时候很",{"2":{"840":1}}],["relu在正区间的梯度为常数1",{"2":{"104":1}}],["relu激活函数在",{"2":{"840":1}}],["relu激活使得模型能够学习非线性化能力",{"2":{"99":1}}],["relu被用作卷积层和全连接层之间的激活函数",{"2":{"840":1}}],["relu可以更好地处理稀疏激活和非线性特征",{"2":{"838":1}}],["relu6",{"0":{"841":1},"2":{"110":2,"841":5,"845":1}}],["relusquaredactivation",{"2":{"110":1}}],["relu2",{"2":{"110":1}}],["relu函数的优点是计算简单",{"2":{"104":1}}],["relu函数在输入大于0时输出等于输入",{"2":{"104":1}}],["relu函数是修正线性单元函数",{"2":{"104":1}}],["relu成为许多",{"2":{"103":1}}],["relu",{"0":{"104":1,"840":1},"2":{"96":1,"97":1,"99":3,"103":2,"104":3,"107":2,"108":1,"110":3,"113":1,"125":2,"212":1,"213":1,"334":1,"394":1,"642":1,"785":1,"838":4,"840":11,"841":5,"844":1,"1087":2,"1115":1,"1143":2,"1205":1,"1207":2,"1215":7,"1216":1,"1217":1,"1218":1,"1257":3,"1295":1,"2086":2}}],["recv",{"2":{"1573":1,"1575":1,"1590":1}}],["recursive",{"0":{"1646":1},"2":{"1330":1,"1646":1}}],["recurse",{"2":{"1214":5}}],["recurse=true",{"2":{"1214":2}}],["recurrence",{"2":{"543":1}}],["recurrent",{"0":{"850":1,"872":1},"1":{"873":1,"874":1,"875":1},"2":{"260":1,"288":1,"292":2,"543":3,"850":1,"1404":1}}],["recht",{"2":{"1175":1}}],["reciprocal",{"2":{"1087":2}}],["recipe",{"2":{"89":1,"95":1}}],["recently",{"2":{"985":1}}],["recentering",{"2":{"320":1}}],["record",{"2":{"1087":1,"1284":1}}],["recompute",{"0":{"946":1,"966":1}}],["recommendation",{"2":{"209":2,"233":1,"2082":1}}],["recognition",{"2":{"294":1,"768":1,"906":1,"2079":1,"2081":1,"2082":1}}],["recsys",{"2":{"233":1}}],["rectifiers",{"2":{"156":1}}],["rectifier",{"2":{"156":1}}],["rectified",{"2":{"103":1,"104":1,"156":1,"840":2,"842":3}}],["recall2",{"2":{"1331":2}}],["recall",{"2":{"122":2,"156":2,"437":1,"1331":2}}],["resnext50",{"2":{"1308":1}}],["resnext101",{"2":{"1308":1}}],["resnet34",{"2":{"1308":1}}],["resnet152",{"2":{"1308":1}}],["resnet101",{"2":{"1308":2}}],["resnet18",{"2":{"1272":1,"1303":1,"1304":1,"1308":2}}],["resnet50",{"2":{"1282":1,"1283":2,"1306":1,"1307":2,"1308":2}}],["resnet始终在学习残差函数",{"2":{"301":1}}],["resnet有两篇论文",{"2":{"300":1}}],["resnet论文中者给出了例子如下图",{"2":{"299":1}}],["resnet是何凯明大神的经典之作",{"2":{"286":1}}],["resnet",{"0":{"286":1},"2":{"115":1,"300":1,"510":1,"1150":1,"1177":1,"1283":1,"1300":1,"1303":1}}],["resume",{"2":{"1308":2}}],["result2",{"2":{"1687":6}}],["result1",{"2":{"1687":6}}],["resulttype",{"2":{"1485":1}}],["resulttype=",{"2":{"1481":1,"1485":3}}],["results",{"2":{"807":3,"808":3,"809":3,"810":3,"1566":4}}],["result",{"2":{"8":1,"592":1,"1083":1,"1086":1,"1100":5,"1114":3,"1214":1,"1254":1,"1632":1,"1646":3,"1706":2,"1710":7,"1715":3,"1729":3,"1741":3,"1762":3,"1788":4,"1789":4,"1824":2,"1842":2,"1905":4,"1924":2,"1927":6,"1933":3,"2061":3,"2154":2}}],["response",{"2":{"1566":4}}],["responses",{"2":{"429":1}}],["respectively",{"2":{"1254":1}}],["rescale",{"2":{"1254":6}}],["rescaling",{"2":{"320":1}}],["resize",{"2":{"1083":3,"1087":6,"1254":1,"1902":1}}],["residual等技术",{"2":{"396":1}}],["residual",{"0":{"497":1},"2":{"294":1,"296":1,"302":2,"344":2,"349":2,"361":1,"446":1,"470":1,"497":1,"517":1}}],["res",{"2":{"805":2,"1713":1,"1891":9}}],["resourceholder",{"2":{"1676":5}}],["resource>>",{"2":{"1891":1}}],["resource>",{"2":{"1482":4,"1891":5}}],["resource=",{"2":{"1481":1}}],["resource",{"2":{"1411":2,"1412":1,"1481":2,"1671":1,"1676":3,"1764":1,"1891":5}}],["resources>",{"2":{"1482":2}}],["resources",{"2":{"724":1,"1481":2,"1482":1,"1891":3}}],["resolve",{"2":{"1087":2}}],["resolution",{"2":{"232":1}}],["reshapes",{"2":{"819":1}}],["reshape不会错误",{"2":{"658":1}}],["reshape",{"0":{"818":1,"819":1},"1":{"819":1,"820":1,"821":1,"822":1},"2":{"503":1,"658":1,"700":3,"810":1,"819":2,"831":1,"1078":1,"1079":1,"1080":1,"1087":3,"1102":2,"1202":2,"1205":2,"1217":4,"1283":1,"1345":2,"1398":4}}],["reset",{"2":{"702":1,"874":2,"1299":2,"1695":1,"2086":2}}],["researchers",{"2":{"2081":1}}],["research",{"0":{"2081":1},"2":{"429":1,"768":1,"1195":1,"2073":1,"2087":1}}],["reserved",{"2":{"571":2}}],["reserve",{"2":{"395":1}}],["restart",{"2":{"2066":1}}],["restarts",{"2":{"1243":1,"1244":2,"1308":1}}],["restricted",{"2":{"104":1,"156":1}}],["rest",{"2":{"20":1,"47":1,"2070":1}}],["repository",{"2":{"1989":1,"1990":1}}],["replicate",{"2":{"1214":1}}],["replace操作",{"2":{"1098":1}}],["replacement",{"2":{"591":1,"1087":1}}],["replace",{"0":{"1743":1},"2":{"590":2,"591":2,"592":2,"1098":2,"1713":4,"1732":1,"1743":2,"1758":1}}],["represents",{"2":{"2086":1}}],["representations",{"2":{"131":1,"235":1,"237":1,"282":1,"292":1,"490":1,"688":1,"713":1,"719":1,"740":5,"759":1,"768":1,"772":1,"1312":1,"1339":1}}],["representation",{"2":{"5":1,"137":1,"350":1,"513":2,"638":1,"713":1,"717":1,"1214":1,"1455":1,"2062":1}}],["repr",{"2":{"1083":3,"1087":1,"1214":3,"1227":3}}],["repeat=1",{"2":{"1284":2}}],["repeats",{"2":{"1087":4,"1284":1}}],["repeat",{"2":{"201":3,"503":2,"1087":4}}],["rep复制出相应的份数",{"2":{"201":1}}],["rep",{"2":{"8":1,"201":3,"503":5}}],["x始终是加一",{"2":{"2023":1}}],["x|∣δy∣",{"2":{"2018":1}}],["x|",{"2":{"2018":1}}],["x||",{"2":{"692":1}}],["xstep=​k​​1​​=​δy​​δx​​",{"2":{"2018":1}}],["xstep=1k=δxδyxstep",{"2":{"2018":1}}],["xstep=1xstep",{"2":{"2018":1}}],["xstepx​i+1​​=x​i​​+xstep",{"2":{"2018":1}}],["xshell",{"2":{"1583":1}}],["x和y增长哪个更快",{"2":{"2016":1}}],["xchg指令",{"0":{"1423":1}}],["x∣z",{"2":{"1377":2}}],["x∣y",{"2":{"908":8}}],["x几乎没有改变",{"2":{"1363":1}}],["xmlstring",{"2":{"1489":2}}],["xml中添加insert语句",{"2":{"1486":1}}],["xml中添加select语句",{"2":{"1485":1}}],["xml中的namespace改为为usermapper的路径",{"2":{"1484":1}}],["xml配置文件",{"2":{"1481":1}}],["xml数据文件中等等",{"2":{"1477":1}}],["xml",{"2":{"1476":1,"1481":7,"1482":3,"1485":3,"1486":1,"1487":1,"1488":1}}],["xm",{"2":{"1342":4,"1343":1}}],["xmax",{"2":{"191":2}}],["xpu",{"2":{"1214":2}}],["xpos",{"2":{"768":1}}],["x²",{"2":{"1114":1}}],["xor",{"2":{"1085":3,"1087":6,"1635":1}}],["x∼u",{"2":{"1007":2}}],["x​i+1​​",{"2":{"2018":1}}],["x​i​​w​q​​w​k​⊤​​x​j​⊤​​+β​i",{"2":{"1340":1}}],["x​i​​w​q​​",{"2":{"1339":1}}],["x​i​​",{"2":{"847":2,"943":1,"961":1,"1322":1,"1323":3,"2018":1}}],["x​m​​",{"2":{"1342":4,"1343":1}}],["x​k​​+p​k​​",{"2":{"1335":1}}],["x​k​​",{"2":{"1335":1}}],["x​0​​",{"2":{"1323":2,"2018":1}}],["x​l+1​​=f",{"2":{"1004":1}}],["x​l​2​​",{"2":{"1003":4}}],["x​l​​=f",{"2":{"1003":1}}],["x​l​​",{"2":{"1003":7}}],["x​2​​",{"2":{"1002":1,"1322":1,"1462":1}}],["x​n​​",{"2":{"1002":1,"1322":1,"1323":1,"1342":4}}],["x​1​​",{"2":{"1002":1,"1322":1,"1323":1,"1462":1,"2018":1}}],["x​1​​+⋯+x​n​​",{"2":{"1002":1}}],["x​",{"2":{"943":14,"961":14}}],["x​t​​",{"2":{"855":1}}],["x​j​​w​v​​+r​i",{"2":{"1339":1}}],["x​j​​w​v​​+p​j​​w​v​​",{"2":{"1339":1}}],["x​j​​w​k​​+r​i",{"2":{"1339":1}}],["x​j​​",{"2":{"847":1}}],["xφ",{"2":{"844":4}}],["x≤x",{"2":{"844":2}}],["xjwv+ri",{"2":{"1339":1}}],["xjwv+pjwv",{"2":{"1339":1}}],["xjwk+ri",{"2":{"1339":1}}],["xj",{"2":{"766":2,"847":1}}],["xj+pj",{"2":{"762":2}}],["xlm",{"2":{"1315":9}}],["xlogy",{"2":{"1087":4}}],["xl+1=f",{"2":{"1004":1}}],["xl2",{"2":{"1003":4}}],["xl=f",{"2":{"1003":1}}],["xlx",{"2":{"1003":2}}],["xl的位置编码类似",{"2":{"761":1}}],["xl的论文transformer",{"2":{"760":1}}],["xl",{"2":{"760":2,"766":1,"768":1,"1003":5,"1367":1}}],["xlnet式位置编码源自transformer",{"2":{"760":1}}],["xlnet",{"0":{"760":1},"2":{"569":1,"698":1,"741":1,"768":1}}],["xencxencx",{"2":{"727":2}}],["xdecxdecx",{"2":{"727":1}}],["xdyyc",{"2":{"575":2}}],["xdxyxyc",{"2":{"575":1}}],["xyz",{"2":{"1713":2}}],["xy",{"2":{"575":1,"1002":2}}],["x可能是上一层的输出或者是整个解码器的输出",{"2":{"533":1}}],["x⊤i⋅δixi⊤⋅δix",{"2":{"485":1}}],["xavier",{"0":{"1000":1},"2":{"449":1,"701":1,"999":1}}],["xavier初始化可以参考论文",{"2":{"449":1}}],["xavier初始化方法根据输入和输出神经元的数量来调整权重的初始值",{"2":{"403":1}}],["x∈rdinput×dmodelx∈rdinput×dmodelx∈r^",{"2":{"419":1}}],["x∈rn×c×h×w𝑥∈𝑅𝑁×𝐶×𝐻×𝑊𝑥∈𝑅^",{"2":{"340":1}}],["x从1开始不断增大",{"2":{"399":1}}],["x是token的索引",{"2":{"701":1}}],["x是tgt",{"2":{"538":2}}],["x是从0到100的一个不断增大的数",{"2":{"399":1}}],["x是前一层的输出",{"2":{"344":1}}],["x是decoder第一部分的输出",{"2":{"82":1}}],["x的梯度",{"2":{"1094":1}}],["x的词数会不断增加",{"2":{"532":1,"533":1}}],["x的形状是",{"2":{"532":1,"533":1}}],["x的形状都不会改变",{"2":{"343":1}}],["x的形状和解码器的输入一样",{"2":{"343":1}}],["x的shape为",{"2":{"36":1,"343":1}}],["x代表来自上一层",{"2":{"343":1}}],["x0x",{"2":{"1323":1}}],["x0x0x^0",{"2":{"501":1}}],["x0",{"2":{"334":2,"1308":1,"1323":1,"1377":1,"2018":1}}],["x++",{"2":{"2008":1}}],["x+3",{"2":{"845":3}}],["x+0",{"2":{"844":3}}],["x+layernorm",{"2":{"523":3}}],["x+",{"2":{"523":1}}],["x+f",{"2":{"332":1}}],["x+multiheadattention",{"2":{"330":2}}],["x+sublayer",{"2":{"330":2,"519":6,"523":2}}],["x表示模型的输入",{"2":{"748":1}}],["x表示",{"2":{"330":1}}],["x^",{"2":{"844":1,"943":14,"961":14,"1002":1}}],["x^l",{"2":{"501":1}}],["x^k",{"2":{"313":1}}],["x^5",{"2":{"313":1}}],["x5x^1",{"2":{"313":1}}],["x5x1",{"2":{"313":1}}],["x4",{"2":{"267":1}}],["x3",{"2":{"267":1,"878":1}}],["x3c",{"2":{"66":1,"76":1,"201":1,"315":3,"375":1,"380":13,"383":1,"384":8,"385":3,"395":6,"398":3,"399":7,"408":5,"423":2,"428":6,"557":17,"558":13,"571":15,"572":1,"579":4,"580":1,"582":11,"583":12,"584":3,"588":6,"590":2,"591":1,"679":5,"702":2,"1083":5,"1085":2,"1102":1,"1235":5,"1236":3,"1350":2,"1481":43,"1482":26,"1485":10,"1486":2,"1487":2,"1488":2,"1489":6,"1523":2,"1590":2,"1594":4,"1606":5,"1607":71,"1608":36,"1611":17,"1615":2,"1616":13,"1619":23,"1620":8,"1621":23,"1623":13,"1624":32,"1625":20,"1629":1,"1633":42,"1634":15,"1638":6,"1639":6,"1640":6,"1645":54,"1646":38,"1647":19,"1648":31,"1649":41,"1650":101,"1653":8,"1654":12,"1659":13,"1660":13,"1661":13,"1662":7,"1663":35,"1665":86,"1667":23,"1668":17,"1670":9,"1671":2,"1672":7,"1673":34,"1674":46,"1675":26,"1676":9,"1677":12,"1680":26,"1683":27,"1684":19,"1685":21,"1687":26,"1688":26,"1691":37,"1693":14,"1694":24,"1695":41,"1696":2,"1698":30,"1699":31,"1700":17,"1701":13,"1704":40,"1705":42,"1706":24,"1707":31,"1708":34,"1709":13,"1710":3,"1712":27,"1713":129,"1714":51,"1715":47,"1718":10,"1719":68,"1720":44,"1721":44,"1722":44,"1723":9,"1724":32,"1725":51,"1726":7,"1728":55,"1729":52,"1736":8,"1737":8,"1738":8,"1739":10,"1741":11,"1742":11,"1743":10,"1744":10,"1746":10,"1747":10,"1749":10,"1750":15,"1751":10,"1752":18,"1754":14,"1755":8,"1756":16,"1761":12,"1762":22,"1763":21,"1772":6,"1774":12,"1778":6,"1779":12,"1784":24,"1788":13,"1789":41,"1791":17,"1792":17,"1797":36,"1799":11,"1800":11,"1801":11,"1802":11,"1803":6,"1805":15,"1806":21,"1807":37,"1811":14,"1813":26,"1814":21,"1816":5,"1817":80,"1820":30,"1821":34,"1824":29,"1825":37,"1829":14,"1831":26,"1832":21,"1834":5,"1835":80,"1838":30,"1839":34,"1842":29,"1843":37,"1849":5,"1853":11,"1857":11,"1861":11,"1866":12,"1867":6,"1868":8,"1869":20,"1874":25,"1879":1,"1883":30,"1887":37,"1891":27,"1895":6,"1897":18,"1898":5,"1902":27,"1905":7,"1906":14,"1907":12,"1908":19,"1909":9,"1910":13,"1911":25,"1912":28,"1914":43,"1917":12,"1921":24,"1922":26,"1923":8,"1924":13,"1925":31,"1926":39,"1927":40,"1928":23,"1929":26,"1930":57,"1931":4,"1933":41,"1999":25,"2003":13,"2004":9,"2005":11,"2006":33,"2007":7,"2008":5,"2059":45,"2060":20,"2061":10,"2062":79,"2063":18,"2069":3,"2070":2,"2153":2}}],["xtx",{"2":{"855":1}}],["xtxtx",{"2":{"334":1}}],["xt=x0+f0",{"2":{"334":2}}],["xt=p",{"2":{"240":1}}],["xt+1",{"2":{"334":2}}],["xt+1=norm",{"2":{"329":2,"332":2}}],["xt+1=xt+ft",{"2":{"329":2,"334":1}}],["xt+ft",{"2":{"329":2,"332":2}}],["xt",{"2":{"329":4,"332":2,"334":4,"1398":1}}],["xt∣ht",{"2":{"240":2}}],["xt−1t",{"2":{"334":1}}],["xt−1√t",{"2":{"334":1}}],["xt−1",{"2":{"240":2}}],["xkx",{"2":{"1335":1}}],["xkxkx",{"2":{"748":1}}],["xk+pkx",{"2":{"1335":1}}],["xk+pk",{"2":{"748":2}}],["xki−μi",{"2":{"313":1}}],["xkixikx^k",{"2":{"313":1}}],["xk",{"2":{"201":6,"1345":16}}],["xk和xv的形状",{"2":{"201":1}}],["xk和xv",{"2":{"201":1}}],["xq",{"2":{"201":11,"1345":19}}],["x=f",{"2":{"1442":2}}],["x=",{"2":{"178":2,"241":2,"399":1,"613":2,"748":3,"1322":3,"1323":3,"1440":1}}],["x⋅ki",{"2":{"128":1}}],["x⋅k^⊤",{"2":{"125":1}}],["x⋅k⊤",{"2":{"125":2}}],["x√2",{"2":{"106":1}}],["xw+b",{"2":{"105":4,"113":1}}],["xw",{"2":{"99":2,"105":2,"519":1}}],["xw1+b1",{"2":{"99":4,"519":2}}],["xxx",{"2":{"105":1,"106":1,"662":1,"1533":1}}],["xx",{"2":{"71":1,"722":1}}],["xn∗dxn∗dx",{"2":{"445":1}}],["xnx",{"2":{"418":1,"1342":1}}],["xnx1",{"2":{"418":1}}],["xnxnx",{"2":{"58":1}}],["xn",{"2":{"178":2,"241":2,"1002":1,"1322":1,"1323":1,"1342":3}}],["xn+1xn+1x",{"2":{"58":3}}],["xi+1=xi+xstepx",{"2":{"2018":1}}],["xi+1",{"2":{"2018":1}}],["xiwqwk⊤xj⊤+βi",{"2":{"1340":1}}],["xiwq",{"2":{"1339":1}}],["xix",{"2":{"1322":1,"1323":2}}],["xixix",{"2":{"148":2,"172":1,"178":1,"485":4}}],["xinghaochen",{"2":{"361":1}}],["xik−μi",{"2":{"313":1}}],["xi生成一个隐状态hi",{"2":{"249":1}}],["xi",{"2":{"54":2,"178":2,"183":1,"191":10,"249":1,"613":2,"766":2,"847":2,"943":1,"961":1,"1323":1,"2018":1}}],["xiaogp",{"2":{"47":2,"95":1,"543":1}}],["xvf",{"2":{"1535":1}}],["xv+c",{"2":{"105":4}}],["xv",{"2":{"54":2,"105":2,"201":4,"1345":4}}],["x2x",{"2":{"1462":1}}],["x27",{"2":{"999":1,"1184":2}}],["x2可以看做是第二个单词",{"2":{"878":1}}],["x23",{"2":{"334":1}}],["x2√3",{"2":{"334":1}}],["x26",{"2":{"74":1,"79":1,"343":1,"380":1,"394":1,"395":2,"522":1,"1481":2,"1537":2,"1590":4,"1594":6,"1611":7,"1612":2,"1614":4,"1619":4,"1629":1,"1633":3,"1641":2,"1645":1,"1648":2,"1649":3,"1650":4,"1674":1,"1675":1,"1684":2,"1685":5,"1691":9,"1694":1,"1704":1,"1708":1,"1712":7,"1713":1,"1726":1,"1729":4,"1739":2,"1741":1,"1742":1,"1743":1,"1744":1,"1747":1,"1749":1,"1750":3,"1751":1,"1752":1,"1761":2,"1762":1,"1763":4,"1770":1,"1772":2,"1774":4,"1778":1,"1779":2,"1784":4,"1788":4,"1789":10,"1791":2,"1792":6,"1797":1,"1807":1,"1825":1,"1843":1,"1883":2,"1887":10,"1891":1,"1898":1,"1902":6,"1912":3,"1914":15,"1921":1,"1926":6,"1928":2,"1930":1,"1931":1,"1933":3,"2005":2,"2006":2,"2007":2,"2059":2,"2060":3,"2061":1,"2062":3,"2063":1}}],["x2",{"2":{"54":2,"106":1,"178":2,"267":1,"845":1,"878":1,"1002":1,"1308":1,"1322":1,"1331":5}}],["x1+⋯+xn",{"2":{"1002":1}}],["x1+p1",{"2":{"748":2}}],["x1可以看做是第一个单词",{"2":{"878":1}}],["x1和x2互相不知道对方的信息",{"2":{"519":1}}],["x1和x2通过某种信息交换和杂糅",{"2":{"519":1}}],["x12",{"2":{"334":1}}],["x1√2",{"2":{"334":1}}],["x1x1x",{"2":{"172":1}}],["x1",{"2":{"54":2,"178":2,"241":2,"267":1,"313":1,"878":1,"1002":1,"1308":2,"1322":1,"1323":1,"1377":1,"1462":1,"2018":1}}],["x",{"2":{"8":8,"36":9,"38":8,"39":12,"54":6,"71":2,"82":12,"83":4,"99":3,"104":6,"105":6,"106":17,"108":16,"109":14,"110":10,"113":2,"114":9,"117":1,"122":3,"125":5,"134":11,"148":1,"161":6,"168":1,"172":3,"178":8,"183":4,"191":20,"192":1,"201":20,"230":5,"240":3,"241":3,"294":2,"300":3,"301":36,"304":5,"313":1,"329":6,"330":12,"332":8,"334":11,"343":20,"344":29,"346":6,"359":1,"394":10,"398":6,"399":10,"410":5,"418":1,"419":1,"446":2,"463":1,"470":1,"472":5,"478":1,"485":1,"501":1,"502":1,"503":5,"504":4,"510":1,"513":1,"519":6,"520":20,"522":5,"523":15,"529":16,"530":1,"532":5,"533":13,"538":4,"557":3,"612":6,"613":1,"614":1,"640":7,"674":1,"692":3,"700":2,"701":2,"704":2,"709":1,"731":6,"736":3,"745":1,"748":2,"762":1,"766":3,"781":1,"807":13,"808":15,"809":11,"810":17,"820":3,"821":2,"822":2,"825":6,"827":3,"838":2,"839":17,"840":10,"841":6,"844":21,"845":8,"846":5,"847":3,"856":1,"879":3,"903":6,"908":17,"912":2,"914":3,"943":74,"944":5,"961":74,"963":1,"977":1,"1002":15,"1003":22,"1004":7,"1007":1,"1069":5,"1083":2,"1085":1,"1087":1,"1093":2,"1094":3,"1095":5,"1096":2,"1098":9,"1099":1,"1114":7,"1115":1,"1116":6,"1133":1,"1134":1,"1147":1,"1180":8,"1202":6,"1205":14,"1211":6,"1213":2,"1215":60,"1216":23,"1217":6,"1218":35,"1254":5,"1257":24,"1295":8,"1299":2,"1308":7,"1322":7,"1323":4,"1331":6,"1339":4,"1340":4,"1342":7,"1343":5,"1345":8,"1377":18,"1398":17,"1438":2,"1440":16,"1442":4,"1462":1,"1512":1,"1544":1,"1548":1,"1603":1,"1615":2,"1630":3,"1632":3,"1641":2,"1650":12,"1699":3,"1709":6,"1788":7,"1883":6,"1905":2,"1906":2,"1907":7,"1914":8,"1925":2,"2003":6,"2004":2,"2006":5,"2007":2,"2008":1,"2018":17,"2021":4,"2059":3,"2060":4,"2086":8}}],["g++",{"2":{"1605":1,"1916":4,"1917":3}}],["gfortran",{"2":{"1589":1}}],["gg",{"2":{"1547":1}}],["gcount",{"2":{"1813":1,"1831":1}}],["gcc",{"2":{"1589":1,"1605":3}}],["gc",{"2":{"1554":1}}],["gcp",{"2":{"1499":1}}],["gcd",{"2":{"1087":2}}],["g​",{"2":{"1184":2}}],["ghost",{"2":{"1168":2}}],["g表示具有g个组的分组查询",{"2":{"937":1,"953":1}}],["gmail等",{"2":{"885":1}}],["g=16",{"2":{"810":1}}],["goto",{"2":{"1631":1}}],["got",{"2":{"1226":1}}],["godbole",{"2":{"1124":1,"1195":1}}],["good",{"2":{"1010":1,"1814":2,"1832":2}}],["goodfellow",{"2":{"543":1}}],["google把第一项位置去掉",{"2":{"1339":1}}],["google把这种方法引入到了nlp领域",{"2":{"575":1}}],["googlenet",{"2":{"1308":1}}],["googlegroups",{"2":{"1196":1}}],["google",{"2":{"235":1,"844":1,"845":1,"1124":1,"1195":1,"1197":1,"1312":1,"1526":1}}],["gound",{"2":{"399":1}}],["gzip",{"2":{"1535":1}}],["gz",{"2":{"370":4,"1309":1,"1535":1}}],["guaranteed",{"2":{"1931":1}}],["guards",{"2":{"1628":1}}],["gui",{"2":{"1645":1}}],["guide",{"2":{"1201":1,"1219":1,"1404":1}}],["gunzip",{"2":{"1535":1}}],["guo",{"2":{"361":1}}],["guessous",{"2":{"95":1}}],["gn将channel方向分group",{"2":{"810":1}}],["gn将channel分组",{"2":{"341":1}}],["gnns",{"2":{"513":1}}],["gn的极端情况就是ln和in",{"2":{"341":1}}],["gn",{"2":{"325":1,"338":3,"340":1,"341":1,"808":1}}],["giving",{"2":{"1659":1}}],["givebirth",{"2":{"1659":2}}],["given",{"2":{"504":1,"513":1,"590":1,"592":2,"1208":1,"1254":1}}],["gilmer",{"2":{"1124":1,"1195":1}}],["git",{"2":{"432":1,"1309":2,"1332":2,"1347":1,"1989":2,"1990":3}}],["github上找",{"2":{"1481":1}}],["github",{"2":{"90":1,"156":1,"361":2,"370":1,"373":1,"387":2,"429":1,"432":1,"503":1,"513":1,"638":1,"768":1,"1067":1,"1073":1,"1195":1,"1196":1,"1198":1,"1309":1,"1332":1,"1347":1,"1476":2,"1989":1,"1990":1}}],["girl",{"2":{"370":1,"557":1}}],["giant",{"2":{"370":1}}],["gianni",{"2":{"292":1}}],["glove和fasttext等模型可以将单词映射到低维连续向量空间",{"2":{"711":1}}],["glove",{"2":{"676":1,"706":1,"709":1}}],["glorot条件和xavier方法是在2010年提出的",{"2":{"1000":1}}],["glorot条件",{"2":{"999":1}}],["glorot",{"0":{"999":1},"2":{"449":1,"999":2}}],["globalvar",{"2":{"1649":4}}],["globalavgpool",{"2":{"816":1}}],["global",{"0":{"816":1},"2":{"285":1,"442":1,"758":2,"1649":1}}],["glm",{"2":{"347":1,"1341":1}}],["glue",{"2":{"1315":2}}],["glu其实不算是一种激活函数",{"2":{"105":1}}],["glu激活则提出于2016年发表的论文",{"2":{"105":1}}],["glu",{"0":{"105":1},"2":{"96":1,"103":1,"105":6}}],["gsgsg^s和gogog^o是两个可学习的极性感知系数矩阵",{"2":{"213":1}}],["gsai",{"2":{"156":1}}],["gan",{"2":{"1472":1}}],["gan训练",{"2":{"1214":1}}],["ganesh",{"2":{"513":1}}],["gamma​cycle​i​​terations​​",{"2":{"1241":1}}],["gammacycleiterationsgamma^",{"2":{"1241":1}}],["gamma=0",{"2":{"1231":3,"1235":1,"1236":1,"1239":1,"1246":1,"1247":1}}],["gamma=args",{"2":{"1215":1}}],["gamma和ββ",{"2":{"313":1}}],["gamma",{"2":{"313":1,"343":4,"360":1,"640":1,"807":2,"808":2,"809":2,"810":2,"1189":1,"1190":2,"1191":1,"1215":3,"1235":1,"1236":1,"1239":2}}],["garfield大杂烩",{"2":{"233":1}}],["garden",{"2":{"156":1}}],["gap对整个网络在结构上做正则化防止过拟合",{"2":{"816":1}}],["gap",{"2":{"222":1}}],["gaussian",{"0":{"844":1},"2":{"103":1,"106":2,"156":1}}],["gate",{"2":{"110":2,"229":1,"231":1,"865":1,"866":1,"868":1,"874":4}}],["gated激活函数",{"2":{"108":1}}],["gated",{"0":{"872":1},"1":{"873":1,"874":1,"875":1},"2":{"98":1,"105":2,"107":2,"108":1,"156":1}}],["gather和reduce",{"2":{"420":1}}],["gather",{"2":{"8":3,"114":2,"201":3,"976":3,"1087":2,"1330":1,"1573":1,"1575":1}}],["grdients",{"2":{"1106":1}}],["greet",{"2":{"1729":6}}],["greeting",{"2":{"1713":11,"1803":3}}],["greedy",{"2":{"83":2,"424":1,"901":1}}],["grep",{"2":{"1516":2}}],["gregkamradt",{"2":{"768":1}}],["greater",{"2":{"1087":8,"1883":1}}],["great",{"2":{"757":3,"2073":1,"2081":1}}],["grothendieck",{"2":{"513":1}}],["ground",{"2":{"406":1,"528":1}}],["groupid>mysql",{"2":{"1481":1}}],["groupid>",{"2":{"1481":2}}],["groupid>org",{"2":{"1481":1}}],["grouped",{"0":{"936":1,"952":1},"1":{"937":1,"953":1}}],["groups",{"2":{"338":1,"385":1,"666":1,"810":2,"1226":9,"1227":4,"1233":1,"1235":1,"1236":1,"1237":1,"1238":1,"1240":1,"1243":1,"1246":1,"1247":1}}],["group",{"0":{"775":1,"810":1},"2":{"338":5,"361":1,"423":1,"640":1,"810":1,"1226":3,"1227":3,"1513":2,"1530":1}}],["groupnorm永远不再batch维度上做平均",{"2":{"810":1}}],["groupnorm",{"0":{"338":1},"2":{"293":1,"503":1,"810":4}}],["groupbylength+padding",{"2":{"90":1}}],["grid",{"2":{"490":1,"1283":5}}],["gru网络来避免梯度消失问题",{"2":{"885":1}}],["gru的张量操作较少",{"2":{"874":1}}],["gru摒弃了细胞状态",{"2":{"874":1}}],["gru是循环神经网络的新一代",{"2":{"874":1}}],["gru就是典型代表",{"2":{"861":1}}],["gru",{"0":{"872":1,"873":1},"1":{"873":1,"874":1,"875":1},"2":{"249":1,"334":1}}],["granted",{"2":{"2060":1}}],["grafana",{"2":{"1499":1}}],["grayscale",{"2":{"1283":1}}],["gram和cbow非常类似",{"2":{"714":1}}],["gram正好相反",{"2":{"714":1}}],["gram嵌入和交叉注意力机制的优点",{"2":{"614":1}}],["gram模型的大小几乎是n的指数倍",{"2":{"242":1}}],["gram模型",{"2":{"242":1}}],["gram",{"2":{"127":1,"204":1,"288":1,"477":1,"714":1}}],["grades",{"2":{"1825":5,"1843":5}}],["grade",{"2":{"1825":7,"1843":7}}],["grad属性",{"2":{"1214":1}}],["grad=false",{"2":{"1120":1}}],["grad=true时",{"2":{"1107":1}}],["grad=true",{"2":{"1082":1,"1092":1,"1093":1,"1094":1,"1095":3,"1096":2,"1098":7,"1101":1,"1102":2,"1106":1,"1114":2,"1116":3,"1117":3,"1120":1}}],["grad=true的",{"2":{"659":1}}],["grad=true才有最终的grad",{"2":{"659":1}}],["grads=true的不进行梯度计算",{"2":{"665":1}}],["gradient=none",{"2":{"1083":1}}],["gradient",{"0":{"1022":1,"1025":1,"1026":1,"1033":1,"1188":1,"2076":1},"1":{"1023":1,"1024":1,"1025":1,"1026":1,"1027":1,"1034":1,"1035":1,"1036":1},"2":{"296":2,"298":1,"542":1,"543":1,"661":1,"1023":1,"1025":1,"1026":1,"1027":7,"1067":1,"1096":2,"1104":1,"1155":1,"1213":2,"1223":1,"1243":1,"1244":1}}],["gradients",{"2":{"134":1,"148":1,"156":1,"334":1,"483":1,"513":1,"1093":2,"1101":1,"2076":1,"2086":5}}],["grad",{"0":{"661":1,"1093":1,"1104":1,"1109":1,"1111":1,"1117":1,"1119":1},"2":{"119":1,"315":3,"383":2,"385":1,"394":1,"485":1,"659":2,"660":2,"661":10,"665":1,"702":6,"834":1,"1039":1,"1082":7,"1083":1,"1086":2,"1087":15,"1092":2,"1093":2,"1094":1,"1095":11,"1096":9,"1097":4,"1098":16,"1099":9,"1100":2,"1101":4,"1102":10,"1104":10,"1106":4,"1107":4,"1109":1,"1110":7,"1111":1,"1113":1,"1114":8,"1115":1,"1116":5,"1117":12,"1118":3,"1119":3,"1202":1,"1205":4,"1213":4,"1214":15,"1215":2,"1218":1,"1223":2,"1227":1,"1231":2,"1244":1,"1295":1,"1398":12,"2086":2}}],["graphs",{"0":{"2075":1},"2":{"1089":1,"2075":1}}],["graph=true",{"2":{"1096":2,"1101":3}}],["graph=false",{"2":{"1083":1}}],["graph=none",{"2":{"1083":1}}],["graphics",{"2":{"795":1}}],["graph",{"0":{"1299":1},"2":{"94":1,"513":1,"1083":1,"1089":1,"1090":1,"1227":1,"1282":3,"1283":1,"1287":2,"1290":1,"1404":1,"2075":1}}],["gpt等模型所用的就是这种位置编码",{"2":{"1337":1}}],["gpt使用的是可学习的位置编码",{"2":{"749":1}}],["gpt使用从左到右的transformer",{"2":{"721":1}}],["gpt是微调方法",{"2":{"721":1}}],["gpt属于这一类",{"2":{"718":1}}],["gpt系列模型是基于因果解码器架构开发的",{"2":{"541":1}}],["gpt4技术原理五",{"2":{"513":1}}],["gpt",{"2":{"89":1,"145":1,"156":3,"513":2,"540":1,"542":2,"543":1,"569":4,"595":1,"670":1,"721":1,"731":1,"844":1,"940":1,"945":1,"962":1,"965":1,"1312":3,"1315":1,"1316":16,"1317":1}}],["gpu架构与编程",{"2":{"2009":1}}],["gpu显存是很宝贵的",{"2":{"1106":1}}],["gpu节点并行处理它们是可取的",{"2":{"977":1}}],["gpu0和gpu2组成一个cp组",{"2":{"976":1}}],["gpu在fp16",{"2":{"968":1}}],["gpu上和llama",{"2":{"980":1}}],["gpu上的张量核心",{"2":{"968":1}}],["gpu上接受少至十二小时的训练后达到翻译质量的新的最佳结果",{"2":{"911":1}}],["gpu擅长并行高强度并行计算",{"2":{"796":1}}],["gputil",{"2":{"423":1}}],["gpus",{"0":{"1306":1},"2":{"156":1,"185":1,"233":1,"422":1,"1308":2}}],["gpu",{"0":{"797":1,"798":1},"1":{"798":1,"799":1},"2":{"34":2,"154":1,"201":4,"217":1,"364":2,"415":1,"423":13,"775":1,"795":1,"974":1,"980":1,"986":1,"1083":1,"1132":1,"1215":1,"1255":1,"1308":5,"1942":1,"1960":1,"2077":1}}],["ger",{"2":{"1087":1}}],["german",{"2":{"557":1}}],["ge",{"2":{"1085":1,"1087":4}}],["geometry",{"2":{"1779":4}}],["geometric",{"2":{"1015":1,"1087":1}}],["george",{"2":{"1124":1,"1195":1}}],["geoffrey",{"2":{"104":1,"1151":1}}],["gemm",{"0":{"804":1},"2":{"180":1}}],["geqrf",{"2":{"1087":2}}],["geq",{"2":{"178":1}}],["geluactivation",{"2":{"110":2}}],["gelu函数的数学表达式为gelu",{"2":{"106":1}}],["gelu通过高斯误差函数",{"2":{"106":1}}],["gelus",{"2":{"106":1,"156":1}}],["gelu和swiglu的三种变体",{"2":{"103":1}}],["gelu",{"0":{"106":1,"844":1},"2":{"96":1,"103":3,"106":2,"110":9,"621":1,"844":8,"1086":2}}],["generalist",{"2":{"735":1}}],["generalization",{"2":{"542":1}}],["general",{"2":{"263":1,"292":1,"724":1,"726":2,"1315":2}}],["generating",{"2":{"557":1}}],["generation",{"2":{"429":1,"906":2,"982":1,"983":1,"1318":1}}],["generative",{"2":{"233":1,"429":1,"513":1,"721":1,"736":1,"740":1,"1312":1}}],["generateschema",{"2":{"2070":1}}],["generated",{"2":{"361":2,"428":1}}],["generate",{"2":{"84":3,"428":1,"2070":1}}],["generator则使用编码器的全部输出",{"2":{"472":1}}],["generator使用的并不是编码器的所有输出",{"2":{"472":1}}],["generator进行最后的预测",{"2":{"450":1}}],["generator类的对象",{"2":{"450":1}}],["generator类包括linear层和softmax层",{"2":{"449":1}}],["generator类对象",{"2":{"398":1}}],["generator会把decoder的输出变成输出词的概率",{"2":{"449":1}}],["generator对应上图的编号10",{"2":{"449":1}}],["generator输出的概率分布",{"2":{"399":1}}],["generator就是一个分类头",{"2":{"397":1}}],["generator的调用放在了loss",{"2":{"385":1}}],["generator",{"2":{"83":4,"364":1,"385":1,"398":4,"399":1,"410":1,"423":2,"424":2,"428":1,"449":2,"450":4,"472":5,"529":1,"703":4,"1087":28,"1254":2}}],["genericlambda",{"2":{"1906":4}}],["generic",{"0":{"1906":1},"2":{"83":1,"343":1,"532":1,"1904":1,"1914":1}}],["gen",{"2":{"83":2,"383":2,"399":1,"424":2}}],["getperson",{"2":{"1921":2}}],["getval",{"2":{"1868":2}}],["getvalue",{"2":{"1640":2,"1922":2,"1927":2}}],["getcount",{"2":{"1868":3}}],["getcircumference",{"2":{"1678":1}}],["getline",{"2":{"1813":6,"1820":2,"1825":1,"1831":6,"1838":2,"1843":1,"1933":1}}],["getfilename",{"2":{"1763":2}}],["getter",{"2":{"1709":1}}],["getname",{"2":{"1685":3,"1927":3}}],["getarea",{"2":{"1678":1,"1693":3}}],["getattr",{"2":{"723":1,"1214":1}}],["getbalance",{"2":{"1677":2}}],["getenv",{"2":{"1594":1}}],["getmapper",{"2":{"1481":1,"1485":1,"1486":1,"1487":1,"1488":1}}],["getstatus",{"2":{"1922":1}}],["getstate",{"2":{"1214":1,"1227":1}}],["getstudentinfo",{"2":{"1921":1}}],["getstudent",{"2":{"1921":4}}],["getscore",{"2":{"1680":2}}],["getsession",{"2":{"1481":2,"1485":1,"1486":1,"1487":1,"1488":1}}],["getresourceasstream",{"2":{"1481":1}}],["getitem",{"2":{"1085":1,"1250":2,"1295":1}}],["get",{"2":{"8":1,"122":3,"201":1,"375":1,"590":3,"592":4,"700":2,"834":1,"1083":1,"1087":1,"1214":9,"1215":1,"1239":1,"1273":2,"1298":1,"1304":1,"1481":1,"1566":1,"1611":2,"1695":1,"1700":3,"1813":6,"1820":1,"1821":2,"1831":6,"1838":1,"1839":2,"1912":2,"1926":6}}],["gqa来减少通信量",{"2":{"976":1}}],["gqa和mqa的消融分析",{"2":{"956":1}}],["gqa不适用于编码器",{"2":{"937":1,"953":1}}],["gqa消除了这种分区的浪费",{"2":{"937":1,"953":1}}],["gqa使我们能够随着模型的增大而保持带宽和容量的相同比例减少",{"2":{"937":1,"953":1}}],["gqa将头分组",{"2":{"652":1}}],["gqa",{"0":{"936":1,"937":1,"952":1,"953":1},"1":{"937":1,"953":1},"2":{"47":2,"205":1,"503":1,"937":4,"938":1,"953":4,"954":1,"956":1,"971":1,"976":1}}],["g",{"2":{"46":1,"240":1,"267":3,"340":2,"341":1,"591":1,"614":5,"739":1,"768":1,"810":2,"986":1,"1184":7,"1254":1,"1342":8,"1343":1,"1520":1,"1547":1,"1554":2,"1779":4,"1917":3,"1969":1,"1999":1,"2149":2,"2152":2,"2153":4,"2154":1}}],["gt",{"2":{"28":1,"47":1,"80":6,"164":2,"265":1,"267":4,"408":1,"473":1,"579":1,"588":1,"661":5,"698":4,"733":1,"740":1,"840":2,"941":6,"960":6,"976":3,"1045":1,"1085":1,"1087":4,"1104":5,"1168":1,"1228":2,"1275":1,"1331":3,"1363":1,"1392":6,"1478":1,"1479":1,"1480":3,"1524":1,"1551":1,"1559":1,"1563":3,"1604":1,"1619":2,"1624":1,"1628":2,"1629":4,"1630":9,"1633":2,"1635":7,"1651":1,"1673":2,"1695":2,"1699":1,"1700":4,"1701":1,"1712":4,"1715":1,"1725":1,"1729":3,"1811":1,"1813":2,"1817":1,"1819":1,"1820":2,"1823":1,"1824":2,"1829":1,"1831":2,"1835":1,"1837":1,"1838":2,"1841":1,"1842":2,"1894":3,"1912":1,"1930":1,"2018":6,"2059":2,"2070":5,"2094":4}}],["iy​i​​",{"2":{"2023":1}}],["ibatis",{"2":{"1481":4}}],["i−1",{"2":{"1343":2}}],["i−j",{"2":{"1339":4}}],["i−organization​​",{"2":{"1324":1}}],["i−organizationseq",{"2":{"1324":1}}],["i−person​​+t​o",{"2":{"1324":1}}],["i−person+to",{"2":{"1324":1}}],["i++",{"2":{"1594":1,"1620":1,"1621":3,"1623":1,"1710":2,"1797":1}}],["i+",{"2":{"1180":1}}],["i+1",{"2":{"532":2,"590":1,"592":1,"1000":1,"1157":1,"1180":1,"1322":3,"1324":1,"1336":1,"2018":4,"2023":2}}],["iperson",{"2":{"1324":1}}],["ip",{"2":{"1307":2,"1526":1,"1583":1,"1594":4,"1611":2,"2090":1}}],["ipu",{"2":{"1087":1,"1214":2}}],["ipow",{"2":{"1083":1}}],["igammac",{"2":{"1087":2}}],["igamma",{"2":{"1087":2}}],["ignoring",{"2":{"572":1}}],["ignore",{"2":{"399":2,"1085":2,"1218":1,"1813":6,"1814":1,"1831":6,"1832":1}}],["i0",{"2":{"1087":2}}],["ixor",{"2":{"1085":3}}],["irecv",{"2":{"1574":1,"1575":1}}],["irshift",{"2":{"1085":3}}],["irange",{"2":{"395":1}}],["i​",{"2":{"999":2}}],["iloc",{"2":{"1250":2}}],["ilshift",{"2":{"1085":3}}],["ilsvrc",{"2":{"840":1,"1302":1}}],["illustrated",{"2":{"429":1}}],["i时各引入一个可训练的相对位置向量avijaijva",{"2":{"759":1}}],["i|",{"2":{"692":1}}],["i|distance",{"2":{"692":1}}],["i映射到集合",{"2":{"613":1}}],["i映射到语义向量ziziz",{"2":{"418":1}}],["ies",{"2":{"567":1}}],["ied",{"2":{"567":1}}],["i∈",{"2":{"503":2,"1343":2}}],["i作为其跨度集合",{"2":{"485":1}}],["i计算出来",{"2":{"418":1}}],["i要依赖全部元素x1",{"2":{"418":1}}],["ios",{"2":{"1820":8,"1821":7,"1838":8,"1839":7,"1902":6}}],["iostream>",{"2":{"1606":1,"1607":1,"1608":1,"1616":1,"1619":1,"1620":1,"1621":2,"1623":1,"1624":1,"1625":2,"1633":1,"1645":1,"1646":1,"1647":1,"1648":1,"1649":1,"1650":1,"1659":1,"1660":1,"1661":1,"1662":1,"1663":1,"1665":1,"1667":1,"1668":1,"1670":1,"1671":1,"1672":1,"1673":2,"1674":1,"1675":1,"1676":1,"1677":1,"1680":1,"1683":4,"1684":2,"1685":2,"1687":2,"1688":2,"1691":1,"1696":1,"1698":1,"1699":1,"1700":1,"1701":1,"1704":1,"1705":1,"1706":2,"1707":1,"1708":1,"1709":1,"1712":1,"1713":4,"1714":1,"1715":2,"1718":1,"1719":6,"1720":4,"1721":4,"1722":4,"1723":1,"1724":3,"1725":4,"1728":3,"1729":4,"1736":1,"1737":1,"1738":1,"1739":1,"1741":1,"1742":1,"1743":1,"1744":1,"1746":1,"1747":1,"1749":1,"1750":1,"1751":1,"1752":1,"1754":1,"1755":1,"1756":1,"1761":1,"1762":1,"1763":1,"1788":1,"1789":1,"1791":1,"1792":1,"1797":1,"1799":1,"1800":1,"1801":1,"1802":1,"1803":1,"1805":1,"1806":1,"1807":1,"1811":1,"1813":1,"1814":1,"1816":1,"1817":1,"1820":2,"1821":1,"1824":1,"1825":1,"1829":1,"1831":1,"1832":1,"1834":1,"1835":1,"1838":2,"1839":1,"1842":1,"1843":1,"1849":1,"1853":1,"1857":1,"1861":1,"1874":1,"1883":1,"1887":1,"1891":1,"1897":1,"1902":1,"1905":1,"1906":1,"1907":1,"1908":1,"1909":1,"1910":1,"1911":1,"1912":1,"1914":1,"1921":1,"1922":1,"1923":1,"1924":1,"1925":1,"1926":1,"1927":1,"1928":2,"1929":1,"1930":2,"1933":1,"1999":1,"2003":1,"2004":1,"2005":1,"2006":5,"2007":1,"2008":1,"2059":5,"2060":2,"2061":1,"2062":3,"2063":1}}],["iostream",{"2":{"1604":2,"1606":1,"1628":1,"1729":1,"1811":1,"1829":1}}],["iomanip",{"2":{"1817":1,"1835":1}}],["iomanip>",{"2":{"1608":1,"1673":1,"1817":1,"1835":1}}],["iota",{"0":{"1747":1},"2":{"1732":1,"1747":2,"1758":1}}],["ioexception",{"2":{"1481":2}}],["ioerror",{"2":{"373":2}}],["ior",{"2":{"1085":3}}],["io",{"0":{"1809":1,"1810":1,"1811":1,"1818":1,"1827":1,"1828":1,"1829":1,"1836":1},"1":{"1810":1,"1811":1,"1812":1,"1813":1,"1814":1,"1815":1,"1816":1,"1817":1,"1818":1,"1819":2,"1820":2,"1821":2,"1822":1,"1823":1,"1824":1,"1825":1,"1826":1,"1828":1,"1829":1,"1830":1,"1831":1,"1832":1,"1833":1,"1834":1,"1835":1,"1836":1,"1837":2,"1838":2,"1839":2,"1840":1,"1841":1,"1842":1,"1843":1,"1844":1},"2":{"373":1,"513":1,"557":2,"945":1,"965":1,"1253":1,"1481":3,"1810":5,"1811":1,"1822":1,"1826":6,"1828":5,"1829":1,"1840":1,"1844":6}}],["i较大时",{"2":{"326":1}}],["i较小时",{"2":{"326":1}}],["i都有影响",{"2":{"326":1}}],["i都会给每个输出词yiyiy",{"2":{"260":1}}],["i可以看成所有环境词的加权和",{"2":{"326":1}}],["it学习",{"2":{"2045":1}}],["it学习指南",{"0":{"2001":1}}],["it2",{"2":{"1897":4}}],["itruediv",{"2":{"1083":1}}],["item",{"2":{"1087":1,"1211":4,"1215":3,"1218":1,"1266":1,"1273":2,"1295":2,"1331":3,"2086":1}}],["items",{"2":{"557":1,"591":5,"1566":1}}],["iters=2",{"2":{"1246":1,"1247":1}}],["iters=4",{"2":{"1237":1,"1238":1,"1240":1}}],["iters",{"2":{"1237":1,"1238":1,"1240":1,"1244":2,"1440":2}}],["iter之中得到了from",{"2":{"557":1}}],["iteration",{"2":{"1284":3}}],["iterations",{"2":{"1241":1,"1284":2}}],["iteratively",{"2":{"592":1}}],["iterate",{"2":{"590":1}}],["iterators",{"2":{"1225":1,"1713":2}}],["iterator",{"2":{"557":13,"1214":7,"1615":1,"1713":2,"1714":2,"1718":1,"1719":6,"1720":7,"1721":7,"1722":7,"1724":4,"1725":5,"1897":2,"1930":2}}],["iterable",{"2":{"557":1}}],["iter次",{"2":{"385":1}}],["iter=1",{"2":{"385":1,"399":1}}],["iter=config",{"2":{"364":1,"385":1,"423":1}}],["iter转换为map",{"2":{"375":1}}],["iter",{"2":{"364":1,"372":1,"375":11,"381":1,"385":5,"399":2,"410":1,"423":1,"557":2,"1083":1,"1279":8,"1283":1,"1284":2,"1440":1,"1720":3,"1721":3,"1722":3}}],["it",{"2":{"261":6,"504":1,"513":1,"590":1,"591":2,"592":1,"688":1,"1100":1,"1254":3,"1304":1,"1305":1,"1615":2,"1713":3,"1714":3,"1718":3,"1719":9,"1720":11,"1721":11,"1722":11,"1724":8,"1725":15,"1754":3,"1807":3,"1897":5,"1922":3,"1933":2,"2067":1,"2073":3,"2075":2,"2077":1,"2078":1,"2081":1,"2083":1,"2087":1}}],["its",{"2":{"235":1,"513":1,"2081":1,"2087":1}}],["i造成影响",{"2":{"260":1}}],["ics",{"2":{"309":2,"314":2}}],["iclr高分",{"2":{"740":1}}],["iclr",{"2":{"233":1}}],["icml2024高分",{"2":{"47":1}}],["i≠maxsoftmax",{"2":{"191":2}}],["i远小于xmaxxmaxx",{"2":{"191":1}}],["ik",{"2":{"189":1,"765":2}}],["i中每个元素均是均值为0",{"2":{"189":1}}],["i出来",{"2":{"189":1}}],["iiosnail",{"2":{"361":1,"429":1}}],["ii",{"2":{"185":1,"735":1}}],["iii",{"2":{"178":1}}],["i∥只跟当前位置i有关",{"2":{"176":1}}],["i∥∥k",{"2":{"176":1}}],["i⋅k",{"2":{"176":1}}],["iv",{"2":{"173":1,"271":1}}],["iadd",{"2":{"1085":1}}],["iand",{"2":{"1085":3}}],["ian",{"2":{"543":1}}],["ia",{"2":{"173":1,"271":1}}],["ij",{"2":{"172":1,"759":4,"762":1,"766":4}}],["i=1",{"2":{"170":1,"178":1,"189":4,"271":1,"313":1,"613":1,"692":3,"903":1}}],["i对堆叠起来就得到矩阵q∈rl×dqq∈rl×dq",{"2":{"161":1}}],["i对于预测结果f",{"2":{"134":1}}],["i被加入或减去到",{"2":{"148":1}}],["i而",{"2":{"148":1}}],["i是当前位置i和和序列中所有位置间的关系",{"2":{"758":1}}],["i是考虑了全局依赖之后的产物",{"2":{"418":1}}],["i是不可控的",{"2":{"326":1}}],["i是前向传播的输入",{"2":{"148":1}}],["i是一个由1和0两种元素组成的向量",{"2":{"19":1}}],["i就是输入xixix",{"2":{"134":1}}],["i和kikik",{"2":{"189":1}}],["i和k矩阵单独一行kikik",{"2":{"189":1}}],["i和sjsjs",{"2":{"172":1}}],["i和pipip",{"2":{"125":1}}],["i和mimim",{"2":{"125":1}}],["i的内积以及一个softmax",{"2":{"125":1}}],["imitate",{"2":{"2073":1}}],["immediate",{"2":{"1574":1}}],["img",{"2":{"1250":11,"1254":2}}],["imul",{"2":{"1085":1}}],["imod",{"2":{"1085":1}}],["imoneoi",{"2":{"90":1}}],["imag",{"2":{"1082":1,"1350":1,"1712":8}}],["images",{"2":{"930":1,"1254":4,"1283":5,"1302":1,"2021":1,"2079":1}}],["image",{"2":{"167":3,"232":1,"259":2,"294":1,"326":2,"882":1,"1207":2,"1250":5,"1253":7,"1254":28,"1273":3,"1283":2,"1302":2,"1360":1,"1361":1,"1363":1,"2021":2,"2079":2,"2081":1,"2082":1}}],["imagenet",{"0":{"1300":1},"2":{"156":1,"840":1,"1150":1,"1177":1,"1300":1,"1302":2,"1303":3,"1304":1,"1306":1,"1307":2,"1308":2}}],["im",{"2":{"370":1,"557":1}}],["implicit",{"0":{"1684":1},"2":{"1087":2}}],["implications",{"2":{"513":1}}],["impl",{"2":{"395":1,"1083":1,"1208":3,"1214":3}}],["implements",{"2":{"1300":1}}],["implemented",{"2":{"1083":11}}],["implementation",{"2":{"840":1}}],["implementing",{"2":{"638":1}}],["implement",{"0":{"1384":1},"2":{"394":1,"399":1,"1254":1}}],["importance",{"2":{"1036":1}}],["important",{"2":{"449":1,"1763":1}}],["import",{"0":{"1278":1},"2":{"346":1,"422":1,"552":2,"557":2,"801":1,"802":1,"804":1,"808":1,"816":1,"1211":1,"1212":1,"1213":1,"1215":9,"1216":2,"1217":2,"1218":3,"1239":2,"1243":2,"1251":1,"1253":8,"1257":6,"1272":1,"1278":7,"1282":1,"1298":1,"1299":1,"1398":1,"1481":6,"1566":1,"2086":3}}],["imprint",{"2":{"148":2,"485":1}}],["improves",{"2":{"399":1}}],["improved",{"2":{"233":1,"735":1}}],["improve",{"2":{"103":1,"104":1,"105":1,"156":2}}],["improving",{"2":{"41":1,"43":1,"47":2,"393":1,"498":1,"513":1,"601":1,"638":3}}],["id​i​​和di+1d",{"2":{"2023":1}}],["ide",{"2":{"1605":1,"1729":1}}],["ideas",{"2":{"2075":1,"2078":1,"2081":1}}],["idea",{"2":{"1474":1}}],["identical",{"2":{"522":1,"935":1,"951":1,"1254":1}}],["identically",{"2":{"99":1,"419":1}}],["identity",{"2":{"299":1,"300":1,"301":2,"302":1}}],["idiv",{"2":{"1083":1,"1085":1}}],["id转换为dmodeldmodeld",{"2":{"700":1}}],["id转化成一个token",{"2":{"455":1}}],["idf",{"2":{"676":1,"711":1}}],["id|>",{"2":{"571":4}}],["id行的数据作为embedding",{"2":{"458":1}}],["id的值",{"2":{"458":1}}],["id=",{"2":{"1481":2,"1485":3,"1486":1,"1487":1,"1488":1,"1489":2}}],["id=vocab",{"2":{"375":1}}],["id=2",{"2":{"65":1,"384":1,"558":1}}],["idx0",{"2":{"591":2}}],["idx2",{"2":{"591":4}}],["idx1",{"2":{"591":6}}],["idx=config",{"2":{"723":1}}],["idx=4",{"2":{"590":1}}],["idx=padding",{"2":{"834":1}}],["idx=pad",{"2":{"423":1}}],["idx=0",{"2":{"83":1,"399":1,"424":1,"702":1,"834":1}}],["idx",{"2":{"84":2,"364":1,"385":1,"399":8,"423":4,"590":3,"591":21,"592":14,"702":13,"834":4,"1087":1,"1215":5,"1250":4,"1251":2,"1330":14}}],["ids中所有可以合并的pair都被替代了",{"2":{"592":1}}],["ids=",{"2":{"423":1,"590":1}}],["ids",{"2":{"76":4,"571":1,"572":2,"573":2,"590":10,"591":1,"592":15,"700":13,"723":25,"1331":2,"1807":2}}],["id进行填充",{"2":{"65":1}}],["id",{"2":{"65":3,"201":5,"384":10,"455":1,"458":1,"558":8,"571":3,"572":2,"592":1,"608":1,"700":6,"702":2,"723":1,"1078":8,"1227":1,"1308":1,"1481":2,"1485":4,"1486":2,"1487":2,"1488":3,"1524":1,"1594":3,"1728":4,"1805":2,"1807":11,"1808":2,"1921":2}}],["i水平堆叠后的结果",{"2":{"36":1}}],["i",{"2":{"28":3,"54":3,"71":9,"83":1,"125":4,"145":1,"148":2,"161":7,"170":2,"172":5,"173":4,"176":5,"178":8,"183":4,"185":1,"189":7,"191":17,"194":1,"201":4,"210":1,"216":1,"245":3,"270":5,"271":6,"277":1,"313":9,"326":1,"381":1,"383":1,"385":4,"391":1,"395":2,"398":3,"399":2,"405":6,"407":8,"408":3,"409":4,"410":1,"418":2,"427":5,"428":5,"445":3,"453":2,"472":1,"485":9,"503":4,"516":3,"528":10,"529":4,"536":1,"537":3,"564":1,"571":5,"572":3,"579":1,"580":1,"582":6,"583":6,"585":1,"590":7,"592":2,"612":3,"613":1,"614":6,"692":4,"735":1,"745":2,"759":3,"760":1,"762":1,"766":1,"847":2,"899":2,"903":4,"915":1,"927":7,"943":4,"944":24,"961":4,"967":2,"974":3,"975":2,"986":1,"999":11,"1000":7,"1007":4,"1098":4,"1099":1,"1100":2,"1102":2,"1132":1,"1157":1,"1180":2,"1202":1,"1205":3,"1244":3,"1254":5,"1273":2,"1280":1,"1320":5,"1322":9,"1323":5,"1324":8,"1328":7,"1329":7,"1330":9,"1336":3,"1339":20,"1340":14,"1343":3,"1344":11,"1345":3,"1398":5,"1486":2,"1487":2,"1488":2,"1516":1,"1519":1,"1541":1,"1545":2,"1574":1,"1589":2,"1590":1,"1594":3,"1620":3,"1621":9,"1623":3,"1629":5,"1630":4,"1634":3,"1647":7,"1667":6,"1668":7,"1670":4,"1671":2,"1684":6,"1696":4,"1706":7,"1708":2,"1710":6,"1712":6,"1714":3,"1719":15,"1736":2,"1737":2,"1738":2,"1797":4,"1879":1,"1891":2,"2018":4,"2023":1,"2149":1}}],["i^tw",{"2":{"760":1}}],["i^t",{"2":{"189":1}}],["i^t中每个元素也是均值为0",{"2":{"189":1}}],["i^",{"2":{"148":1,"485":2,"999":3}}],["i^l所触发前缀序列的下一个token",{"2":{"128":1}}],["i^l所捕获的模式的一种补充",{"2":{"128":1}}],["i^l是分数最高的mlimilm",{"2":{"128":1}}],["i^l进行比较",{"2":{"128":1}}],["i^l⋅𝐸",{"2":{"128":1}}],["i^l=𝑠𝑜𝑓𝑡𝑚𝑎𝑥",{"2":{"128":1}}],["i^l",{"2":{"128":6}}],["i^l对应的value",{"2":{"128":1}}],["i^k即为batchnorm输出的第",{"2":{"313":1}}],["i^k=",{"2":{"313":1}}],["i^k",{"2":{"28":1}}],["i^k和wviwivw",{"2":{"12":1,"16":1}}],["i^v",{"2":{"12":1,"16":1,"28":1}}],["i^q",{"2":{"12":1,"16":1,"28":1}}],["isalpha",{"2":{"1933":1}}],["is>",{"2":{"1912":4}}],["istringstream",{"2":{"1823":1,"1826":1,"1841":1,"1844":1,"1933":1}}],["istft",{"2":{"1083":1,"1087":1}}],["isempty",{"2":{"1713":1}}],["isend和mpi",{"2":{"1574":1,"1575":1}}],["isleapyear",{"2":{"1642":1,"1729":6}}],["isstudent",{"2":{"1607":4}}],["issue",{"2":{"1254":1}}],["isinstance",{"2":{"1226":1,"1254":4}}],["isinf",{"2":{"1087":1}}],["isolation图可以更轻松地在目标超参数的不同值之间进行同类比较",{"2":{"1150":1}}],["isolation图上的每个点对应着在优化某些",{"2":{"1150":1}}],["isolation图是基本超参数轴图的特例",{"2":{"1150":1}}],["isreal",{"2":{"1087":1}}],["isposinf",{"2":{"1087":1}}],["isneginf",{"2":{"1087":1}}],["isnan",{"2":{"1087":1}}],["isfinite",{"2":{"1087":1}}],["isclose",{"2":{"1087":1}}],["isub",{"2":{"1085":1}}],["is",{"0":{"911":1,"2073":1},"2":{"8":2,"36":1,"38":1,"39":1,"67":2,"76":1,"82":1,"114":2,"115":1,"122":2,"130":4,"145":1,"160":3,"181":1,"199":2,"201":4,"233":2,"235":2,"257":1,"259":1,"292":1,"343":1,"344":1,"346":1,"364":2,"370":1,"375":5,"380":1,"394":2,"423":8,"428":2,"429":1,"432":1,"446":2,"477":1,"503":2,"513":2,"522":1,"523":1,"533":1,"557":2,"564":3,"573":1,"590":1,"591":6,"658":1,"661":1,"688":1,"692":1,"700":2,"702":2,"722":1,"723":6,"737":1,"740":3,"747":1,"820":1,"933":2,"935":2,"945":1,"951":2,"965":1,"1010":1,"1076":1,"1083":1,"1086":7,"1087":26,"1104":1,"1110":1,"1114":2,"1208":1,"1211":1,"1215":2,"1216":1,"1217":1,"1218":1,"1254":7,"1255":1,"1284":2,"1303":1,"1304":2,"1308":2,"1312":1,"1328":2,"1329":1,"1330":5,"1607":1,"1633":1,"1654":2,"1659":3,"1665":2,"1671":1,"1674":2,"1691":3,"1695":3,"1698":2,"1699":2,"1737":1,"1738":1,"1761":1,"1764":1,"1797":6,"1806":3,"1807":2,"1820":4,"1821":1,"1825":2,"1838":4,"1839":1,"1843":2,"1891":1,"1909":3,"1911":3,"1912":2,"1929":1,"1933":1,"2061":1,"2062":1,"2063":1,"2073":3,"2081":1,"2083":1,"2086":3,"2087":3}}],["ifcfg",{"2":{"2095":1}}],["ifconfig",{"2":{"1526":1}}],["ifstream",{"2":{"1761":1,"1819":1,"1820":1,"1821":2,"1825":1,"1826":1,"1837":1,"1838":1,"1839":2,"1843":1,"1844":1,"1933":1}}],["ifdef",{"2":{"1632":1}}],["ifndef",{"2":{"1628":1,"1632":1,"1916":2,"1923":1,"1999":1}}],["ifloordiv",{"2":{"1085":1}}],["ifp",{"2":{"612":1}}],["if",{"0":{"1619":1,"1922":1},"2":{"8":1,"36":1,"67":2,"76":1,"114":1,"119":1,"181":1,"199":2,"201":4,"372":1,"374":1,"375":2,"380":1,"385":3,"394":2,"395":4,"399":1,"402":1,"422":1,"423":4,"449":1,"503":4,"557":2,"572":2,"590":3,"591":3,"592":3,"612":1,"700":5,"702":3,"723":6,"933":2,"1076":1,"1098":2,"1102":2,"1211":2,"1215":6,"1216":2,"1217":1,"1218":2,"1226":2,"1235":3,"1236":3,"1237":5,"1238":5,"1240":5,"1246":5,"1247":5,"1250":2,"1254":8,"1255":1,"1328":2,"1329":1,"1330":4,"1398":1,"1436":1,"1440":1,"1481":1,"1566":1,"1594":1,"1608":2,"1611":1,"1619":12,"1625":2,"1630":2,"1631":3,"1633":1,"1646":2,"1649":1,"1672":2,"1677":1,"1683":1,"1684":1,"1695":2,"1706":1,"1713":1,"1715":2,"1719":1,"1720":1,"1721":1,"1722":1,"1724":1,"1725":1,"1726":1,"1728":2,"1729":3,"1754":1,"1761":2,"1762":1,"1763":1,"1806":1,"1807":1,"1811":1,"1814":4,"1820":3,"1821":1,"1825":6,"1829":1,"1832":4,"1838":3,"1839":1,"1843":6,"1874":1,"1883":1,"1887":3,"1891":1,"1902":5,"1911":2,"1920":2,"1922":5,"1926":5,"1927":6,"1928":2,"1930":1,"1932":1,"1933":8,"1985":1,"1999":1,"2060":1,"2153":1}}],["innovate",{"2":{"2087":1}}],["inner",{"2":{"513":1,"1087":1,"2003":1}}],["inroads",{"2":{"2087":1}}],["inherits",{"2":{"1784":1,"1849":1}}],["inherently",{"2":{"480":1,"513":1}}],["inline",{"0":{"1923":1},"2":{"1632":1,"1709":6,"1920":1,"1923":2}}],["inbox",{"2":{"885":1}}],["inbedder使用较短的问答对作为训练数据",{"2":{"733":1}}],["ing",{"2":{"567":3,"576":1}}],["industry",{"0":{"2082":1},"2":{"2087":1}}],["indices的维度",{"2":{"1086":1}}],["indices",{"2":{"558":2,"834":1,"1085":2,"1086":2,"1087":16,"1227":3,"1330":9}}],["indices`",{"2":{"558":1}}],["indicates",{"2":{"557":1}}],["index序列",{"2":{"704":1}}],["index为3",{"2":{"700":1}}],["index为2",{"2":{"700":1}}],["index=1",{"2":{"557":1}}],["index=0",{"2":{"557":1,"1218":1}}],["index=pad",{"2":{"399":1}}],["index设置为",{"2":{"399":1}}],["index",{"2":{"387":1,"399":2,"520":1,"530":1,"557":4,"592":2,"700":1,"834":1,"1085":2,"1087":64,"1215":1,"1250":3,"1295":3,"1330":3,"1476":1,"1700":6,"1719":1,"1752":1,"1797":6,"1912":7,"1922":1}}],["insufficient",{"2":{"1874":1}}],["insufficientfundsexception",{"2":{"1766":1}}],["inside",{"2":{"1726":5}}],["insights",{"2":{"429":1}}],["inspectdb",{"2":{"2070":1}}],["inspection",{"2":{"591":1}}],["inspired",{"2":{"591":1}}],["inserted",{"2":{"1933":2}}],["inserter",{"2":{"1883":1}}],["insert>",{"2":{"1486":1}}],["insert",{"0":{"1486":1},"2":{"557":1,"572":1,"1330":1,"1481":1,"1486":2,"1713":2,"1719":2,"1720":1,"1721":1,"1722":1,"1724":5,"1725":2,"1933":1}}],["install",{"0":{"792":1},"2":{"1276":1,"1301":2,"1309":1,"1404":1,"1537":2,"1584":4,"1987":2,"1996":1,"1999":2}}],["instancenorm",{"2":{"809":1,"810":1}}],["instancenorm2d",{"2":{"809":2}}],["instancenorm是对h",{"2":{"341":1}}],["instance",{"0":{"337":1,"809":1},"2":{"293":1,"314":1,"337":2,"640":1,"809":1}}],["instances",{"2":{"181":1}}],["instruction",{"2":{"638":2,"726":1}}],["insteading",{"2":{"572":1}}],["instead",{"2":{"19":1,"938":1,"954":1,"1244":1,"1254":2,"1909":2}}],["increase",{"2":{"1868":2}}],["increased",{"2":{"8":1}}],["increment",{"2":{"1621":1,"1633":1,"1729":2,"1868":2,"1924":3}}],["incremental",{"2":{"623":1}}],["include>",{"2":{"1482":8}}],["include",{"0":{"1976":1},"2":{"557":1,"1087":4,"1589":1,"1590":1,"1594":3,"1604":1,"1606":1,"1607":1,"1608":2,"1616":1,"1619":1,"1620":1,"1621":2,"1623":1,"1624":2,"1625":2,"1628":4,"1633":1,"1645":3,"1646":1,"1647":1,"1648":1,"1649":1,"1650":1,"1659":1,"1660":1,"1661":1,"1662":1,"1663":1,"1665":2,"1667":1,"1668":1,"1670":1,"1671":1,"1672":1,"1673":4,"1674":2,"1675":2,"1676":1,"1677":2,"1680":2,"1683":5,"1684":2,"1685":4,"1687":2,"1688":2,"1691":3,"1695":4,"1696":1,"1698":1,"1699":1,"1700":1,"1701":1,"1704":2,"1705":1,"1706":2,"1707":1,"1708":1,"1709":1,"1710":1,"1712":1,"1713":9,"1714":2,"1715":4,"1718":2,"1719":13,"1720":9,"1721":9,"1722":9,"1723":2,"1724":6,"1725":8,"1728":5,"1729":7,"1736":3,"1737":3,"1738":3,"1739":3,"1741":3,"1742":3,"1743":3,"1744":3,"1746":3,"1747":3,"1749":3,"1750":3,"1751":3,"1752":3,"1754":3,"1755":3,"1756":3,"1761":2,"1762":2,"1763":3,"1788":1,"1789":1,"1791":1,"1792":1,"1797":3,"1799":2,"1800":2,"1801":2,"1802":2,"1803":2,"1805":3,"1806":3,"1807":3,"1811":1,"1813":2,"1814":2,"1816":1,"1817":2,"1820":5,"1821":2,"1824":3,"1825":4,"1829":1,"1831":2,"1832":2,"1834":1,"1835":2,"1838":5,"1839":2,"1842":3,"1843":4,"1849":1,"1853":1,"1857":1,"1861":1,"1874":1,"1883":4,"1887":3,"1891":3,"1895":2,"1897":3,"1902":4,"1905":1,"1906":2,"1907":2,"1908":1,"1909":1,"1910":1,"1911":3,"1912":3,"1914":4,"1916":2,"1921":3,"1922":3,"1923":3,"1924":2,"1925":3,"1926":4,"1927":4,"1928":6,"1929":4,"1930":5,"1931":1,"1933":10,"1976":1,"1999":3,"2003":1,"2004":1,"2005":1,"2006":5,"2007":1,"2008":1,"2059":5,"2060":2,"2061":2,"2062":7,"2063":3}}],["includes>",{"2":{"1482":4}}],["includes",{"2":{"429":1}}],["inception",{"2":{"429":1,"1308":1,"1361":2}}],["invaliddepositamountexception",{"2":{"1766":1}}],["invariance",{"2":{"320":2,"742":1}}],["invert",{"2":{"1085":1}}],["inverted",{"2":{"591":3}}],["inverse=false",{"2":{"1083":2}}],["inverse",{"2":{"402":1,"591":1,"1087":2}}],["inp",{"2":{"1101":4}}],["inp+1",{"2":{"1101":1}}],["inplace的操作",{"2":{"661":1,"1104":1}}],["inplace>",{"2":{"395":4}}],["inplace",{"0":{"660":1,"1097":1},"2":{"395":1,"1081":1,"1083":1,"1097":2,"1214":4}}],["inputfile",{"2":{"1820":4,"1825":4,"1838":4,"1843":4}}],["input1",{"2":{"1102":2}}],["input3",{"2":{"1087":1}}],["input2",{"2":{"1087":3}}],["input的输入序列会按照seq",{"2":{"656":1}}],["input序列长度大于embedding时候的seq",{"2":{"656":1}}],["input自身相关的self",{"2":{"649":1,"931":1}}],["input或者target",{"2":{"649":1,"931":1}}],["input=target",{"2":{"381":1}}],["inputstream",{"2":{"1481":4}}],["inputs=none",{"2":{"1083":1}}],["inputs相关的embedding模块包括input",{"2":{"457":1}}],["inputs和outputs的上面分别有一个embedding模块",{"2":{"457":1}}],["inputs",{"2":{"163":1,"698":3,"700":1,"723":5,"1086":1,"1214":1,"1244":3,"1273":2,"1298":3,"1386":1,"2086":2}}],["input",{"0":{"1447":1,"1449":1,"1812":1,"1830":1},"1":{"1813":1,"1814":1,"1831":1,"1832":1},"2":{"8":1,"28":1,"76":4,"99":2,"101":2,"114":1,"130":1,"201":1,"235":1,"315":6,"326":2,"343":1,"346":2,"395":17,"419":2,"428":1,"449":1,"454":1,"457":1,"460":1,"522":1,"572":1,"592":1,"621":2,"623":1,"700":17,"702":4,"704":2,"723":9,"765":1,"768":1,"770":1,"773":1,"775":2,"783":1,"801":2,"802":4,"804":2,"805":2,"807":2,"809":2,"810":1,"814":2,"815":2,"816":6,"834":6,"835":2,"839":4,"840":5,"841":2,"842":6,"843":4,"844":2,"845":2,"846":2,"847":2,"866":1,"1003":1,"1039":1,"1083":1,"1086":1,"1087":2,"1095":1,"1096":1,"1098":1,"1100":1,"1202":2,"1205":9,"1207":2,"1211":2,"1212":7,"1213":5,"1214":1,"1215":3,"1216":1,"1223":4,"1231":4,"1259":2,"1262":2,"1263":2,"1267":2,"1272":2,"1273":15,"1297":2,"1298":2,"1299":2,"1398":5,"1810":1,"1825":1,"1828":1,"1843":1,"1933":2,"1993":1}}],["in和gn总述",{"2":{"361":1}}],["intuitive",{"2":{"2078":1}}],["intptr2",{"2":{"1911":7}}],["intptr",{"2":{"1911":9}}],["intvalue",{"2":{"1728":7}}],["intcontainer",{"2":{"1701":2}}],["intarray",{"2":{"1700":3}}],["int>>",{"2":{"1797":1,"1933":1}}],["int>",{"2":{"1615":2,"1645":2,"1695":10,"1700":1,"1701":1,"1714":2,"1718":2,"1719":10,"1720":9,"1721":9,"1722":9,"1723":1,"1724":6,"1725":9,"1726":1,"1736":1,"1737":1,"1738":1,"1739":1,"1741":2,"1742":2,"1743":1,"1744":1,"1746":1,"1747":1,"1749":1,"1751":1,"1752":1,"1754":1,"1755":1,"1756":1,"1797":2,"1799":1,"1800":1,"1801":1,"1806":2,"1879":1,"1883":2,"1897":3,"1898":1,"1907":2,"1911":2,"1914":1,"1922":1,"1925":2,"1926":3,"1927":1,"1928":4,"1933":1,"2061":2}}],["inttensor",{"2":{"1350":1}}],["int`",{"2":{"1254":1}}],["int32",{"2":{"700":1,"1075":1}}],["introduce",{"0":{"1224":1},"1":{"1225":1,"1226":1,"1227":1}}],["introduction",{"0":{"2072":1},"1":{"2073":1,"2074":1,"2075":1,"2076":1,"2077":1,"2078":1,"2079":1,"2080":1,"2081":1,"2082":1,"2083":1,"2084":1,"2085":1,"2086":1,"2087":1},"2":{"429":1,"2043":1}}],["intra",{"2":{"216":1,"287":1}}],["int类型",{"2":{"343":1}}],["into",{"2":{"148":1,"156":1,"370":1,"483":1,"513":1,"572":1,"573":1,"591":3,"810":3,"981":1,"1216":1,"1217":1,"1218":1,"1481":1,"1486":1,"2087":1}}],["intelligence",{"2":{"2073":1}}],["intel",{"2":{"1214":1,"1569":1}}],["intenum",{"2":{"1083":1}}],["intended",{"2":{"591":1}}],["integrates",{"2":{"2083":1}}],["integrated",{"2":{"134":1,"156":1}}],["integration",{"2":{"1086":1}}],["integer",{"0":{"1912":1},"2":{"590":1,"1607":1,"1701":1,"1824":1,"1842":1,"1904":1,"1912":6}}],["integers",{"2":{"590":2,"591":2,"592":3}}],["interest",{"2":{"1874":2}}],["interestrate",{"2":{"1873":1,"1874":3}}],["intersection",{"2":{"1331":1}}],["interval",{"2":{"1215":2}}],["interleave",{"2":{"1087":2}}],["interleaved=true",{"2":{"503":2}}],["interface",{"2":{"1083":1,"1481":1,"1485":1}}],["interpolation",{"2":{"768":1,"1087":4}}],["interpretability的几个流派",{"2":{"513":1}}],["interpretability",{"2":{"475":1}}],["interpreting",{"2":{"122":1,"156":1,"437":1,"513":2}}],["international",{"2":{"740":1}}],["internal",{"2":{"309":2,"314":1,"361":2,"1083":2,"1088":1,"1817":1,"1835":1}}],["interactions",{"2":{"772":1}}],["interaction",{"2":{"616":1,"624":2}}],["interacting",{"2":{"499":1}}],["interactive",{"2":{"233":1,"399":1,"513":1}}],["inter",{"2":{"216":1}}],["intermediate",{"2":{"110":5}}],["int",{"2":{"76":2,"114":5,"201":4,"346":1,"557":2,"558":1,"571":5,"572":3,"573":3,"591":8,"592":4,"700":2,"702":6,"723":1,"1003":1,"1082":5,"1083":1,"1085":7,"1086":12,"1087":268,"1208":12,"1214":4,"1227":1,"1254":10,"1330":3,"1345":2,"1436":2,"1481":2,"1485":1,"1486":2,"1487":2,"1488":4,"1576":1,"1590":11,"1594":11,"1603":1,"1606":1,"1607":21,"1608":3,"1611":14,"1612":4,"1613":2,"1614":11,"1615":1,"1616":1,"1619":1,"1620":2,"1621":6,"1622":1,"1623":4,"1624":1,"1625":5,"1629":3,"1630":3,"1632":1,"1633":11,"1634":10,"1638":2,"1639":3,"1640":4,"1641":10,"1645":16,"1646":7,"1647":9,"1648":9,"1649":4,"1650":11,"1653":5,"1654":3,"1655":3,"1659":1,"1660":1,"1661":12,"1662":12,"1663":15,"1664":2,"1665":9,"1667":9,"1668":13,"1669":2,"1670":3,"1671":5,"1672":3,"1673":3,"1674":5,"1675":3,"1676":3,"1677":1,"1680":1,"1683":8,"1684":4,"1685":2,"1687":12,"1688":2,"1691":1,"1693":3,"1694":9,"1695":5,"1696":5,"1698":4,"1699":2,"1700":8,"1701":1,"1702":4,"1704":1,"1705":13,"1706":24,"1707":13,"1708":6,"1709":5,"1710":21,"1712":2,"1713":7,"1714":5,"1715":3,"1718":1,"1719":11,"1720":4,"1721":4,"1722":4,"1723":1,"1724":3,"1725":4,"1728":9,"1729":23,"1736":2,"1737":2,"1738":2,"1739":2,"1741":2,"1742":1,"1743":1,"1744":1,"1746":2,"1747":1,"1749":1,"1750":2,"1751":1,"1752":1,"1754":1,"1755":1,"1756":1,"1761":2,"1762":5,"1763":1,"1772":1,"1774":1,"1778":2,"1779":1,"1784":4,"1788":4,"1789":1,"1791":1,"1792":1,"1797":5,"1799":2,"1800":2,"1801":2,"1802":3,"1803":1,"1805":3,"1806":2,"1807":4,"1811":2,"1813":2,"1814":2,"1816":1,"1817":2,"1820":3,"1821":1,"1824":3,"1825":8,"1829":2,"1831":2,"1832":2,"1834":1,"1835":2,"1838":3,"1839":1,"1842":3,"1843":8,"1849":1,"1853":4,"1857":4,"1861":4,"1866":1,"1867":4,"1868":4,"1869":1,"1874":1,"1879":1,"1883":9,"1887":1,"1891":2,"1897":1,"1902":2,"1905":6,"1906":3,"1907":1,"1908":1,"1909":1,"1910":2,"1911":1,"1912":3,"1914":2,"1916":12,"1921":5,"1922":2,"1923":4,"1924":5,"1925":5,"1926":7,"1927":1,"1928":4,"1929":1,"1930":2,"1931":1,"1933":1,"1999":27,"2003":3,"2004":3,"2005":4,"2006":10,"2007":3,"2008":2,"2019":1,"2059":13,"2060":8,"2061":4,"2062":3,"2063":5}}],["int64",{"2":{"65":1,"384":2,"395":1,"558":2,"1205":1}}],["infile",{"2":{"1821":11,"1839":11}}],["infiniband",{"2":{"1589":1}}],["info",{"2":{"1664":5,"1665":8}}],["infos=false",{"2":{"1083":1}}],["informed",{"2":{"513":1}}],["information",{"2":{"5":1,"906":1}}],["infty",{"2":{"1003":2}}],["infer",{"0":{"1274":1},"2":{"1273":2,"1274":3,"1298":1,"1299":1}}],["inferred",{"2":{"820":1}}],["inferencesession",{"2":{"1273":2,"1298":1}}],["inferencemode",{"2":{"1121":1}}],["inference",{"0":{"955":1,"1121":1},"1":{"956":1,"957":1},"2":{"150":1,"201":1,"428":3,"429":6,"472":1,"513":3,"529":1,"894":1,"1087":1,"1118":1,"1121":1,"1274":1}}],["infra",{"2":{"543":1}}],["inf",{"2":{"70":1,"71":1,"84":2,"201":1,"503":1,"592":2,"1216":1,"1218":1}}],["in",{"0":{"1123":1,"1300":1},"2":{"8":2,"23":1,"36":3,"41":1,"47":2,"82":1,"83":3,"91":1,"95":1,"119":1,"122":3,"126":1,"130":1,"131":1,"136":1,"143":1,"144":1,"145":1,"156":13,"160":1,"161":7,"167":1,"181":1,"201":2,"210":1,"233":1,"259":1,"263":1,"288":1,"292":3,"302":1,"316":1,"320":1,"325":1,"326":2,"334":1,"337":3,"338":1,"340":1,"341":1,"343":4,"361":3,"364":2,"370":5,"381":1,"383":1,"384":1,"385":2,"387":1,"399":3,"410":1,"423":3,"424":1,"428":3,"437":2,"449":1,"450":1,"472":1,"477":1,"507":1,"513":4,"522":3,"529":2,"532":1,"542":1,"543":1,"557":7,"558":1,"562":1,"571":2,"572":5,"587":2,"590":2,"591":13,"592":11,"638":1,"688":1,"703":1,"713":1,"723":1,"731":1,"736":2,"740":2,"751":2,"764":1,"768":2,"808":1,"820":1,"927":4,"941":3,"944":4,"960":3,"981":1,"985":3,"1000":1,"1006":1,"1036":1,"1082":1,"1083":1,"1086":1,"1089":1,"1090":1,"1098":3,"1099":1,"1102":1,"1123":1,"1202":1,"1205":2,"1211":1,"1214":1,"1215":3,"1216":2,"1217":2,"1218":5,"1223":2,"1226":1,"1231":5,"1233":1,"1234":1,"1235":1,"1236":1,"1237":1,"1238":1,"1239":1,"1240":1,"1241":2,"1242":2,"1243":1,"1244":3,"1245":1,"1246":1,"1247":1,"1251":1,"1254":8,"1273":2,"1279":1,"1283":1,"1284":2,"1295":2,"1299":1,"1304":1,"1328":1,"1329":3,"1330":2,"1331":1,"1343":1,"1398":3,"1440":1,"1566":4,"1799":1,"1800":1,"1801":1,"1802":1,"1806":5,"1820":1,"1838":1,"1883":1,"1891":1,"1902":1,"1920":1,"1930":1,"2059":12,"2075":1,"2081":1,"2086":2,"2087":2}}],["initcapture",{"2":{"1907":2}}],["initialbalance",{"2":{"1874":6}}],["initial",{"2":{"1243":1,"1303":1,"1308":1,"1607":5,"1633":1,"1774":2,"1779":2}}],["initializing",{"2":{"1308":1}}],["initializer=create",{"2":{"700":1}}],["initializer",{"2":{"700":4,"1925":1,"1933":1}}],["initializes",{"2":{"571":1}}],["initialize",{"2":{"449":1}}],["initialization",{"0":{"988":1,"1000":1,"1001":1},"1":{"1002":1,"1003":1,"1004":1,"1005":1,"1006":1,"1007":1},"2":{"47":1,"700":1,"988":2,"1036":1,"1404":1,"1621":1,"1671":1,"1764":1,"1920":1}}],["init所示",{"2":{"1180":1}}],["init",{"2":{"8":7,"23":2,"66":1,"83":2,"110":2,"113":2,"114":5,"201":13,"343":6,"344":4,"346":4,"380":1,"398":1,"399":2,"423":2,"449":1,"450":2,"472":1,"503":6,"522":2,"523":2,"532":2,"533":2,"571":1,"591":1,"592":2,"668":1,"701":2,"702":2,"703":2,"723":2,"1010":1,"1078":1,"1085":4,"1120":1,"1205":4,"1206":2,"1208":1,"1211":2,"1212":2,"1213":2,"1214":1,"1215":4,"1216":8,"1217":6,"1218":10,"1227":1,"1250":2,"1254":3,"1257":2,"1295":3,"1345":2,"1575":1,"1590":1,"1594":1,"2086":2}}],["=k=y1−y0x1−x0k",{"2":{"2018":1}}],["=k=v",{"2":{"533":1}}],["=`",{"2":{"1635":1}}],["=out",{"2":{"1393":1}}],["=∣∣μ​p​​−μ​q​​∣∣​2​​+tr",{"2":{"1361":1}}],["=∣∣μp−μq∣∣2+tr",{"2":{"1361":1}}],["=||",{"2":{"1361":1}}],["=r​θ",{"2":{"1343":1}}],["=r",{"2":{"1343":1}}],["=rθ",{"2":{"1343":1}}],["=rowsum",{"2":{"944":1}}],["=rowmax",{"2":{"944":1}}],["=g",{"2":{"1342":2}}],["=t",{"2":{"1324":1}}],["=λ×g∣g∣",{"2":{"1184":1}}],["=每秒处理的样本数量",{"2":{"1132":1}}],["=∫​−∞​+∞​​p",{"2":{"1003":1}}],["=∫−∞+∞p",{"2":{"1003":1}}],["=n​l​​",{"2":{"1003":2}}],["=n​l​​var",{"2":{"1003":3}}],["=nl",{"2":{"1003":2}}],["=nlvar",{"2":{"1003":3}}],["=n",{"2":{"1003":5}}],["=q",{"2":{"944":1}}],["=qtk||q||",{"2":{"175":2}}],["=qtk",{"2":{"175":2}}],["=e−net",{"2":{"1393":1}}],["=e",{"2":{"1002":3,"1003":4,"1389":1,"1393":1}}],["=e​m",{"2":{"943":1,"961":1}}],["=e^",{"2":{"943":1,"961":1}}],["=em",{"2":{"943":1,"961":1}}],["=ex−e−xex+e−xtanh",{"2":{"839":1}}],["=exp",{"2":{"210":3,"847":1,"944":1}}],["=exi∑vj=1exj=exmax−δi∑vj=1exmax−δj=exmaxe−δiexmax∑vj=1e−δj=e−δi∑vj=1e−δjsoftmax",{"2":{"191":1}}],["=exi∑vj=1exj=exmax−δi∑vj=1exmax−δjsoftmax",{"2":{"191":1}}],["=exi∑vj=1exjsoftmax",{"2":{"54":1,"191":1}}],["=exi∑j=1vexj=exmax−δi∑j=1vexmax−δj=exmaxe−δiexmax∑j=1ve−δj=e−δi∑j=1ve−δjsoftmax",{"2":{"191":1}}],["=exi∑j=1vexj=exmax−δi∑j=1vexmax−δjsoftmax",{"2":{"191":1}}],["=exi∑j=1vexjsoftmax",{"2":{"54":1,"191":1}}],["=exi∑j=1nexjp",{"2":{"178":1}}],["=exi∑nj=1exjpi=softmax",{"2":{"178":1}}],["=ℓ",{"2":{"943":2,"961":2}}],["=concat",{"2":{"927":2}}],["=ax+by+c=0f",{"2":{"2021":1}}],["=argmax​y​​​p",{"2":{"908":1}}],["=argmax",{"2":{"908":1}}],["=argmaxyp",{"2":{"908":2}}],["=attention",{"2":{"170":3,"271":3}}],["=p",{"2":{"908":1,"1322":3,"1339":2,"1377":1}}],["=−​t​​1​​​i​∑​t​​log",{"2":{"899":1}}],["=−1t∑itlog",{"2":{"899":1}}],["=−1",{"2":{"184":1}}],["=m",{"2":{"943":3,"961":3}}],["=min",{"2":{"841":3}}],["=max",{"2":{"99":2,"104":2,"840":3,"943":5,"961":5}}],["=​n​l​​​​2​​",{"2":{"1003":1}}],["=​2​​1​​n​l​​var",{"2":{"1003":1}}],["=​2​​1​​var",{"2":{"1003":1}}],["=​2​​1​​e",{"2":{"1003":1}}],["=​ℓ",{"2":{"943":2,"961":2}}],["=​i​∑​​f",{"2":{"943":1,"961":1}}],["=​i=1​∑​t​​logp",{"2":{"903":1}}],["=​p",{"2":{"908":1,"1377":1}}],["=​∑​j​​exp",{"2":{"847":1}}],["=​e​x​​+e​−x​​​​e​x​​−e​−x​​​​",{"2":{"839":1}}],["=​1+e​−x​​​​1​​",{"2":{"839":1}}],["=input",{"2":{"698":1}}],["=i∑j=1weight",{"2":{"71":1}}],["=400k",{"2":{"572":1}}],["=d",{"2":{"1003":1}}],["=dropout",{"2":{"394":2}}],["=dk∑i=11=dkvar",{"2":{"189":1}}],["=dk∑i=1var",{"2":{"189":2}}],["=γ⊙x−μ^bσ^b+β",{"2":{"343":1}}],["=γ⊙x−^μb^σb+β",{"2":{"343":1}}],["=h",{"2":{"301":3}}],["=x​m​⊤​​w​q​​r​θ",{"2":{"1343":1}}],["=x​6​​relu6",{"2":{"845":1}}],["=xm⊤wqrθ",{"2":{"1343":1}}],["=x×tanh",{"2":{"846":2}}],["=xrelu6",{"2":{"845":1}}],["=x",{"2":{"845":2,"1343":1}}],["=xφ",{"2":{"844":2}}],["=xp",{"2":{"844":2}}],["=x+f",{"2":{"301":1}}],["=x+",{"2":{"301":1}}],["=x⋅sigmoid",{"2":{"845":2}}],["=x⋅σ",{"2":{"108":4,"109":2,"110":2}}],["=x⋅φ",{"2":{"106":2}}],["=𝑥",{"2":{"301":1}}],["=𝐹",{"2":{"140":3}}],["=var",{"2":{"189":2,"999":6,"1002":6}}],["=log",{"2":{"183":1}}],["=logexi−c∑j−1dexi−c=xi−c−log∑j−1dexi−c𝑙𝑜𝑔",{"2":{"183":1}}],["=logexi−c∑dj−1exi−c=xi−c−logd∑j−1exi−c𝑙𝑜𝑔",{"2":{"183":1}}],["=lengthsource∑i=1similarity",{"2":{"170":1,"271":1}}],["=w∗max",{"2":{"1360":2}}],["=wv",{"2":{"175":4}}],["=w",{"2":{"175":1,"1003":1,"1360":1,"1388":1,"1389":1,"1393":1,"1395":2}}],["=w1tq+w2tkw^t",{"2":{"175":1}}],["=w^t",{"2":{"175":1}}],["=wt1q+wt2kwt",{"2":{"175":1}}],["=wt",{"2":{"175":2}}],["=∑if",{"2":{"943":1,"961":1}}],["=∑i=1tlogp",{"2":{"903":1}}],["=∑i=1dk1=dkvar",{"2":{"189":1}}],["=∑i=1dkvar",{"2":{"189":2}}],["=∑i=1lengthsourcesimilarity",{"2":{"170":1,"271":1}}],["=∑j=1iweight",{"2":{"71":1}}],["=ℎ+ℎ",{"2":{"141":1}}],["=0e",{"2":{"1003":2,"1004":2}}],["=0",{"2":{"134":1,"976":1,"1003":1,"1004":1,"1388":1,"1389":3,"1393":9,"1395":1}}],["=f^",{"2":{"1004":1}}],["=fp",{"2":{"612":2}}],["=f",{"2":{"125":3,"766":1,"943":2,"961":2,"1003":1,"1004":1}}],["=swish",{"2":{"109":2}}],["=softmax",{"2":{"57":3,"125":3,"186":2,"194":6,"267":1,"510":2,"918":3,"941":1,"960":1,"1339":1}}],["=10000^",{"2":{"1343":1,"1344":4}}],["=1000",{"2":{"1083":1}}],["=1",{"2":{"110":1,"134":1,"184":1,"976":1,"1000":6,"1003":3,"1004":3,"1393":1}}],["=11+e−xsigmoid",{"2":{"839":1}}],["=11+e−x",{"2":{"108":1,"109":1}}],["=11+e−xσ",{"2":{"108":1,"109":1}}],["=12nlvar",{"2":{"1003":1}}],["=12var",{"2":{"1003":1}}],["=12e",{"2":{"1003":1}}],["=12",{"2":{"106":2}}],["=σ",{"2":{"105":2}}],["=2nlvar",{"2":{"1003":1}}],["=2",{"2":{"46":1,"1393":1}}],["=>",{"2":{"36":1}}],["==模型保存的几种参数类型==",{"2":{"668":1}}],["==onnx==模型保存必须输入对应的input",{"2":{"668":1}}],["==动态图==",{"2":{"668":1}}],["==状态字典==",{"2":{"667":1}}],["==self",{"2":{"666":1}}],["======",{"2":{"1442":1}}],["=========loss",{"2":{"1205":1}}],["=============",{"2":{"1398":1}}],["================model",{"2":{"1205":2}}],["============output",{"2":{"1283":1}}],["============images",{"2":{"1283":1}}],["===========w",{"2":{"1098":1}}],["==========y",{"2":{"1098":1}}],["=======loss",{"2":{"1205":1}}],["=====",{"2":{"1442":1}}],["====",{"2":{"423":2}}],["==>",{"2":{"36":1}}],["==",{"2":{"23":1,"67":1,"74":2,"84":5,"199":1,"382":1,"385":6,"394":1,"395":3,"399":2,"402":1,"423":1,"590":2,"591":1,"700":1,"723":1,"933":1,"1215":2,"1216":3,"1217":2,"1218":3,"1226":1,"1237":4,"1238":4,"1240":4,"1246":4,"1247":5,"1254":2,"1273":1,"1295":1,"1328":2,"1329":1,"1330":5,"1398":1,"1440":1,"1594":2,"1608":2,"1619":1,"1625":2,"1630":2,"1633":2,"1635":1,"1646":1,"1712":1,"1728":1,"1729":6,"1736":1,"1737":1,"1738":1,"1761":1,"1762":1,"1924":1,"1999":1}}],["=",{"2":{"8":10,"23":7,"28":3,"36":5,"38":1,"39":3,"54":1,"65":2,"66":7,"67":5,"71":5,"74":7,"76":6,"79":3,"82":3,"83":21,"84":9,"99":1,"104":1,"105":4,"106":2,"108":3,"109":4,"110":11,"113":3,"114":6,"119":3,"148":1,"161":6,"167":2,"169":1,"170":2,"173":2,"175":4,"178":5,"183":1,"186":1,"189":5,"191":10,"192":1,"197":1,"199":5,"201":63,"230":1,"240":2,"241":3,"270":2,"271":3,"301":1,"315":9,"326":7,"329":2,"330":2,"332":1,"334":2,"343":13,"344":9,"346":10,"359":1,"364":2,"371":3,"372":3,"373":4,"374":2,"375":7,"380":12,"381":2,"382":4,"383":4,"384":7,"385":13,"394":8,"395":5,"396":1,"398":5,"399":25,"402":8,"410":4,"422":3,"423":17,"424":9,"428":10,"446":1,"449":5,"450":5,"451":1,"472":14,"485":1,"503":46,"510":2,"517":1,"520":9,"522":3,"523":5,"528":2,"529":14,"530":13,"532":3,"533":8,"552":1,"557":16,"558":12,"571":12,"572":6,"590":4,"591":27,"592":21,"612":1,"615":1,"640":1,"646":1,"659":1,"661":4,"679":8,"685":1,"692":4,"694":2,"698":2,"700":10,"701":2,"702":20,"703":11,"723":25,"762":1,"766":2,"768":1,"801":5,"802":9,"804":6,"805":7,"807":16,"808":13,"809":10,"810":12,"814":4,"815":4,"816":9,"819":2,"820":6,"821":1,"822":1,"825":1,"826":2,"827":1,"828":1,"829":1,"831":1,"832":4,"833":6,"834":10,"835":3,"839":10,"840":6,"841":3,"842":9,"843":6,"844":5,"845":4,"846":4,"847":4,"856":2,"899":2,"903":1,"908":2,"912":1,"914":2,"915":1,"927":2,"933":6,"941":2,"943":13,"944":4,"960":2,"961":13,"970":1,"971":1,"1003":5,"1004":1,"1023":1,"1039":2,"1069":4,"1070":5,"1071":4,"1072":9,"1075":4,"1076":7,"1078":4,"1082":6,"1083":16,"1085":4,"1086":22,"1087":371,"1092":9,"1093":5,"1094":4,"1095":7,"1096":10,"1097":4,"1098":22,"1099":8,"1100":3,"1101":2,"1102":17,"1104":4,"1106":2,"1107":1,"1114":4,"1116":7,"1132":1,"1133":1,"1134":1,"1179":1,"1180":1,"1184":1,"1188":1,"1189":3,"1190":3,"1191":5,"1192":6,"1193":6,"1195":5,"1202":13,"1205":36,"1207":3,"1208":6,"1210":1,"1211":12,"1212":5,"1213":7,"1214":41,"1215":76,"1216":46,"1217":39,"1218":83,"1221":2,"1223":4,"1226":11,"1227":9,"1231":10,"1233":3,"1234":2,"1235":7,"1236":6,"1237":7,"1238":7,"1239":3,"1240":8,"1241":3,"1242":4,"1243":7,"1244":7,"1245":3,"1246":9,"1247":9,"1250":13,"1251":1,"1253":3,"1254":27,"1255":4,"1257":18,"1258":1,"1259":4,"1262":3,"1263":5,"1266":5,"1267":7,"1269":2,"1270":2,"1272":2,"1273":10,"1274":1,"1279":1,"1280":1,"1282":2,"1283":11,"1295":15,"1296":8,"1297":5,"1298":5,"1299":4,"1322":2,"1328":5,"1329":7,"1330":29,"1331":8,"1336":2,"1339":3,"1340":1,"1342":1,"1343":1,"1345":32,"1350":9,"1373":1,"1377":1,"1388":6,"1389":4,"1392":2,"1393":8,"1394":8,"1395":14,"1396":1,"1398":46,"1436":1,"1440":7,"1442":3,"1481":7,"1485":10,"1486":4,"1487":5,"1488":4,"1489":4,"1560":1,"1566":7,"1570":1,"1590":1,"1594":10,"1603":1,"1607":15,"1608":3,"1611":18,"1612":7,"1613":2,"1614":13,"1615":5,"1619":5,"1620":3,"1621":8,"1623":14,"1624":5,"1625":4,"1629":17,"1630":20,"1632":1,"1633":16,"1634":7,"1635":11,"1638":1,"1639":3,"1640":1,"1641":1,"1645":1,"1646":3,"1647":10,"1648":7,"1649":3,"1650":31,"1651":4,"1655":2,"1656":1,"1663":2,"1667":4,"1668":8,"1669":2,"1670":3,"1671":2,"1672":7,"1673":1,"1674":1,"1676":2,"1677":3,"1680":2,"1683":10,"1684":7,"1685":2,"1687":8,"1688":4,"1691":3,"1693":4,"1694":8,"1695":8,"1696":3,"1698":4,"1699":4,"1700":2,"1704":3,"1705":2,"1706":9,"1707":3,"1708":3,"1709":3,"1710":9,"1712":12,"1713":42,"1714":7,"1715":13,"1718":3,"1719":16,"1720":14,"1721":14,"1722":14,"1724":8,"1725":17,"1728":9,"1729":13,"1736":2,"1737":2,"1738":2,"1739":2,"1741":1,"1742":1,"1743":1,"1746":2,"1749":1,"1750":1,"1751":1,"1752":1,"1754":3,"1755":2,"1756":2,"1759":1,"1762":1,"1763":1,"1774":3,"1779":1,"1788":3,"1789":4,"1792":1,"1797":3,"1799":1,"1800":1,"1801":1,"1802":1,"1803":1,"1805":1,"1806":2,"1807":6,"1816":2,"1817":3,"1820":1,"1821":3,"1824":4,"1825":6,"1834":2,"1835":3,"1838":1,"1839":3,"1842":4,"1843":6,"1853":3,"1857":4,"1861":4,"1866":2,"1874":2,"1879":4,"1883":5,"1887":20,"1891":3,"1897":3,"1898":1,"1902":3,"1905":1,"1906":3,"1907":10,"1908":2,"1910":5,"1911":4,"1914":5,"1917":11,"1921":6,"1922":7,"1923":3,"1924":3,"1925":2,"1926":8,"1927":3,"1928":8,"1929":7,"1930":7,"1931":1,"1933":5,"1999":6,"2003":2,"2005":4,"2006":4,"2007":1,"2008":1,"2017":1,"2018":11,"2021":2,"2027":1,"2059":8,"2060":6,"2061":2,"2062":3,"2086":15,"2153":2,"2154":3}}],["=512",{"2":{"7":1}}],["n第二行",{"2":{"1616":1}}],["nurl",{"2":{"1566":1}}],["nullopt",{"2":{"1927":2}}],["nullptr",{"2":{"1611":7,"1612":1,"1633":2,"1647":3,"1648":1,"1672":7,"1683":1,"1694":2,"1706":2,"1714":1,"1887":5,"1897":1}}],["null",{"2":{"1481":3,"1616":1,"1713":1,"1715":3,"1911":3}}],["num++",{"2":{"1729":1}}],["num2",{"2":{"1729":7,"1824":3,"1842":3}}],["num1",{"2":{"1729":7}}],["numpy2",{"2":{"1072":1}}],["numpy",{"2":{"807":1,"1072":17,"1078":3,"1079":1,"1083":2,"1087":1,"1254":5,"1278":1,"1298":2,"1398":1}}],["numeric",{"2":{"1814":2,"1832":2}}],["numeric>",{"2":{"1746":1,"1747":1}}],["numerical",{"2":{"498":1,"688":1}}],["numel",{"2":{"119":1,"395":1,"1087":1}}],["num和token这三个维度本身就支持并行",{"2":{"420":1}}],["num⋅warmup",{"2":{"402":2}}],["num−0",{"2":{"402":2}}],["num是当前训练步数",{"2":{"402":1}}],["num是注意力头数",{"2":{"198":1}}],["num",{"2":{"34":3,"119":1,"198":2,"199":3,"315":1,"364":1,"372":1,"402":2,"420":2,"423":1,"503":18,"571":4,"591":2,"592":3,"700":1,"702":9,"1087":3,"1211":1,"1215":1,"1216":19,"1217":2,"1218":27,"1239":1,"1284":1,"1328":1,"1329":14,"1330":16,"1332":1,"1645":13,"1646":3,"1648":2,"1664":1,"1665":7,"1709":3,"1729":2,"1797":2,"1799":2,"1800":2,"1801":2,"1802":2,"1806":2,"1814":3,"1817":5,"1824":2,"1832":3,"1835":5,"1842":2,"2061":2,"2062":2,"2086":4}}],["nums",{"2":{"28":1,"1645":3,"2061":2}}],["number++",{"2":{"1625":2}}],["numbers",{"2":{"428":1,"688":2,"1254":1,"1623":8,"1645":6,"1718":3,"1719":29,"1720":16,"1721":16,"1722":16,"1723":7,"1724":14,"1759":2,"1797":5,"1799":4,"1800":4,"1801":3,"1802":2,"1806":4,"1883":12,"1914":5,"1922":5,"2061":2,"2077":1}}],["number",{"2":{"23":1,"160":1,"422":1,"591":1,"592":1,"621":1,"688":1,"1085":11,"1086":2,"1087":125,"1110":1,"1215":1,"1254":2,"1308":4,"1625":8,"1665":1,"1728":1,"2061":1,"2083":1,"2086":2}}],["n−m​d​​w​k​​x​n​​",{"2":{"1343":1}}],["n−mdwkxnq",{"2":{"1343":1}}],["nhql",{"2":{"1217":1}}],["nkhd",{"2":{"1217":1}}],["nqhd",{"2":{"1217":1}}],["ncall",{"2":{"1101":1}}],["nccl",{"2":{"423":1,"1305":1,"1306":1,"1307":2}}],["nsecond",{"2":{"1101":1}}],["nsp",{"2":{"721":1,"1315":3}}],["nr",{"2":{"1082":2,"1110":2}}],["n​d​​w​k​​x​n​​",{"2":{"1343":1}}],["n​i+1​​var",{"2":{"1000":1}}],["n​i​​var",{"2":{"1000":1}}],["n​2​​d​2​​m​−1​​",{"2":{"945":1,"965":1}}],["n​2​​",{"2":{"941":1,"960":1,"1317":1}}],["n^",{"2":{"941":1,"945":1,"960":1,"965":1,"1317":1}}],["n^2d^2m^",{"2":{"945":1,"965":1}}],["n^2",{"2":{"180":1,"210":1}}],["npos",{"2":{"1713":2}}],["np",{"2":{"807":4,"808":3,"809":3,"810":4,"1072":6,"1078":1,"1254":2,"1278":1,"1279":2,"1398":17,"1589":1,"1594":1}}],["nprocs=ngpus",{"2":{"422":1}}],["nbest",{"2":{"1330":23}}],["nbsp",{"2":{"769":40,"770":16,"772":16,"773":8,"774":8,"775":16,"776":24,"777":16,"778":8,"779":8,"783":8,"785":24,"794":8,"795":8,"796":8,"810":8,"812":8,"813":8,"814":16,"815":8,"816":24,"819":8,"820":8,"821":8,"822":8,"823":8,"825":8,"826":8,"827":8,"828":8,"829":8,"831":8,"834":8,"835":8,"838":56,"839":8,"840":16,"841":16,"842":8,"843":8,"844":8,"845":8,"846":16,"847":8,"848":8,"850":24,"851":16,"860":16,"861":16,"862":8,"863":8,"865":8,"866":8,"867":8,"868":8,"874":32,"878":16,"879":8,"881":8,"882":24,"883":16,"885":16,"886":16,"888":16,"889":8,"890":8,"891":8,"892":24,"894":16,"895":8,"896":8,"897":8,"898":24,"899":24,"901":24,"902":16,"903":16,"904":24,"906":8,"908":48,"909":16,"911":40,"912":16,"914":8,"915":8,"916":8,"920":8,"921":8,"924":8,"926":8,"928":8,"931":16,"932":8,"933":16,"934":16,"935":24,"937":32,"938":8,"940":16,"941":24,"942":16,"943":24,"944":16,"945":8,"946":8,"948":8,"951":24,"953":32,"954":8,"956":16,"959":24,"960":24,"961":24,"962":8,"963":16,"964":24,"965":24,"966":8,"968":8,"969":8,"971":8,"973":8,"974":16,"975":16,"976":48,"977":48,"978":8,"980":24,"981":16,"982":16,"983":32,"985":16,"986":24,"988":16,"990":16,"991":24,"992":8,"993":8,"994":16,"995":16,"998":8,"999":8,"1000":40,"1001":8,"1003":88,"1004":16,"1011":8,"1012":16,"1014":8,"1015":8,"1016":8,"1017":16,"1018":8,"1019":8,"1021":8,"1023":8,"1025":8,"1026":8,"1027":8,"1028":8,"1034":16,"1036":8,"1041":8,"1042":16,"1048":16,"1052":8,"1057":8,"1059":8,"1112":8,"1113":24,"1114":48,"1115":8,"1116":8,"1117":48,"1118":8,"1119":16,"1120":24,"1121":40,"1122":32,"1123":32,"1210":8,"1219":8,"1221":16,"1222":32,"1223":24,"1225":32,"1228":24,"1229":16,"1287":36,"1288":18,"1290":9,"1291":36,"1292":9,"1293":9,"1312":64,"1313":48,"1314":8,"1315":32,"1316":32,"1317":24,"1318":8,"1320":32,"1322":32,"1323":24,"1324":24,"1326":8,"1334":8,"1335":8,"1336":24,"1337":24,"1338":8,"1339":40,"1340":48,"1341":16,"1342":40,"1343":32,"1344":32,"1360":8,"1361":24,"1364":16,"1370":16,"1371":24,"1372":16,"1373":16,"1374":8,"1375":8,"1376":8,"1377":24,"1392":7,"1438":8,"1439":8,"1443":24,"1450":8,"1455":16,"1456":24,"1457":8,"1459":8,"1460":8,"1461":8,"1462":8,"1464":16,"1465":8,"1466":8,"1469":8}}],["nbatches",{"2":{"29":1,"36":5,"383":4}}],["nth",{"0":{"1752":1},"2":{"1732":1,"1752":3}}],["ntest",{"2":{"1215":1}}],["nt",{"2":{"1086":1}}],["ntk",{"2":{"768":1}}],["ntokens对损失进行正则化",{"2":{"398":1}}],["ntokens",{"2":{"380":1,"381":1,"385":5,"398":1,"399":1,"410":1}}],["n的结构可以处理的问题有",{"2":{"882":1}}],["n的bias",{"2":{"757":1}}],["n的patch序列p=",{"2":{"613":1}}],["nv",{"0":{"735":1},"2":{"731":1,"732":1,"735":4,"740":1}}],["nvidia公司提供的gpu并行计算框架",{"2":{"1569":1}}],["nvidia",{"0":{"793":1},"1":{"794":1,"795":1,"796":1},"2":{"387":1,"790":1,"794":1,"968":1}}],["n代表句子序列长度",{"2":{"698":1}}],["nfd",{"2":{"552":2}}],["n是解码器层的个数",{"2":{"532":1}}],["n是batch",{"2":{"341":1}}],["nx",{"2":{"517":1}}],["n+t",{"2":{"504":2}}],["n+1",{"2":{"58":4}}],["n×d",{"2":{"941":2,"942":2,"959":2,"960":2}}],["n×n×n",{"2":{"436":1}}],["n×h×w",{"2":{"315":1,"340":1}}],["n=k×k×cn=k",{"2":{"1003":1}}],["n=num",{"2":{"974":1}}],["n=1024",{"2":{"941":1,"960":1}}],["n=6",{"2":{"372":1,"423":1,"448":1,"449":1,"703":2}}],["n=2",{"2":{"83":1,"399":1,"424":1}}],["nähe",{"2":{"370":1,"557":1}}],["ngpus",{"2":{"364":2,"422":3,"423":2}}],["ngpt",{"0":{"350":1},"1":{"351":1,"352":1,"353":1,"354":1,"355":1,"356":1,"357":1},"2":{"293":1,"350":1,"352":2,"354":2,"361":1,"513":1}}],["nmt就是这样",{"2":{"909":1}}],["nmt",{"0":{"909":1},"2":{"281":1,"907":1}}],["nlohmann",{"2":{"1989":1,"1990":1}}],["nlg",{"2":{"1317":3}}],["nlhd",{"2":{"1217":1}}],["nll",{"2":{"1215":2}}],["nlp面经",{"0":{"2144":1}}],["nlp领域的词汇表大小",{"2":{"698":1}}],["nlp领域又为何钟情layernorm",{"2":{"312":1}}],["nlp处理的是语言",{"2":{"689":1}}],["nlp分词模型",{"2":{"638":1}}],["nlp三大subword模型详解",{"2":{"638":1}}],["nlp语境下的layernorm",{"2":{"341":1}}],["nlp中的tokenization方法总结",{"2":{"638":1}}],["nlp中的tokenization",{"2":{"638":1}}],["nlp中使用ln",{"2":{"326":1}}],["nlp中",{"2":{"318":1}}],["nlp的文本序列本质上是一个时间序列",{"2":{"316":1}}],["nlp",{"0":{"326":1,"670":1,"905":1,"906":1},"1":{"906":1,"907":1,"908":1,"909":1},"2":{"233":2,"293":1,"323":1,"326":1,"333":1,"341":1,"432":1,"474":1,"638":4,"711":1,"740":1,"808":1,"844":1,"878":1,"889":1,"906":1,"1015":1,"1312":2,"1315":2,"1318":2,"1404":4,"2079":1}}],["nlu",{"2":{"156":1,"1315":2,"1317":3,"1318":1}}],["n²d",{"2":{"212":1}}],["ndemonstrating",{"2":{"1891":1}}],["ndwkxn",{"2":{"1343":1}}],["ndimension",{"2":{"1087":1}}],["ndim",{"2":{"1082":1}}],["ndims",{"2":{"700":1}}],["ndarrays",{"2":{"1254":1}}],["ndarray",{"2":{"1072":7,"1078":1,"1083":1}}],["nd×n",{"2":{"1003":1}}],["nd+n​2​​",{"2":{"945":1,"965":1}}],["nd+n2",{"2":{"945":1,"965":1}}],["ndss",{"2":{"233":1}}],["nd^2",{"2":{"216":1}}],["nd2",{"2":{"216":2}}],["nd²",{"2":{"212":1}}],["nd",{"2":{"210":3,"945":1,"965":1,"1078":5}}],["n2d2m−1",{"2":{"945":1,"965":1}}],["n2−8",{"2":{"765":1}}],["n2",{"2":{"180":2,"210":2,"941":1,"960":1,"1317":1}}],["nabla",{"2":{"1188":1,"1189":1,"1190":2,"1191":2,"1192":2,"1193":3}}],["nadam",{"0":{"1193":1},"2":{"1130":1}}],["nado",{"2":{"1124":1,"1195":1}}],["narrow",{"2":{"1087":3}}],["nag",{"0":{"1033":1},"1":{"1034":1,"1035":1,"1036":1},"2":{"1035":1}}],["name2",{"2":{"1927":4}}],["name1",{"2":{"1927":3}}],["name>",{"2":{"1489":2}}],["namespace=",{"2":{"1481":1}}],["namespace",{"0":{"1484":1},"2":{"1481":1,"1606":1,"1607":1,"1608":1,"1619":1,"1620":1,"1621":2,"1623":1,"1624":1,"1625":2,"1683":4,"1684":2,"1685":2,"1687":2,"1688":2,"1691":1,"1713":3,"1714":1,"1715":2,"1824":1,"1825":1,"1842":1,"1843":1,"1930":2,"1931":4,"1933":1,"1999":1,"2003":1,"2004":1,"2005":1,"2006":5,"2007":1,"2008":1,"2059":1}}],["names=",{"2":{"1083":1}}],["names",{"2":{"1082":1,"1083":7,"1087":7,"1489":2,"1807":2}}],["named",{"2":{"768":1,"906":1,"1214":5}}],["name=word",{"2":{"700":1}}],["name=",{"2":{"700":1,"1481":4,"1487":1}}],["name=path",{"2":{"571":1}}],["name",{"2":{"571":1,"700":3,"722":1,"723":1,"1214":8,"1215":1,"1216":1,"1218":1,"1273":7,"1281":1,"1298":1,"1332":2,"1398":1,"1440":1,"1481":1,"1485":2,"1486":2,"1487":1,"1532":1,"1537":2,"1616":1,"1654":4,"1665":21,"1673":3,"1674":8,"1675":5,"1685":14,"1691":11,"1728":6,"1729":3,"1750":2,"1805":2,"1807":4,"1825":8,"1843":8,"1897":2,"1921":5,"1927":5,"1986":1}}],["nansum",{"2":{"1087":1}}],["nanquantile",{"2":{"1087":2}}],["nanmedian",{"2":{"1087":5}}],["nanmean",{"2":{"1087":1}}],["nan",{"2":{"503":1,"840":1,"1087":6,"1115":2,"1153":1,"1182":1}}],["nanda∗",{"2":{"233":1}}],["nat服务未开启",{"2":{"2091":1}}],["nat模式",{"2":{"2090":1}}],["nativebatchnormbackward0>",{"2":{"315":3}}],["nature子刊揭示社会层级结构的网格表征",{"2":{"233":1}}],["natural",{"2":{"167":2,"259":2,"572":1,"2079":1}}],["nair和",{"2":{"104":1}}],["naive",{"2":{"89":1}}],["ni+1var",{"2":{"1000":1}}],["nivar",{"2":{"1000":1}}],["nicely",{"2":{"591":1}}],["ni",{"2":{"361":1}}],["nips",{"2":{"95":1}}],["nikolas",{"2":{"47":1,"233":2}}],["neo",{"2":{"1316":2}}],["nelement",{"2":{"1087":1}}],["nelson",{"2":{"233":1}}],["ne",{"2":{"1085":1,"1087":4}}],["neginf",{"2":{"1087":2}}],["neg",{"2":{"1083":2,"1085":1,"1086":1,"1087":4}}],["negative",{"2":{"384":1,"402":1,"726":2,"1087":2}}],["nestedtensor",{"2":{"1086":1}}],["nestedtensors",{"2":{"1086":1}}],["nested",{"2":{"1086":9,"1087":1,"1797":2}}],["nesterov",{"0":{"1033":1,"1190":1},"1":{"1034":1,"1035":1,"1036":1},"2":{"1034":1,"1130":1,"1143":4,"1144":2}}],["ness",{"2":{"504":1}}],["ner",{"0":{"1319":1},"1":{"1320":1,"1321":1,"1322":1,"1323":1,"1324":1,"1325":1,"1326":1},"2":{"880":1,"906":1,"1309":2}}],["near",{"2":{"370":1,"557":1}}],["neel",{"2":{"233":1}}],["needed",{"2":{"557":1,"591":1,"2076":1}}],["needs",{"2":{"375":1,"1217":1}}],["need",{"0":{"911":1},"2":{"115":1,"233":1,"235":1,"292":1,"429":1,"432":1,"446":2,"513":1,"747":1,"935":1,"951":1,"981":1,"1010":1,"1254":4,"1312":1}}],["neq",{"2":{"191":1,"1003":1}}],["neural",{"0":{"283":1,"497":2,"850":1,"1379":1,"1451":1},"2":{"156":3,"175":1,"228":1,"257":1,"283":1,"284":1,"285":1,"288":3,"292":7,"393":1,"429":2,"449":1,"490":1,"493":1,"495":1,"496":1,"497":2,"498":1,"513":3,"543":2,"574":1,"601":1,"605":1,"638":5,"698":1,"769":2,"796":1,"850":1,"885":1,"907":1,"1241":1,"1242":1,"1254":1,"1404":2,"1438":1,"1456":2,"1457":1,"1464":2,"2086":3}}],["neurips",{"2":{"156":1}}],["neurons",{"2":{"126":1,"143":1,"156":1,"477":1,"487":1,"513":1}}],["neuron",{"0":{"135":1},"2":{"96":1,"133":1,"475":1}}],["newdir",{"2":{"1930":7}}],["newfunction",{"2":{"1909":4}}],["newfile",{"2":{"1510":1}}],["newheight",{"2":{"1789":2}}],["newwidth",{"2":{"1789":2}}],["newlength",{"2":{"1789":2}}],["newline",{"2":{"1616":1}}],["newradius",{"2":{"1779":2}}],["newuser",{"2":{"1530":2}}],["new>",{"2":{"1083":3}}],["newids",{"2":{"590":4}}],["newsql",{"2":{"1951":1}}],["news",{"2":{"373":3}}],["newgeluactivation",{"2":{"110":1}}],["new",{"0":{"1647":2,"1668":2},"2":{"110":1,"201":1,"395":1,"543":1,"575":1,"578":1,"584":1,"585":1,"587":1,"590":1,"592":2,"623":2,"944":4,"1083":1,"1087":14,"1253":1,"1254":20,"1255":2,"1284":1,"1481":1,"1485":1,"1486":1,"1520":2,"1554":7,"1630":2,"1647":16,"1648":5,"1666":2,"1668":12,"1669":6,"1671":2,"1672":4,"1676":2,"1678":1,"1680":1,"1683":2,"1688":4,"1694":3,"1695":1,"1696":1,"1700":1,"1706":1,"1714":2,"1797":2,"1820":1,"1838":1,"1866":2,"1874":1,"1887":3,"1891":1,"1909":1,"1911":1,"2075":1,"2078":1,"2081":1}}],["netstat",{"2":{"1527":1}}],["neto1net",{"2":{"1392":1}}],["neto1=w7×outh1+w9×outh2+w11×outh3+b2∗1net",{"2":{"1393":1}}],["neto1=w7×outh1+w9×outh2+w11×outh3+b2×1net",{"2":{"1389":1}}],["neto1=0",{"2":{"1389":1}}],["net​o1​​",{"2":{"1392":1}}],["net​o1​​=w​7​​×out​h1​​+w​9​​×out​h2​​+w​11​​×out​h3​​+b​2​​∗1",{"2":{"1393":1}}],["net​o1​​=w​7​​×out​h1​​+w​9​​×out​h2​​+w​11​​×out​h3​​+b​2​​×1",{"2":{"1389":1}}],["net​o1​​=0",{"2":{"1389":1}}],["net​h1​​=0",{"2":{"1388":1}}],["net​h1​​=w​1​​×l​1​​+w​2​​×l​2​​+b​1​​∗1",{"2":{"1388":1}}],["neth1=0",{"2":{"1388":1}}],["neth1=w1×l1+w2×l2+b1∗1net",{"2":{"1388":1}}],["net的前世今生与核心知识",{"2":{"1365":1}}],["net1",{"2":{"1215":3}}],["netron",{"2":{"930":1}}],["netflix",{"2":{"692":1}}],["net理论上可以对给定的样本和样本类别进行任意分类",{"2":{"320":1}}],["network提取真实数据和生成数据的特征向量",{"2":{"1361":1}}],["network的gated",{"2":{"301":1}}],["networks中",{"2":{"419":1}}],["networks的gated",{"2":{"301":1}}],["networks的实现与resnet相反",{"2":{"301":1}}],["networks进行比对",{"2":{"301":1}}],["networks全文翻译",{"2":{"156":1}}],["networks",{"0":{"283":1,"497":1,"1429":1},"2":{"101":1,"105":1,"125":1,"134":1,"155":2,"156":6,"283":1,"287":1,"292":3,"298":2,"301":1,"302":1,"361":1,"393":1,"449":1,"497":1,"498":1,"513":2,"517":1,"543":4,"1241":1,"1242":1,"1254":1}}],["network",{"0":{"850":1,"1451":1},"2":{"97":1,"156":2,"160":1,"288":1,"361":2,"445":1,"466":1,"601":1,"638":1,"769":3,"796":1,"850":1,"885":1,"1216":1,"1218":2,"1404":3,"1438":1,"1456":2,"1457":2,"1464":2,"2086":3,"2095":1}}],["net",{"2":{"95":1,"320":1,"661":1,"768":2,"1102":4,"1104":1,"1215":2,"1244":1,"1257":2,"1258":1,"1259":1,"1262":1,"1266":1,"1267":1,"1269":1,"1282":2,"1302":2,"1309":1,"1388":1,"1389":1,"1392":2,"1393":9,"1394":8,"1398":5}}],["nextafter",{"2":{"1087":2}}],["next",{"2":{"83":4,"428":5,"472":4,"529":4,"542":1,"592":1,"632":1,"721":1,"732":1,"734":3,"1283":1,"1284":2,"1315":1,"1328":4,"1329":6,"1330":10}}],["nostatic",{"2":{"2070":1}}],["nostalgebraist",{"2":{"513":1}}],["noboolalpha",{"2":{"1817":3,"1835":3}}],["noexcept",{"2":{"1764":3,"1887":2}}],["noisy",{"2":{"1350":2}}],["noiser",{"0":{"1354":1}}],["noise",{"2":{"395":9,"500":1,"1155":1,"1350":3}}],["norouzi",{"2":{"1194":1}}],["norm3",{"2":{"1218":2}}],["norm2",{"2":{"1216":2,"1217":2,"1218":4}}],["norm1",{"2":{"1216":2,"1217":2,"1218":4}}],["norm相当于白化",{"2":{"1180":1}}],["norm代替",{"2":{"1168":1}}],["norm通常可以用layer",{"2":{"1168":1}}],["norm采用固定值的batch",{"2":{"1136":1}}],["norm会对batch",{"0":{"1136":1}}],["norm可能得到的效果会更好",{"2":{"807":1}}],["norm也都可以作为单独的小积木块",{"2":{"449":1}}],["norm也更好优化",{"2":{"334":1}}],["norm与deepnorm",{"2":{"361":1}}],["norm兼具post",{"2":{"347":1}}],["norm层的权重参数",{"2":{"346":1}}],["norm层之后跟随一个残差连接",{"2":{"344":1}}],["norm→attention→add→norm→feed",{"2":{"344":1}}],["norm→feed",{"2":{"330":2}}],["normbalization",{"2":{"338":1}}],["norm拓展到b这个维度",{"2":{"337":1}}],["norm对transformer而言也是如此",{"2":{"337":1}}],["norm对cnn而言是在归一化时在特征维度上做选择的最小集",{"2":{"337":1}}],["norm对参数非常敏感",{"2":{"333":1}}],["norm虽然理论上行可以达到更好的算法上限",{"2":{"335":1}}],["norm能达到的上界高",{"2":{"335":1}}],["norm则恰恰相反",{"2":{"335":1}}],["norm结构往往过度依赖恒等分支",{"2":{"335":1}}],["norm两者模型效果的数学推导",{"2":{"335":1}}],["norm和post",{"2":{"335":1}}],["norm由于并不是所有的参数都参与正则化",{"2":{"334":1}}],["norm更加明显",{"2":{"334":1}}],["norm训练出来的效果是更好的",{"2":{"334":1}}],["norm主要思路是",{"2":{"334":1}}],["norm如果调整到sub",{"2":{"334":1}}],["norm需要很仔细地调参才能取得好的结果",{"2":{"333":1}}],["norm的技术博客",{"2":{"640":1}}],["norm的特点如下",{"2":{"347":1}}],["norm的层次结构更为",{"2":{"335":1}}],["norm的表现不如同样深度的post",{"2":{"335":1}}],["norm的公式进一步展开得到如下",{"2":{"334":1}}],["norm的效果不如post",{"2":{"334":1,"361":1}}],["norm的位置加的有问题",{"2":{"334":1}}],["norm的训练更加不稳定",{"2":{"333":1}}],["norm的基础上",{"2":{"329":1}}],["norm中",{"2":{"332":1}}],["norm每norm一次就削弱一次残差的恒等分支的权重",{"2":{"332":1}}],["norm指的是残差连接后使用layer",{"2":{"330":1}}],["normal",{"2":{"503":5,"1087":2}}],["normalizer",{"2":{"552":2,"1329":1}}],["normalizers",{"2":{"552":3}}],["normalized",{"2":{"341":1,"350":1,"513":1,"807":2,"808":2,"809":2,"810":2,"1087":1,"1211":2}}],["normalize则是不考虑其他数据",{"2":{"322":1}}],["normalize是考虑本batch内所有数据",{"2":{"322":1}}],["normalize",{"2":{"313":1,"326":1,"552":1,"1215":1,"1283":1}}],["normalize过后的线性attention",{"2":{"210":1}}],["normalization对小batchsize效果差",{"2":{"810":1}}],["normalization有效地将粒子限制在时间变化为轴的单位球体sd−1sd−1",{"2":{"499":1}}],["normalization会怎样",{"2":{"361":1}}],["normalization和layer",{"2":{"361":1}}],["normalization和instance",{"2":{"316":1}}],["normalization首先将",{"2":{"338":1}}],["normalization则更适用于占用显存比较大的任务",{"2":{"338":1}}],["normalization优化了bn在比较小的mini",{"2":{"338":1}}],["normalization不是对一个batch内的特征维度进行处理",{"2":{"322":1}}],["normalization不会受到不同长度句子的影响",{"2":{"317":1}}],["normalization等工作",{"2":{"316":1}}],["normalization往往用于cv中",{"2":{"316":1}}],["normalization或spatio",{"2":{"315":1}}],["normalization或batch",{"2":{"294":1}}],["normalization是在c维上计算",{"2":{"315":1}}],["normalization是在c维度上计算",{"2":{"315":2}}],["normalization是transformer经常采用的方案",{"2":{"312":1}}],["normalization可分为很多种",{"2":{"311":1}}],["normalization",{"0":{"806":1,"809":1,"810":1},"1":{"807":1,"808":1,"809":1,"810":1,"811":1,"812":1},"2":{"294":2,"310":1,"314":3,"315":3,"316":1,"320":4,"325":1,"326":2,"334":1,"337":1,"338":1,"344":1,"346":3,"348":1,"358":1,"361":13,"499":1,"517":4,"545":1,"812":4,"1217":1,"1303":1}}],["normalization操作",{"2":{"294":1}}],["normalizing",{"2":{"156":1}}],["normattention",{"2":{"330":1}}],["normattention→add",{"2":{"330":1}}],["norm是在pre",{"2":{"329":1}}],["norm是layer",{"2":{"294":1}}],["norm之后再add",{"2":{"329":1}}],["norm在同样参数配置下没有post",{"2":{"335":1}}],["norm在接预测层之前xtxtx",{"2":{"334":1}}],["norm在所有序列模型中都不吃香",{"2":{"316":1}}],["norm在transformer的位置是哪里",{"2":{"294":1}}],["norm模块用于在多头自注意力机制和前馈神经网络之间添加残差连接和归一化操作",{"2":{"294":1}}],["norms",{"2":{"8":1}}],["norm",{"0":{"328":2,"331":1,"334":1,"337":1,"347":1,"811":1,"812":1},"1":{"329":2,"330":2,"331":2,"332":3,"333":3,"334":2,"335":2},"2":{"8":1,"83":2,"201":9,"293":6,"316":2,"326":5,"329":14,"330":8,"332":4,"333":5,"334":24,"335":2,"337":1,"343":6,"344":7,"346":2,"347":2,"354":1,"361":2,"394":2,"396":1,"398":4,"399":1,"410":3,"446":1,"472":3,"517":1,"519":4,"522":5,"529":1,"532":2,"640":4,"702":13,"723":1,"808":2,"812":2,"1083":2,"1136":1,"1168":3,"1180":1}}],["noqa",{"2":{"571":1}}],["now",{"2":{"429":1,"1254":1}}],["noam机制就是这个进程的具体体现",{"2":{"402":1}}],["noam机制主要是受人类的学习机制启发",{"2":{"402":1}}],["noam",{"0":{"402":1},"2":{"402":2}}],["node0",{"2":{"1307":2}}],["nodes",{"0":{"1307":1},"2":{"1308":1}}],["node2vec",{"2":{"676":1}}],["node",{"0":{"1306":1},"2":{"364":2,"381":1,"385":5,"399":1,"410":1,"423":3,"1082":1,"1110":1,"1307":2,"1308":5,"1590":1}}],["nonwhitespaces",{"2":{"572":1}}],["nonlinear",{"2":{"499":2}}],["nonlinearity",{"2":{"320":1,"361":2}}],["nonlinearities",{"2":{"156":1}}],["nonzero",{"2":{"399":1,"1085":1,"1087":7}}],["nonvolitional",{"2":{"163":1}}],["non",{"0":{"642":1},"2":{"156":1,"513":1,"801":2,"802":1,"814":1,"815":1,"1086":1,"1087":7,"1208":2,"1214":5,"1284":1,"1761":1,"1781":1}}],["nonpad",{"2":{"55":1}}],["none=true",{"2":{"385":1}}],["none",{"0":{"1738":1},"2":{"8":1,"23":1,"36":1,"67":2,"76":2,"83":1,"114":1,"199":2,"201":3,"375":4,"380":1,"394":2,"399":1,"503":2,"557":3,"590":1,"659":1,"702":6,"723":8,"808":4,"816":1,"933":2,"1085":9,"1086":10,"1087":234,"1208":1,"1211":3,"1214":17,"1216":1,"1217":1,"1218":1,"1227":23,"1250":1,"1298":1,"1308":1,"1330":2,"1566":1,"1732":1,"1738":5}}],["nothing",{"2":{"592":1}}],["notes",{"0":{"639":1},"1":{"640":1,"641":1,"642":1,"643":1,"644":1,"645":1,"646":1,"647":1,"648":1,"649":1,"650":1,"651":1,"652":1,"653":1,"654":1},"2":{"1254":1}}],["note",{"2":{"591":1,"1195":1,"1222":1,"1245":1,"1254":1,"1304":1,"1784":1,"2062":1}}],["notimplementederror",{"2":{"591":3}}],["notion",{"2":{"233":1}}],["not",{"2":{"36":1,"67":2,"114":1,"115":1,"181":1,"199":2,"201":1,"233":2,"372":1,"374":1,"380":1,"394":2,"428":1,"446":1,"503":1,"513":1,"590":1,"591":1,"592":1,"702":1,"723":2,"820":1,"827":1,"933":2,"981":1,"1083":11,"1087":8,"1215":2,"1216":1,"1217":1,"1218":1,"1226":1,"1254":2,"1462":1,"1481":1,"1619":1,"1719":1,"1720":1,"1721":1,"1722":1,"1724":1,"1725":1,"1736":1,"1754":1,"1755":1,"1806":1,"1807":1,"1902":1,"1911":1,"1922":1,"1927":2,"1933":1,"2063":1}}],["no",{"2":{"8":2,"141":1,"572":2,"591":2,"592":1,"660":1,"661":1,"665":1,"834":1,"1086":1,"1095":2,"1097":1,"1098":2,"1104":1,"1115":1,"1116":2,"1118":1,"1143":1,"1215":5,"1695":1}}],["n",{"0":{"880":2,"881":1,"882":1,"883":1},"2":{"8":16,"10":2,"16":2,"57":4,"58":2,"83":3,"119":1,"135":1,"178":2,"180":2,"194":1,"201":46,"204":2,"210":1,"217":1,"241":2,"288":1,"314":2,"315":7,"316":1,"325":1,"326":4,"337":1,"340":1,"341":6,"343":6,"385":2,"402":1,"418":1,"445":2,"449":2,"461":2,"463":1,"477":1,"480":2,"485":1,"503":5,"504":23,"511":6,"522":7,"530":1,"532":4,"564":1,"571":8,"579":1,"580":1,"582":4,"583":4,"585":1,"591":7,"613":3,"621":2,"703":2,"740":2,"765":3,"804":3,"879":2,"883":1,"899":1,"912":2,"941":8,"944":4,"946":2,"957":2,"960":8,"963":1,"966":2,"974":1,"994":3,"995":3,"1000":11,"1002":2,"1003":4,"1004":3,"1006":2,"1007":4,"1078":2,"1087":5,"1101":3,"1156":1,"1166":2,"1205":1,"1215":5,"1217":13,"1279":8,"1284":1,"1308":15,"1312":1,"1322":4,"1323":2,"1342":11,"1343":5,"1398":2,"1459":1,"1515":2,"1553":2,"1556":1,"1590":1,"1594":8,"1607":2,"1611":4,"1616":1,"1623":2,"1632":2,"1633":6,"1646":11,"1651":5,"1654":4,"1710":2,"1716":3,"1739":4,"1751":2,"1752":1,"1788":1,"1789":3,"1797":1,"1813":12,"1814":1,"1817":4,"1831":12,"1832":1,"1835":4,"1883":5,"1891":1,"1902":3,"1924":5,"1931":2,"2059":1,"2063":6,"2155":6}}],["nns",{"2":{"1456":1}}],["nnz",{"2":{"1086":1}}],["nnn",{"2":{"178":1}}],["nn",{"0":{"662":1,"1122":1,"1200":1,"1201":1,"1202":1,"1207":1,"1208":1,"1209":1,"1214":1},"1":{"1202":1,"1203":1,"1210":1,"1211":1,"1212":1,"1213":1,"1214":1},"2":{"8":1,"23":3,"38":1,"39":1,"82":2,"83":1,"110":11,"113":4,"114":1,"119":3,"201":4,"315":5,"326":2,"343":7,"344":3,"346":5,"361":3,"394":3,"398":1,"399":3,"429":2,"449":4,"450":1,"503":9,"522":2,"523":1,"529":3,"532":1,"533":1,"558":1,"700":2,"701":2,"702":11,"703":3,"723":6,"801":5,"802":6,"804":3,"807":2,"808":3,"809":2,"810":3,"814":4,"815":4,"816":5,"834":4,"835":1,"839":4,"840":2,"841":1,"842":3,"843":2,"844":1,"845":1,"846":1,"847":1,"1096":1,"1109":1,"1116":1,"1117":3,"1120":1,"1122":2,"1200":1,"1201":1,"1202":6,"1205":12,"1206":1,"1207":6,"1210":3,"1211":3,"1212":4,"1213":4,"1214":4,"1215":23,"1216":17,"1217":21,"1218":29,"1257":10,"1278":2,"1283":1,"1295":4,"1296":1,"1345":1,"2086":7}}],["根目录",{"2":{"1505":1,"1999":1}}],["根节点被拆分",{"2":{"986":1}}],["根本不知道生成模型生成了什么东西",{"2":{"1374":1}}],["根本就存不进缓存",{"2":{"948":1,"978":1}}],["根本无法执行归一化计算",{"2":{"316":1}}],["根据我们的正常思维",{"2":{"2124":1}}],["根据我们的经验",{"2":{"1175":1}}],["根据不同情况",{"2":{"2115":1}}],["根据不同任务的需要",{"2":{"1466":1}}],["根据更改修正主页index内容",{"2":{"2048":1}}],["根据公式迭代得到的",{"2":{"2018":1}}],["根据已知的起点坐标",{"2":{"2017":1}}],["根据利率计算利息并将其存入余额",{"2":{"1873":1}}],["根据姓名构造",{"2":{"1825":1,"1843":1}}],["根据键排序键值对",{"2":{"1807":1}}],["根据提示完成安装",{"2":{"1605":1}}],["根据id删除用户",{"2":{"1488":1}}],["根据id删除一个用户",{"2":{"1488":1}}],["根据id查询用户",{"2":{"1485":2}}],["根据贝叶斯公式",{"2":{"1377":1}}],["根据分析和推断的结果",{"2":{"1291":1}}],["根据输入和操作的语义来确定操作的数据类型",{"2":{"1291":1}}],["根据循环学习率策略",{"2":{"1241":1}}],["根据param",{"2":{"1227":1}}],["根据梯度范数的第90百分位数选择梯度截断阈值",{"2":{"1184":1}}],["根据问题的不同",{"2":{"1163":1}}],["根据上述问题的答案",{"2":{"1146":1}}],["根据实验目标",{"2":{"1142":1}}],["根据团队的资源预算计费",{"2":{"1134":1}}],["根据这个定义",{"2":{"1117":1}}],["根据需要重新分配内存",{"2":{"1083":1}}],["根据平方梯度的整个历史收缩学习率",{"2":{"1048":1}}],["根据以上知识",{"2":{"1918":1}}],["根据以上的猜测",{"2":{"1019":1}}],["根据以下表达式转换xx",{"2":{"343":1}}],["根据两个随机变量积的方差公式可得",{"2":{"1003":1}}],["根据过个随机变量和的方差推导公式为",{"2":{"1003":1}}],["根据链式法则",{"2":{"994":1}}],["根据z",{"2":{"912":1}}],["根据loss相对于给定参数的梯度来调整parameters",{"2":{"1105":1}}],["根据lm和tm寻找最佳y的过程",{"2":{"908":1}}],["根据llama3源码来看",{"2":{"8":1}}],["根据调整后的结果来选择best",{"2":{"904":1}}],["根据总分还是平均分",{"2":{"904":1}}],["根据绝对位置k来定义位置编码",{"2":{"745":1}}],["根据位置编码表示的是序列中元素的绝对位置信息还是相对位置信息",{"2":{"742":1}}],["根据当前上下文对word",{"2":{"717":1}}],["根据其邻居更新其向量",{"2":{"709":1}}],["根据信息复杂度动态地将字节分组为",{"2":{"612":1}}],["根据评估不断删除排序靠后的subword",{"2":{"601":1}}],["根据模型需求",{"2":{"555":1}}],["根据算法规则生成词表",{"2":{"554":1}}],["根据选定的模型或算法",{"2":{"554":1}}],["根据奥卡姆剃刀原理",{"2":{"542":1}}],["根据自回归语言建模目标训练的纯因果解码器模型表现出最强的零样本泛化能力",{"2":{"542":1}}],["根据自注意力的思想和人脑的机制",{"2":{"168":1}}],["根据你对",{"2":{"536":1}}],["根据权重统计和玩具模型的见解",{"2":{"477":1}}],["根据原始嵌入计算查询",{"0":{"463":1}}],["根据标准化操作的维度",{"2":{"311":1}}],["根据泛逼近定理",{"2":{"296":1}}],["根据注意力分布来计算输入信息的加权平均",{"2":{"267":1}}],["根据极性明确地将",{"2":{"213":1}}],["根据规则",{"2":{"204":1}}],["根据并行度进行拆分",{"2":{"201":2}}],["根据交叉熵损失训练的模型通常更倾向于用多义表示更多特征",{"2":{"118":1}}],["根据",{"2":{"83":1,"128":1,"1242":1,"1485":1}}],["根据transformer回路理论",{"2":{"45":1}}],["根源",{"0":{"4":1},"2":{"0":1}}],["zoo",{"2":{"1691":6}}],["zxcvbn",{"2":{"1486":1}}],["z∣x",{"2":{"1377":10}}],["z|x",{"2":{"1377":1}}],["z2",{"2":{"1331":4}}],["zachary",{"2":{"1124":1,"1195":1}}],["zartbot",{"2":{"233":2}}],["z​i​",{"2":{"999":1}}],["z​i​​",{"2":{"999":1,"1000":1}}],["z^",{"2":{"999":2}}],["zelda",{"2":{"1194":1}}],["zellig",{"2":{"713":1}}],["zeroing",{"2":{"1101":1}}],["zerotensor",{"2":{"1086":1}}],["zero",{"2":{"122":1,"385":1,"402":1,"542":2,"629":1,"659":2,"838":2,"1039":1,"1086":1,"1087":3,"1096":1,"1098":3,"1101":1,"1102":2,"1202":1,"1205":2,"1214":1,"1215":1,"1218":1,"1223":2,"1227":1,"1231":2,"1244":1,"1295":1,"1762":1,"2086":4}}],["zeros",{"2":{"76":1,"84":1,"201":3,"343":1,"395":1,"428":1,"472":1,"503":5,"529":1,"723":2,"1070":1,"1087":2,"1095":1,"1096":1,"1211":2,"1218":1,"1330":3}}],["z=attention",{"2":{"394":1}}],["zu",{"2":{"370":1}}],["zwei",{"2":{"370":1,"557":1}}],["z是",{"2":{"263":1}}],["zn",{"2":{"241":2}}],["z1",{"2":{"241":2}}],["z向量可能是",{"2":{"224":1}}],["z的形状为",{"2":{"199":2}}],["zh",{"2":{"1476":1,"1566":1}}],["zhang",{"2":{"361":1,"513":1}}],["zhaohongfei",{"2":{"95":1}}],["zhihu",{"2":{"156":4,"713":1,"768":1}}],["zhuanlan",{"2":{"156":4}}],["zjukg",{"2":{"156":1}}],["ziz",{"2":{"1000":1}}],["ziziz",{"2":{"418":1}}],["zi",{"2":{"999":2}}],["zi=∑j=1max",{"2":{"71":1}}],["zi=max",{"2":{"71":1}}],["zip",{"2":{"36":2,"590":1}}],["zz",{"2":{"71":1,"1544":1}}],["z",{"0":{"1449":2},"2":{"8":1,"71":6,"145":2,"225":5,"241":2,"344":8,"543":1,"820":2,"912":3,"1000":5,"1082":2,"1093":2,"1094":2,"1095":8,"1096":1,"1098":6,"1099":2,"1116":1,"1331":5,"1377":9,"1615":2,"1619":2,"2059":5}}],["为保证绘制逼真",{"2":{"2019":1}}],["为保证输入序列的长度一致",{"2":{"555":1}}],["为虚拟互动提供了更具情感表达的体验",{"2":{"2011":1}}],["为游戏中的人物动态表现设定了新标准",{"2":{"2011":1}}],["为整数赋予有意义的名称",{"2":{"1728":1}}],["为处理复杂数据奠定基础",{"2":{"1727":1}}],["为它分配内存",{"2":{"1647":1}}],["为真",{"2":{"1630":1}}],["为真则",{"2":{"184":1}}],["为假",{"2":{"1630":1}}],["为止",{"2":{"1620":1}}],["为算法优化提供基础",{"2":{"1602":1}}],["为高性能计算和infiniband网络优化的mpi实现",{"2":{"1569":1}}],["为初始化",{"2":{"1344":1}}],["为线性链条件随机场",{"2":{"1322":1}}],["为线性结构",{"2":{"840":1}}],["为max",{"2":{"1155":1}}],["为下一轮实验确定适当的目标",{"2":{"1139":1}}],["为下一层再次通过自注意力交换信息做好准备",{"2":{"101":1,"466":1}}],["为前缀的属性",{"2":{"1114":1}}],["为满足上述公式继续推导",{"2":{"1000":1}}],["为tanh激活函数",{"2":{"1000":1}}],["为解决这一问题",{"2":{"981":1}}],["为解决这个问题",{"2":{"977":1}}],["为0",{"2":{"944":2}}],["为实现这一目标",{"2":{"940":1,"959":1,"975":1}}],["为句子中的谓词和论元分配语义角色",{"2":{"906":1}}],["为隐藏层的第t步的状态",{"2":{"855":1}}],["为比较大的正值和比较小的负值时都会接近于",{"2":{"838":1}}],["为扩大感受野",{"2":{"778":1}}],["为后续的transformer层",{"2":{"709":1}}],["为不同的词分配小的数字",{"2":{"691":1}}],["为每个序列元素分配一个标签",{"2":{"906":1}}],["为每个词构造一个唯一的长度为|v||v||v|的向量",{"2":{"681":1}}],["为每个头计算注意力",{"0":{"32":1},"1":{"33":1,"34":1},"2":{"0":1}}],["为行文方便",{"2":{"679":1}}],["为模型提供了必要的语义",{"2":{"674":1}}],["为模型预测下一个词提供了数学基础和概率指导",{"2":{"473":1}}],["为啥效果好了",{"0":{"648":1}}],["为啥拆多头",{"0":{"648":1}}],["为啥attention的时候要除以dk",{"0":{"647":1}}],["为lcm带来了诸多优势",{"2":{"629":1}}],["为三位数",{"2":{"595":1}}],["为词汇表中的每个token分配一个概率",{"2":{"473":1}}],["为一个",{"2":{"334":1}}],["为优化",{"2":{"301":1}}],["为跳层连接",{"2":{"298":1}}],["为长期存在的长距离依赖问题提供了稳健的解决方案",{"2":{"279":1}}],["为机器立心",{"2":{"233":1}}],["为其在多种场景下的出色表现奠定了基础",{"2":{"225":1}}],["为何",{"2":{"326":1}}],["为何会有如此退化的现象",{"2":{"299":1}}],["为何说这样参数量非常低",{"2":{"224":1}}],["为何要用深度神经网络",{"0":{"1465":1}}],["为何要再进行一次训练",{"2":{"707":1}}],["为何要再训练",{"0":{"707":1}}],["为何要把输入数据",{"2":{"677":1}}],["为何要在每一步针对每一个待预测词都生成一个新的上下文向量",{"2":{"516":1}}],["为何要在注意力机制中加入softmax",{"2":{"180":1}}],["为何要使用三个不同的权重矩阵",{"2":{"172":1}}],["为何要从一个序列中的每一个",{"2":{"172":1}}],["为何要区分mha和mlp",{"2":{"120":1}}],["为何要指定kernel大小为1",{"2":{"101":1}}],["为softmax",{"2":{"210":1}}],["为更好发挥注意力权重的潜力并提高序列推荐在学习高阶依赖关系方面的能力",{"2":{"209":1}}],["为进一步改进当前严重依赖mlps的深度学习模型提供了新的可能性",{"2":{"155":1}}],["为应对上述挑战",{"2":{"152":1}}],["为节点",{"2":{"130":1}}],["为学习过程提供了深度和复杂性",{"2":{"120":1}}],["为什么选择",{"0":{"1964":1}}],["为什么推荐使用",{"2":{"1715":1}}],["为什么引用必须初始化",{"2":{"1650":1}}],["为什么人工智能班级学习",{"2":{"1602":1}}],["为什么编译器和运行程序会自动生成",{"2":{"1589":1}}],["为什么将学习率和其他优化参数称为超参数",{"0":{"1185":1}}],["为什么在优化的探索阶段使用quasi",{"0":{"1175":1}}],["为什么有些论文有复杂的学习率衰减方案",{"0":{"1173":1}}],["为什么有效",{"2":{"740":1}}],["为什么是",{"2":{"1016":1}}],["为什么参数初始化很重要",{"0":{"990":1}}],["为什么多头效果更好呢",{"2":{"927":1}}],["为什么会选择向量作为载体",{"2":{"677":1}}],["为什么会出现这种情况",{"2":{"334":1}}],["为什么需要异常处理",{"0":{"1761":1}}],["为什么需要mybatis",{"0":{"1479":1}}],["为什么需要持久化服务呢",{"2":{"1477":1}}],["为什么需要这份调优手册",{"0":{"1127":1},"2":{"1125":1}}],["为什么需要添加这两种mask码呢",{"2":{"932":1}}],["为什么需要",{"2":{"627":1}}],["为什么需要多层的transformer堆叠",{"2":{"20":1}}],["为什么大多数llm只使用decoder",{"2":{"543":1}}],["为什么现在的llm都是decoder",{"2":{"542":2,"543":1}}],["为什么pre",{"2":{"334":1,"361":1}}],["为什么cnn仅在channel维度上统计呢",{"2":{"325":1}}],["为什么",{"2":{"320":1,"722":1,"740":1,"773":1,"1648":1,"2003":1,"2004":1,"2005":1,"2006":5,"2008":1}}],["为什么transformer块使用layernorm而不是batchnorm",{"2":{"294":1}}],["为什么transformer中计算注意力权重时需要做缩放",{"2":{"233":1}}],["为什么不应该调整batch",{"0":{"1186":1},"2":{"1131":2}}],["为什么不让解码器自主学习关注源序列中的相关部分",{"2":{"284":1}}],["为什么不合用一个权重矩阵呢",{"2":{"172":1}}],["为什么不直接使用",{"2":{"172":1}}],["为什么要使用多文件编程",{"2":{"1916":1}}],["为什么要使用函数",{"2":{"1729":1}}],["为什么要使用数组",{"2":{"1623":1}}],["为什么要学习计算机图形学",{"0":{"2010":1}}],["为什么要学习",{"2":{"1601":1}}],["为什么要用头文件",{"2":{"1628":1}}],["为什么要用",{"2":{"1233":1}}],["为什么要定义torch",{"2":{"1201":1}}],["为什么要进行缩放",{"0":{"924":1}}],["为什么要把一个词用多维向量表示",{"2":{"683":1}}],["为什么要向量化",{"2":{"680":1}}],["为什么要除以",{"2":{"186":1}}],["为什么要引入权重矩阵",{"2":{"172":1}}],["为什么要取qkv这些名字",{"2":{"162":1}}],["为什么ffn要先升维后降维",{"2":{"116":1}}],["为false的位置是需要被mask掉的",{"2":{"66":1}}],["为避免不平衡负载",{"2":{"42":1}}],["为注意力机制引入了灵活性",{"2":{"42":1}}],["为此我们大体有两个选择",{"2":{"1334":1}}],["为此将损失定义如下图标号4",{"2":{"230":1}}],["为此",{"2":{"41":1,"1127":1}}],["为例来看transformer计算时候的信息流",{"2":{"415":1}}],["为例来看看自注意力的流程",{"2":{"170":1}}],["为例来解释下图",{"2":{"263":1}}],["为例",{"2":{"36":1,"89":1,"267":1,"450":1,"453":1,"460":1,"545":1,"564":1,"623":1,"744":1}}],["为了见名知意",{"2":{"2153":1}}],["为了去尽量多的地方",{"2":{"2130":1}}],["为了组成最大的数",{"2":{"2124":1}}],["为了代码清晰或者避免歧义",{"2":{"1638":1}}],["为了规范操作",{"2":{"1488":1}}],["为了阅读美观",{"2":{"1404":1}}],["为了能利用上token的相对位置信息",{"2":{"1342":1}}],["为了能够方便地将多头结果拼合起来",{"2":{"35":1}}],["为了引入相对位置信息",{"2":{"1339":1}}],["为了面向普通大众",{"2":{"1185":1}}],["为了检查出这一问题",{"2":{"1179":1}}],["为了检查搜索空间边界",{"2":{"1147":1}}],["为了公平比较",{"2":{"1150":1}}],["为了尽量减少不可微分函数的影响",{"2":{"1115":1}}],["为了计算这些梯度",{"2":{"1105":1}}],["为了计算自注意力块的输入",{"2":{"620":1}}],["为了为这些请求腾出空间",{"2":{"986":1}}],["为了系统地利用这些重用机会",{"2":{"985":1}}],["为了减少激活内存占用",{"2":{"976":1}}],["为了减少训练过程中的激活内存",{"2":{"957":1}}],["为了减少kv缓存",{"2":{"956":1}}],["为了减轻这种偏差",{"2":{"553":1}}],["为了数值稳定性",{"2":{"943":1,"961":1}}],["为了展示seq2seq模型的运行过程",{"2":{"890":1}}],["为了建模序列问题",{"2":{"878":1}}],["为了更好地管理这些空间",{"2":{"1648":1}}],["为了更好的比对",{"2":{"337":1}}],["为了更好的说明",{"2":{"64":1,"244":1,"519":1,"530":1}}],["为了更好的并行",{"2":{"31":1}}],["为了更细粒度地排除子图不进行梯度计算",{"2":{"1116":1}}],["为了更新细胞状态",{"2":{"866":1}}],["为了赋予网络这样的",{"2":{"850":1}}],["为了充分利用路由权重和解码器嵌入",{"2":{"739":1}}],["为了使",{"2":{"1594":1}}],["为了使模型能够理解文本或图像",{"2":{"708":1}}],["为了使得模型结果能够更好地泛化到未知长度",{"2":{"194":1}}],["为了使得模型的参数量大体保持不变",{"2":{"109":1}}],["为了进一步说明词与词之间的关系",{"2":{"685":1}}],["为了进一步研究",{"2":{"320":1}}],["为了抵消这种影响",{"2":{"647":1,"924":1}}],["为了探索在sonar空间中进行语言建模的最佳实践",{"2":{"631":1}}],["为了防止引用循环",{"2":{"1114":1}}],["为了防止梯度",{"2":{"621":1}}],["为了防止计算出错",{"2":{"346":1}}],["为了高效分配计算资源",{"2":{"612":1}}],["为了合并",{"2":{"581":1}}],["为了合并计算结果",{"2":{"420":1}}],["为了以最有效的方式构建语料库",{"2":{"576":1}}],["为了并行操作",{"2":{"525":1}}],["为了并行提速",{"2":{"443":1}}],["为了方便这些残差连接",{"2":{"914":1}}],["为了方便残差连接",{"2":{"518":1}}],["为了方便说明",{"2":{"501":1}}],["为了改进梯度的统计情况",{"2":{"503":1}}],["为了改进这一现象",{"2":{"230":1}}],["为了让模型能够理解自然语言文本",{"2":{"545":1}}],["为了让模型能够利用词的顺序信息",{"2":{"518":1}}],["为了让前面的token不能观察到后面token的信息",{"2":{"525":1}}],["为了让自注意力发挥作用",{"2":{"488":1}}],["为了让cnn可以处理长序列",{"2":{"247":1}}],["为了讲述方便",{"2":{"460":1}}],["为了做更好的训练",{"2":{"407":1}}],["为了标记每一个文本序列的起止位置",{"2":{"377":1}}],["为了确保安全共享",{"2":{"983":1}}],["为了确保数据的高质量",{"2":{"369":1}}],["为了确保模型在这一时点上不会受到未来词汇的干扰",{"2":{"59":1}}],["为了平衡不同类型的数据",{"2":{"368":1}}],["为了平衡效率与全局信息捕捉能力",{"2":{"217":1}}],["为了训练适应特定应用的llm",{"2":{"367":1}}],["为了训练稳定性",{"2":{"335":1}}],["为了控制它们的影响",{"2":{"356":1}}],["为了克服这一挑战",{"2":{"618":1}}],["为了克服这种弊端",{"2":{"267":1}}],["为了克服编码器和解码器之间的瓶颈",{"2":{"284":1}}],["为了克服针对组合性和可扩展性的这些限制",{"2":{"224":1}}],["为了准确给出最终的答案",{"2":{"246":1}}],["为了弥补这一点",{"2":{"232":1}}],["为了弥补这一差距",{"2":{"121":1}}],["为了设计一个长期神经记忆模块",{"2":{"230":1}}],["为了区分开不同通道之间的主次关系",{"2":{"213":1}}],["为了简化说明",{"2":{"462":1}}],["为了简化",{"2":{"213":1}}],["为了简单",{"2":{"80":1,"344":1}}],["为了缓解标准自注意力机制的效率瓶颈",{"2":{"210":1}}],["为了优化多语言的压缩效率",{"2":{"553":1}}],["为了优化",{"2":{"210":1}}],["为了保持高吞吐量",{"2":{"968":1}}],["为了保持向量的可区分性",{"2":{"709":1}}],["为了保持整体架构线性复杂度",{"2":{"210":1}}],["为了保持交互的全局性",{"2":{"204":1}}],["为了保持序列的长度一致",{"2":{"50":1}}],["为了提高",{"2":{"1653":1}}],["为了提高开发效率和安全性",{"2":{"1611":1}}],["为了提高计算效率",{"2":{"636":1}}],["为了提高其训练稳定性",{"2":{"351":1}}],["为了提高某个位置j的相对重要性",{"2":{"176":1}}],["为了提供更灵活的模型表示能力和捕捉数据中的复杂依赖关系",{"2":{"172":1}}],["为了提升性能",{"2":{"1169":1}}],["为了提升效率",{"2":{"169":1}}],["为了提升训练效率",{"2":{"89":1,"406":1}}],["为了识别不一样的模式",{"2":{"172":1}}],["为了理解一句话的意思",{"2":{"167":1}}],["为了解释和可视化",{"2":{"161":1}}],["为了解决菱形继承带来的问题",{"2":{"1662":1}}],["为了解决上述挑战",{"2":{"975":1}}],["为了解决上述问题",{"2":{"284":1}}],["为了解决relu",{"2":{"842":1}}],["为了解决有限记忆的问题",{"2":{"228":1}}],["为了解决线性注意力中常见的注意力权重分布信息熵过高的问题",{"2":{"211":1,"213":1}}],["为了解决效率和效果问题",{"2":{"89":1}}],["为了解决这个问题",{"2":{"88":1,"213":1}}],["为了避免无限循环",{"2":{"1646":1}}],["为了避免分母为0",{"2":{"1044":1}}],["为了避免混淆",{"2":{"436":1}}],["为了避免不必要的复杂性",{"2":{"213":1}}],["为了避免在一个参数空间中进行多次编辑",{"2":{"143":1}}],["为了避免模型在生成序列时看到未来的信息",{"2":{"50":1}}],["为了实现这一点",{"2":{"536":1,"1222":1}}],["为了实现这种在度量空间进行转换的流映射",{"2":{"499":1}}],["为了实现这种理想训练方式",{"2":{"406":1}}],["为了实现超大批量大小",{"2":{"402":1}}],["为了实现简单",{"2":{"344":1}}],["为了实现终生学习编辑",{"2":{"143":1}}],["为了实现head之间的信息交换",{"2":{"41":1}}],["为了达到这个目的",{"2":{"143":1}}],["为了在训练时候模拟实际推理的效果",{"2":{"81":1}}],["为了节约空间",{"2":{"74":1}}],["为了加快训练速度",{"2":{"57":1}}],["为了符合模型的输入方式",{"2":{"53":1,"376":1}}],["为了降低计算开销和显存占用",{"2":{"46":1}}],["为了最大限度的增强表达能力",{"2":{"46":1}}],["为方便理解",{"2":{"24":1}}],["为",{"2":{"17":1,"59":1,"80":1,"83":1,"89":1,"399":1,"934":1,"944":1,"1003":1,"1049":2,"1386":5,"1485":1,"1520":1,"1554":3,"1594":2,"1671":1,"1728":1}}],["为多头注意力机制模块输入与输出张量的通道维度",{"2":{"7":1}}],["wmnw",{"2":{"1443":1}}],["w9+=0",{"2":{"1395":1}}],["w8+=0",{"2":{"1395":1}}],["w6+=0",{"2":{"1395":1}}],["w5+=0",{"2":{"1395":1}}],["w7+=0",{"2":{"1395":1}}],["w7+=w7+δw7=w7−η∂etotal∂w7=0",{"2":{"1395":1}}],["w7w",{"2":{"1391":1,"1395":1}}],["wd=小狗",{"2":{"1566":1}}],["wd",{"2":{"1308":2}}],["wl^",{"2":{"1004":2}}],["wl",{"2":{"1003":10,"1004":1}}],["wl⋅xl",{"2":{"1003":1}}],["wliwilw",{"2":{"128":2}}],["w∼u",{"2":{"1000":2,"1007":4}}],["w∼n",{"2":{"1000":2,"1003":2,"1004":2,"1006":4}}],["w​mn​​",{"2":{"1443":1}}],["w​1000​​=",{"2":{"1396":1}}],["w​10​+​​=0",{"2":{"1395":1}}],["w​12​+​​=0",{"2":{"1395":1}}],["w​11​+​​=0",{"2":{"1395":1}}],["w​1​+​​=0",{"2":{"1395":1}}],["w​1​​∗z​1​​+⋅+w​n​​∗z​n​​",{"2":{"1000":1}}],["w​9​+​​=0",{"2":{"1395":1}}],["w​8​+​​=0",{"2":{"1395":1}}],["w​6​+​​=0",{"2":{"1395":1}}],["w​5​+​​=0",{"2":{"1395":1}}],["w​4​+​​=0",{"2":{"1395":1}}],["w​3​+​​=0",{"2":{"1395":1}}],["w​2​+​​=0",{"2":{"1395":1}}],["w​7​+​​=0",{"2":{"1395":1}}],["w​7​+​​=w​7​​+δw​7​​=w​7​​−η​∂w​7​​​​∂e​total​​​​=0",{"2":{"1395":1}}],["w​7​​",{"2":{"1391":1,"1395":1}}],["w​l​​",{"2":{"1003":10,"1004":1}}],["w​l​​⋅x​l​​",{"2":{"1003":1}}],["w​i​​",{"2":{"1000":3}}],["w​i​v​​∈r​d​model​​×d​v​​​​",{"2":{"927":1}}],["w​i​k​​∈r​d​model​​×d​k​​​​",{"2":{"927":1}}],["w​i​q​​∈r​d​model​​×d​k​​​​",{"2":{"927":1}}],["w​o​​∈r​hd​v​​×d​model​​​​",{"2":{"927":1}}],["w​o​​",{"2":{"927":1}}],["w​sx​​x​t​​+w​hh​​s​t−1​​",{"2":{"856":1}}],["wsxxt+whhst−1",{"2":{"856":1}}],["wrapper",{"2":{"1227":1,"1589":1}}],["wrapped",{"2":{"1214":2}}],["wrap",{"2":{"1083":12,"1087":1}}],["wrwrw",{"2":{"760":1}}],["written",{"2":{"1825":1,"1843":1}}],["writecontent",{"2":{"1902":3}}],["writer",{"2":{"1215":2,"1279":5,"1280":2,"1282":4,"1283":5}}],["write",{"2":{"591":11,"935":1,"951":1,"1254":2,"1816":2,"1820":2,"1834":2,"1838":2,"2078":1}}],["writings",{"2":{"429":1}}],["w4+=0",{"2":{"1395":1}}],["w44",{"2":{"694":1}}],["w43",{"2":{"694":1}}],["w42",{"2":{"694":1}}],["w41",{"2":{"694":1}}],["wwidlow",{"2":{"587":1}}],["www",{"2":{"47":1,"95":1,"105":1,"156":2,"233":1,"292":1,"361":1,"387":2,"429":1,"513":2,"543":1,"638":1,"740":1,"768":1,"1302":1,"1476":1,"1566":2,"1605":3}}],["w>",{"2":{"579":4,"580":1,"582":11,"583":12,"584":3,"588":6}}],["w上应用dla",{"2":{"479":1}}],["w上操作",{"2":{"315":1}}],["wn",{"2":{"361":1}}],["w归一化",{"2":{"341":1}}],["w做归一化",{"2":{"341":2}}],["w是一个面",{"2":{"341":1}}],["w是feature的高",{"2":{"341":1}}],["w至一个维度",{"2":{"341":1}}],["wtk",{"2":{"175":1}}],["wt",{"2":{"172":1,"175":1,"1398":1}}],["wprojwprojw",{"2":{"145":2}}],["wfcwfcw",{"2":{"145":2}}],["w3+=0",{"2":{"1395":1}}],["w34",{"2":{"694":1}}],["w33",{"2":{"694":1}}],["w32",{"2":{"694":1}}],["w31",{"2":{"694":1}}],["w3",{"2":{"114":2,"267":8}}],["w2+=0",{"2":{"1395":1}}],["w2+b2max",{"2":{"519":2}}],["w2+b2ffn",{"2":{"99":2}}],["w24",{"2":{"694":2}}],["w23",{"2":{"694":3}}],["w22",{"2":{"694":1}}],["w21",{"2":{"694":2}}],["w2ffn",{"2":{"125":2}}],["w2",{"2":{"113":1,"114":2,"1102":13,"1398":5}}],["w2为原始ffn的第二层全连接权重矩阵",{"2":{"109":1}}],["w2w2w",{"2":{"501":1}}],["w2w",{"2":{"46":1}}],["w2w1",{"2":{"46":1}}],["wazg123456",{"2":{"2069":1,"2070":1}}],["wall",{"2":{"1917":3,"1984":1}}],["water",{"2":{"1665":1}}],["watervehicle",{"2":{"1664":3,"1665":6}}],["want",{"2":{"1254":2}}],["waiting",{"2":{"1413":1}}],["wait=1",{"2":{"1284":2}}],["wait",{"2":{"1215":1,"2070":1}}],["waite",{"2":{"1090":1}}],["wavelets",{"2":{"513":1}}],["warn",{"2":{"1214":1}}],["warning",{"2":{"384":1,"1083":1,"1728":1,"1922":1}}],["warming",{"2":{"1284":1}}],["warm",{"2":{"335":1,"1243":1,"1244":2}}],["warmup时长",{"2":{"1158":1}}],["warmup=1",{"2":{"1284":2}}],["warmup=config",{"2":{"423":1}}],["warmup=400",{"2":{"83":1,"402":1,"424":1}}],["warmup阶段用较小的学习率可以有助于模型在训练初期快速收敛",{"2":{"402":1}}],["warmup阶段",{"2":{"402":1}}],["warmup",{"0":{"401":1},"2":{"372":1,"401":1,"402":4,"423":1,"1183":6}}],["warmup会留给模型足够多的时间进行",{"2":{"333":1}}],["wasserstein",{"2":{"499":1}}],["was",{"2":{"261":2,"449":1}}],["way",{"2":{"47":1,"591":1,"1308":1}}],["win32",{"2":{"1985":1}}],["wingspan",{"2":{"1654":2}}],["win",{"2":{"1087":1}}],["windows系统win",{"2":{"2093":1}}],["windows10",{"2":{"2089":1}}],["windows",{"0":{"1969":1},"2":{"768":1,"1605":4,"1616":1,"1985":1}}],["window",{"2":{"370":1,"768":1,"814":2,"815":2,"1087":1}}],["wi",{"2":{"1000":2,"1386":1}}],["wiw",{"2":{"1000":1}}],["wish就是希望的意思",{"2":{"845":1}}],["wise和",{"2":{"722":1}}],["wise线性映射",{"2":{"172":1}}],["wise的",{"2":{"101":1}}],["wise了",{"2":{"101":1}}],["wise",{"0":{"101":1},"2":{"96":1,"101":4,"143":1,"213":1,"419":2,"642":2,"829":1,"912":1,"941":1,"960":1,"977":1}}],["wiv∈rdmodel×dv",{"2":{"927":1}}],["wiv",{"2":{"503":1}}],["wiki",{"2":{"1566":1}}],["wikipedia",{"2":{"769":1,"885":1,"1566":1}}],["wik∈rdmodel×dk",{"2":{"927":1}}],["wik",{"2":{"503":1}}],["wiq∈rdmodel×dk",{"2":{"927":1}}],["wiq",{"2":{"503":1}}],["wid",{"2":{"584":1,"585":1,"587":2}}],["wide",{"2":{"1308":2}}],["wideresnet",{"2":{"1178":1}}],["widest",{"2":{"578":1}}],["widely",{"2":{"167":1,"259":1}}],["width=350",{"2":{"399":1}}],["width",{"2":{"315":1,"700":1,"1708":3,"1772":4,"1774":4,"1789":5,"1791":3,"1792":5}}],["wi⋅vi",{"2":{"270":2}}],["wi=fscore",{"2":{"270":2}}],["wires",{"2":{"167":3,"259":2}}],["wild",{"2":{"1611":1}}],["wildcardname",{"2":{"1489":4}}],["wild可解释性",{"2":{"156":1}}],["will",{"2":{"76":1,"428":2,"557":1,"572":2,"592":2,"1254":2,"1283":1,"1284":2,"2062":1}}],["withdrawal",{"2":{"1874":1}}],["withdraw",{"2":{"1677":2,"1766":2,"1873":1,"1874":4}}],["within",{"2":{"702":1}}],["without",{"2":{"235":1,"292":1,"358":1,"572":1,"768":1,"807":1,"809":1}}],["with",{"0":{"283":1,"968":1,"973":1,"1028":1,"1102":1,"1123":1,"2078":1},"1":{"969":1,"970":1,"971":1,"972":1,"1029":1,"1030":1,"1031":1,"1032":1},"2":{"8":1,"43":1,"47":4,"83":1,"90":1,"95":1,"101":1,"105":1,"111":1,"115":1,"137":1,"156":1,"210":1,"232":1,"233":2,"283":1,"292":1,"343":1,"344":1,"348":1,"350":1,"361":1,"380":1,"449":1,"477":1,"513":3,"532":1,"543":2,"557":2,"558":1,"561":1,"571":1,"573":1,"574":1,"590":1,"591":4,"592":3,"601":1,"605":1,"616":1,"638":7,"665":1,"723":1,"736":1,"759":1,"763":1,"765":2,"768":5,"801":3,"802":2,"807":1,"809":1,"810":2,"834":2,"947":1,"982":1,"1034":1,"1035":1,"1072":1,"1082":1,"1086":2,"1087":2,"1095":1,"1096":1,"1097":1,"1098":1,"1116":2,"1130":1,"1208":2,"1214":3,"1215":1,"1227":1,"1243":1,"1244":1,"1254":1,"1284":3,"1303":4,"1306":1,"1307":2,"1329":1,"1330":2,"1339":1,"1340":1,"1341":1,"1566":1,"1605":1,"1807":3,"1933":1,"2077":1,"2078":1,"2079":2,"2083":1,"2086":1}}],["wgmma在异步代理中执行以计算下一个块",{"2":{"973":1}}],["wgwgw^g",{"2":{"501":1}}],["wg",{"2":{"46":2}}],["why",{"2":{"387":1,"542":1,"543":1,"768":1}}],["while",{"0":{"1620":1},"2":{"557":1,"590":1,"592":1,"1086":1,"1350":2,"1436":2,"1620":11,"1625":2,"1631":3,"1723":1,"1729":1,"1814":1,"1820":1,"1825":1,"1832":1,"1838":1,"1843":1,"1933":2,"2075":1,"2153":1}}],["whitespaces",{"2":{"572":3}}],["white",{"2":{"370":1,"557":2}}],["which",{"2":{"76":1,"688":1,"1086":1,"1254":1,"1284":1,"1308":1,"1533":1,"2076":1,"2077":1,"2086":1}}],["wheels",{"2":{"1664":1,"1665":8}}],["whether",{"2":{"557":1,"572":2}}],["wherehead​i​​=attention",{"2":{"927":1}}],["whereheadi=attention",{"2":{"927":1}}],["where",{"2":{"245":1,"927":1,"1087":2,"1329":3,"1330":6,"1485":3,"1487":1,"1488":1,"1489":2}}],["when",{"2":{"201":1,"572":1,"711":1,"723":1,"740":1,"1284":2,"1308":1,"1330":1,"2077":1}}],["whats",{"2":{"387":1}}],["what",{"0":{"2073":1},"2":{"18":1,"20":2,"47":1,"437":1,"446":1,"513":2,"542":1,"860":1,"1713":1,"1762":2,"1763":6,"1764":1,"1902":1,"2062":1,"2063":1}}],["whv",{"2":{"9":1}}],["whk",{"2":{"9":1}}],["whq",{"2":{"9":1}}],["w1000=",{"2":{"1396":1}}],["w10+=0",{"2":{"1395":1}}],["w1+=0",{"2":{"1395":1}}],["w1∗z1+⋅+wn∗zn",{"2":{"1000":1}}],["w14",{"2":{"694":2}}],["w13",{"2":{"694":2}}],["w12+=0",{"2":{"1395":1}}],["w12",{"2":{"694":2}}],["w11+=0",{"2":{"1395":1}}],["w11",{"2":{"694":2}}],["w11w12w13w14w21w22w23w24w31w32w33w34w41w42w43w44",{"2":{"694":1}}],["w11w12w13w14w21w23w23w24",{"2":{"694":2}}],["w1w1w",{"2":{"501":1}}],["w1",{"0":{"1394":1},"2":{"114":2,"325":1,"1102":13,"1398":5}}],["w1和self",{"2":{"113":1}}],["w1∈rdinput×dmodelw1∈rdinput×dmodelw",{"2":{"101":1}}],["w1v",{"2":{"9":1}}],["w1k",{"2":{"9":1}}],["w1q",{"2":{"9":1}}],["w",{"2":{"9":9,"12":2,"28":3,"46":2,"99":1,"105":6,"109":1,"113":4,"125":1,"126":2,"145":1,"148":1,"156":1,"161":9,"175":6,"230":1,"267":4,"270":2,"315":3,"325":5,"326":3,"337":1,"340":1,"341":5,"394":2,"485":1,"519":1,"579":6,"580":1,"582":20,"583":16,"584":1,"585":2,"587":2,"588":1,"591":2,"762":1,"770":1,"804":1,"807":1,"808":1,"809":1,"810":1,"927":6,"957":3,"988":1,"1000":7,"1003":14,"1004":7,"1006":2,"1007":2,"1092":7,"1094":2,"1095":3,"1096":7,"1098":14,"1254":20,"1308":3,"1339":7,"1340":12,"1343":5,"1389":2,"1392":3,"1393":7,"1394":11,"1395":10,"1398":7,"1442":4,"1512":1,"1544":1,"1547":1,"1557":3,"1654":2,"1772":2,"1774":5,"1789":2,"1791":2,"1792":2}}],["w0v",{"2":{"9":1}}],["w0k",{"2":{"9":1}}],["w0q",{"2":{"9":1}}],["w^o矩阵",{"2":{"36":1}}],["w^o操作前后",{"2":{"10":1}}],["w^",{"2":{"10":1,"12":1,"856":3,"927":2,"1000":2,"1004":1,"1396":1}}],["w^q",{"2":{"9":1,"71":1,"172":5,"503":1}}],["w^v参数",{"2":{"201":1}}],["w^v在整体运行过程中是共享的",{"2":{"172":1}}],["w^v这三个矩阵实际上是模型学会的分配q",{"2":{"172":1}}],["w^v进行矩阵乘法",{"2":{"161":1}}],["w^v计算后的q",{"2":{"36":1,"198":1}}],["w^v的部分",{"2":{"24":1}}],["w^v和w^o矩阵",{"2":{"23":1}}],["w^v",{"2":{"9":2,"26":3,"30":3,"36":1,"71":1,"172":3,"503":1}}],["w^k",{"2":{"9":2,"23":1,"24":1,"26":3,"30":3,"36":2,"71":1,"161":1,"198":1,"201":1,"503":1}}],["well",{"2":{"2078":1,"2083":1,"2086":1,"2087":1}}],["welbl",{"2":{"638":1}}],["werror",{"2":{"1984":1}}],["were",{"2":{"8":1,"167":1,"181":1,"259":1}}],["wednesday",{"2":{"1728":6}}],["weekday",{"2":{"1728":4}}],["weakptr",{"2":{"1695":2}}],["weak",{"2":{"1695":3,"1890":1}}],["wearing",{"2":{"557":1}}],["wear",{"2":{"239":4}}],["webassembly",{"2":{"1961":1}}],["website",{"2":{"513":1,"2085":1}}],["web",{"0":{"1493":1},"2":{"373":3,"1602":1}}],["weixin",{"2":{"768":1}}],["wei",{"2":{"543":1}}],["weiße",{"2":{"370":1,"557":1}}],["weighted",{"2":{"209":1,"731":1}}],["weighting",{"2":{"89":1}}],["weight矩阵的第i行中只剩下了前i个权重值",{"2":{"71":1}}],["weight",{"0":{"1446":1},"2":{"17":1,"71":2,"209":1,"346":4,"429":2,"661":2,"702":7,"834":5,"988":3,"1003":1,"1046":1,"1064":1,"1087":7,"1098":1,"1104":2,"1106":1,"1107":1,"1109":1,"1205":4,"1221":1,"1308":2,"1404":1,"1441":1}}],["weights",{"2":{"8":1,"204":1,"233":2,"503":14,"834":1,"1087":1,"1216":2,"1218":2,"1386":1}}],["we",{"2":{"8":1,"23":1,"160":1,"201":1,"399":1,"402":1,"428":3,"591":2,"592":1,"700":1,"1254":9,"1299":1,"1329":1,"1330":2}}],["wvv",{"2":{"762":1}}],["wvvj=",{"2":{"762":1}}],["wvi",{"2":{"503":1}}],["wv∈rd×dvwv∈rd×dv",{"2":{"161":1}}],["wvh",{"2":{"9":1}}],["wv1",{"2":{"9":1}}],["wv0",{"2":{"9":1}}],["wvw^t",{"2":{"172":5}}],["wvw^q",{"2":{"9":1,"24":1,"26":3,"30":3,"161":1,"198":1,"201":1}}],["wvwt",{"2":{"172":5}}],["wvwvw",{"2":{"355":1}}],["wvwvw^v权重矩阵相乘",{"2":{"463":1}}],["wvwvw^v这三个权重矩阵的计算过程可以并行化",{"2":{"417":1}}],["wvwvw^v这三个投影层以及最后的投影层wowow^o",{"2":{"8":1}}],["wvwvw^v",{"2":{"7":1,"45":1}}],["wvwv",{"2":{"71":1}}],["wvwq",{"2":{"9":1,"24":1,"26":3,"30":3,"161":1,"198":1,"201":1}}],["wv",{"2":{"8":1,"9":2,"201":2,"1216":2,"1218":2,"1345":2}}],["wki",{"2":{"503":1}}],["wkiwikw",{"2":{"12":1,"16":1}}],["wk和wv进行线性计算",{"2":{"201":1}}],["wkt",{"2":{"175":1}}],["wk+wq",{"2":{"175":2}}],["wk≈v",{"2":{"145":1}}],["wkwkw",{"2":{"764":1,"765":1}}],["wkwkw^",{"2":{"10":1}}],["wkwkw^k和",{"2":{"463":1}}],["wkwkw^k",{"2":{"7":1,"8":1,"45":1,"417":1}}],["wkwk",{"2":{"71":1}}],["wkh",{"2":{"9":1}}],["wk1",{"2":{"9":1}}],["wk0",{"2":{"9":1}}],["wk",{"2":{"8":1,"9":4,"24":2,"26":6,"30":6,"161":2,"198":2,"201":4,"1216":2,"1218":2,"1345":2}}],["wqi",{"2":{"503":1}}],["wq∈rd×dqwq∈rd×dq",{"2":{"161":1}}],["wqwqw",{"2":{"355":1,"764":1,"765":1}}],["wqwqw^",{"2":{"10":1}}],["wqwqw^q",{"2":{"7":1,"8":1}}],["wqwq",{"2":{"71":1}}],["wqwkwqwkw^qw^k叫做qk回路",{"2":{"45":1}}],["wqwtk𝑊𝑄𝑊𝐾𝑇𝑊",{"2":{"19":1}}],["wqh",{"2":{"9":1}}],["wq1",{"2":{"9":1}}],["wq0",{"2":{"9":1}}],["wq",{"2":{"8":1,"9":3,"26":2,"30":2,"172":10,"175":2,"201":2,"1216":2,"1218":2,"1345":2,"1518":1,"1544":1}}],["woof",{"2":{"1685":3,"1690":1,"1691":1,"1693":2}}],["wooden",{"2":{"370":1}}],["wo∈rhdv×dmodel",{"2":{"927":1}}],["womultihead",{"2":{"927":1}}],["women",{"2":{"685":1}}],["woman",{"2":{"557":1}}],["working",{"2":{"2075":1,"2077":1,"2079":1}}],["workings",{"2":{"513":1}}],["work",{"0":{"968":1},"1":{"969":1,"970":1,"971":1,"972":1},"2":{"542":1,"947":1}}],["works",{"2":{"513":1,"2073":1,"2078":1}}],["workers",{"2":{"1215":1,"1254":1,"1308":2}}],["worker",{"2":{"154":2,"364":1,"385":1,"422":3,"423":2,"1578":1}}],["wordcounts",{"2":{"1933":4}}],["word和h",{"2":{"595":1}}],["wordpiece的单词表征向量学习到的是上下文独立的表征",{"2":{"698":1}}],["wordpiece的算法如下",{"2":{"599":1}}],["wordpiece引入了一个假设",{"2":{"599":1}}],["wordpiece就会把他们合并放入词表",{"2":{"598":1}}],["wordpiece按token间的互信息进行合并",{"2":{"598":1}}],["wordpiece与bpe类似",{"2":{"598":1}}],["wordpiece可以更有效地处理词汇的变体和未知词汇",{"2":{"597":1}}],["wordpiece算法出自论文",{"2":{"597":1}}],["wordpiece和unigram",{"2":{"596":1}}],["wordpiece",{"0":{"597":1},"1":{"598":1,"599":1,"600":1},"2":{"554":1,"564":1,"567":1,"601":3,"604":1,"638":10}}],["word",{"0":{"460":1},"2":{"83":4,"172":1,"428":4,"448":1,"456":1,"460":1,"472":4,"529":4,"545":1,"557":2,"563":1,"564":1,"565":1,"569":3,"576":1,"587":1,"676":2,"688":2,"690":1,"700":6,"710":1,"713":2,"714":1,"717":1,"718":1,"723":3,"731":1,"736":2,"740":3,"751":1,"1217":2,"1312":1,"1933":8,"2079":1}}],["words",{"2":{"74":1,"79":1,"380":1,"571":1,"574":1,"587":1,"606":1,"638":3,"688":1,"700":1,"714":1,"740":1,"764":1,"1914":5,"1933":1}}],["word2vec虽然在训练时考虑到了局部上下文",{"2":{"715":1}}],["word2vec有两种主要的架构",{"2":{"714":1}}],["word2vec的核心都是一个",{"2":{"715":1}}],["word2vec的第一篇论文是",{"2":{"713":1}}],["word2vec的总体思路是",{"2":{"713":1}}],["word2vec",{"0":{"712":1},"1":{"713":1,"714":1,"715":1},"2":{"50":1,"429":1,"676":1,"706":1,"709":1,"711":1,"740":1}}],["world",{"2":{"8":1,"201":1,"423":1,"595":2,"700":1,"1306":1,"1307":2,"1308":4,"1577":1,"1590":3,"1594":13,"1606":1,"1624":1,"1708":1,"1713":12,"1715":3,"1803":1,"1816":1,"1824":1,"1834":1,"1842":1,"1902":1,"1906":2,"1914":1,"1928":1,"1929":2,"1966":2,"1973":1,"1975":1,"1987":1,"2073":1}}],["wowvwowvw^ow^v叫做ov回路",{"2":{"45":1}}],["wowow^o四个权重矩阵刻画",{"2":{"45":1}}],["wowow^o矩阵",{"0":{"10":1},"2":{"0":1}}],["wowow^",{"2":{"10":1}}],["wo",{"2":{"8":1,"201":2}}],["上次还是高三那会",{"2":{"2056":1}}],["上上周放假重温了",{"2":{"2056":1}}],["上周五",{"2":{"2051":1}}],["上每次增加一个单位",{"2":{"2016":1}}],["上测试",{"2":{"1594":1}}],["上生成",{"2":{"1594":1}}],["上升",{"2":{"1440":1}}],["上节中",{"2":{"1323":1}}],["上方我们添加了一个crf层",{"2":{"1320":1}}],["上方是编码器输入对应的掩码操作",{"2":{"63":1}}],["上打开一个新的讨论主题",{"2":{"1196":1}}],["上训练的",{"2":{"1150":1}}],["上读写中间结果",{"2":{"967":1}}],["上比标准注意力快3x倍",{"2":{"945":1,"965":1}}],["上实例化大型的𝑁×𝑁注意力矩阵",{"2":{"940":1,"962":1}}],["上三角的值全为0",{"2":{"934":1}}],["上三角化",{"2":{"201":1}}],["上句中的",{"2":{"933":1}}],["上式每一项可以分别理解为",{"2":{"1340":1}}],["上式三个权重矩阵w每个时间步",{"2":{"856":1}}],["上式计算量太大",{"2":{"844":1}}],["上进行了",{"2":{"1177":1}}],["上进行",{"2":{"726":2}}],["上进行思考",{"2":{"628":1}}],["上进行统计",{"2":{"325":1}}],["上表现出色",{"2":{"611":1}}],["上一篇笔记我们已经讲了进程信号量机制相关的知识",{"2":{"1426":1}}],["上一篇笔记我们已经讲了进程通信相关的知识",{"2":{"1405":1}}],["上一篇我们知道为了克服自注意力矩阵带来的影响",{"2":{"743":1}}],["上一位置最佳tags的索引",{"2":{"1330":1}}],["上一个是tgt",{"2":{"533":1}}],["上一轮的输出会当作下一轮的输入以补充信息",{"2":{"241":1}}],["上篇",{"2":{"513":1}}],["上的",{"2":{"1227":2}}],["上的张量",{"2":{"1083":1}}],["上的被引用次数却只有",{"2":{"844":1}}],["上的概率测度空间",{"2":{"499":1}}],["上的流映射",{"2":{"499":1}}],["上的均值",{"2":{"343":1}}],["上常微分方程",{"2":{"494":1}}],["上计算统计量",{"2":{"326":1}}],["上有较高的注意力权重",{"2":{"194":1}}],["上都有较高的注意力权重",{"2":{"194":1}}],["上并行化嵌入查找和聚合",{"2":{"154":1}}],["上",{"2":{"122":1,"169":1,"204":1,"280":1,"292":1,"536":1,"745":1,"766":1,"935":1,"951":1,"980":1,"986":1,"1140":2,"1214":3,"1228":1,"1330":1,"1547":1,"1557":1,"1605":2}}],["上界会更宽松",{"2":{"94":1}}],["上述属性那几个属性最为关键",{"2":{"1226":1}}],["上述子模块在",{"2":{"1206":1}}],["上述做法还有优化空间吗",{"2":{"1203":1}}],["上述正是模型中可以使用控制流语句的原因",{"2":{"1108":1}}],["上述代码有一个细节",{"2":{"701":1}}],["上述流程的后面三步在transformer的架构图中对应蓝色框部分",{"2":{"698":1}}],["上述这些成员变量通过参数传给初始化函数",{"2":{"533":1}}],["上述方法是从纯粹的数学角度出发",{"2":{"498":1}}],["上述方案在注意力计算时会遇到问题",{"2":{"54":1}}],["上述选择概率是随着训练的推进不断调整的",{"2":{"411":1}}],["上述例句则对应的掩码是",{"2":{"382":2}}],["上述操作分别通过如下语句完成",{"2":{"381":1}}],["上述操作其实是一种搜索+合并的运算",{"2":{"265":1}}],["上述表达与实际思路有所出入",{"2":{"318":1}}],["上述的意外指标是基于一个损失函数",{"2":{"230":1}}],["上述将的是对ffn进行知识编辑",{"2":{"144":1}}],["上述结构对输入x的每一行进行相同的信息变换",{"2":{"99":1}}],["上述结果还揭示了通用函数逼近能力与秩崩溃速率之间的权衡",{"2":{"93":1}}],["上述结果表明",{"2":{"93":1}}],["上图最上边的解码过程可以看出",{"2":{"1320":1}}],["上图是dropout示意图",{"2":{"1017":1}}],["上图是不同嵌入大小d",{"2":{"480":1}}],["上图给出了由仅解码器llm和潜在注意力层组成的架构设计",{"2":{"735":1}}],["上图给出了fast动作token化的流水线概述",{"2":{"637":1}}],["上图给出了标准注意力和pattention的对比",{"2":{"621":1}}],["上图给出了xor分类的解决方案",{"2":{"320":1}}],["上图对两种网络进行对比",{"2":{"496":1}}],["上图显示了transformer如何准确地复制了在海马体中观察到的那些模式",{"2":{"490":1}}],["上图3号标签是随机挑出了100个模型最后一层里残差分布和前馈分布不同的例子",{"2":{"306":1}}],["上图展示了用不同的方式对字节进行分组",{"2":{"613":1}}],["上图展示了反向传播的印记和偏移机制",{"2":{"485":1}}],["上图展示了依据",{"2":{"485":1}}],["上图展示了通过x⊤⋅δx⊤",{"2":{"485":1}}],["上图展示了在预训练模型bertbase和不同语料库下",{"2":{"131":1}}],["上图展示了修剪后保留编码器头的功能",{"2":{"20":1}}],["上图的右侧显示了2022年至2024年slm使用的前馈网络类型的趋势",{"2":{"98":1}}],["上图中",{"2":{"7":1,"489":1,"614":1}}],["上面我们已经说了",{"2":{"1363":1}}],["上面绘制了每个试验预算的最佳性能的箱线图",{"2":{"1177":1}}],["上面这段话也许过于学术性",{"2":{"689":1}}],["上面流程在下图中也有对应的展示",{"2":{"545":1}}],["上面部分",{"2":{"519":1}}],["上面对应的掩码如下",{"2":{"450":1}}],["上面的命令将启动",{"2":{"1589":1}}],["上面的论述最终指向一点",{"2":{"316":1}}],["上面的操作相当于把一个进程拆分成8个独立的子进程进行操作",{"2":{"10":1}}],["上面公式运算结果就接近于0",{"2":{"192":1}}],["上面代码的",{"2":{"83":1}}],["上面运算逻辑对应的是multiheadedattention的forward",{"2":{"36":1}}],["上面重点说的是将输入切分",{"2":{"13":1}}],["上下文并行",{"2":{"976":1}}],["上下文并行主要是针对self",{"2":{"420":1}}],["上下文长度冲向",{"2":{"768":1}}],["上下文长度和鲁棒性等",{"2":{"351":1}}],["上下文方案",{"2":{"766":1}}],["上下文模式",{"0":{"766":1},"2":{"741":1}}],["上下文相似的词在词义上也一定存在相似性",{"2":{"713":1}}],["上下文相关信息",{"2":{"698":1}}],["上下文相关性",{"2":{"568":1}}],["上下文无关信息",{"2":{"698":1}}],["上下文分析器的输出结果会被输入第二个模型",{"2":{"635":1}}],["上下文在lcm所设计的抽象空间内表达",{"2":{"628":1}}],["上下文向量表示当前输出单词所需的源语言信息",{"2":{"267":1}}],["上下文决定一切",{"0":{"259":1},"2":{"259":1}}],["上下文记忆瓶颈被打破",{"2":{"233":1}}],["上下文记忆",{"2":{"229":1}}],["上下文信息融合",{"2":{"7":1}}],["上下文逻辑",{"2":{"4":1,"12":1,"33":1}}],["再做截止时间是",{"2":{"2135":1}}],["再做学问",{"2":{"2054":1}}],["再选择",{"2":{"2131":2}}],["再结合一点点数学",{"2":{"2121":1}}],["再去深刻领悟其中的奥妙",{"2":{"2054":1}}],["再说了",{"2":{"2054":1}}],["再免费那也是费的学校的电",{"2":{"2054":1}}],["再写入另一个文件",{"2":{"1825":1,"1843":1}}],["再写相同功能的非模板函数",{"2":{"1702":1}}],["再判断条件",{"2":{"1620":1}}],["再执行循环体",{"2":{"1620":1}}],["再完善",{"2":{"1596":1}}],["再系统",{"2":{"1596":1}}],["再长就处理不了了",{"2":{"1337":1}}],["再训练方差或试验方差",{"2":{"1152":1}}],["再拼接到一张图上作为训练数据",{"2":{"1015":1}}],["再后来",{"2":{"907":1}}],["再nlp中对最后一个维度求均值和方差",{"2":{"808":1}}],["再用",{"2":{"777":1}}],["再使用",{"2":{"776":1}}],["再堆叠在一起",{"2":{"776":1}}],["再输入模型",{"2":{"745":1}}],["再输入一个多层注意力网络",{"2":{"731":1}}],["再得到表征",{"2":{"731":1}}],["再到",{"2":{"726":1}}],["再通过截断正态分布赋予权重",{"2":{"723":1}}],["再对后几层进行精调",{"2":{"718":1}}],["再进一步",{"2":{"1441":1}}],["再进一步微调更新",{"2":{"709":1}}],["再进行评测",{"2":{"222":1}}],["再次访问会导致错误",{"2":{"1694":1}}],["再次编译并运行程序",{"2":{"1664":1}}],["再次回到步骤",{"2":{"1620":1}}],["再次测试",{"2":{"1484":1}}],["再次",{"2":{"696":1}}],["再次使用分词表将",{"2":{"431":1}}],["再见",{"2":{"638":1}}],["再将注意力权重a与v进行线性组合便得到了交叉注意力模块的输出向量",{"2":{"537":1}}],["再将结果列表进行concat",{"2":{"29":1,"32":1}}],["再加上位置编码",{"2":{"515":1}}],["再加上一些恒等映射结构",{"2":{"301":1}}],["再加入positional",{"2":{"445":1}}],["再细化来说",{"2":{"442":1,"443":1}}],["再构建一个批量如下",{"2":{"408":1}}],["再除以该页的总字数",{"2":{"340":1}}],["再除以这本书的字符总数",{"2":{"340":1}}],["再除以这行的标准差",{"2":{"319":1}}],["再除以每个页码下的字符总数",{"2":{"340":1}}],["再计算每个",{"2":{"338":1}}],["再把标准化的输出进行映射",{"2":{"313":1}}],["再把这些张量联接",{"2":{"7":1}}],["再学习一个线性映射",{"2":{"313":1}}],["再来进行准确翻译",{"2":{"257":1}}],["再汇总所有组的结果",{"2":{"216":1}}],["再补充几个token",{"2":{"194":1}}],["再分析注意力",{"0":{"200":1},"2":{"157":1}}],["再配合swish激活函数做哈达马积的操作",{"2":{"109":1}}],["再多一些头",{"2":{"20":1}}],["再再以某种方式整合",{"2":{"14":1}}],["再比如",{"2":{"3":1,"679":1}}],["右边或者右上",{"2":{"2022":1}}],["右边是应用了dropout之后的网络结构",{"2":{"1017":1}}],["右对齐",{"2":{"1673":1,"1817":1,"1835":1}}],["右值引用与移动语义",{"0":{"1885":1,"2007":1},"1":{"1886":1,"1887":1,"1888":1}}],["右值",{"2":{"1629":1,"1887":1}}],["右切换分屏",{"2":{"1557":1}}],["右图",{"2":{"940":1,"962":1}}],["右下角的堆叠条形图显示了来自不同公司和机构的模型数量",{"2":{"540":1}}],["右面大蓝框是编码器的交叉注意力模块",{"2":{"537":1}}],["右面是该论文提出的方案collabhead",{"2":{"19":1}}],["右移",{"2":{"1630":2,"2059":1}}],["右移动",{"2":{"1547":1}}],["右移之后变成",{"2":{"528":1}}],["右移的原因解释如下",{"2":{"528":1}}],["右侧真除法使用运算符",{"2":{"1083":1}}],["右侧除法的行为",{"2":{"1083":1}}],["右侧减法",{"2":{"1083":1}}],["右侧是解码器栈",{"2":{"515":1}}],["右侧上方的残差网络定义了有限变换的离散序列",{"2":{"496":1}}],["右侧给出了memoryformer的一个组成部分",{"2":{"153":1}}],["右填充的结果是",{"2":{"378":1}}],["右积核技巧",{"2":{"210":1}}],["右上全为false",{"2":{"74":1}}],["右上角",{"2":{"10":1}}],["右上角o意为输出output的意思",{"2":{"10":2}}],["右",{"2":{"7":1,"210":1,"629":1,"757":1,"839":1}}],["左对齐",{"2":{"1817":1,"1835":1}}],["左操作数隐式地绑定到",{"2":{"1712":1}}],["左移",{"2":{"1630":2,"2059":1}}],["左值",{"2":{"1629":2}}],["左图",{"2":{"940":1,"962":1}}],["左边是完整的神经网络",{"2":{"1017":1}}],["左边和右边的所有单词",{"2":{"721":1}}],["左边的小蓝框是解码器中的掩码多头自注意力模块",{"2":{"537":1}}],["左下角",{"2":{"537":1}}],["左下方红色虚线框是编码器",{"2":{"537":1}}],["左上角的矩阵",{"2":{"537":1}}],["左上角全为1",{"2":{"74":1}}],["左填充的结果是",{"2":{"378":1}}],["左填充",{"0":{"378":1}}],["左",{"2":{"7":1,"210":1,"629":1,"757":1,"839":1,"1547":1}}],["pf",{"2":{"1611":4}}],["ps",{"2":{"1522":1,"1524":1}}],["psbashps",{"2":{"1522":1}}],["pseudo",{"2":{"960":1}}],["p神经元",{"2":{"1461":1}}],["pwd=",{"2":{"1487":1}}],["pwd=86sj",{"2":{"1302":1}}],["pwd",{"2":{"1481":1,"1485":7,"1486":2,"1487":1}}],["pwatermark",{"2":{"1363":1}}],["pvp",{"2":{"1339":1}}],["pk",{"2":{"1336":4,"1339":1}}],["pkp",{"2":{"1335":2}}],["pkpkp",{"2":{"748":1}}],["p​1​​",{"2":{"2018":1}}],["p​0​​",{"2":{"2018":1}}],["p​i​​w​q​​w​k​⊤​​p​j​⊤​​",{"2":{"1340":1}}],["p​ij​​v​j​​",{"2":{"944":1}}],["p​ij​​=exp",{"2":{"944":1}}],["p​v​​",{"2":{"1339":1}}],["p​max​​",{"2":{"1339":2}}],["p​min​​",{"2":{"1339":2}}],["p​j​​w​v​​",{"2":{"1339":1}}],["p​j​​w​k​​",{"2":{"1339":1}}],["p​k",{"2":{"1336":4}}],["p​k​​",{"2":{"1335":2,"1339":1}}],["p的概率清0",{"2":{"1018":1}}],["pp3",{"2":{"1611":1}}],["pp2",{"2":{"1611":4}}],["ppt",{"2":{"1050":1}}],["pp",{"2":{"976":1,"1611":7}}],["ppl",{"2":{"935":1,"951":1}}],["p根据老师的提示来预测",{"2":{"897":1}}],["p∈r2k×dp∈r2k×dp∈r^",{"2":{"763":1}}],["pmax",{"2":{"1339":2}}],["pmatrix",{"2":{"694":5}}],["pmin",{"2":{"1339":2}}],["pmet",{"2":{"144":1,"156":2}}],["p或top",{"2":{"636":1}}],["p初始化为零",{"2":{"623":1}}],["p^",{"2":{"623":4,"899":2}}],["p得到的分数",{"2":{"621":1}}],["p表示",{"2":{"621":1}}],["pjwvp",{"2":{"1339":1}}],["pjwkp",{"2":{"1339":1}}],["pjpjp",{"2":{"614":1}}],["pj",{"2":{"613":2}}],["p将长度为n的字节序列x=",{"2":{"613":1}}],["p需要满足增量patch化的属性",{"2":{"612":1}}],["p0",{"2":{"591":2}}],["pdf",{"2":{"429":1,"513":1,"638":1,"713":2,"740":3,"768":1}}],["pd",{"2":{"399":1,"1250":1,"1253":1}}],["pth",{"2":{"1258":1,"1259":1}}],["ptr的所有权已经被移动到lambda表达式内部",{"2":{"1907":1}}],["ptr来管理std",{"2":{"1902":1}}],["ptr2",{"2":{"1695":8}}],["ptr1",{"2":{"1695":12}}],["ptr++",{"2":{"1633":1}}],["ptr",{"2":{"1072":6,"1078":6,"1087":1,"1611":3,"1633":24,"1634":3,"1647":6,"1669":2,"1671":4,"1672":11,"1695":22,"1705":2,"1714":3,"1715":2,"1890":3,"1891":11,"1902":2,"1907":4,"1911":7,"2005":2,"2006":2}}],["pt",{"2":{"372":2,"374":3,"423":2,"1215":1,"1255":1,"1262":1,"1263":2,"1269":2,"1270":1,"1296":2,"1297":2}}],["py的api还有一个create",{"2":{"2070":1}}],["py内容",{"2":{"2070":1}}],["py和数据分析",{"2":{"2048":1}}],["pyi",{"2":{"1078":1}}],["py∣x",{"2":{"908":1}}],["py∣xp",{"2":{"908":1}}],["pyo3",{"2":{"572":1}}],["py",{"2":{"361":1,"1303":3,"1304":2,"1306":1,"1307":2,"1308":1,"1309":3,"1332":1,"1398":1,"2043":1,"2068":1,"2069":2,"2070":3}}],["pytroch",{"2":{"1073":1,"1078":1}}],["pytorch代码编译成优化内核",{"2":{"1293":1}}],["pytorch会生成一个独立的可运行脚本",{"2":{"1291":1}}],["pytorch会进行类型推断",{"2":{"1291":1}}],["pytorch会对模型函数进行静态分析",{"2":{"1291":1}}],["pytorch会执行以下步骤来进行捕获",{"2":{"1291":1}}],["pytorch在保存时对张量进行了打包",{"2":{"1114":1}}],["pytorch有一个内置的微分引擎",{"2":{"1105":1}}],["pytorch实现",{"0":{"1065":1}}],["pytorch网站上给出的定义是",{"2":{"688":1}}],["pytorch中就是通过",{"2":{"1110":1}}],["pytorch中有三个相似的矩阵操作",{"2":{"805":1}}],["pytorch中",{"0":{"1230":1},"1":{"1231":1,"1232":1,"1233":1,"1234":1,"1235":1,"1236":1,"1237":1,"1238":1,"1239":1,"1240":1,"1241":1,"1242":1,"1243":1,"1244":1,"1245":1,"1246":1,"1247":1,"1248":1},"2":{"661":1,"1104":1}}],["pytorch的相关说明参见",{"2":{"370":1}}],["pytorch使用",{"0":{"315":1},"2":{"293":1}}],["pytorchgelutanh",{"2":{"110":1}}],["pytorch里面就有memory",{"2":{"78":1}}],["pytorch",{"0":{"84":1,"657":1,"659":1,"702":1,"788":1,"789":1,"791":1,"792":1,"1037":1,"1046":1,"1050":1,"1054":1,"1061":1,"1088":1,"1091":1,"1092":1,"1098":1,"1112":1,"1201":1,"1285":1,"1286":1,"1300":1,"2072":1,"2073":1,"2074":1,"2080":1,"2084":1},"1":{"658":1,"659":1,"660":1,"661":1,"662":1,"790":1,"791":1,"1038":1,"1039":1,"1089":1,"1090":1,"1092":1,"1093":1,"1094":1,"1095":1,"1096":1,"1097":1,"1098":1,"1099":1,"1100":1,"1101":1,"1102":1,"1113":1,"1114":1,"1115":1,"1116":1,"1117":1,"1118":1,"1119":1,"1120":1,"1121":1,"1122":1,"1123":1,"1202":1,"1203":1,"1286":1,"1287":1,"1288":1,"2073":1,"2074":1,"2075":2,"2076":2,"2077":2,"2078":2,"2079":2,"2080":1,"2081":2,"2082":2,"2083":2,"2084":1,"2085":2,"2086":2,"2087":1},"2":{"49":1,"84":1,"95":2,"110":1,"361":2,"370":1,"399":1,"429":6,"661":1,"702":1,"740":1,"790":1,"801":1,"802":1,"804":1,"807":1,"808":1,"809":1,"810":1,"814":1,"815":1,"816":1,"819":1,"820":1,"821":1,"822":1,"825":1,"827":1,"828":1,"829":1,"831":1,"834":1,"835":1,"836":1,"839":2,"840":1,"841":1,"842":3,"843":2,"844":1,"845":1,"846":1,"847":1,"933":1,"1050":1,"1054":1,"1061":1,"1065":1,"1073":2,"1078":3,"1082":2,"1088":3,"1089":2,"1090":1,"1098":1,"1104":1,"1105":1,"1112":1,"1114":3,"1118":1,"1121":1,"1201":1,"1215":1,"1230":1,"1233":1,"1234":2,"1235":1,"1236":1,"1237":1,"1238":1,"1239":1,"1240":1,"1241":1,"1242":1,"1243":1,"1244":1,"1245":1,"1246":1,"1247":1,"1249":1,"1254":2,"1275":2,"1301":2,"1308":2,"1332":1,"1404":10,"1602":1,"2043":1,"2073":2,"2075":1,"2076":1,"2077":1,"2078":1,"2079":3,"2081":1,"2082":1,"2083":1,"2085":1,"2086":1,"2087":2}}],["pythong",{"2":{"2154":1}}],["pythongard",{"2":{"1099":1}}],["python用时和",{"2":{"1651":1}}],["pythonpath=",{"2":{"1309":1,"1332":1}}],["pythondata",{"2":{"1242":1}}],["pythondef",{"2":{"65":1,"67":1,"74":1,"79":1,"83":2,"199":1,"364":1,"372":1,"374":1,"375":1,"381":1,"382":1,"383":1,"384":1,"385":1,"394":1,"399":1,"402":1,"422":1,"423":1,"424":1,"449":1,"472":1,"522":1,"529":1,"557":4,"558":1,"572":1,"573":1,"700":1,"808":1,"809":1,"810":1,"933":1,"1075":1,"1076":1,"1078":1,"1083":1,"1085":1,"1087":1,"1092":1,"1095":1,"1096":1,"1097":1,"1098":1,"1102":1,"1202":1,"1207":1,"1255":1,"1258":1,"1259":1,"1263":1,"1266":1,"1267":1,"1269":1,"1270":1,"1273":1,"1274":1,"1279":1,"1282":1,"1283":1,"1296":1,"1297":1,"1298":1,"1329":1,"1330":1,"1331":1,"1350":1,"2153":1}}],["pythonlmbda",{"2":{"1234":1}}],["pythonscheduler",{"2":{"1231":1,"1244":2}}],["pythonself",{"2":{"398":1,"1226":1}}],["python3",{"2":{"1078":1,"1309":1}}],["pythont",{"2":{"828":1}}],["pythontensor",{"2":{"74":1,"315":1}}],["pythonx",{"2":{"820":1,"821":1,"822":1,"825":1,"827":1,"1093":1,"1114":2,"1116":1}}],["pythona",{"2":{"819":1,"826":1,"829":1,"831":1,"833":5}}],["pythonact2cls",{"2":{"110":1}}],["pythoninp",{"2":{"1101":1}}],["pythoninput",{"2":{"810":1}}],["pythonimport",{"2":{"119":1,"315":1,"373":1,"801":1,"802":1,"804":1,"805":1,"807":1,"808":1,"814":1,"815":1,"816":1,"832":1,"839":1,"1039":1,"1069":1,"1070":1,"1071":1,"1082":1,"1086":1,"1094":1,"1211":1,"1212":1,"1213":1,"1216":1,"1217":1,"1218":1,"1239":1,"1243":1,"1272":1,"1278":1,"1566":1,"2086":1}}],["pythonnvidia",{"2":{"792":1}}],["pythonvocab",{"2":{"679":1}}],["python伊甸园",{"2":{"543":1}}],["pythonm",{"2":{"835":1,"839":1,"840":1,"841":1,"842":3,"843":2,"844":1,"845":1,"846":1,"847":1}}],["pythonmemory",{"2":{"528":1}}],["pythonmodel",{"2":{"371":1,"703":1}}],["pythonoptim",{"2":{"1222":1}}],["pythonoptimizer",{"2":{"402":1,"1221":1,"1231":2,"1241":1,"1245":1}}],["pythonout",{"2":{"451":1,"528":1}}],["pythonfor",{"2":{"410":1,"587":1,"1099":1,"1223":2}}],["pythonfrom",{"2":{"346":1,"552":1,"1215":1,"1251":1,"1253":1,"1257":1}}],["pythoncriterion",{"2":{"399":1}}],["pythonclass",{"2":{"8":1,"23":1,"38":1,"39":1,"66":1,"82":2,"83":1,"110":1,"114":1,"201":3,"343":2,"344":2,"346":1,"380":1,"394":3,"398":1,"399":1,"410":1,"472":1,"523":1,"529":3,"532":1,"533":1,"558":1,"571":1,"591":1,"592":1,"702":1,"1205":2,"1250":1}}],["pythonreturn",{"2":{"330":1}}],["python",{"0":{"1600":1,"1602":1,"2078":1,"2153":1},"1":{"1601":1,"1602":1,"1603":1,"1604":1,"1605":1,"1606":1,"1607":1,"1608":1},"2":{"36":1,"70":1,"74":3,"76":1,"84":1,"110":2,"113":1,"326":1,"343":1,"373":2,"382":2,"385":1,"408":1,"428":1,"450":2,"461":1,"477":1,"503":1,"522":1,"578":1,"579":1,"580":1,"582":4,"583":4,"584":1,"588":1,"590":1,"592":1,"701":1,"702":1,"723":1,"807":1,"809":1,"834":1,"976":1,"1069":1,"1072":1,"1082":2,"1083":1,"1086":2,"1100":1,"1113":1,"1114":1,"1116":1,"1118":1,"1208":1,"1210":1,"1214":1,"1227":1,"1233":1,"1235":1,"1236":1,"1237":1,"1238":1,"1240":1,"1246":1,"1247":1,"1250":1,"1254":1,"1280":1,"1281":1,"1284":1,"1295":1,"1299":1,"1309":2,"1328":1,"1332":1,"1345":1,"1398":1,"1440":1,"1566":1,"1601":1,"1602":7,"1604":3,"1651":2,"2069":2,"2070":3,"2078":1,"2083":1}}],["peek",{"2":{"1813":1,"1831":1}}],["people",{"2":{"557":1,"1309":1,"1750":4,"2073":1}}],["penalization",{"2":{"399":2}}],["pe",{"2":{"394":2,"520":1,"530":1}}],["pe表示位置嵌入",{"2":{"335":1}}],["peft",{"2":{"224":1}}],["permissions",{"2":{"1930":1,"2060":2}}],["permission",{"2":{"1784":1,"2060":2}}],["permutation",{"2":{"742":1}}],["permute和transpose会让stride属性改变",{"2":{"658":1}}],["permute",{"0":{"818":1,"822":1},"1":{"819":1,"820":1,"821":1,"822":1},"2":{"658":1,"785":1,"822":1,"1080":1,"1087":2,"1330":1}}],["perceptron",{"2":{"1457":1,"1461":1,"1464":1}}],["perceptive",{"2":{"274":1}}],["perf",{"2":{"1961":1}}],["perfetto",{"2":{"1161":1}}],["perform",{"2":{"542":1,"543":1}}],["performer",{"2":{"512":1}}],["performing",{"2":{"201":1}}],["performance",{"2":{"95":1,"111":1,"156":1,"1305":1}}],["perplexity",{"2":{"399":1,"768":1}}],["person>",{"2":{"1750":1}}],["person类的age成员只被继承一次",{"2":{"1662":1}}],["person类的age成员被继承两次",{"2":{"1661":1}}],["person",{"2":{"1320":4,"1324":8,"1661":6,"1662":7,"1750":6}}],["perspective",{"2":{"420":1,"499":1,"513":1,"768":1}}],["perspectives",{"2":{"136":1,"156":1}}],["persistent=false",{"2":{"723":2,"1211":1}}],["persistent",{"2":{"229":1,"1208":3,"1211":1,"1214":1}}],["per",{"0":{"1222":1},"2":{"160":1,"364":2,"423":3,"688":1,"1087":3,"1242":3,"1280":1,"1308":1,"1332":1}}],["punning",{"2":{"1728":1}}],["pub",{"2":{"1594":1}}],["public继承保留了对基类数据的直接访问控制",{"2":{"1853":1}}],["publicfunc",{"2":{"1653":1}}],["publicvar",{"2":{"1653":1,"1853":4,"1857":6,"1861":5}}],["public",{"0":{"1845":1,"1850":1,"1851":1,"1852":1,"1866":1},"1":{"1846":1,"1847":1,"1848":1,"1849":1,"1850":1,"1851":2,"1852":2,"1853":2,"1854":1,"1855":1,"1856":1,"1857":1,"1858":1,"1859":1,"1860":1,"1861":1,"1862":1,"1863":1,"1864":1,"1865":1,"1866":1,"1867":1,"1868":1,"1869":1},"2":{"1436":1,"1481":6,"1485":1,"1486":1,"1487":1,"1488":1,"1638":1,"1639":1,"1640":1,"1641":2,"1653":1,"1654":4,"1655":2,"1656":4,"1659":5,"1660":5,"1661":8,"1662":9,"1663":3,"1665":8,"1674":3,"1675":1,"1676":1,"1677":3,"1683":2,"1685":6,"1688":10,"1691":7,"1693":9,"1694":2,"1696":1,"1700":1,"1701":1,"1712":2,"1728":4,"1763":2,"1772":1,"1774":1,"1775":1,"1778":2,"1779":2,"1784":3,"1788":1,"1789":1,"1791":1,"1792":1,"1825":1,"1843":1,"1846":1,"1849":2,"1851":2,"1853":6,"1855":1,"1857":5,"1859":1,"1861":5,"1863":3,"1864":1,"1866":6,"1867":2,"1868":2,"1869":8,"1871":1,"1873":1,"1874":5,"1887":1,"1891":1,"1902":2,"2006":6,"2063":1}}],["purpose",{"2":{"726":2}}],["puredemon",{"2":{"156":1}}],["pure",{"2":{"115":1}}],["puma",{"2":{"713":4}}],["push",{"2":{"395":3,"1714":3,"1719":4,"1720":3,"1721":2,"1722":3,"1723":4,"1726":4,"1797":1,"1799":2,"1800":2,"1801":1,"1825":1,"1843":1,"1891":1}}],["pulley",{"2":{"370":1}}],["putty",{"2":{"1583":1}}],["putzt",{"2":{"370":1}}],["put",{"2":{"204":1,"233":2,"810":1,"1087":4,"1485":2,"1816":2,"1834":2}}],["p矩阵也不需要被显式分配出来",{"2":{"180":1}}],["png",{"2":{"2021":1}}],["pn",{"2":{"178":2}}],["p2p",{"2":{"974":1}}],["p2",{"2":{"178":2,"1633":3}}],["p1```markdown",{"2":{"1633":1}}],["p117",{"2":{"1426":1}}],["p1",{"2":{"178":2,"591":2,"1633":2}}],["p=softmax",{"2":{"941":2,"960":2}}],["p=0",{"2":{"835":1}}],["p=",{"2":{"178":2,"613":2,"1083":1}}],["p=dropout",{"2":{"23":1}}],["phong着色",{"2":{"2009":1}}],["phony",{"2":{"1917":1}}],["physx",{"2":{"1937":1}}],["physical",{"2":{"983":1,"1086":1,"1087":2}}],["physics",{"2":{"147":1,"156":1,"513":1}}],["phase",{"2":{"429":1,"1242":1}}],["phrases",{"2":{"740":1}}],["phrase",{"2":{"237":1,"282":1,"292":1}}],["phi",{"2":{"106":3,"844":3}}],["pojo",{"2":{"1481":2,"1485":3,"1486":1,"1487":1}}],["pop",{"2":{"1714":2,"1719":2,"1720":2,"1721":2,"1722":3,"1723":2,"1726":3}}],["popularity",{"2":{"2081":1}}],["popular",{"2":{"1300":1,"2073":1,"2078":1}}],["pope等人",{"2":{"937":1,"953":1}}],["pooled",{"2":{"1481":1}}],["pool2d",{"2":{"1215":1,"1257":1}}],["pool",{"2":{"735":2,"814":2,"815":2,"986":1,"1215":2}}],["pooling只是取令牌嵌入的平均值",{"2":{"735":1}}],["pooling作为句向量",{"2":{"734":1}}],["pooling效果就较好",{"2":{"731":1}}],["pooling是指llm的最后一层的hidden",{"2":{"731":1}}],["pooling策略",{"0":{"731":1}}],["pooling",{"0":{"813":1,"814":1,"816":1},"1":{"814":1,"815":1,"816":1},"2":{"694":1,"731":5,"735":2,"769":1,"813":1,"816":1}}],["potential",{"2":{"507":1,"625":1}}],["points",{"2":{"1087":1}}],["pointers",{"0":{"1614":1},"2":{"1647":1}}],["pointer",{"0":{"1611":1,"1614":1},"2":{"1078":2,"1611":3,"1614":3,"1633":3,"1650":1,"1672":1}}],["pointwise",{"0":{"829":1},"2":{"838":1,"944":3,"1081":1}}],["point",{"2":{"498":1,"513":1,"668":1,"1087":2,"1155":1,"1607":1,"1712":1}}],["portion",{"2":{"1594":3}}],["port",{"2":{"422":1}}],["power=1",{"2":{"1240":1}}],["powerlr",{"2":{"1240":1}}],["powers",{"2":{"456":1}}],["power",{"2":{"402":1,"1087":5,"1240":1}}],["powernorm",{"2":{"361":1}}],["pow",{"2":{"346":2,"1083":3,"1085":1,"1087":4,"1094":1,"1101":1,"1114":2}}],["possible",{"2":{"1329":6,"1330":2}}],["possibly",{"2":{"591":1}}],["postgresql",{"2":{"1492":1}}],["postnet",{"2":{"632":1}}],["posts",{"2":{"513":1}}],["post",{"0":{"328":1,"331":1},"1":{"329":1,"330":1,"331":1,"332":2,"333":2,"334":1,"335":1},"2":{"293":2,"329":7,"331":2,"332":3,"333":5,"334":3,"347":1,"349":1,"361":1,"545":1,"1083":1,"1183":2,"1208":2,"1214":1,"1226":3,"1227":3}}],["pos",{"2":{"201":15,"503":3,"1083":1,"1713":6}}],["posinf",{"2":{"1087":2}}],["positive",{"2":{"135":5,"1083":1,"1087":1}}],["positioned",{"2":{"2087":1}}],["position2position不会带来额外的信息",{"2":{"763":1}}],["position表示",{"2":{"130":2}}],["positionwisefeedforward",{"2":{"113":2,"394":1,"449":2,"703":1}}],["position",{"0":{"101":1,"1334":1},"1":{"1335":1,"1336":1,"1337":1,"1338":1,"1339":1,"1340":1,"1341":1,"1342":1,"1343":1,"1344":1,"1345":1},"2":{"96":1,"101":3,"419":1,"449":5,"590":1,"701":2,"703":5,"722":2,"723":20,"742":2,"751":1,"759":1,"767":1,"768":5,"1217":2,"1330":1,"1339":1,"1341":3,"1713":2,"1754":1,"1756":2,"2063":1}}],["positionalencoding其实是做了相加操作",{"2":{"704":1}}],["positionalencoding",{"2":{"394":1,"449":2,"703":1}}],["positional",{"0":{"459":1},"2":{"20":1,"394":1,"457":3,"698":2,"704":2,"749":1,"751":1,"758":2,"760":1,"764":1,"768":2,"1218":4,"1308":1}}],["positions",{"2":{"5":1,"74":1,"382":1,"764":1,"1217":2,"1330":1,"1821":1,"1839":1}}],["posid",{"2":{"90":8}}],["policy",{"2":{"1227":1}}],["polymorphism",{"0":{"1686":1,"1687":1,"1688":1},"1":{"1687":1,"1688":1,"1690":1,"1691":1}}],["polynomiallr",{"0":{"1240":1},"2":{"1240":1}}],["polygamma",{"2":{"1087":2}}],["polyak",{"2":{"1028":1}}],["poloclub",{"2":{"513":1}}],["polar",{"2":{"1345":1}}],["polarity",{"2":{"211":1,"233":1}}],["polaformer",{"0":{"211":1},"1":{"212":1,"213":1},"2":{"157":1,"211":2,"213":2,"233":1}}],["pole",{"2":{"167":2,"259":1}}],["poles",{"2":{"167":1,"259":1}}],["pobj",{"2":{"131":1}}],["pid",{"2":{"1524":1}}],["pid>",{"2":{"1523":2}}],["pitts",{"2":{"1459":1}}],["piwqwk⊤pj⊤p",{"2":{"1340":1}}],["pil",{"2":{"1253":3}}],["pillow",{"2":{"1250":1}}],["pil=𝑠𝑜𝑓𝑡𝑚𝑎𝑥",{"2":{"128":1}}],["pickle",{"2":{"1214":2,"1227":2}}],["picklable",{"2":{"1214":1}}],["pivots",{"2":{"1087":1}}],["pivot=true",{"2":{"1083":1}}],["pingcap",{"2":{"1954":1}}],["pingbashping",{"2":{"1526":1}}],["pinverse",{"2":{"1087":1}}],["pin",{"2":{"1087":9,"1215":1}}],["pinned",{"2":{"1087":1}}],["pinecone的创建者edo",{"2":{"696":1}}],["pijvjp",{"2":{"944":1}}],["pij=exp",{"2":{"944":1}}],["pi",{"2":{"844":1,"1243":1,"1244":1,"1604":2,"1673":3,"1817":5,"1835":5,"1908":9,"1910":1}}],["piantadosi2024why",{"2":{"740":1}}],["pieces",{"2":{"1217":1}}],["piece",{"2":{"569":9}}],["piecewise",{"2":{"402":1}}],["pi≥0pi≥0p",{"2":{"178":1}}],["pi=softmax",{"2":{"178":1}}],["pip",{"2":{"1276":1,"1301":1}}],["pipip",{"2":{"614":1}}],["pipi",{"2":{"178":1}}],["pipeline和词表tgt",{"2":{"384":1,"558":1}}],["pipeline和词表src",{"2":{"384":1,"558":1}}],["pipeline",{"0":{"977":1},"2":{"65":1,"384":6,"558":6,"977":1,"1578":1}}],["plc",{"2":{"1956":1}}],["plot",{"0":{"1279":1,"1280":1}}],["plm",{"2":{"711":1}}],["plain",{"2":{"1476":1}}],["platonic",{"2":{"513":1}}],["platform=linux",{"2":{"1605":1}}],["platform=windows",{"2":{"1605":1}}],["platform=mac",{"2":{"1605":1}}],["platform",{"2":{"513":1}}],["place操作的适用性",{"2":{"1123":1}}],["place操作",{"2":{"660":1}}],["place",{"0":{"1123":1},"2":{"490":1,"1123":1,"1214":2}}],["placed",{"2":{"76":1}}],["player",{"2":{"2087":1}}],["playbook",{"2":{"1127":1,"1195":2,"1196":1}}],["play",{"2":{"405":3}}],["playhouse",{"2":{"370":1}}],["plienhar",{"2":{"429":2}}],["pli",{"2":{"128":1}}],["plipilp",{"2":{"128":1}}],["pli=softmax",{"2":{"128":1}}],["p",{"0":{"1459":1},"2":{"47":1,"67":5,"95":1,"119":3,"128":1,"135":1,"145":2,"156":5,"178":6,"185":1,"198":2,"199":6,"200":10,"233":1,"240":1,"292":1,"361":1,"387":1,"394":5,"395":8,"429":1,"449":3,"499":2,"513":1,"543":1,"571":6,"592":2,"598":2,"612":2,"613":1,"614":1,"638":1,"740":1,"745":1,"762":1,"768":1,"844":1,"899":17,"903":2,"908":30,"933":5,"941":4,"944":2,"946":1,"960":4,"966":1,"1003":1,"1087":9,"1227":2,"1284":2,"1308":2,"1322":9,"1335":1,"1336":3,"1339":5,"1340":3,"1361":9,"1377":17,"1459":1,"1520":1,"1549":2,"1668":7,"1699":3,"1704":4,"1705":10,"1784":4,"1805":1,"1866":4,"1925":5,"1926":3,"1928":3,"1930":4,"2018":2}}],["pragma",{"2":{"1628":1}}],["practice",{"2":{"1254":1}}],["prices",{"2":{"1623":2}}],["privatemember",{"2":{"1778":3}}],["privatevar",{"2":{"1653":1,"1853":3,"1857":3,"1861":3}}],["private",{"0":{"1845":1,"1858":1,"1859":1,"1860":1,"1868":1},"1":{"1846":1,"1847":1,"1848":1,"1849":1,"1850":1,"1851":1,"1852":1,"1853":1,"1854":1,"1855":1,"1856":1,"1857":1,"1858":1,"1859":2,"1860":2,"1861":2,"1862":1,"1863":1,"1864":1,"1865":1,"1866":1,"1867":1,"1868":1,"1869":1},"2":{"1481":4,"1653":1,"1654":1,"1655":1,"1674":3,"1676":1,"1677":3,"1685":2,"1693":1,"1700":1,"1701":1,"1728":3,"1763":1,"1772":1,"1774":1,"1775":1,"1778":2,"1779":1,"1784":4,"1785":1,"1788":1,"1789":1,"1791":1,"1792":1,"1846":1,"1853":2,"1857":2,"1859":2,"1861":4,"1863":4,"1864":1,"1868":2,"1871":1,"1873":1,"1874":4,"1887":1,"1902":2,"1975":1,"1980":1,"1984":1,"1985":2,"1991":1,"2063":1}}],["privacy",{"2":{"364":1}}],["primary",{"2":{"1481":1}}],["prime",{"2":{"999":3,"1004":1,"1393":1}}],["primer",{"2":{"513":1}}],["priority",{"2":{"1083":1}}],["printing",{"2":{"2086":1}}],["printtuple",{"2":{"1912":2}}],["printtuplehelper",{"2":{"1912":2}}],["printargs",{"2":{"1912":2}}],["printarray",{"2":{"1634":2}}],["printcuboidinfo",{"2":{"1791":3,"1792":2}}],["printcount",{"2":{"1639":3}}],["printwidth",{"2":{"1772":3}}],["printwith",{"2":{"1701":6}}],["printlength",{"2":{"1729":2}}],["println",{"2":{"1436":1,"1481":1,"1485":1,"1486":1,"1487":1,"1488":1}}],["printmessage",{"2":{"1645":3,"1708":3}}],["printdate",{"2":{"1642":1}}],["printvalue",{"2":{"1638":1}}],["printf",{"2":{"1590":1,"1594":2,"1710":2}}],["printstring",{"2":{"1929":4}}],["printstacktrace",{"2":{"1481":1}}],["printsquare",{"2":{"1645":3}}],["prints",{"2":{"592":1}}],["printed",{"2":{"591":1,"2086":1}}],["print",{"2":{"74":1,"83":1,"119":2,"315":3,"422":2,"423":4,"424":1,"428":1,"529":1,"557":2,"591":1,"592":1,"804":1,"832":3,"1069":1,"1070":2,"1071":1,"1072":6,"1078":14,"1082":1,"1092":1,"1093":2,"1094":1,"1095":3,"1096":4,"1098":6,"1101":3,"1102":1,"1114":4,"1202":1,"1205":4,"1207":1,"1211":2,"1212":5,"1213":5,"1215":3,"1216":2,"1218":2,"1239":1,"1243":1,"1251":1,"1253":1,"1254":1,"1255":2,"1257":1,"1259":1,"1263":1,"1267":1,"1270":2,"1273":1,"1274":1,"1283":2,"1284":1,"1295":1,"1296":1,"1297":1,"1298":1,"1299":1,"1308":2,"1398":8,"1440":3,"1566":6,"1660":6,"1663":11,"1883":4,"1887":5,"1929":3,"2086":2,"2154":1}}],["present",{"2":{"2034":1}}],["preserved",{"2":{"557":1}}],["prev",{"2":{"1556":1}}],["preventing",{"2":{"393":1}}],["pred",{"2":{"1215":3,"1331":3}}],["predicted",{"2":{"1295":2}}],["prediction任务",{"2":{"732":1,"734":1}}],["prediction",{"2":{"428":2,"529":1,"542":1,"543":1,"632":1,"721":1,"734":2,"1315":1,"1363":1}}],["predict",{"2":{"288":2,"399":2,"894":1,"1331":1}}],["prefetch",{"2":{"1161":1}}],["prefix=",{"2":{"1214":2}}],["prefix和keep",{"2":{"1214":1}}],["prefix",{"2":{"372":1,"423":2,"541":1,"591":5,"1214":8,"1701":4,"1989":1}}],["prefill和decode",{"2":{"420":1}}],["prefill",{"2":{"69":1,"83":1,"977":1}}],["prelu",{"2":{"838":2,"842":3,"1087":1}}],["prenet对输入的sonar嵌入进行归一化处理",{"2":{"632":1}}],["prenet",{"2":{"632":1}}],["pretty",{"2":{"591":1}}],["pretokenizer",{"2":{"553":1}}],["pretraining在注意力上添加两个标记位置嵌入之间的点积logit",{"2":{"751":1}}],["pretraining",{"2":{"542":1}}],["pretrained=false",{"2":{"1282":1}}],["pretrained",{"2":{"126":1,"130":1,"143":1,"156":2,"834":2,"1308":2,"1312":1}}],["precision2",{"2":{"1331":2}}],["precision",{"0":{"973":1},"2":{"1086":1,"1331":2}}],["precise",{"2":{"144":1,"156":2}}],["precompute",{"2":{"201":1,"1345":2}}],["pre",{"0":{"328":1,"334":1},"1":{"329":1,"330":1,"331":1,"332":1,"333":1,"334":1,"335":1},"2":{"172":1,"293":2,"329":6,"333":1,"334":10,"335":4,"344":1,"349":1,"361":1,"396":1,"501":1,"545":1,"577":1,"612":1,"721":1,"726":1,"727":1,"740":3,"747":1,"764":1,"768":2,"938":1,"954":1,"1208":11,"1214":6,"1226":3,"1227":3,"1308":1,"1360":1,"2079":3}}],["preprocessing",{"2":{"592":1,"1254":1,"1604":1}}],["preprint",{"2":{"156":1,"361":1,"638":2,"768":1}}],["prepend",{"2":{"572":1,"1087":1,"1214":4,"1227":4}}],["preparing",{"2":{"370":1}}],["prepbn",{"0":{"348":1},"2":{"293":1,"361":1}}],["prep",{"2":{"131":1}}],["providing",{"2":{"2079":1}}],["provides",{"2":{"1305":1,"2079":1}}],["provided",{"2":{"557":1}}],["protectedvar",{"2":{"1853":4,"1857":4,"1861":4}}],["protected",{"0":{"1655":1,"1845":1,"1854":1,"1855":1,"1856":1,"1867":1},"1":{"1846":1,"1847":1,"1848":1,"1849":1,"1850":1,"1851":1,"1852":1,"1853":1,"1854":1,"1855":2,"1856":2,"1857":2,"1858":1,"1859":1,"1860":1,"1861":1,"1862":1,"1863":1,"1864":1,"1865":1,"1866":1,"1867":1,"1868":1,"1869":1},"2":{"1652":1,"1655":5,"1665":3,"1677":1,"1691":1,"1728":1,"1775":1,"1784":3,"1785":2,"1846":1,"1851":1,"1853":3,"1855":3,"1857":5,"1859":1,"1861":3,"1863":5,"1864":1,"1867":2,"1871":1,"1874":3}}],["proto",{"2":{"1083":2}}],["programming",{"2":{"2078":1}}],["program",{"2":{"1589":6,"1762":1}}],["progressive",{"2":{"348":1,"361":1}}],["prometheus",{"2":{"1499":1}}],["promote",{"2":{"1360":1}}],["prompteol的使用显著增强了moee方法的稳定性和性能",{"2":{"739":1}}],["prompteol方式效果更好",{"2":{"736":1}}],["prompteol由一篇发表于2023年的论文",{"2":{"736":1}}],["prompteol",{"2":{"736":2,"739":1}}],["prompting",{"2":{"504":1,"513":1}}],["prompt",{"2":{"135":2,"983":1,"1316":1}}],["promptsum",{"2":{"736":1}}],["promptsth",{"2":{"736":1}}],["prompts",{"2":{"135":4}}],["prof",{"2":{"1284":4}}],["profile",{"2":{"1227":3,"1284":2}}],["profileractivity",{"2":{"1284":2}}],["profiler的入口点",{"2":{"1227":1}}],["profiler",{"2":{"1161":1,"1284":9}}],["professor",{"2":{"543":1}}],["prod",{"2":{"1087":3,"1350":14}}],["produces",{"2":{"1330":1}}],["produce",{"2":{"522":1}}],["production",{"2":{"1157":1}}],["product形式",{"2":{"929":1}}],["product",{"0":{"916":1},"1":{"917":1,"918":1,"919":1,"920":1,"921":1,"922":1,"923":1,"924":1},"2":{"7":2,"9":1,"16":1,"67":1,"78":1,"154":1,"173":1,"186":1,"199":1,"210":1,"233":1,"434":1,"647":1,"757":1,"921":2,"924":1,"933":1,"1216":1,"1218":1,"1607":3,"1710":6,"1729":2}}],["pros",{"2":{"1067":1}}],["property",{"2":{"1083":1,"1481":4}}],["properties",{"2":{"399":1,"1482":2}}],["propagation",{"0":{"1390":1},"1":{"1391":1,"1392":1,"1393":1,"1394":1},"2":{"1047":1,"1404":1,"1438":2,"1442":1}}],["proj𝑊𝑝r𝑜𝑗",{"2":{"145":2}}],["proj𝑊𝑝𝑟𝑜𝑗",{"2":{"126":1}}],["proj",{"2":{"110":8,"145":2,"503":8}}],["projects",{"2":{"2070":1,"2079":1}}],["project",{"0":{"1972":1},"2":{"1966":1,"1976":1,"1980":3,"1987":1,"1991":1,"1997":1,"1999":2}}],["projector",{"0":{"733":1}}],["projection",{"2":{"485":1,"624":1,"764":1,"1086":1}}],["projections",{"2":{"36":1}}],["projecting",{"2":{"148":1,"483":1,"513":1}}],["projected",{"2":{"36":1}}],["probable",{"2":{"1330":1}}],["probability",{"2":{"499":1,"1215":1}}],["probabilities就是线性层后经过softmax的概率分布",{"2":{"473":1}}],["probabilities",{"0":{"471":1},"1":{"472":1,"473":1}}],["probabilistic",{"2":{"429":1}}],["problem",{"2":{"840":1}}],["problems",{"2":{"480":1,"513":1}}],["prob",{"2":{"83":2,"130":1,"428":2,"472":2,"529":2,"530":1,"723":1}}],["processcontainer",{"2":{"1914":14}}],["processfile",{"2":{"1761":3}}],["processor",{"2":{"1214":1}}],["processes",{"2":{"422":1,"983":1,"1308":1}}],["processed",{"2":{"65":3,"384":6,"558":2,"1761":1}}],["processing",{"0":{"1305":1},"1":{"1306":1,"1307":1},"2":{"167":2,"259":2,"545":1,"795":2,"1214":1,"1305":1,"1308":1,"2079":2}}],["process",{"2":{"82":1,"230":1,"423":6,"429":1,"450":1,"703":1,"982":1,"983":1,"1227":1,"1273":1,"1594":1,"2077":1}}],["pruned",{"2":{"20":1,"47":1}}],["paxos",{"2":{"1952":1}}],["pa3",{"2":{"1614":10}}],["pa2",{"2":{"1614":7,"1629":1}}],["pa1",{"2":{"1614":7}}],["pa",{"2":{"1611":10,"1614":6,"1629":2}}],["pan",{"2":{"1302":1}}],["pandas",{"2":{"1253":1}}],["panicexception",{"2":{"572":1}}],["paging",{"2":{"981":1}}],["pageattention的内存共享大大减少了复杂采样算法",{"2":{"983":1}}],["pages",{"2":{"983":1}}],["page",{"0":{"949":1,"979":1},"1":{"980":1,"981":1,"982":1,"983":1},"2":{"949":1,"979":1}}],["pagedattention是vllm的核心技术",{"2":{"983":1}}],["pagedattention跟踪物理块的引用计数并实现写时复制机制",{"2":{"983":1}}],["pagedattention中的不同序列可以通过将它们的逻辑块映射到相同的物理块来共享块",{"2":{"983":1}}],["pagedattention通过其block",{"2":{"983":1}}],["pagedattention还有另一个关键优势",{"2":{"983":1}}],["pagedattention内核能够高效地识别和获取这些块",{"2":{"981":1}}],["pagedattention将每个序列的kv",{"2":{"981":1}}],["pagedattention允许在非连续的内存空间中存储连续的key和value",{"2":{"981":1}}],["pagedattention",{"0":{"981":1},"2":{"949":1,"979":1,"981":1,"982":1}}],["paged",{"2":{"206":1,"986":1}}],["paper说学习率衰减计划也能转移",{"2":{"1158":1}}],["paper",{"2":{"950":1,"987":1,"1310":2}}],["papers",{"2":{"740":1}}],["paperweekly",{"2":{"156":1,"513":1,"638":1}}],["package",{"0":{"1977":1,"1991":1},"2":{"1537":2,"1977":1,"1991":1}}],["packages",{"2":{"1078":1}}],["packaged",{"2":{"724":1}}],["pack",{"2":{"724":1}}],["packing",{"2":{"88":2,"89":4,"90":4,"95":1,"217":1}}],["packing和mask",{"0":{"87":1},"1":{"88":1,"89":1,"90":1},"2":{"49":1}}],["pairs",{"2":{"590":1,"592":1,"623":1}}],["pair",{"0":{"1805":1},"2":{"567":1,"575":1,"590":8,"591":2,"592":18,"638":2,"724":1,"764":1,"1725":2,"1795":1,"1805":5,"1807":5,"1921":2,"1925":5,"1933":1}}],["pair=",{"2":{"375":1,"557":2,"590":1}}],["pascal",{"2":{"429":1}}],["passwd",{"2":{"1530":1}}],["password",{"2":{"1481":1,"2069":2,"2070":2}}],["passed",{"2":{"1254":1}}],["passing",{"2":{"509":1,"1573":1}}],["pass",{"0":{"970":1,"971":1},"2":{"343":1,"522":1,"1215":1,"1650":3,"1729":3,"2086":3}}],["past",{"2":{"76":6,"723":3}}],["pay",{"2":{"209":2,"233":1}}],["patience",{"2":{"1245":1}}],["pattention就是让",{"2":{"621":1}}],["pattention层引入了一个额外的维度",{"2":{"621":1}}],["pattention使用一组可训练的",{"2":{"621":1}}],["pattention使用一组可学习的token来表示模型参数",{"2":{"620":1}}],["pattention机制",{"0":{"621":1}}],["pattention",{"2":{"620":1,"621":3,"623":1}}],["patterns",{"2":{"591":1}}],["pattern",{"2":{"126":1,"127":1,"591":5,"623":1,"1516":3}}],["patch比token更高效",{"2":{"638":1}}],["patch表示是键",{"2":{"615":1}}],["patch表示是key",{"2":{"614":1}}],["patch的平均大小是使用给定patch函数在训练和推理期间处理数据的主要因素",{"2":{"613":1}}],["patch函数fpfpf",{"2":{"613":1}}],["patch化",{"0":{"613":1}}],["patch方案fpfpf",{"2":{"612":1}}],["patch",{"2":{"612":2,"614":10,"1227":3}}],["patch和token之间的一个关键区别是",{"2":{"612":1}}],["patch指的是没有固定词汇表的动态分组序列",{"2":{"612":1}}],["patches",{"2":{"610":1,"638":1,"1208":1}}],["pat",{"2":{"571":3}}],["patricia",{"2":{"233":1}}],["path",{"2":{"160":1,"372":2,"423":4,"557":3,"571":5,"1250":4,"1273":4,"1303":1,"1308":4,"1330":1,"1332":1,"1589":2,"1930":8,"1933":1,"2009":1}}],["pathways",{"2":{"8":1,"47":2}}],["parentprotected",{"2":{"1784":4}}],["parentprivate",{"2":{"1784":3}}],["parent",{"2":{"1784":10,"1785":3}}],["parse",{"2":{"1215":1}}],["parser",{"2":{"1215":13}}],["partitioned",{"2":{"981":1}}],["partitioning",{"0":{"968":1},"1":{"969":1,"970":1,"971":1,"972":1},"2":{"947":1}}],["particularly",{"2":{"2087":1}}],["particular",{"2":{"499":1}}],["particles",{"2":{"499":1}}],["particle",{"2":{"498":3,"499":1,"513":1}}],["partial",{"0":{"1751":1},"2":{"148":2,"192":2,"485":2,"591":1,"731":1,"999":4,"1392":10,"1393":8,"1394":38,"1395":2,"1442":2,"1732":1,"1751":2}}],["paris",{"2":{"122":1}}],["para",{"2":{"1258":2,"1259":2}}],["param参数",{"2":{"1488":1}}],["param中设置的值即可",{"2":{"1485":1}}],["param属性",{"2":{"1485":1}}],["parametric",{"2":{"842":1}}],["parametertype=",{"2":{"1485":1,"1486":1,"1487":1,"1488":1}}],["parametertype",{"2":{"1485":1}}],["parameter参数",{"2":{"1226":1}}],["parameter交互计算方面采用了固定的线性投影方法",{"2":{"618":1}}],["parameterized",{"2":{"348":1,"361":1}}],["parameter的作用就是将这个两个参数作为模型参数",{"2":{"343":1}}],["parameter",{"0":{"1203":1,"1222":1},"2":{"139":1,"233":1,"343":2,"346":2,"393":1,"429":1,"503":4,"616":1,"620":1,"621":3,"623":3,"624":4,"702":2,"733":1,"772":1,"1117":1,"1208":1,"1210":1,"1214":7,"1218":1,"1221":1,"1222":1,"1226":1,"1441":1}}],["parameters和buffer到cuda",{"2":{"662":1}}],["parameters到cuda",{"2":{"662":1}}],["parameters",{"0":{"988":1,"1210":1},"2":{"83":1,"111":1,"119":3,"402":1,"423":1,"424":1,"449":2,"616":2,"621":1,"623":2,"638":1,"662":1,"664":2,"702":1,"807":2,"809":2,"988":1,"1039":1,"1202":2,"1205":6,"1208":2,"1210":1,"1214":5,"1215":3,"1218":1,"1221":1,"1222":2,"1231":2,"1239":1,"1241":1,"1242":1,"1243":1,"1245":1,"1254":1,"1266":1,"1267":1,"1296":1,"2086":3}}],["param",{"2":{"147":2,"383":3,"385":1,"666":1,"807":6,"1214":1,"1226":11,"1227":9,"1243":1,"1259":2,"1485":2}}],["params的可迭代对象转换为",{"2":{"1226":1}}],["params",{"2":{"141":1,"201":16,"1202":2,"1214":1,"1222":3,"1224":1,"1225":1,"1226":4,"1227":2,"1254":1}}],["parallelism",{"0":{"968":1,"977":1},"1":{"969":1,"970":1,"971":1,"972":1},"2":{"217":1,"420":2,"947":1,"977":2}}],["parallelized",{"2":{"160":1}}],["parallel=true",{"2":{"8":1,"114":1,"201":1}}],["parallel",{"0":{"976":1,"977":1,"1305":1},"1":{"1306":1,"1307":1},"2":{"8":4,"201":4,"908":1,"974":1,"975":1,"976":3,"983":1,"1214":1,"1308":3,"1564":1}}],["padded",{"2":{"1087":1}}],["padding的除外",{"2":{"382":1}}],["padding的功能在tgt",{"2":{"77":1}}],["padding有左填充和右填充之分",{"2":{"378":1}}],["padding=3",{"2":{"1283":1}}],["padding=1",{"2":{"802":2}}],["padding=128",{"2":{"65":1,"375":1,"384":1,"558":1}}],["padding=",{"2":{"801":2,"802":1}}],["padding=max",{"2":{"375":1}}],["padding=config",{"2":{"364":1,"423":1}}],["padding数值一般来说是0",{"2":{"54":1}}],["padding",{"0":{"60":1,"376":1,"933":1},"1":{"61":1,"62":1,"63":1,"64":1,"65":1,"66":1,"67":1,"377":1,"378":1},"2":{"49":1,"50":3,"59":1,"65":3,"74":1,"77":3,"78":1,"79":1,"82":2,"83":2,"84":10,"89":1,"90":5,"316":1,"326":2,"364":1,"372":1,"375":1,"380":1,"382":3,"384":7,"387":1,"399":7,"423":2,"424":1,"650":1,"702":15,"723":1,"773":1,"801":2,"802":1,"834":5,"932":1,"933":1,"934":1,"1086":1,"1087":1}}],["pad>`",{"2":{"557":1}}],["pad>`部分进行了mask",{"2":{"399":1}}],["pad>",{"2":{"399":3,"423":2}}],["pad>的标签",{"2":{"399":1}}],["pad>所在的index填充为0",{"2":{"399":1}}],["pad>在词典中对应的序号",{"2":{"399":1}}],["pad优化",{"2":{"377":1}}],["pad=2",{"2":{"66":1,"380":1}}],["pad部分置为false",{"2":{"66":1,"382":1}}],["pad",{"2":{"65":3,"66":3,"74":3,"79":2,"80":2,"84":2,"87":2,"88":1,"364":1,"375":1,"380":5,"382":1,"384":3,"385":1,"423":3,"558":1,"571":1,"723":1,"1086":1,"1330":4}}],["palm",{"2":{"8":1,"47":2,"541":1}}],["有兴趣的可以自己去研究",{"2":{"2117":1}}],["有次回寝室",{"2":{"2054":1}}],["有朋自远方来",{"2":{"2053":1}}],["有缓冲",{"2":{"1811":1,"1829":1}}],["有纯虚函数",{"2":{"1693":1}}],["有限元分析",{"2":{"1957":1}}],["有限",{"2":{"1648":1}}],["有限状态自动机",{"2":{"137":1}}],["有多个cin时",{"2":{"1813":1,"1831":1}}],["有多种实现",{"2":{"1569":1}}],["有多好",{"2":{"334":1}}],["有参数",{"2":{"1824":1,"1842":1}}],["有参",{"2":{"1481":1}}],["有很多人天天都在焦虑",{"2":{"2056":1}}],["有很多重复代码块",{"2":{"1479":1}}],["有很多相似的上下文",{"2":{"713":1}}],["有规律可循",{"2":{"1375":1}}],["有某种衰减方案可以使模型更有可能达到良好的学习率",{"2":{"1171":1}}],["有足够的计算资源来进行调参实验",{"2":{"1138":1}}],["有必要在开始下一步前对此进行诊断和矫正",{"2":{"1132":1}}],["有密切的关系",{"2":{"1105":1}}],["有哪些方法可以调用",{"2":{"1073":1}}],["有哪些属性",{"2":{"1073":1}}],["有何不同",{"2":{"1017":1}}],["有何联系",{"2":{"775":1}}],["有没有什么办法可以解决这个问题呢",{"2":{"967":1}}],["有不同的理解",{"2":{"2054":1}}],["有不同的解码方式可供选择",{"2":{"889":1}}],["有不同的关注点",{"2":{"9":1}}],["有短期记忆问题",{"2":{"861":1}}],["有可能是当前的局部最优",{"2":{"2115":1}}],["有可能出现神经网络输出为",{"2":{"840":1}}],["有可能会比欧氏距离给出一个更高的距离值",{"2":{"692":1}}],["有固定的梯度",{"2":{"840":1}}],["有几个kernel",{"2":{"773":1}}],["有几种机制可以在",{"2":{"1116":1}}],["有几种不同的度量方法可以用来衡量两个向量之间的距离",{"2":{"692":1}}],["有几种非线性操作",{"2":{"327":1}}],["有几种常用的激活函数",{"2":{"103":1}}],["有只鸡在吃不明生物",{"2":{"755":1}}],["有个不明生物在吃鸡",{"2":{"755":1}}],["有三个句子",{"2":{"704":1}}],["有三种主流算法",{"2":{"567":1}}],["有望提供更稳健和和更可解释的相似度度量",{"2":{"692":1}}],["有关虚拟机创建的网络问题",{"0":{"2088":1},"1":{"2089":1,"2090":1,"2091":1,"2092":1,"2093":1,"2094":1,"2095":1}}],["有关requires",{"2":{"1214":1}}],["有关详细讨论",{"2":{"1136":1}}],["有关推断模式的实现细节",{"2":{"1121":1}}],["有关推断模式的详细信息",{"2":{"1121":1}}],["有关更多信息",{"2":{"1114":1}}],["有关",{"2":{"692":1,"1322":1}}],["有较高相似度的词",{"2":{"691":1}}],["有序容器",{"0":{"1796":1},"1":{"1797":1,"1798":1,"1799":1,"1800":1,"1801":1,"1802":1,"1803":1},"2":{"1795":1}}],["有序容器与关联容器",{"0":{"1794":1},"1":{"1795":1,"1796":1,"1797":1,"1798":1,"1799":1,"1800":1,"1801":1,"1802":1,"1803":1,"1804":1,"1805":1,"1806":1,"1807":1,"1808":1}}],["有序并且非常容易索引",{"2":{"679":1}}],["有序假设",{"2":{"274":1}}],["有其固有的局限性",{"2":{"610":1}}],["有1～4个范围的字节",{"2":{"607":1}}],["有点类似英语中的词根词缀拼词法",{"2":{"567":1}}],["有利于在自回归模型",{"2":{"542":1}}],["有利于分布与权重的相互协调",{"2":{"313":1}}],["有了以上背景知识",{"2":{"1442":1}}],["有了对分块计算",{"2":{"974":1}}],["有了上下文的定义之后",{"2":{"713":1}}],["有了这些积木块",{"2":{"449":1}}],["有了htℎtℎ",{"2":{"240":1}}],["有时需要清除缓冲区",{"2":{"1813":1,"1831":1}}],["有时可以处理一些",{"2":{"1180":1}}],["有时也被称为",{"2":{"1119":1}}],["有时也被称为转置卷积",{"2":{"779":1}}],["有时称为intra",{"2":{"911":1}}],["有时",{"2":{"709":1,"1184":1}}],["有时候教材上解释成",{"2":{"2054":1}}],["有时候根据业务的需求",{"2":{"1488":1}}],["有时候",{"2":{"560":1}}],["有时这被认为是一个独立的仿射层",{"2":{"360":1}}],["有时是四维张量",{"2":{"34":1}}],["有所下降",{"2":{"935":1,"951":1}}],["有所上升",{"2":{"935":1,"951":1}}],["有所不同",{"2":{"341":1}}],["有所收获",{"2":{"235":1}}],["有一群小孩和若干块饼干",{"2":{"2147":1}}],["有一定的初步认识",{"2":{"2120":1}}],["有一定要求",{"2":{"316":1}}],["有一些同步的机制必须在临界区段的进入点与离开点实现",{"2":{"1413":1}}],["有一些论述",{"2":{"713":1}}],["有一个共同特征",{"2":{"492":1}}],["有一个天然的硬伤",{"2":{"316":1}}],["有一种理论称为叠加假说",{"2":{"118":1}}],["有读者会问",{"2":{"307":1}}],["有效",{"2":{"1164":1}}],["有效的正则化策略",{"2":{"1011":1}}],["有效的自然语言模型需要可以理解每个单词",{"2":{"715":1}}],["有效的减少了词表的数量",{"2":{"576":1}}],["有效解决了梯度消失问题",{"2":{"304":1}}],["有效地管理kv",{"2":{"981":1}}],["有效地管理注意力key",{"2":{"980":1}}],["有效地重叠块的传输与块级计算",{"2":{"975":1}}],["有效地跳过了下游组件的进一步计算",{"2":{"479":1}}],["有效地将时间和空间复杂度从",{"2":{"212":1}}],["有效地控制了模型的复杂度",{"2":{"116":1}}],["有选择性地依据词之间的相关性来吸取整个序列中每一个其他元素的信息",{"2":{"261":1}}],["有长距离的依赖关系",{"2":{"246":1}}],["有意义的事就是好好活",{"2":{"2056":1}}],["有意义的",{"2":{"221":1}}],["有意义的为false",{"2":{"66":1}}],["有各种优化的分类",{"2":{"203":1}}],["有些类似",{"2":{"2105":1}}],["有些人很习惯去评论别人",{"2":{"2056":1}}],["有些进程在临界区外无休止的等待",{"2":{"1412":1}}],["有些模型可能只需要100次训练",{"2":{"1183":1}}],["有些操作需要保存中间结果以便执行反向传播",{"2":{"1114":1}}],["有些算子并不能和",{"2":{"785":1}}],["有些方法着重研究前向传播的隐状态和权重的映射上",{"2":{"482":1}}],["有些小积木块被用某些类进行了封装",{"2":{"449":1}}],["有些工作就将焦点转移到",{"2":{"446":1}}],["有些文献也称为",{"2":{"310":1}}],["有些数字会非常大",{"2":{"190":1}}],["有些则更加难以捉摸",{"2":{"9":1}}],["有许多优点",{"2":{"183":1}}],["有能力改变条件概率p",{"2":{"176":1}}],["有助于减少错误",{"2":{"1629":1}}],["有助于我们更好地理解这门语言的特点和应用场景",{"2":{"1603":1}}],["有助于更好地使用和理解这些工具",{"2":{"1602":1}}],["有助于找到更好的局部最小值或接近全局最优解",{"2":{"1026":1}}],["有助于网络的收敛和训练",{"2":{"838":1}}],["有助于实现跨语言的文本检索",{"2":{"696":1}}],["有助于模型更好地理解和生成文本",{"2":{"561":1}}],["有助于模型更快地收敛并提高性能",{"2":{"393":1}}],["有助于模型捕捉更丰富的语义信息和依赖关系",{"2":{"172":1}}],["有助于防止过拟合",{"2":{"198":1}}],["有助于提升模型性能和泛化能力",{"2":{"172":1}}],["有助于缓解梯度消失问题",{"2":{"104":1}}],["有研究者尝试使用relu6",{"2":{"841":1}}],["有研究发现",{"2":{"674":1}}],["有研究人员认为",{"2":{"363":1}}],["有研究人员发现",{"2":{"163":1}}],["有研究表明",{"2":{"91":1,"93":1,"129":1,"329":1}}],["有论文对将glu的门控放在了权重w的部分",{"2":{"105":1}}],["有",{"2":{"37":1,"217":1,"344":1,"394":1,"420":1,"1130":1}}],["有两种类型的工作模式",{"2":{"1154":1}}],["有两种类型的资源成本与增加",{"2":{"1134":1}}],["有两种类型",{"2":{"739":1}}],["有两种流行的方法可以获得一系列令牌的嵌入",{"2":{"735":1}}],["有两种常用归一化类型",{"2":{"311":1}}],["有两种参数",{"2":{"199":1}}],["有两种方式来计算每个头的注意力",{"2":{"32":1}}],["有两个参数",{"2":{"344":1}}],["有两个隐藏层",{"2":{"301":1}}],["有两个输入",{"2":{"265":1}}],["有两个方向的尝试比较重要",{"2":{"123":1}}],["有两个关于掩码的变量",{"2":{"79":1}}],["有两个比较确切的例子",{"2":{"5":1}}],["有的甚至把中间两项都去掉了",{"2":{"758":1}}],["有的方案将其中某些项变成可训练的参数",{"2":{"758":1}}],["有的研究人员提出了相对编码",{"2":{"743":1}}],["有的文章中并没有区分它们",{"2":{"676":1}}],["有的特征分裂成具有规模的更细粒度的特征",{"2":{"477":1}}],["有的注意力头关注标点符号",{"2":{"20":1}}],["有的注意力头关注sep符号",{"2":{"20":1}}],["有的注意力头关注下一个token",{"2":{"20":1}}],["有的注意力头关注所有的词",{"2":{"20":1}}],["有的头主要识别词法信息",{"2":{"13":1}}],["有的头可能主要识别语法信息",{"2":{"13":1}}],["有的头可能主要识别位置信息",{"2":{"13":1}}],["有的头可能识别不到啥信息",{"2":{"13":1}}],["有人把rope看作是混合位置编码",{"2":{"767":1}}],["有人看教练部署",{"2":{"5":1}}],["有人负责看重点球员",{"2":{"5":1}}],["有人负责总体把握",{"2":{"5":1}}],["有人负责从防守球员角度看",{"2":{"5":1}}],["有人负责从从进攻球员角度看",{"2":{"5":1}}],["不大于胃口",{"2":{"2157":1}}],["不影响你正常理解算法思维",{"2":{"2146":1}}],["不吃面经咋活",{"0":{"2141":1}}],["不妨继续往下看看",{"2":{"2140":1}}],["不妨选择他们的中点",{"2":{"2022":1}}],["不放弃",{"2":{"2056":1}}],["不放在",{"2":{"2043":1}}],["不抛弃",{"2":{"2056":1}}],["不解之言",{"0":{"2055":1}}],["不愠",{"2":{"2054":2}}],["不自觉的产物",{"2":{"2054":1}}],["不然为何称他为",{"2":{"2054":1}}],["不亦君子乎",{"2":{"2053":1}}],["不亦乐乎",{"2":{"2053":1}}],["不亦说乎",{"2":{"2053":1,"2054":1}}],["不吹嘘",{"2":{"2051":1}}],["不拖拉",{"2":{"2051":1}}],["不拥有字符串数据",{"2":{"1929":1}}],["不拥有对象的生命周期控制权",{"2":{"1695":1}}],["不合法",{"2":{"1867":1,"1868":1}}],["不合理初始化的问题",{"0":{"991":1}}],["不支持随机访问",{"2":{"1799":1}}],["不支持迭代器遍历",{"2":{"1723":1}}],["不具有传递性",{"2":{"1793":1}}],["不具备局部意义和绝对含义",{"2":{"707":1}}],["不允许重复元素",{"2":{"1724":1}}],["不允许复制",{"2":{"1695":2}}],["不进行越界检查",{"2":{"1713":1}}],["不进行分布式训练",{"2":{"372":1}}],["不适合内联的情况包括包含循环",{"2":{"1709":1}}],["不适用于共享内存架构",{"2":{"1579":1}}],["不直接拷贝指针成员变量",{"2":{"1694":1}}],["不拷贝指针指向的内存内容",{"2":{"1694":1}}],["不足",{"2":{"1653":1}}],["不推荐过度使用",{"2":{"1762":1}}],["不推荐使用",{"2":{"1631":1}}],["不推荐在复杂逻辑中使用",{"2":{"1619":1}}],["不等于",{"2":{"1619":1,"1630":1,"1635":1}}],["不存在空引用",{"2":{"1612":1}}],["不占用额外的内存空间",{"2":{"1612":1}}],["不必深究其具体数值",{"2":{"1611":1}}],["不共享内存",{"2":{"1572":1}}],["不让空闲",{"2":{"1565":1}}],["不让注意力计算看到未来的单词",{"2":{"450":1}}],["不懂就问ai或者浏览器",{"2":{"1561":1}}],["不保存更改",{"2":{"1544":1}}],["不保存",{"2":{"1544":1}}],["不保存退出",{"2":{"1518":1}}],["不写的话不会提交到数据库",{"2":{"1486":1,"1487":1,"1488":1}}],["不光滑等不太好的性质",{"2":{"1460":1}}],["不满足这些属性的对象的示例包括集合",{"2":{"1225":1}}],["不平衡的数据集",{"2":{"1165":1}}],["不太可能完美迁移",{"2":{"1158":1}}],["不太可能转移",{"2":{"1158":1}}],["不值得这样做",{"2":{"1152":1}}],["不值得部署",{"2":{"1137":1}}],["不幸运",{"2":{"1149":1}}],["不幸的是",{"2":{"1115":1,"1132":1,"1145":1,"1185":1}}],["不良的搜索空间边界和可接受的搜索空间边界示例",{"2":{"1147":1}}],["不运行任何实验",{"2":{"1133":1}}],["不可访问",{"2":{"1863":3}}],["不可变性",{"2":{"1613":1}}],["不可重新赋值指向",{"2":{"1612":1}}],["不可行",{"2":{"1147":1,"1182":1}}],["不可避免地",{"2":{"1127":1}}],["不可训练的",{"2":{"765":1}}],["不释放",{"2":{"1106":1}}],["不释放tensor的梯度",{"2":{"1099":1}}],["不像adam",{"2":{"1059":1}}],["不容易受到单个样本或噪声的影响",{"2":{"1025":1}}],["不用抱怨这一切",{"2":{"2054":1}}],["不用mybatis依旧可以做到",{"2":{"1479":1}}],["不用担心它们之间的重叠",{"2":{"976":1}}],["不用训练",{"2":{"629":1}}],["不做近似地完成超长",{"2":{"974":1}}],["不确定具体长度",{"2":{"883":1}}],["不重要",{"2":{"844":1}}],["不包含",{"2":{"1704":1}}],["不包含9",{"2":{"832":1}}],["不包含7",{"2":{"832":1}}],["不考虑自赋值问题",{"2":{"1887":1}}],["不考虑学习率衰减的固定学习率",{"2":{"1179":1}}],["不考虑其与其他位置的关系",{"2":{"745":1}}],["不考虑词嵌入层",{"2":{"119":1}}],["不相关的单词略有不同",{"2":{"709":1}}],["不相似词的向量则更远离",{"2":{"686":1}}],["不论两个",{"2":{"689":1}}],["不屈不挠的",{"2":{"683":1}}],["不连续",{"2":{"658":1}}],["不见森林",{"2":{"626":1}}],["不完全总结如下",{"2":{"542":1}}],["不显式区分",{"2":{"541":1}}],["不希望注意力在预测某个词时候就能关注到其后面的单词",{"2":{"525":1,"528":1}}],["不带掩码",{"2":{"525":1}}],["不受计算限制",{"2":{"1149":1}}],["不受其约束",{"2":{"810":1}}],["不受数据增强跟负样本采样策略的影响",{"2":{"727":1}}],["不受干扰",{"2":{"449":1}}],["不受距离的影响",{"2":{"261":1}}],["不管",{"2":{"1339":1}}],["不管一个给定的工作负载是否是计算约束",{"2":{"1154":1}}],["不管这n个向量原来是多少维的",{"2":{"684":1}}],["不管解码器本次输出是什么",{"2":{"407":1}}],["不管是从中学的书本还是后来自己的钻研",{"2":{"2054":1}}],["不管是单核单处理器",{"2":{"1565":1}}],["不管是自注意力模块还是ffn",{"2":{"523":1}}],["不管是",{"2":{"344":1}}],["不断要求用户输入数字",{"2":{"1620":1}}],["不断进化的语言",{"2":{"1603":1}}],["不断进行改进",{"2":{"706":1}}],["不断改进生产中使用的模型",{"2":{"1139":1}}],["不断地将前一个时刻",{"2":{"886":1}}],["不断迭代更新",{"2":{"709":1}}],["不断重复上述过程",{"2":{"431":1,"902":1}}],["不断尝试",{"2":{"402":1}}],["不断加深整个网络",{"2":{"247":1}}],["不展示验证集相关代码",{"2":{"364":1}}],["不依赖minibatch",{"2":{"811":1}}],["不依赖",{"2":{"337":1}}],["不依赖于其他数据",{"2":{"322":1}}],["不要对一件没有做过的事儿说没有意义",{"2":{"2056":1}}],["不要尝试通过std",{"2":{"1929":1}}],["不要尝试修改",{"2":{"1929":1}}],["不要过早地捕获异常",{"2":{"1764":1}}],["不要过度分散注意力",{"2":{"1598":1}}],["不要过度调整靠近输入层的参数",{"2":{"333":1}}],["不要在析构函数中抛出异常",{"2":{"1764":1}}],["不要在训练中调整",{"2":{"1155":1}}],["不要滥用运算符重载",{"2":{"1712":1}}],["不要释放已经释放过的内存",{"2":{"1672":1}}],["不要传递一个",{"2":{"1222":1}}],["不要盲目使用余弦相似度",{"2":{"692":1}}],["不要硬编码",{"2":{"682":1}}],["不要浪费计算资源",{"2":{"54":1}}],["不对后两个维度",{"2":{"326":1}}],["不对齐问题带来的最大困境是",{"2":{"245":1}}],["不一定全局最优",{"2":{"901":1}}],["不一定正确",{"2":{"407":1}}],["不一定等同于b对a的重要性",{"2":{"172":1}}],["不一致",{"2":{"316":1}}],["不需要手动编写",{"2":{"1917":1}}],["不需要修改可以传const引用",{"2":{"1914":1}}],["不需要修改实参",{"2":{"1650":1}}],["不需要分号",{"2":{"1632":1}}],["不需要单独设置参数类型",{"2":{"1485":1}}],["不需要掌握",{"0":{"1112":1},"1":{"1113":1,"1114":1,"1115":1,"1116":1,"1117":1,"1118":1,"1119":1,"1120":1,"1121":1,"1122":1,"1123":1}}],["不需要每次循环都进行缩放",{"2":{"969":1}}],["不需要高质量的标注数据",{"2":{"734":1}}],["不需要",{"2":{"676":1}}],["不需要严格时序依次迭代",{"2":{"418":1}}],["不需要排队进入",{"2":{"415":1}}],["不需要对这部分参数进行正则化",{"2":{"334":1}}],["不需要保存",{"2":{"322":1}}],["不需要存全局的均值和方差",{"2":{"322":1}}],["不需要batch的均值和方差",{"2":{"322":1}}],["不需要学习",{"2":{"313":1}}],["不需要掩码",{"2":{"81":1,"525":1}}],["不是也不是时间远",{"2":{"2054":1}}],["不是亲戚朋友",{"2":{"2054":1}}],["不是",{"2":{"1926":1,"1928":1}}],["不是变量或常量",{"2":{"1632":1}}],["不是大写字母",{"2":{"1619":1}}],["不是简单的",{"2":{"1615":1}}],["不是字典类型的话",{"2":{"1226":1}}],["不是一个参数",{"2":{"1211":1}}],["不是一个真正的词表分布",{"2":{"128":1}}],["不是线性而是有大量的分叉",{"2":{"542":1}}],["不是神经元",{"2":{"488":1}}],["不是排队进入",{"2":{"419":1}}],["不是担心输入的梯度会消失吗",{"2":{"304":1}}],["不利于大规模快速训练和部署",{"2":{"291":1}}],["不但要从",{"2":{"277":1}}],["不仅可以包含数据成员",{"2":{"1728":1}}],["不仅可以有效过滤无关信息",{"2":{"746":1}}],["不仅可以激发大模型的推理能力",{"2":{"736":1}}],["不仅仅是动态图",{"0":{"1286":1}}],["不仅仅是实体需要向量化",{"2":{"680":1}}],["不仅仅是待翻译词之前的输入",{"2":{"277":1}}],["不仅像原始",{"2":{"624":1}}],["不仅降低了各个网络的学习难度",{"2":{"298":1}}],["不仅是输入和隐藏层单元的激活值要中心化",{"2":{"298":1}}],["不仅能够存储大量信息",{"2":{"121":1}}],["不只像rnn那样只传递一个编码器最终的隐状态",{"2":{"273":1}}],["不只是发生了叠加",{"2":{"137":1}}],["不更新权重",{"2":{"228":1}}],["不如开发专家模块",{"2":{"222":1}}],["不如让几个人一起来看一遍比赛",{"2":{"5":1}}],["不再拥有该对象",{"2":{"1695":1}}],["不再依赖于动态图和python解释器",{"2":{"1291":1}}],["不再对任何数据有所响应",{"2":{"840":1}}],["不再让位置信息构成绝对的抑制",{"2":{"746":1}}],["不再变化",{"2":{"715":1}}],["不再像传统语言模型那样逐词预测",{"2":{"628":1}}],["不再赘述",{"2":{"457":1}}],["不再需要解码过程中的隐向量",{"2":{"416":1}}],["不再以计算单元为中心",{"2":{"206":1}}],["不再影响全局概率的预测",{"2":{"62":1}}],["不够高",{"2":{"163":1}}],["不截断",{"2":{"90":1}}],["不过也算是基础了解pytorch吧",{"2":{"2072":1}}],["不过我可以跟你说",{"2":{"2054":1}}],["不过孔子先生更厉害",{"2":{"2054":1}}],["不过这里有一个字需要特别强调",{"2":{"1478":1}}],["不过这里采用了cfg",{"2":{"1363":1}}],["不过",{"2":{"542":1,"846":1,"1228":1}}],["不过目前",{"2":{"314":1}}],["不过实践中",{"2":{"88":1}}],["不过现在执行变换的权重矩阵从一组",{"2":{"9":1}}],["不常用到",{"2":{"78":1}}],["不使用前次推理的输出作为下一次推理的增加输入",{"2":{"406":1}}],["不使用",{"2":{"77":3}}],["不应该注意的部分",{"2":{"77":1}}],["不能复制",{"2":{"1911":1}}],["不能用于虚函数",{"2":{"1905":1}}],["不能以数字开头",{"2":{"1729":1}}],["不能在运行时改变",{"2":{"1714":1}}],["不能实例化抽象类",{"2":{"1693":1}}],["不能修改常量引用",{"2":{"1729":1}}],["不能修改常量成员变量的值",{"2":{"1640":1}}],["不能修改任何成员变量的值",{"2":{"1640":1}}],["不能调用非",{"2":{"1640":1}}],["不能取地址",{"2":{"1629":1}}],["不能省略",{"2":{"1621":1}}],["不能指向其他地址",{"2":{"1614":2}}],["不能通过迭代器修改",{"2":{"1724":1}}],["不能通过",{"2":{"1614":2}}],["不能通过该指针修改指向的值",{"2":{"1614":1}}],["不能再指向其他变量",{"2":{"1612":1}}],["不能直接将字符串指针转换为",{"2":{"1683":1}}],["不能直接解引用",{"2":{"1611":1}}],["不能直接求",{"2":{"1377":1}}],["不能存储负数",{"2":{"1607":1}}],["不能写错",{"2":{"1481":1}}],["不能同时进入关于同一组共享变量的临界区域",{"2":{"1409":1}}],["不能跟别的类别的高斯中心离得太近",{"2":{"1375":1}}],["不能为空",{"2":{"1226":1}}],["不能很好地泛化到训练时未见过的更长序列",{"2":{"749":1}}],["不能重复",{"2":{"689":1}}],["不能被进一步压缩",{"2":{"575":1}}],["不能泄露天机",{"2":{"409":1}}],["不能访问未来的输入",{"2":{"382":1}}],["不能没有",{"2":{"334":1}}],["不能反映出原始分值中的相对强度或置信度",{"2":{"180":1}}],["不能仅仅关注mlp层",{"2":{"131":1}}],["不能看目标序列的第i个元素及其后面的部分",{"2":{"409":1}}],["不能看到之后的",{"2":{"89":1}}],["不能看见未来的信息",{"2":{"59":1,"934":1}}],["不能计算当前词和后面的词的注意力",{"2":{"58":1}}],["不会自动成为",{"2":{"1786":1}}],["不会自动进行数组越界检查",{"2":{"1634":1}}],["不会被插入",{"2":{"1724":1}}],["不会将",{"2":{"1704":1}}],["不会执行",{"2":{"1630":2,"1672":1}}],["不会进行四舍五入",{"2":{"1607":1}}],["不会进行元素间的信息交换",{"2":{"101":1}}],["不会影响梯度计算",{"2":{"1094":1}}],["不会消失",{"2":{"840":1}}],["不会发生梯度消失问题",{"2":{"840":1}}],["不会发生变化",{"2":{"706":1}}],["不会随着不同上下文场景的变化而改变",{"2":{"715":1}}],["不会重新拷贝数据",{"2":{"658":1}}],["不会产生oov问题",{"2":{"566":1}}],["不会相互影响",{"2":{"437":1}}],["不会有长依赖的问题",{"2":{"415":1}}],["不会受到未来信息的影响",{"2":{"50":1}}],["不会跨组操作",{"2":{"33":1}}],["不理会它与后续",{"2":{"50":1}}],["不改变原数据",{"2":{"35":1,"36":1}}],["不同商家之间的视野",{"0":{"2051":1},"2":{"2046":1}}],["不同情况的位置确定是如何的呢",{"2":{"2022":1}}],["不同类型的运算符重载示例",{"2":{"1712":1}}],["不同点",{"0":{"1470":1}}],["不同stage的attention模块是固定attention",{"2":{"1363":1}}],["不同于常规位置编码对将",{"2":{"1340":1}}],["不同于rnn",{"2":{"1334":1}}],["不同实现与性能优化",{"0":{"1228":1}}],["不同优化算法效果对比",{"0":{"1055":1},"1":{"1056":1,"1057":1}}],["不同词汇是怎么流畅地组合成句子的",{"2":{"908":1}}],["不同参数l",{"2":{"778":1}}],["不同神经元的感知区域相互重叠从而覆盖整个视野",{"2":{"769":1}}],["不同事物被广泛的联系在一起",{"2":{"689":1}}],["不同语言的结构决定了适合它们的分词策略",{"2":{"568":1}}],["不同语言的结构差异意味着对词汇表的需求也不同",{"2":{"560":1}}],["不同位置上的相似性或者相关性",{"2":{"535":1}}],["不同单词之间的相关性有多大",{"2":{"512":1}}],["不同单词对目标单词的影响力往往不同",{"2":{"260":1}}],["不同维度的参数会得到相同的更新",{"2":{"992":1}}],["不同维度的两个tensor",{"2":{"829":1}}],["不同维度的表示",{"2":{"510":1}}],["不同维度上的特征进行混合",{"2":{"101":1}}],["不同模型支持的句子最大长度不同",{"2":{"453":1}}],["不同模型架构的差异开始显现",{"2":{"147":1}}],["不同在于",{"2":{"349":1}}],["不同正则化方法的区别只是操作的信息维度不同",{"2":{"321":1}}],["不同正则化的区别只是操作的信息维度不同",{"2":{"311":1}}],["不同句子相同位置的词汇不具有相同性质",{"2":{"316":1}}],["不同卷积层会提供不同层次的特征",{"2":{"247":1}}],["不同",{"2":{"180":1,"354":1,"976":1,"1590":1}}],["不同注意力头的效果",{"2":{"131":1}}],["不同论文提出了不同思路和方案",{"2":{"122":1}}],["不同的",{"2":{"2054":1}}],["不同的编译器和标准库可能以不同的方式实现",{"2":{"1678":1}}],["不同的线程可以并行执行",{"2":{"1563":1}}],["不同的训练参数",{"2":{"1396":1}}],["不同的后缀名",{"2":{"1260":1}}],["不同的优化器类之间其内容可能不同",{"2":{"1227":1}}],["不同的学习率在不同的时间效果最好",{"2":{"1171":1}}],["不同的随机初始化",{"2":{"1152":1}}],["不同的模型架构可能允许更大的batch",{"2":{"1132":1}}],["不同的输入样本有不同的均值和方差",{"2":{"808":1}}],["不同的距离度量",{"2":{"689":1}}],["不同的概念将是",{"2":{"628":1}}],["不同的id序列可能翻译出不同的句子",{"2":{"595":1}}],["不同的subword序列会产生完全不同的id序列表示",{"2":{"595":1}}],["不同的任务可能需要不同的分词粒度",{"2":{"568":1}}],["不同的任务和应用场景可能需要不同类型的mask操作",{"2":{"50":1}}],["不同的分词方式会导致模型对数值的理解差异",{"2":{"560":1}}],["不同的自然语言处理任务可能需要不同大小的词汇表",{"2":{"560":1}}],["不同的块中的标记",{"2":{"204":1}}],["不同的xixix",{"2":{"172":1}}],["不同的注意力头可以在不同的处理单元上并行计算",{"2":{"417":1}}],["不同的注意力头可以学习到序列中不同位置之间的不同依赖关系",{"2":{"9":1}}],["不同的注意力头起不同的作用",{"2":{"130":1}}],["不同的是swiglu采用两个权重矩阵和输入分别变换",{"2":{"109":1}}],["不同的序列长度并不一定会一致",{"2":{"53":1}}],["不同的头拿到相同的输入",{"2":{"34":1}}],["不同的q去查找不同方面的相关性",{"2":{"5":1}}],["不同头",{"2":{"9":1}}],["不加偏置项",{"2":{"8":1}}],["第七节课",{"0":{"1643":1},"1":{"1644":1,"1645":1,"1646":1,"1647":1,"1648":1,"1649":1,"1650":1}}],["第十课",{"0":{"1636":1},"1":{"1637":1,"1638":1,"1639":1,"1640":1,"1641":1}}],["第五部分",{"0":{"1765":1}}],["第五课",{"0":{"1626":1},"1":{"1627":1,"1628":1,"1629":1,"1630":1,"1631":1,"1632":1,"1633":1,"1634":1}}],["第五步是第二个残差连接和层归一化",{"2":{"519":1}}],["第2种",{"2":{"1489":1}}],["第2本书第6页",{"2":{"340":1}}],["第19次课",{"0":{"1760":1},"1":{"1761":1,"1762":1,"1763":1,"1764":1,"1765":1,"1766":1}}],["第12讲",{"0":{"1681":1},"1":{"1682":1,"1683":1,"1684":1,"1685":1,"1686":1,"1687":1,"1688":1}}],["第11次课",{"0":{"1658":1},"1":{"1659":1,"1660":1,"1661":1,"1662":1,"1663":1}}],["第1种",{"2":{"1489":1}}],["第10课",{"0":{"1652":1}}],["第1000次迭代结果",{"2":{"1396":1}}],["第100次迭代结果",{"2":{"1396":1}}],["第10次迭代结果",{"2":{"1396":1}}],["第1个通道平均值的数字",{"2":{"315":1}}],["第几次循环",{"2":{"1251":1}}],["第4项相对位置信息实际上只是一个只依赖于",{"2":{"762":1}}],["第i个token所能看得到的其他token是通过抽样的方式决定的",{"2":{"727":1}}],["第3步",{"0":{"465":1}}],["第",{"0":{"463":1,"464":1,"1692":1,"1697":1,"1711":1,"1717":1,"1767":1,"1794":1,"1809":1,"1827":1,"2057":1},"1":{"1693":1,"1694":1,"1695":1,"1698":1,"1699":1,"1700":1,"1701":1,"1712":1,"1713":1,"1714":1,"1715":1,"1718":1,"1719":1,"1720":1,"1721":1,"1722":1,"1723":1,"1724":1,"1725":1,"1768":1,"1769":1,"1770":1,"1771":1,"1772":1,"1773":1,"1774":1,"1775":1,"1776":1,"1777":1,"1778":1,"1779":1,"1780":1,"1781":1,"1782":1,"1783":1,"1784":1,"1785":1,"1786":1,"1787":1,"1788":1,"1789":1,"1790":1,"1791":1,"1792":1,"1793":1,"1795":1,"1796":1,"1797":1,"1798":1,"1799":1,"1800":1,"1801":1,"1802":1,"1803":1,"1804":1,"1805":1,"1806":1,"1807":1,"1808":1,"1810":1,"1811":1,"1812":1,"1813":1,"1814":1,"1815":1,"1816":1,"1817":1,"1818":1,"1819":1,"1820":1,"1821":1,"1822":1,"1823":1,"1824":1,"1825":1,"1826":1,"1828":1,"1829":1,"1830":1,"1831":1,"1832":1,"1833":1,"1834":1,"1835":1,"1836":1,"1837":1,"1838":1,"1839":1,"1840":1,"1841":1,"1842":1,"1843":1,"1844":1,"2058":1,"2059":1,"2060":1,"2061":1,"2062":1}}],["第六篇",{"2":{"233":1}}],["第二句",{"2":{"2054":1}}],["第二行的首地址",{"2":{"1705":1}}],["第二轮",{"0":{"1159":1},"2":{"1157":1,"1159":1}}],["第二阶段是做下游任务时",{"2":{"718":1}}],["第二点是如何确定新的多维向量的维度",{"2":{"684":1}}],["第二部分会详细讲解vim操作命令",{"2":{"1517":1}}],["第二部分",{"0":{"1622":1,"1729":1,"1762":1},"1":{"1623":1,"1624":1},"2":{"407":1,"429":1,"543":1}}],["第二版论文把resnet分成三个部分h",{"2":{"302":1}}],["第二次迭代",{"0":{"583":1}}],["第二次是",{"2":{"428":1,"532":1,"533":1}}],["第二次模型推理输出",{"2":{"239":1}}],["第二次推理则会根据任务类型来选择具体的tuner",{"2":{"225":1}}],["第二项",{"2":{"145":1,"1339":1}}],["第二层权重修改",{"2":{"1443":1}}],["第二层",{"2":{"1397":1}}],["第二层计算",{"0":{"1389":1},"2":{"1467":1}}],["第二层上是与罕见单词相关的聚类",{"2":{"185":1}}],["第二层可以认为是value",{"2":{"126":1}}],["第二层降维回归到输入维度",{"2":{"109":1}}],["第二层不使用激活函数",{"2":{"99":1}}],["第二个模型",{"2":{"1315":1}}],["第二个聊天会话继续",{"2":{"986":1}}],["第二个子层是一个简单的",{"2":{"914":1}}],["第二个子层执行交叉注意力操作",{"2":{"533":1}}],["第二个参数",{"2":{"770":1}}],["第二个layer",{"2":{"517":1}}],["第二个残差连接",{"2":{"517":1}}],["第二个函数是g函数",{"2":{"498":1}}],["第二个矩阵ff2ff2ff",{"2":{"485":1}}],["第二个线性变换将维度降回到",{"2":{"466":1}}],["第二个线性层则计算",{"2":{"118":1}}],["第二个线性层",{"2":{"99":1}}],["第二个block包含两层mlp",{"2":{"446":1}}],["第二个batch有60个字",{"2":{"398":1}}],["第二个和第三个输出词的概率分布中",{"2":{"398":1}}],["第二个",{"2":{"259":1,"1614":1}}],["第二个变体",{"2":{"231":1}}],["第二个句子的特征值是1",{"2":{"722":1}}],["第二个句子的",{"2":{"167":1,"259":1,"261":1}}],["第二个全连接层",{"2":{"113":1}}],["第二个为",{"2":{"80":1}}],["第二类叫做",{"2":{"84":1}}],["第二步是自注意力机制",{"2":{"519":1}}],["第二步",{"2":{"79":2}}],["第二遍从进攻球员角度看",{"2":{"5":1}}],["第三句或许在未来我也能够书写进去",{"2":{"2054":1}}],["第三句的",{"2":{"2054":1}}],["第三部分",{"0":{"1763":1}}],["第三节课",{"0":{"1610":1}}],["第三次课",{"0":{"1609":1},"1":{"1610":1,"1611":1,"1612":1,"1613":1,"1614":1,"1615":1,"1616":1}}],["第三次推理输出结束符号",{"2":{"239":1}}],["第三层计算",{"2":{"1467":1}}],["第三层权重修改",{"2":{"1443":1}}],["第三个子层是ffn",{"2":{"533":1}}],["第三步是残差连接和层归一化",{"2":{"519":1}}],["第三",{"2":{"222":1}}],["第三行说明时刻",{"2":{"74":1}}],["第三遍从防守球员角度看",{"2":{"5":1}}],["第一部分",{"0":{"1618":1,"1728":1,"1761":1},"1":{"1619":1,"1620":1,"1621":1}}],["第一行的首地址",{"2":{"1705":1}}],["第一行",{"2":{"1616":1}}],["第一行只有第一列是",{"2":{"74":1}}],["第一轮",{"0":{"1158":1},"2":{"1157":1}}],["第一轮模型推理输出",{"2":{"239":1}}],["第一轮模型的输入是",{"2":{"239":1}}],["第一种是self",{"2":{"727":1}}],["第一阶段是利用语言模型进行预训练",{"2":{"718":1}}],["第一点可以理解",{"2":{"684":1}}],["第一句中的",{"2":{"261":1}}],["第一次执行到定义语句时",{"2":{"1649":1}}],["第一次迭代",{"0":{"582":1}}],["第一次是",{"2":{"428":1}}],["第一次推理分析任务类型",{"2":{"225":1}}],["第一次把它放到了生产环境里",{"2":{"214":1}}],["第一项",{"2":{"145":1}}],["第一层计算",{"2":{"1467":1}}],["第一层权重修改",{"2":{"1443":1}}],["第一层求解",{"0":{"1388":1}}],["第一层的输出随后被用作第二层的输入",{"2":{"437":1}}],["第一层的输入是嵌入矩阵",{"2":{"437":1}}],["第一层的激活函数为",{"2":{"99":1}}],["第一层",{"2":{"185":1,"1397":1}}],["第一层升维",{"2":{"109":1}}],["第一类叫做",{"2":{"84":1}}],["第一个枚举常量的值为",{"2":{"1728":1}}],["第一个",{"0":{"1606":1},"2":{"1614":1}}],["第一个发射分数",{"2":{"1328":1}}],["第一个子层是一个multi",{"2":{"914":1}}],["第一个子层执行掩码多头自注意力计算",{"2":{"533":1}}],["第一个句子的特征值是0",{"2":{"722":1}}],["第一个单词是cls标志",{"2":{"722":1}}],["第一个模型继续按标准",{"2":{"1315":1}}],["第一个模型",{"2":{"635":1}}],["第一个解码器层的输入有两个",{"2":{"526":1}}],["第一个layer",{"2":{"517":1}}],["第一个残差连接",{"2":{"517":1}}],["第一个函数是",{"2":{"498":1}}],["第一个线性变换将输入的维度从512",{"2":{"466":1}}],["第一个线性层生成",{"2":{"118":1}}],["第一个线性层及激活函数组合",{"2":{"116":1}}],["第一个线性层通常会扩展输入的维度",{"2":{"99":1}}],["第一个线性层",{"2":{"99":1}}],["第一个block主要包含了token",{"2":{"446":1}}],["第一个batch有6个字",{"2":{"398":1}}],["第一个输出词的概率分布中",{"2":{"398":1}}],["第一个的34和第二个的78视为整体进行归一化处理",{"2":{"315":1}}],["第一个编码器层的qkv由输向量x组成的矩阵x进行线性变化而来",{"2":{"161":1}}],["第一个参数矩阵会得到记忆系数",{"2":{"128":1}}],["第一个全连接层",{"2":{"113":1}}],["第一个序列长度为",{"2":{"80":1}}],["第一个字只能与第一个字计算相关性",{"2":{"70":1}}],["第一步会用token",{"2":{"519":1}}],["第一步",{"2":{"79":2}}],["第一遍从总体上粗略看",{"2":{"5":1}}],["第四部分",{"0":{"1764":1}}],["第四课",{"0":{"1727":1},"1":{"1728":1,"1729":1}}],["第四步是ffn层",{"2":{"519":1}}],["第四个参数是",{"2":{"344":1}}],["第四个用来融合多头结果",{"2":{"23":1}}],["第四篇",{"2":{"292":1}}],["第四遍则综合之前的理解再总体看一遍",{"2":{"5":1}}],["一说到",{"2":{"2112":1}}],["一生",{"2":{"2056":1}}],["一看",{"2":{"2051":1}}],["一周的七天",{"2":{"1728":1}}],["一只未命名的小猫被创建了",{"2":{"1675":1}}],["一元减",{"2":{"1635":1}}],["一元加",{"2":{"1635":1}}],["一元加法",{"2":{"1083":3}}],["一一对应",{"2":{"1485":1}}],["一部分是第一项",{"2":{"1450":1}}],["一部分为当前位置的梯度",{"2":{"1036":1}}],["一下",{"2":{"1330":1}}],["一项研究将包括",{"2":{"1147":1}}],["一项研究指定了一组要运行的超参数配置以供后续分析",{"2":{"1144":1}}],["一份关于如何调试和如何减少训练失败的文档在两年前是不可能写出来的",{"2":{"1127":1}}],["一组实验的目标是比较目标超参数的不同值",{"2":{"1150":1}}],["一组仅使用tp",{"2":{"977":1}}],["一组键",{"2":{"154":1}}],["一对多",{"2":{"908":1}}],["一开始最好是用一个比较简单的工作流",{"2":{"1134":1}}],["一开始会完全是瞎预测",{"2":{"895":1}}],["一开始x为1",{"2":{"399":1}}],["一状态",{"2":{"864":1}}],["一直是机器翻译的巅峰技术",{"2":{"908":1}}],["一直是位置编码设计的重中之重",{"2":{"758":1,"766":1}}],["一直保持随机初始化的状态",{"2":{"296":1}}],["一小部分来自高质量标记数据的全面整合",{"2":{"725":1}}],["一词而引起的潜在混淆",{"2":{"1185":1}}],["一词就可以看出来其中精髓",{"2":{"717":1}}],["一词并不需要了解整个句子的上下文之后",{"2":{"257":1}}],["一句话最大长度是10个字",{"2":{"704":1}}],["一样增大",{"2":{"1159":1}}],["一样将输入数据进行",{"2":{"624":1}}],["一样运行",{"2":{"489":1}}],["一系列属性初始化",{"2":{"1214":1}}],["一系列方法具体情况",{"2":{"664":1}}],["一系列交替的",{"2":{"614":1}}],["一系列物品",{"2":{"163":1}}],["一共是四项注意力的组合",{"2":{"758":1}}],["一共6个token",{"2":{"585":1}}],["一共10个token",{"2":{"585":1}}],["一共10个子词",{"2":{"580":1}}],["一串文本就变成了一串整数组成的向量",{"2":{"545":1}}],["一问一答",{"2":{"536":1}}],["一位",{"2":{"528":1}}],["一面查看在组装过程中的玩具",{"2":{"524":1}}],["一起为了祖国的美好未来而尽心尽力那也是志同道合",{"2":{"2054":1}}],["一起贩卖毒品那也是志同道合",{"2":{"2054":1}}],["一起工作",{"2":{"976":1}}],["一起做标准化",{"2":{"318":1,"323":1,"326":1}}],["一起进行点乘操作",{"2":{"32":1}}],["一文搞懂transformer架构的三种注意力机制",{"2":{"292":1}}],["一文看懂",{"2":{"292":1}}],["一来cnn可以并行处理",{"2":{"290":1}}],["一来可以使得序列中的重要元素被投射以较高注意力",{"2":{"260":1}}],["一心二用容易出错",{"2":{"288":1}}],["一些常用的",{"2":{"1624":1}}],["一些优化算法",{"2":{"1223":1}}],["一些研究表明",{"2":{"838":1}}],["一些研究引入了bitfit或者lora作为peft模块",{"2":{"733":1}}],["一些具有专用神经元的特征随着规模增加而出现",{"2":{"477":1}}],["一些权重的目的是防止模型过分依赖某些特定的输入",{"2":{"394":1}}],["一些翻译结果并不能与英语中的词汇一一对应到",{"2":{"245":1}}],["一些注意力头的ov矩阵可以直接将",{"2":{"122":1}}],["一些注意力权重太低",{"2":{"18":1,"20":1}}],["一步步来",{"0":{"2104":1}}],["一步上结束",{"2":{"1149":1}}],["一步到位捕捉到全局的联系",{"2":{"291":1}}],["一步错",{"2":{"239":1}}],["一步就把所有目标单词预测出来",{"2":{"57":1}}],["一种模拟真实光线传播的渲染方法",{"2":{"2009":1}}],["一种经典的线性链crf的结构图如下",{"2":{"1322":1}}],["一种经典建模方法",{"2":{"694":1}}],["一种用于在运行时自动重用键值缓存的新技术",{"2":{"985":1}}],["一种自然的思路是",{"2":{"896":1}}],["一种自重建方法",{"2":{"727":1}}],["一种特殊结构的神经网络",{"2":{"850":1}}],["一种是逐元素操作",{"2":{"642":1}}],["一种是扩散建模",{"2":{"636":1}}],["一种是src",{"2":{"198":1,"199":1}}],["一种新颖的高效且可扩展的大型语言模型架构",{"2":{"156":1}}],["一是注意力模块同时具有历史和当前上下文",{"2":{"231":1}}],["一是准确的模型产生近似概率分布",{"2":{"185":1}}],["一是通过多样本并行计算能够加快网络的训练速度",{"2":{"57":1}}],["一方面在数值空间中将相近的向量推得更近",{"2":{"709":1}}],["一方面作为",{"2":{"485":1}}],["一方面",{"2":{"180":1,"1127":1,"1137":1}}],["一般认为",{"2":{"1339":1}}],["一般认为参数初始化需满足以下两个必要条件",{"2":{"998":1}}],["一般的认为它的缺点是没有外推性",{"2":{"1337":1}}],["一般说来",{"2":{"1312":1}}],["一般采用默认值0",{"2":{"1045":1}}],["一般初始化的权重为高斯或均匀分布中随机抽取的值",{"2":{"993":1}}],["一般初始化α为全1",{"2":{"319":1}}],["一般我们会进一步进行分解",{"2":{"908":1}}],["一般为",{"2":{"890":1}}],["一般要对数据做归一化",{"2":{"807":1}}],["一般就会从这里出发",{"2":{"758":1}}],["一般来说",{"2":{"707":1,"713":1,"745":1,"767":1,"1136":1,"1148":1,"1335":1}}],["一般来讲",{"2":{"396":1}}],["一般情况下数据的维度都是高维的",{"2":{"1370":1}}],["一般情况下",{"2":{"661":2,"1104":2,"1650":1}}],["一般是指许多指令得以同时进行的计算模式",{"2":{"1564":1}}],["一般是还原后的尺寸与输入图像一致",{"2":{"779":1}}],["一般是",{"2":{"379":1}}],["一般都会先做一个线性变换",{"2":{"172":1}}],["一般会在训练时让长度相近的样本出现在同一个batch中",{"2":{"88":1}}],["一",{"0":{"639":1,"1503":1,"1562":1,"1602":1,"1611":1,"1637":1,"1645":1,"1682":1,"1693":1,"1704":1,"1916":1},"1":{"640":1,"641":1,"642":1,"643":1,"644":1,"645":1,"646":1,"647":1,"648":1,"649":1,"650":1,"651":1,"652":1,"653":1,"654":1,"1504":1,"1505":1,"1506":1,"1507":1,"1508":1,"1509":1,"1510":1,"1511":1,"1512":1,"1513":1,"1514":1,"1515":1,"1516":1,"1517":1,"1518":1,"1519":1,"1520":1,"1521":1,"1522":1,"1523":1,"1524":1,"1525":1,"1526":1,"1527":1,"1528":1,"1529":1,"1530":1,"1531":1,"1532":1,"1533":1,"1534":1,"1535":1,"1536":1,"1537":1,"1538":1,"1539":1,"1563":1,"1564":1,"1565":1,"1566":1,"1683":1,"1684":1},"2":{"156":3,"429":1,"513":1,"740":1}}],["一旦声明",{"2":{"1802":1}}],["一旦初始化后",{"2":{"1614":1}}],["一旦有了这个外号",{"2":{"1612":1}}],["一旦绑定",{"2":{"1612":1}}],["一旦学习停滞不前",{"2":{"1245":1}}],["一旦训练周期数达到里程碑",{"2":{"1236":1}}],["一旦训练好效果会更优",{"2":{"331":1}}],["一旦考虑了所有这些因素带来的影响",{"2":{"1186":1}}],["一旦建立了不会破坏以",{"2":{"1183":1}}],["一旦给定的每次试验时间限制中产生了有用的见解",{"2":{"1157":1}}],["一旦随机分布选择不当",{"2":{"993":1}}],["一旦模型规模很大长度很长时",{"2":{"948":1,"978":1}}],["一旦我们完成了对",{"2":{"1153":1}}],["一旦我们回答了上述问题",{"2":{"1146":1}}],["一旦我们给出每一个维度的定义",{"2":{"712":1}}],["一旦我们计算出代表完整事实",{"2":{"145":1}}],["一旦得到embedding",{"2":{"676":1}}],["一旦词表特别大",{"2":{"184":1}}],["一旦计算出",{"2":{"145":1}}],["一旦检索的事实与",{"2":{"141":1}}],["一旦这类模式有用",{"2":{"12":1}}],["一条知识包括实体和它们之间的关系",{"2":{"136":1}}],["一类通用的序列可能不会收敛到秩为1的子空间",{"2":{"91":1}}],["一定是可以把词义所包含的信息全都囊括在内",{"2":{"712":1}}],["一定可以大致被装进一个几十维的子空间里",{"2":{"684":1}}],["一定程度上控制了词表大小和解决了稀疏字符难以训练的问题",{"2":{"608":1}}],["一定程度缓解了上文提到的计算",{"2":{"20":1}}],["一定要注意一些要点",{"2":{"1167":1}}],["一定要注意",{"2":{"79":1}}],["一次计算成功",{"2":{"971":1}}],["一次一个元素",{"2":{"912":1}}],["一次在",{"2":{"470":1}}],["一次在ffn之前",{"2":{"470":1}}],["一次在ffn层之后",{"2":{"468":1}}],["一次在自注意力机制之后",{"2":{"468":1}}],["一次关于glm4",{"2":{"429":1}}],["一次并行做5次推理来得到所有时间步的结果",{"2":{"408":1}}],["一次即可给出所有序列的预测",{"2":{"391":1}}],["一次返回一个batch对",{"2":{"385":1}}],["一次只需要一个向量就行了",{"2":{"382":1}}],["一次用于计算平均值",{"2":{"327":1}}],["一次性处理完毕",{"2":{"427":1}}],["一次性给到transformer",{"2":{"391":1}}],["一次性并行解码把各个位置上的预测全部输出",{"2":{"390":1}}],["一次性输入整个序列",{"2":{"390":1}}],["一次性输入给解码器就是问题所在",{"2":{"58":1}}],["一次性",{"2":{"222":1}}],["一次",{"2":{"7":1}}],["一场比赛要看四遍",{"2":{"5":1}}],["一个指向",{"2":{"2152":2}}],["一个整数",{"2":{"2150":1}}],["一个整数数组",{"2":{"2149":2}}],["一个十五岁的孩子",{"2":{"2054":1}}],["一个简易的makefile",{"2":{"1917":1}}],["一个简单的加法函数",{"2":{"1706":1}}],["一个简单的思路是训练神经网络并期望它能够记住自己的训练数据",{"2":{"230":1}}],["一个简单的记忆层可以用下面的等式来描述",{"2":{"154":1}}],["一个表示空指针的关键字",{"2":{"1897":1}}],["一个完整的类可以作为另一个类的友元类",{"2":{"1769":1}}],["一个处理用户订单的程序可以分解为获取用户输入",{"2":{"1729":1}}],["一个设计良好的函数可以在程序的多个地方被调用",{"2":{"1729":1}}],["一个设备从左侧开始处理第一个查询块",{"2":{"975":1}}],["一个很长的",{"2":{"1729":1}}],["一个浮点数表示成绩",{"2":{"1728":1}}],["一个类只能有一个析构函数",{"2":{"1676":1}}],["一个子类可以同时继承多个父类",{"2":{"1659":1}}],["一个变量的别名",{"2":{"1650":1}}],["一个存储内存地址的变量",{"2":{"1650":1}}],["一个更实际的例子",{"2":{"1645":1}}],["一个大小为",{"2":{"1623":1}}],["一个重要的里程碑版本",{"2":{"1603":1}}],["一个重置门",{"2":{"874":1}}],["一个主进程负责任务分发",{"2":{"1578":1}}],["一个较早的mpi实现",{"2":{"1569":1}}],["一个广泛使用的开源mpi实现",{"2":{"1569":1}}],["一个基于network",{"2":{"1568":1}}],["一个基于总线",{"2":{"1568":1}}],["一个进程可以包含多个线程",{"2":{"1563":1}}],["一个进程正在访问临界资源",{"2":{"1409":1}}],["一个程序运行起来就相当于一个进程",{"2":{"1563":1}}],["一个深度神经网络可以理解为一个复杂的复合函数",{"2":{"1442":1}}],["一个深度学习模型中的所有数据可划分为如下类别",{"2":{"1441":1}}],["一个典型的深度神经网络图如下",{"2":{"1441":1}}],["一个特定的候选变化最初取得了更好的验证误差",{"2":{"1152":1}}],["一个超参数是冗余还是固定超参数将取决于目标超参数的值",{"2":{"1143":1}}],["一个超参数是目标超参数",{"2":{"1143":1}}],["一个冗余超参数和目标超参数的相互影响越多",{"2":{"1143":1}}],["一个冗余超参数",{"2":{"1143":1}}],["一个好的经验法则",{"2":{"1133":1}}],["一个好的embedding应该对以下两个方面进行建模",{"2":{"689":1}}],["一个函数在执行的过程中",{"2":{"1646":1}}],["一个函数",{"2":{"1023":1}}],["一个函数的output",{"2":{"661":1,"1104":1}}],["一个损失函数",{"2":{"1021":1}}],["一个新的聊天会话开始",{"2":{"986":1}}],["一个新的提示到达",{"2":{"986":1}}],["一个批量的少样本学习查询和一个自一致性采样",{"2":{"986":1}}],["一个批次中包括了五句话",{"2":{"316":1}}],["一个批次中包含了",{"2":{"313":1}}],["一个批次的输入包括若干个独立的句子",{"2":{"316":1}}],["一个使用函数指针作为参数的函数",{"2":{"1706":1}}],["一个使用",{"2":{"976":1}}],["一个最直接的方法就是",{"2":{"908":1}}],["一个最简单的办法就是把这两个矩阵直接相乘",{"2":{"172":1}}],["一个箭头就表示对该向量做一次变换",{"2":{"878":1}}],["一个lstm",{"2":{"864":1}}],["一个向量空间进行一次线性变换并接上一个平移",{"2":{"838":1}}],["一个输出特征点需要多少输入数据和kernel参与运算",{"2":{"773":1}}],["一个输入可能包含",{"2":{"88":1}}],["一个潜在注意力层层后面接上一个mlp层",{"2":{"735":1}}],["一个潜在transformer步骤",{"2":{"613":1}}],["一个巨大的挑战是一个词在不同的情境中可以被使用",{"2":{"715":1}}],["一个词的意义可以被它所出现的上下文定义",{"2":{"713":1}}],["一个词所代表的事物与其他词所代表的事物之间能产生内在联系的往往有成百上千上万种之多",{"2":{"4":1}}],["一个比较容易想到的方法是",{"2":{"712":1}}],["一个概念往往对应于文本文档中的一个句子",{"2":{"628":1}}],["一个计算量较大的潜在transformer会处理patch表示",{"2":{"614":1}}],["一个轻量级的局部编码器",{"2":{"614":1}}],["一个从没有出现过的新单词被编码成为词表里的token",{"2":{"587":1}}],["一个被消解",{"2":{"582":1}}],["一个在开头有一个",{"2":{"579":1}}],["一个实用的建议是不断训练因果解码器",{"2":{"541":1}}],["一个用于ffn",{"2":{"523":1}}],["一个用于自注意力",{"2":{"523":1}}],["一个用于计算标准偏差",{"2":{"327":1}}],["一个encoderlayer就执行完毕",{"2":{"519":1}}],["一个transformer编码块做的事情如下图所示",{"2":{"519":1}}],["一个transformer架构的模型里",{"2":{"119":1}}],["一个句子进来",{"2":{"510":1}}],["一个句子中的文字先后顺序很重要",{"2":{"459":1}}],["一个固定大小的transformer模型理论上可以计算任何可计算函数",{"2":{"504":1}}],["一个能量函数的",{"2":{"499":1}}],["一个对每个token",{"2":{"461":1}}],["一个汉字",{"2":{"363":1,"548":1}}],["一个context可能存不下那么多信息",{"2":{"891":1}}],["一个c",{"2":{"341":1}}],["一个样本序列中可能会有一些",{"2":{"326":1}}],["一个样本是无法求出均值和方差的",{"2":{"316":1}}],["一个卷积核可以得到一个通道的特征图数据",{"2":{"325":1}}],["一个神经层的输入是之前神经层的输出",{"2":{"309":1}}],["一个是输出",{"2":{"1018":1}}],["一个是输入",{"2":{"344":1,"1018":1}}],["一个是是其他粒子对其作用",{"2":{"498":1}}],["一个是该粒子本身的运动",{"2":{"498":1}}],["一个是一个",{"2":{"344":1}}],["一个是memory",{"2":{"287":1}}],["一个是看书时候每个字都看",{"2":{"217":1}}],["一个字符串表示姓名",{"2":{"1728":1}}],["一个字词完全随着另一个字词的出现而紧跟着出现",{"2":{"582":1}}],["一个字",{"2":{"259":1}}],["一个问题是如何根据之前的隐变量",{"2":{"240":1}}],["一个点积操作中没有什么可以学的参数",{"2":{"172":1}}],["一个=0",{"2":{"170":1}}],["一个和",{"2":{"170":1}}],["一个序列",{"2":{"158":1}}],["一个",{"2":{"122":1,"164":1,"170":13,"204":1,"265":1,"267":3,"277":4,"323":2,"325":1,"334":1,"516":1,"547":1,"777":1,"1330":1,"1762":1}}],["一个包含优化选项默认值的字典",{"2":{"1225":1}}],["一个包含完整的反向传播钩子",{"2":{"1214":1}}],["一个包含特殊标记",{"2":{"557":1}}],["一个包含",{"2":{"88":1,"1225":1}}],["一个三维的张量",{"2":{"53":1}}],["一个学词义这样明显的区分",{"2":{"20":1}}],["一个团队合作完成一个软件项目",{"2":{"5":1}}],["实参",{"2":{"1729":1}}],["实参的别名",{"2":{"1650":1}}],["实参的地址",{"2":{"1650":1}}],["实参的值",{"2":{"1650":1}}],["实践代码组织的艺术",{"2":{"1729":1}}],["实践证明",{"2":{"1313":1}}],["实践",{"2":{"1200":1}}],["实践中",{"2":{"1029":1}}],["实践中我们不可能拥有无限的计算资源",{"2":{"116":1}}],["实例化",{"2":{"1699":1,"1700":1,"1701":1}}],["实例化数据集和数据加载器",{"2":{"1296":1}}],["实例化模型",{"2":{"1296":1}}],["实例化到",{"2":{"941":1,"960":1}}],["实例",{"2":{"1083":1,"1087":1}}],["实质上",{"2":{"408":1}}],["实时高保真物理模拟",{"2":{"2011":1}}],["实时风险计算引擎",{"2":{"1946":1}}],["实时交互系统开发",{"2":{"1936":1}}],["实时查看日志",{"2":{"1539":2}}],["实时查看进程资源使用",{"2":{"1522":1}}],["实时动态地适应新任务",{"2":{"225":1}}],["实时选择性地调整权重矩阵中的单一组件",{"2":{"218":1}}],["实验的注释或简短描述",{"2":{"1167":1}}],["实验配置存储位置的链接",{"2":{"1167":1}}],["实验名称",{"2":{"1167":1}}],["实验",{"2":{"214":1}}],["实施多机并行训练程序可能会引入错误和一些棘手的细节",{"2":{"1134":1}}],["实施过程如下图所示",{"2":{"765":1}}],["实施",{"2":{"765":1}}],["实施dropout之后",{"2":{"393":1}}],["实施知识擦除对于消除有偏见的以及有害的知识至关重要",{"2":{"140":1}}],["实施mask",{"0":{"67":1},"2":{"49":1}}],["实体类",{"2":{"1476":1}}],["实体知识和关系知识是可互换的",{"2":{"136":1}}],["实体和关系知识并不简单地存储在相同的位置或以相同的方式表示",{"2":{"136":1}}],["实体和关系知识可能以不同的方式存储和表示",{"2":{"136":1}}],["实体和关系应该是等价的",{"2":{"136":1}}],["实体",{"2":{"135":2}}],["实体的可能性越大",{"2":{"130":1}}],["实际应用场景讨论",{"0":{"1865":1},"1":{"1866":1,"1867":1,"1868":1,"1869":1}}],["实际参数的值会被复制一份传递给函数的形式参数",{"2":{"1729":1}}],["实际参数的数量",{"2":{"1729":1}}],["实际参数",{"2":{"1729":1}}],["实际参数列表",{"2":{"1729":2}}],["实际指向",{"2":{"1683":1}}],["实际地址增加",{"2":{"1633":1}}],["实际差异可能很小",{"2":{"1630":1}}],["实际大小可能因编译器和操作系统而异",{"2":{"1607":1}}],["实际项目",{"2":{"1596":1}}],["实际中",{"2":{"1343":1}}],["实际的正例个数",{"2":{"1331":1}}],["实际正例个数",{"2":{"1331":1}}],["实际为这些模型提供服务是具有挑战性的",{"2":{"980":1}}],["实际伪代码",{"2":{"963":1}}],["实际工程上的",{"0":{"930":1}}],["实际工程中",{"2":{"898":1}}],["实际场景中",{"2":{"878":1}}],["实际效果往往也更好",{"2":{"767":1}}],["实际变量打印如下",{"2":{"557":1}}],["实际操作中",{"2":{"457":1}}],["实际操作会将两种掩码合并",{"2":{"72":1}}],["实际是对特征的每一维统计所有样本的均值和方差",{"2":{"325":1}}],["实际是针对单个",{"2":{"323":1}}],["实际情况",{"0":{"53":1,"57":1},"2":{"49":2}}],["实际上传递的是指向数组首元素的指针",{"2":{"1667":1}}],["实际上等价于",{"2":{"1589":1}}],["实际上只是一个只依赖于",{"2":{"1340":1}}],["实际上我们把重心放在进一步理解问题上",{"2":{"1140":1}}],["实际上我们可以将one",{"2":{"700":1}}],["实际上是",{"2":{"1704":1}}],["实际上是一个",{"2":{"1667":1}}],["实际上是一并输入",{"2":{"427":1}}],["实际上是表达式",{"2":{"1113":1}}],["实际上是高维且稠密",{"2":{"676":1}}],["实际上是通过三重注意力机制建立起了序列内部以及序列之间的全局联系",{"2":{"515":1}}],["实际上是来自不同",{"2":{"5":1}}],["实际上解码器的输入是一个拼接",{"2":{"406":1}}],["实际上也可以被看作是一种稀疏性表现",{"2":{"393":1}}],["实际上lora在扩散模型算法的领域内是可以组合的",{"2":{"222":1}}],["实际上",{"2":{"172":1,"176":1,"222":1,"460":1,"694":1,"710":1,"899":1,"1110":1,"1133":1,"1175":1,"1465":1,"1667":1,"2107":1}}],["实际上去除了batch",{"2":{"88":1}}],["实际上在模仿海马及内嗅皮层处理信息的方式",{"2":{"490":1}}],["实际上在交叉自注意力中",{"2":{"78":1}}],["实际上在代码中会采用大矩阵的方式来进行",{"2":{"29":1}}],["实际上每一个注意力头的",{"2":{"31":1}}],["实际上多头的",{"2":{"30":1}}],["实际上线性层并没有针对多头做切分",{"2":{"30":1}}],["实际上代码实现的时候可以忽略concat",{"2":{"24":1}}],["实际上便是把",{"2":{"12":1}}],["实现实时光照",{"2":{"2009":1}}],["实现实体的识别和概念的绑定",{"2":{"510":1}}],["实现具体功能",{"2":{"1916":1}}],["实现减法运算",{"2":{"1916":1}}],["实现加法运算",{"2":{"1916":1}}],["实现泛型",{"2":{"1906":1}}],["实现泛型编程",{"0":{"1698":1}}],["实现一个简单的lambda函数来排序一个vector",{"2":{"1900":1}}],["实现继承",{"2":{"1868":1}}],["实现以下功能",{"2":{"1716":1}}],["实现以上计算方式的改变只需要在transformer的模型代码中做很少的代码改动",{"2":{"349":1}}],["实现间接调用和函数作为参数传递",{"2":{"1706":1}}],["实现它的深拷贝构造函数",{"2":{"1696":1}}],["实现动态绑定",{"2":{"1688":1}}],["实现数据封装",{"2":{"1677":1}}],["实现上述类的定义和成员函数",{"2":{"1664":1}}],["实现功能的覆盖",{"2":{"1654":1}}],["实现事件驱动编程",{"2":{"1645":1}}],["实现代码复用和模块化编程",{"2":{"1628":1}}],["实现链表等数据结构",{"2":{"1612":1}}],["实现华氏温度到摄氏温度的转换",{"2":{"1608":1}}],["实现的",{"2":{"1602":1}}],["实现的一个",{"2":{"216":1}}],["实现复信",{"2":{"1594":1}}],["实现同步也就是实现临界区进程",{"2":{"1414":1}}],["实现命名实体识别",{"0":{"1319":1},"1":{"1320":1,"1321":1,"1322":1,"1323":1,"1324":1,"1325":1,"1326":1}}],["实现比",{"2":{"1228":1}}],["实现比传统mlps更高的模型性能",{"2":{"155":1}}],["实现则在此基础上在垂直方向上进行融合",{"2":{"1228":1}}],["实现看作是在水平方向上进行融合",{"2":{"1228":1}}],["实现通常更快",{"2":{"1228":1}}],["实现可用",{"2":{"1228":1}}],["实现可能会在版本之间发生重大变化",{"2":{"1175":1}}],["实现可控的可预测的涌现",{"2":{"363":1}}],["实现并不总能正确处理每台设备的批次大小",{"2":{"1168":1}}],["实现绝对值运算符",{"2":{"1083":1}}],["实现负号运算符",{"2":{"1083":1}}],["实现正号运算符",{"2":{"1083":1}}],["实现正交性的一个简单方法是随机性",{"2":{"709":1}}],["实现右侧的位右移运算符",{"2":{"1083":1}}],["实现右侧的位左移运算符",{"2":{"1083":1}}],["实现右侧整数除法运算",{"2":{"1083":1}}],["实现就地幂运算",{"2":{"1083":1}}],["实现幂运算",{"2":{"1083":1}}],["实现逻辑",{"0":{"957":1}}],["实现细节",{"0":{"1090":1},"2":{"944":1,"963":1}}],["实现举例",{"0":{"890":1}}],["实现原理",{"2":{"816":1}}],["实现更特定的功能",{"2":{"1868":1}}],["实现更高效的并行计算",{"2":{"795":1}}],["实现更精确的推理",{"2":{"480":1}}],["实现简单",{"2":{"746":1}}],["实现对数字进行了显著的调整",{"2":{"595":1}}],["实现多个编码层堆叠起来的效果",{"2":{"532":1}}],["实现概念和概念的一个全连接图",{"2":{"510":1}}],["实现warmup",{"2":{"385":1}}],["实现自适应",{"2":{"222":1}}],["实现了细粒度的表情生成与动画控制",{"2":{"2011":1}}],["实现了相对位置编码的效果",{"2":{"767":1}}],["实现了单向注意力先计算右乘",{"2":{"216":1}}],["实现了接近90",{"2":{"111":1}}],["实现全连接层所需的特征变换功能",{"2":{"153":1}}],["实现",{"0":{"22":1,"64":1,"73":1,"110":1,"112":1,"171":1,"196":1,"342":1,"360":1,"410":1,"521":1,"531":1,"699":1,"1046":1,"1050":1,"1054":1,"1061":1,"2014":1},"1":{"23":1,"24":1,"25":1,"26":1,"27":1,"28":1,"29":1,"30":1,"31":1,"32":1,"33":1,"34":1,"35":1,"36":1,"37":1,"38":1,"39":1,"65":1,"66":1,"67":1,"74":1,"75":1,"113":1,"114":1,"172":1,"173":1,"174":1,"175":1,"176":1,"177":1,"178":1,"179":1,"180":1,"181":1,"182":1,"183":1,"184":1,"185":1,"186":1,"187":1,"188":1,"189":1,"190":1,"191":1,"192":1,"193":1,"194":1,"195":1,"197":1,"198":1,"199":1,"200":1,"201":1,"343":1,"344":1,"522":1,"523":1,"532":1,"533":1,"700":1,"701":1,"702":1,"703":1,"704":1,"2015":1,"2016":1,"2017":1,"2018":1,"2019":1,"2020":1,"2021":1,"2022":1,"2023":1,"2024":1,"2025":1,"2026":1,"2027":1,"2028":1,"2029":1,"2030":1},"2":{"0":1,"49":2,"96":2,"157":2,"293":2,"429":1,"801":1,"802":1,"804":1,"807":1,"808":1,"809":1,"810":1,"814":1,"815":1,"816":1,"819":1,"820":1,"821":1,"822":1,"825":1,"827":1,"828":1,"829":1,"831":1,"834":1,"835":1,"839":2,"840":1,"841":1,"842":3,"843":2,"844":1,"845":1,"846":1,"847":1,"1234":1,"1589":1,"1656":1}}],["个地方",{"2":{"2131":1}}],["个地方用到了",{"2":{"37":1}}],["个人参悟",{"0":{"2054":1}}],["个人感悟",{"2":{"2046":1}}],["个人提升",{"0":{"2039":1}}],["个人能力与解决问题之间的差距在哪里",{"2":{"1598":1}}],["个电话本的类",{"2":{"1808":1}}],["个整数",{"2":{"1716":1}}],["个整数的动态数组",{"2":{"1714":1}}],["个整数的数组的指针",{"2":{"1705":1}}],["个整数的数组",{"2":{"1623":2,"1668":1,"1714":1}}],["个双精度浮点数的数组",{"2":{"1623":1}}],["个字节",{"2":{"1607":9,"1611":2}}],["个字符到",{"2":{"1813":2,"1831":2}}],["个字符的字符数组",{"2":{"1704":1}}],["个字符",{"2":{"57":1,"340":1,"1813":4,"1816":1,"1821":2,"1831":4,"1834":1,"1839":2}}],["个字符去预测第",{"2":{"57":1}}],["个其他神经元传递过来的输入信号",{"2":{"1459":1}}],["个分量",{"2":{"1336":1}}],["个候选项中",{"2":{"1330":1}}],["个词来预测下一个词",{"2":{"1312":1}}],["个词分别作为叶子节点",{"2":{"184":1}}],["个训练周期中没有改善",{"2":{"1245":1}}],["个周期",{"2":{"1235":1}}],["个最佳检查点",{"2":{"1166":2}}],["个可调超参数",{"2":{"1130":1}}],["个可学习的",{"2":{"621":1}}],["个输出",{"2":{"1110":1}}],["个输出的softmax",{"2":{"1016":1}}],["个元素在它应处的地方",{"2":{"1752":1}}],["个元素是最小的",{"2":{"1751":1}}],["个元素",{"2":{"957":1,"1623":1,"1633":2}}],["个中间值",{"2":{"946":1,"966":1}}],["个blocks",{"2":{"944":5}}],["个batch",{"2":{"383":1}}],["个elements",{"2":{"944":1,"963":1}}],["个完全相同的层堆叠而成",{"2":{"914":1,"915":1}}],["个时间步",{"2":{"855":1}}],["个时刻的输入",{"2":{"172":1}}],["个时刻的时候只能使用",{"2":{"58":1}}],["个方向进行滑动",{"2":{"783":1}}],["个空格",{"2":{"778":1}}],["个通道的结果",{"2":{"776":1}}],["个通道",{"2":{"776":1}}],["个通道作卷积计算",{"2":{"776":1}}],["个卷积核分别对输入层的",{"2":{"776":1}}],["个head",{"2":{"765":1}}],["个唯一",{"2":{"595":1}}],["个解码层堆叠的栈",{"2":{"532":1}}],["个神经元来施加探针来看它们的分类性能",{"2":{"477":1}}],["个神经元就能表示数以万计的特征",{"2":{"137":1}}],["个参数都是从这个分布里面采样",{"2":{"1003":1}}],["个参数",{"2":{"344":1}}],["个样本计算这些归一化统计数据在实际应用中效果更好",{"2":{"1168":1}}],["个样本第1个通道来求平均",{"2":{"315":1}}],["个样本",{"2":{"315":1}}],["个样本的第",{"2":{"313":2}}],["个切片的统计数据",{"2":{"315":1}}],["个具有",{"2":{"313":1}}],["个具有独立语义逻辑的在不同子空间上小",{"2":{"27":1}}],["个实数的向量",{"2":{"178":1}}],["个不同的地址",{"2":{"1611":2}}],["个不同的单词",{"2":{"698":1}}],["个不同的",{"2":{"135":1}}],["个不同的线程中运行计算",{"2":{"34":1}}],["个矩阵",{"2":{"89":1}}],["个短样本",{"2":{"88":1}}],["个长的样本",{"2":{"88":1}}],["个节点分组为一个流水线预填节点组",{"2":{"977":1}}],["个节点",{"2":{"184":1}}],["个节点的全连接网络",{"2":{"28":1}}],["个节点到d",{"2":{"28":1}}],["个节点到",{"2":{"28":1}}],["个块",{"2":{"20":1}}],["个头文件",{"2":{"1918":1}}],["个头乘以",{"2":{"20":1}}],["个头",{"2":{"20":1}}],["个向量的线性组合来分析梯度",{"2":{"485":1}}],["个向量",{"2":{"16":1,"1335":1}}],["个细分的",{"2":{"12":1}}],["个小的语义逻辑子空间有机地整合成一个总体的",{"2":{"10":1}}],["个",{"2":{"5":1,"7":1,"10":1,"17":1,"20":1,"89":3,"135":1,"402":2,"420":1,"1341":2,"1364":3,"1589":1,"1647":1,"1668":1,"1705":5,"1713":1,"1726":1,"1751":1,"2139":3}}],["个数",{"2":{"5":1,"17":1,"1086":1}}],["lvalue",{"2":{"1629":1}}],["lh",{"2":{"1509":1}}],["lbrace",{"2":{"1343":1}}],["lgamma",{"2":{"1087":2}}],["lglgl",{"2":{"614":1}}],["l+1",{"2":{"1004":2}}],["lc×l",{"2":{"1004":1}}],["lcm又来了",{"2":{"638":1}}],["lcm在半监督任务上学习",{"2":{"632":1}}],["lcm在sonar嵌入空间中进行操作",{"2":{"629":1}}],["lcm配备了",{"2":{"632":1}}],["lcm既不掌握输入语言或模态的信息",{"2":{"629":1}}],["lcm将句子视为一个概念单元",{"2":{"628":1}}],["lcm将token抛弃",{"2":{"627":1}}],["lcm的核心组件是句子嵌入模型sonar",{"2":{"629":1}}],["lcm的核心在于它不再执着于预测下一个词",{"2":{"628":1}}],["lcm的目标是预测下一个句子的嵌入向量",{"2":{"628":1}}],["lcm的出现",{"2":{"627":1}}],["lcms能够在概念层面进行推理",{"2":{"629":1}}],["lcms不再执着于",{"2":{"627":1}}],["lcms",{"2":{"625":1}}],["lcm",{"0":{"625":1,"632":1,"633":1,"634":1,"635":1,"636":1},"1":{"626":1,"627":1,"628":1,"629":1,"630":1,"631":1,"632":1,"633":1,"634":1,"635":1,"636":1},"2":{"628":1,"629":2,"632":1,"1087":2}}],["l​j​​",{"2":{"970":1,"971":1}}],["l​j​​=m​j​​+log",{"2":{"970":1,"971":1}}],["l^",{"2":{"970":2,"971":2}}],["lj",{"2":{"970":1,"971":1}}],["lj=mj+log",{"2":{"970":1,"971":1}}],["l=2",{"2":{"778":1}}],["l=1",{"2":{"778":1}}],["ldflags",{"2":{"1917":2}}],["ldexp",{"2":{"1087":2}}],["ld​c​​∗l",{"2":{"957":1}}],["lda",{"2":{"711":1}}],["ldots",{"2":{"178":2,"899":1,"903":3,"927":1,"943":1,"961":1,"1322":4,"1323":4,"1343":1}}],["l20031220",{"2":{"1481":1}}],["l2",{"2":{"692":1,"702":1,"1098":3,"1099":1,"1179":3,"1386":1}}],["lε≪lglε≪lg",{"2":{"614":1}}],["ly",{"2":{"567":2}}],["lyaofu",{"2":{"233":1}}],["l表示层数",{"2":{"335":1}}],["l表示矩阵k的第lll列对于q的重要性",{"2":{"264":1}}],["ln不依赖于batch的大小和输入sequence的深度",{"2":{"808":1}}],["ln中同层神经元输入拥有相同的均值和方差",{"2":{"808":1}}],["ln一般只用于rnn的场景下",{"2":{"808":1}}],["ln一般用在第三维度",{"2":{"326":1}}],["ln层通过类似于tanh的s形曲线将其输入映射到输出",{"2":{"359":1}}],["ln的区别",{"2":{"808":1}}],["ln的模型设计",{"2":{"349":1}}],["ln的训练稳定性",{"2":{"347":1}}],["ln的良好性能及pre",{"2":{"347":1}}],["ln的输入是每一层神经元的输入",{"2":{"319":1}}],["ln重要的两个部分是平移不变性和缩放不变性",{"2":{"346":1}}],["ln会固定一句话",{"2":{"341":1}}],["ln相对稳定",{"2":{"333":1}}],["ln基本不起作用",{"2":{"326":1}}],["ln是以列为单位",{"2":{"323":1}}],["ln是在每个样本内部",{"2":{"322":1}}],["ln则在通道",{"2":{"321":1}}],["ln",{"2":{"320":7,"322":1,"323":1,"326":1,"329":3,"334":4,"335":2,"338":1,"340":1,"341":1,"347":1,"349":2,"361":2,"503":2,"523":2,"808":4,"846":3}}],["ln以及其计算退化版本",{"2":{"320":1}}],["ln对模型训练的影响也有两点",{"2":{"320":1}}],["ln实际是针对单个",{"2":{"318":1}}],["ln和bn的差异",{"0":{"321":1},"1":{"322":1,"323":1,"324":1,"325":1,"326":1,"327":1},"2":{"293":1}}],["lu",{"2":{"1083":4,"1087":3}}],["lut",{"2":{"701":2}}],["luke",{"2":{"638":2}}],["luong模型里单独使用了一个隐状态~sts~t",{"2":{"285":1}}],["luong使用多种对齐函数",{"2":{"285":1}}],["luong",{"0":{"285":1},"2":{"285":4,"292":4,"892":2,"910":2}}],["lunch",{"2":{"233":1}}],["l是矩阵kkk的第lll列",{"2":{"268":1}}],["l是seq长度",{"2":{"113":1}}],["lsm",{"2":{"1952":1}}],["ls",{"2":{"1507":2,"1509":4,"1513":1}}],["lstsq",{"2":{"1083":1}}],["lstm的核心概念是cell",{"2":{"863":1}}],["lstm的假设无法成立",{"2":{"287":1}}],["lstm的假设是",{"2":{"287":1}}],["lstm适合于处理和预测时间序列中间隔和延迟非常长的重要事件",{"2":{"862":1}}],["lstm在循环压缩下的记忆序列能力存在疑问",{"2":{"287":1}}],["lstm",{"0":{"862":1,"863":1,"864":1,"870":1,"871":1,"873":1},"1":{"863":1,"864":1,"865":2,"866":2,"867":2,"868":2,"869":2,"870":1,"871":1},"2":{"249":1,"287":1,"334":2,"861":1,"862":1,"870":1,"1312":1}}],["lsa",{"2":{"711":1}}],["lshift",{"2":{"1085":3}}],["lsh",{"2":{"153":3,"204":1}}],["l的的点积的方差为dkdkd",{"2":{"189":1}}],["l中每个元素都是正态分布",{"2":{"189":1}}],["lps由于其特征学习能力而较少受到维数灾难的影响",{"2":{"155":1}}],["l1",{"2":{"692":1,"1386":1}}],["l195",{"2":{"361":1}}],["l15h0",{"2":{"130":1}}],["l14h7",{"2":{"130":1}}],["l14h13",{"2":{"130":1}}],["l18h14代表着第18层的第14个注意力头",{"2":{"130":1}}],["llvm",{"2":{"1960":1,"1961":1}}],["lll",{"2":{"943":1,"961":1,"970":1}}],["ll",{"2":{"614":2,"957":1}}],["ll|",{"2":{"571":1}}],["llama长度外推高性价比trick",{"2":{"768":1}}],["llama2",{"2":{"561":1}}],["llamarmsnorm",{"2":{"346":2}}],["llama",{"0":{"1345":1,"1381":1,"1382":1,"1383":1,"1384":1},"2":{"233":1,"346":1,"361":2,"501":1,"513":1,"569":3,"595":3,"768":1,"980":1,"1341":1,"1404":1}}],["llama和mistral的架构表现要比gpt",{"2":{"147":1}}],["llamamlp",{"2":{"110":1}}],["llama3示例",{"0":{"570":1},"1":{"571":1,"572":1,"573":1}}],["llama3的实现如下",{"2":{"114":1,"346":1}}],["llama3",{"0":{"114":1,"201":1},"2":{"96":1,"157":1,"361":1}}],["llm承诺从根本上改变我们在所有行业中使用人工智能的方式",{"2":{"980":1}}],["llm论文日更",{"2":{"740":1}}],["llm以生成高质量text",{"2":{"740":1}}],["llm最后一层的隐状态作为query",{"2":{"735":1}}],["llm看不到未来信息的机制可能会损害文本表征的质量",{"2":{"734":1}}],["llm2vec的第一步是将decoder",{"2":{"734":1}}],["llm2vec虽然涉及两个训练任务",{"2":{"734":1}}],["llm2vec包括修改注意力机制以及两个无监督训练任务",{"2":{"734":1}}],["llm2vec不需要高昂成本的标注训练数据就可以在不增加输入长度的情况下",{"2":{"734":1}}],["llm2vec是一种能将任何decoder",{"2":{"734":1}}],["llm2vec",{"0":{"734":1},"2":{"732":1,"740":1}}],["llm无需额外操作即可全部转换至双向注意力",{"2":{"732":1}}],["llm使用的单向注意力",{"2":{"732":1}}],["llm让靠后的token权重更大",{"2":{"731":1}}],["llm本身的表征直接用于embedding",{"2":{"711":1}}],["llm时代",{"2":{"711":1}}],["llm时代transformer中的positional",{"2":{"638":1}}],["llm还没研究透",{"2":{"638":1}}],["llm提示范式的第一个研究",{"2":{"513":1}}],["llm的所有输入token产生它们的注意力key",{"2":{"981":1}}],["llm的因果注意掩码",{"2":{"735":1}}],["llm的的单向注意力机制改成双向注意力机制",{"2":{"734":1}}],["llm的token粒度其实并不是一个好的表达语义的方式",{"2":{"626":1}}],["llm的prompt竟然是图灵完备的",{"2":{"513":1}}],["llm的几种并行机制",{"2":{"429":1}}],["llm计算的第一部分从词元嵌入矩阵中提取每个词元的相关行",{"2":{"463":1}}],["llm是自回归模型",{"2":{"390":1}}],["llm被注入足够的信息量",{"2":{"386":1}}],["llm推理加速2",{"2":{"361":1}}],["llm注意力attention",{"2":{"233":1}}],["llmforeverybody",{"2":{"156":1}}],["llm对知识的存储能力符合",{"2":{"147":1}}],["llm在处理长上下文",{"2":{"976":1}}],["llm在理解和生成自然语言方面表现出了非凡的能力",{"2":{"138":1}}],["llm在自己构造的高维语言空间中",{"2":{"116":1}}],["llm返回的正确答案应该是",{"2":{"122":1}}],["llm学习",{"2":{"121":1}}],["llm展现出了令人惊叹的能力",{"2":{"121":1}}],["llms的最终或中间隐藏状态",{"2":{"738":1}}],["llms",{"2":{"90":1,"121":3,"123":2,"136":1,"138":2,"139":4,"140":10,"141":2,"218":1,"233":2,"429":1,"561":4,"625":1,"735":1,"768":1}}],["llm",{"0":{"728":1},"1":{"729":1,"730":1,"731":1,"732":1,"733":1},"2":{"47":1,"95":2,"111":1,"121":1,"137":1,"138":2,"141":3,"154":1,"221":3,"222":7,"225":1,"233":1,"335":1,"387":3,"425":1,"429":4,"460":1,"477":2,"482":2,"504":1,"513":2,"542":2,"560":1,"595":2,"638":1,"737":1,"740":2,"985":4,"986":2}}],["less",{"2":{"1087":8,"1510":2}}],["lesswrong",{"2":{"513":1}}],["le",{"2":{"1085":1,"1087":4}}],["leq",{"2":{"844":1}}],["lexical",{"2":{"740":1}}],["lexicographically",{"2":{"557":1}}],["lemmatization",{"2":{"552":1}}],["let",{"2":{"513":1,"592":1,"1254":2}}],["lec",{"2":{"490":1}}],["leonyi",{"2":{"429":1}}],["leviathan",{"2":{"429":1}}],["level",{"2":{"89":1,"156":1,"605":2,"638":1,"2009":1}}],["legacy",{"2":{"395":1}}],["leibler",{"2":{"1377":1}}],["leiter",{"2":{"370":1}}],["lei",{"2":{"361":1}}],["lerp",{"2":{"354":1,"1087":4}}],["leading",{"2":{"2087":1}}],["least",{"2":{"985":1,"1737":1,"1738":1}}],["leak",{"2":{"1647":1}}],["leakeyrelu",{"2":{"842":1}}],["leakyrelu",{"2":{"110":1,"842":1,"1205":2}}],["leaky",{"2":{"110":1,"838":1,"842":2}}],["leaf",{"2":{"591":1,"661":1,"1087":1,"1104":1}}],["learnable",{"0":{"1337":1},"2":{"807":2,"809":2}}],["learn",{"2":{"437":1,"513":1,"542":1,"543":1,"638":2,"2073":1}}],["learns",{"2":{"399":1}}],["learning",{"0":{"283":1,"667":1,"1229":1,"1232":1,"1404":1},"2":{"137":1,"139":1,"142":1,"156":1,"226":1,"230":1,"233":2,"237":1,"257":1,"263":1,"282":1,"283":1,"284":1,"290":2,"292":7,"294":1,"350":1,"411":1,"429":1,"513":2,"543":2,"706":1,"726":1,"727":1,"734":2,"740":4,"747":1,"748":1,"768":1,"985":1,"1023":1,"1036":1,"1046":1,"1054":1,"1067":1,"1143":3,"1144":2,"1183":10,"1195":1,"1196":1,"1215":2,"1218":1,"1221":1,"1229":1,"1239":1,"1241":1,"1242":1,"1243":1,"1303":2,"1308":2,"1312":2,"1316":1,"1332":1,"1340":1,"1404":3,"1455":3,"1456":1,"1472":1,"2049":2,"2073":1,"2078":1,"2079":1,"2081":1,"2083":1,"2086":1,"2087":2}}],["learned",{"2":{"131":1,"749":1}}],["left",{"2":{"106":2,"344":1,"387":1,"523":1,"943":1,"961":1,"1003":1,"1004":1,"1087":5,"1180":1,"1184":2,"1254":4,"1343":1,"1817":1,"1835":1}}],["len维度的并行",{"2":{"972":1}}],["len大于了定义的seq",{"2":{"665":1}}],["len进行切割拼接到batch上吗",{"2":{"656":1}}],["len时候使用mask然后想问的另一个问题",{"2":{"656":1}}],["len时",{"2":{"656":1}}],["len加1",{"2":{"530":1}}],["len代表句子最长的长度",{"2":{"380":1}}],["len个语义向量",{"2":{"267":1}}],["lens的思路和原理是",{"2":{"482":1}}],["lens的作用是通过将",{"2":{"482":1}}],["lens的原理非常简单",{"2":{"147":1}}],["lens把lm",{"2":{"475":1}}],["lens应用于梯度矩阵",{"2":{"148":1,"485":1}}],["lens通过直接将特定神经元或层的输出与unembedding矩阵相乘",{"2":{"147":1}}],["lens",{"2":{"147":1,"148":1,"156":1,"437":1,"475":1,"482":1,"483":1,"513":3}}],["len=max",{"2":{"83":1,"424":1}}],["lenweight",{"2":{"71":1}}],["len∑j=1weight",{"2":{"71":1}}],["len",{"2":{"25":1,"28":2,"30":2,"31":2,"34":6,"35":3,"36":21,"65":1,"66":2,"67":2,"70":2,"71":4,"76":7,"83":5,"84":14,"201":11,"326":2,"346":1,"372":2,"375":1,"380":1,"382":3,"384":3,"423":2,"424":3,"450":3,"503":13,"518":2,"520":11,"522":1,"530":15,"532":1,"533":1,"557":2,"571":1,"572":1,"590":2,"591":1,"592":1,"665":1,"703":2,"704":2,"723":1,"945":2,"965":2,"1083":1,"1215":6,"1216":7,"1217":15,"1226":1,"1244":1,"1250":4,"1254":2,"1273":2,"1280":1,"1295":3,"1331":4,"1345":12,"1350":4,"1566":1,"1710":9,"1713":1,"2153":2}}],["len是句子长度",{"2":{"23":1}}],["length的实验",{"2":{"1157":1}}],["length时",{"2":{"1157":1}}],["length是6",{"2":{"460":1}}],["length是句子长度",{"2":{"198":1}}],["length拆开计算再汇总的做法",{"2":{"420":1}}],["lengths",{"2":{"233":1}}],["length维度拆开",{"2":{"420":1}}],["length维度",{"2":{"180":1}}],["length",{"2":{"17":1,"66":1,"76":6,"79":3,"89":3,"90":2,"160":1,"170":1,"198":5,"199":7,"271":1,"326":3,"420":2,"451":1,"460":2,"700":5,"723":7,"749":1,"760":1,"765":1,"768":3,"808":2,"974":5,"1086":1,"1087":6,"1157":1,"1217":5,"1218":11,"1328":5,"1329":4,"1330":9,"1332":1,"1566":1,"1624":1,"1708":3,"1713":2,"1715":1,"1729":1,"1774":4,"1789":5,"1791":3,"1792":5,"1914":2}}],["lmpi",{"2":{"1589":1}}],["lm可以通过目标语言的语料进行训练",{"2":{"908":1}}],["lmort使用对齐均匀性指标从各层挑出一些合适的h",{"2":{"731":1}}],["lm的一个",{"2":{"485":1}}],["lm展开后",{"2":{"295":1}}],["lms概念对齐",{"2":{"156":1}}],["lm",{"2":{"89":1,"147":2,"148":3,"387":1,"484":2,"485":1,"569":1,"638":1,"894":2,"895":1,"898":1,"908":1}}],["lru",{"2":{"985":1,"986":3}}],["lr=initial",{"2":{"1243":1}}],["lr=base​l​​r∗",{"2":{"1240":1}}],["lr=baselr∗",{"2":{"1240":1}}],["lr=1e",{"2":{"1222":1}}],["lr=learning",{"2":{"1218":1}}],["lr=args",{"2":{"1215":2}}],["lr=config",{"2":{"423":1}}],["lr=0",{"2":{"83":1,"402":1,"424":1,"1039":1,"1202":1,"1205":6,"1215":1,"1221":2,"1231":2,"1239":1,"1241":3,"1242":2,"1245":1,"1266":1,"1267":1,"1296":1,"2086":1}}],["lrate",{"2":{"402":1}}],["lrate=dmodel−0",{"2":{"402":1}}],["lrate=d−0",{"2":{"402":1}}],["lr",{"0":{"1230":1,"1233":1,"1234":1,"1235":1,"1236":1,"1237":1,"1238":1,"1239":1,"1240":1,"1241":1,"1242":1,"1243":1,"1244":1,"1246":1,"1247":1},"1":{"1231":1,"1232":1,"1233":1,"1234":1,"1235":1,"1236":1,"1237":1,"1238":1,"1239":1,"1240":1,"1241":1,"1242":1,"1243":1,"1244":1,"1245":1,"1246":1,"1247":1,"1248":1},"2":{"83":3,"364":1,"372":1,"385":3,"399":1,"402":4,"423":4,"424":3,"667":1,"1159":1,"1173":4,"1179":4,"1215":5,"1222":1,"1230":2,"1231":1,"1233":1,"1234":1,"1235":5,"1236":5,"1237":7,"1238":7,"1239":3,"1240":8,"1241":1,"1242":1,"1243":10,"1246":6,"1247":6,"1248":1,"1303":1,"1308":5,"1398":3,"1404":1,"1440":3}}],["lt",{"2":{"46":1,"80":6,"164":2,"265":1,"267":4,"408":1,"510":1,"579":1,"588":1,"612":5,"613":1,"838":1,"840":2,"976":1,"1085":1,"1087":4,"1524":1,"1551":1,"1559":1,"1604":1,"1607":14,"1619":2,"1624":1,"1628":2,"1629":4,"1630":8,"1633":2,"1635":6,"1673":2,"1678":2,"1695":1,"1699":1,"1700":4,"1701":1,"1712":6,"1715":1,"1725":1,"1729":3,"1789":2,"1811":1,"1816":2,"1817":1,"1819":1,"1820":2,"1823":1,"1824":2,"1829":1,"1834":2,"1835":1,"1837":1,"1838":2,"1841":1,"1842":2,"1894":3,"1912":1,"1930":1,"2014":2,"2019":2,"2031":6,"2059":2}}],["limit",{"2":{"1874":2}}],["limit=",{"2":{"1284":1}}],["limits>",{"2":{"1814":1,"1832":1}}],["limits",{"2":{"768":1,"1155":1,"1340":1,"1814":2,"1832":2}}],["libraries",{"0":{"1975":1,"2079":1},"2":{"1975":1,"1980":1,"1991":1,"1999":1,"2079":2}}],["library",{"0":{"1930":1,"1974":1},"2":{"796":1,"1254":1,"1920":1,"1974":1,"1981":1,"1999":1,"2021":1}}],["lib",{"2":{"1078":1,"1589":1,"1997":4,"1999":16}}],["liberty在博客中这样写道",{"2":{"696":1}}],["liu",{"2":{"768":1,"2021":1,"2034":1,"2111":1}}],["lisa",{"2":{"638":1}}],["lists",{"2":{"2077":1}}],["list>",{"2":{"1720":4,"1721":4,"1799":1,"1801":1,"1914":1}}],["listdir",{"2":{"1250":1}}],["listenviolet",{"2":{"233":1}}],["list",{"0":{"1720":1,"1721":1,"1799":1,"1801":1},"2":{"65":3,"384":6,"399":1,"557":2,"558":4,"572":5,"573":5,"590":2,"591":2,"592":6,"700":1,"1069":1,"1082":1,"1085":2,"1087":22,"1099":2,"1226":4,"1227":1,"1273":4,"1330":3,"1481":4,"1485":1,"1489":2,"1720":17,"1721":15,"1795":2,"1799":3,"1800":1,"1801":3,"1914":2,"1925":3}}],["lite",{"2":{"740":2}}],["literal",{"2":{"572":2,"1087":2,"1929":1}}],["little",{"2":{"370":1}}],["lie",{"2":{"498":1}}],["lipschitz",{"2":{"497":1}}],["like",{"2":{"395":1,"405":4,"407":2,"1071":3,"1095":2,"1098":3,"1101":3,"1254":1,"1345":1,"1489":2,"2077":1,"2079":2,"2082":1}}],["likes",{"2":{"349":1,"361":1}}],["lightning",{"0":{"216":1,"217":1},"2":{"157":2,"214":1,"215":1,"216":5,"217":6,"233":1}}],["lifearchitect",{"2":{"387":1}}],["lifelong",{"2":{"143":1}}],["lifting",{"2":{"20":1,"47":1}}],["liang",{"2":{"95":1}}],["linux虚拟机系统",{"2":{"2089":1}}],["linux等",{"2":{"2001":1}}],["linux内核源码理解能力",{"2":{"1942":1}}],["linux基础",{"2":{"1599":1}}],["linux基础部分",{"0":{"1502":1},"1":{"1503":1,"1504":1,"1505":1,"1506":1,"1507":1,"1508":1,"1509":1,"1510":1,"1511":1,"1512":1,"1513":1,"1514":1,"1515":1,"1516":1,"1517":1,"1518":1,"1519":1,"1520":1,"1521":1,"1522":1,"1523":1,"1524":1,"1525":1,"1526":1,"1527":1,"1528":1,"1529":1,"1530":1,"1531":1,"1532":1,"1533":1,"1534":1,"1535":1,"1536":1,"1537":1,"1538":1,"1539":1,"1540":1,"1541":1,"1542":1,"1543":1,"1544":1,"1545":1,"1546":1,"1547":1,"1548":1,"1549":1,"1550":1,"1551":1,"1552":1,"1553":1,"1554":1,"1555":1,"1556":1,"1557":1,"1558":1,"1559":1,"1560":1},"2":{"2043":1}}],["linux模块",{"0":{"1599":1}}],["linux为主",{"2":{"1594":1}}],["linux安装配置mpich",{"2":{"1594":1}}],["linux环境",{"0":{"1582":1}}],["linux",{"0":{"1504":1,"1968":1},"1":{"1505":1,"1506":1,"1507":1},"2":{"1505":1,"1510":1,"1561":1,"1594":5,"1605":3,"1941":1,"1985":1}}],["lin等人提出使用全局平均池化global",{"2":{"816":1}}],["linking",{"2":{"1604":1}}],["link",{"0":{"1975":1},"2":{"713":2,"1050":2,"1054":1,"1061":1,"1065":1,"1112":1,"1161":1,"1219":1,"1230":1,"1233":1,"1234":1,"1235":1,"1236":1,"1237":1,"1238":1,"1239":1,"1240":1,"1241":1,"1242":1,"1243":1,"1244":1,"1245":1,"1246":1,"1247":1,"1249":1,"1975":1,"1980":1,"1991":1,"1999":1,"2085":1}}],["lindenstrauss",{"2":{"684":1}}],["linestream",{"2":{"1933":2}}],["line",{"2":{"399":1,"557":2,"591":2,"1813":3,"1820":4,"1825":3,"1831":3,"1838":4,"1843":3,"1933":3}}],["linearlr",{"0":{"1238":1},"2":{"1238":1}}],["linear和softmax分别可以作为单独的积木块",{"2":{"449":1}}],["linearity",{"2":{"156":1}}],["linear实例化了两个线性层对象",{"2":{"113":1}}],["linearactivation",{"2":{"110":1}}],["linear",{"0":{"642":1,"804":1,"843":1,"844":1},"2":{"23":1,"36":2,"97":2,"103":3,"104":1,"105":1,"106":2,"107":2,"108":1,"110":4,"113":2,"137":1,"145":1,"156":3,"210":4,"211":1,"217":1,"232":1,"233":2,"348":1,"361":1,"420":1,"503":4,"513":1,"530":1,"624":1,"702":3,"765":2,"768":1,"804":1,"840":2,"842":3,"926":1,"976":2,"1087":4,"1098":3,"1099":1,"1202":8,"1205":8,"1207":3,"1210":2,"1212":3,"1213":3,"1215":4,"1216":8,"1217":6,"1218":9,"1257":2,"1295":2,"1345":3,"2086":3}}],["linears的前三项",{"2":{"36":1}}],["linears",{"2":{"23":1,"36":9}}],["linsight",{"2":{"292":1,"361":1}}],["linformer",{"2":{"204":2}}],["lin",{"2":{"36":2,"387":1}}],["lock",{"2":{"1695":1,"1913":1,"1947":1}}],["locatebashlocate",{"2":{"1532":1}}],["location=device",{"2":{"1255":1,"1263":1}}],["locating",{"2":{"145":1,"156":2}}],["localvar++",{"2":{"1649":1}}],["localvar",{"2":{"1648":3,"1649":3}}],["locally",{"2":{"1214":2}}],["localllama",{"2":{"768":1}}],["localhost",{"2":{"422":1,"1481":1}}],["locality",{"2":{"204":1}}],["localization",{"2":{"136":1,"156":1}}],["local",{"2":{"8":4,"201":18,"204":1,"285":1,"1214":4,"1594":5,"1649":1}}],["loop",{"0":{"1898":1},"2":{"1284":1,"1621":1,"1713":1,"1897":1,"2086":3}}],["loop的一个迭代",{"2":{"975":1}}],["looks",{"2":{"700":1}}],["looked",{"2":{"567":1}}],["looking",{"2":{"567":1}}],["lookup",{"2":{"558":2,"700":4}}],["lookahead",{"2":{"429":1}}],["look",{"2":{"18":1,"20":2,"47":1,"567":4,"591":1}}],["lo",{"2":{"583":4,"584":1,"585":1,"587":1}}],["lowering",{"2":{"1227":1}}],["lowest",{"2":{"577":1,"578":1,"592":2}}],["low",{"0":{"973":1},"2":{"578":1,"579":1,"584":1,"585":1,"587":2}}],["lov",{"2":{"576":1}}],["loving",{"2":{"576":1}}],["loves",{"2":{"576":1}}],["loved",{"2":{"576":1}}],["lovely",{"2":{"567":2}}],["love",{"2":{"245":2,"385":1,"391":1,"398":2,"399":1,"428":2,"528":9,"529":2,"536":1,"567":1}}],["loaddata",{"2":{"2070":1}}],["loading",{"2":{"1308":1}}],["loaded",{"2":{"1297":2}}],["loader",{"2":{"1215":13,"1241":2,"1242":3,"1254":1,"1280":1}}],["loads",{"2":{"1227":1}}],["load",{"2":{"371":3,"372":4,"373":6,"374":4,"571":1,"591":4,"666":1,"723":1,"1208":4,"1214":5,"1226":2,"1227":13,"1255":3,"1259":3,"1263":3,"1267":4,"1270":3,"1296":1,"1297":1}}],["lora",{"2":{"222":2,"396":2,"429":1}}],["logs",{"2":{"1281":1}}],["logsumexp",{"2":{"1087":2,"1329":2}}],["logdir",{"2":{"1281":1}}],["logdir=",{"2":{"1281":2,"1283":1}}],["logdet",{"2":{"1087":1}}],["logging",{"2":{"1215":1}}],["logic",{"2":{"1461":1,"1762":1}}],["logical",{"2":{"1087":8}}],["logits对应着该token取不同字的概率",{"2":{"397":1}}],["logits",{"2":{"180":1,"378":2,"398":1,"431":1,"473":2,"515":1,"530":2,"1096":1,"1207":3}}],["logit",{"2":{"147":3,"473":1,"475":2,"479":1,"482":3,"513":2,"1087":2}}],["logcumsumexp",{"2":{"1087":2}}],["logaddexp2",{"2":{"1087":1}}],["logaddexp",{"2":{"1087":1}}],["log2",{"2":{"1087":2}}],["log1p",{"2":{"1087":2}}],["log10",{"2":{"1087":2}}],["logp",{"2":{"903":2}}],["log⁡n",{"2":{"684":3}}],["logknlogknlog",{"2":{"511":1}}],["logt",{"2":{"504":2}}],["logn",{"2":{"480":1}}],["logmndqkt",{"2":{"194":1}}],["logmn√dqkt",{"2":{"194":1}}],["log",{"0":{"183":1},"2":{"157":1,"183":4,"194":1,"399":1,"504":4,"740":1,"899":6,"903":2,"970":1,"971":1,"1087":5,"1115":2,"1215":6,"1257":1,"1279":1,"1280":1,"1281":1,"1282":1,"1283":1,"1284":1,"1329":2,"1539":3,"2155":2}}],["loses",{"2":{"115":1}}],["loss=f",{"2":{"1442":2}}],["loss的特性有关",{"2":{"1155":1}}],["loss且没有正则化项时",{"2":{"1155":1}}],["loss函数",{"2":{"1014":1}}],["loss关于权值参数的梯度很小",{"2":{"991":1}}],["lossy",{"2":{"591":1}}],["loss",{"0":{"1056":1,"1280":1},"2":{"89":1,"381":3,"385":13,"398":1,"399":10,"410":3,"423":3,"472":1,"1039":1,"1092":2,"1096":5,"1097":2,"1098":15,"1099":3,"1102":3,"1202":3,"1205":9,"1213":2,"1215":13,"1218":4,"1223":7,"1231":6,"1244":2,"1245":2,"1266":3,"1267":2,"1279":2,"1280":4,"1295":7,"1304":1,"1398":15,"1440":1,"1442":2,"2086":10}}],["long的位数时",{"2":{"2062":1}}],["longllama",{"2":{"768":1}}],["longlora",{"2":{"768":2}}],["longer",{"2":{"768":1,"1695":1}}],["longalpaca",{"2":{"768":1}}],["longalign",{"2":{"88":1,"89":1,"95":2}}],["longformer",{"2":{"204":1}}],["long",{"0":{"862":1},"1":{"863":1,"864":1,"865":1,"866":1,"867":1,"868":1,"869":1,"870":1,"871":1},"2":{"89":1,"95":1,"160":1,"204":1,"229":1,"233":2,"292":2,"420":1,"513":1,"723":2,"765":1,"768":1,"862":1,"1085":1,"1087":1,"1328":1,"1330":6,"1607":12,"1821":1,"1839":1,"1910":4,"2062":2}}],["longtensor",{"2":{"83":1,"399":1,"424":1,"428":1,"472":1,"529":1,"702":1,"723":3,"834":3,"1328":1}}],["lam",{"2":{"1569":1}}],["lambd",{"2":{"1087":2}}],["lambdas",{"0":{"1906":1},"2":{"1904":1}}],["lambda作为比较函数用于排序",{"2":{"1883":1}}],["lambda=lmbda",{"2":{"1234":1}}],["lambda=lambda",{"2":{"83":1,"402":1,"423":1,"424":1}}],["lambda=",{"2":{"1233":1}}],["lambda2",{"2":{"1233":2,"1907":1}}],["lambda1",{"2":{"1233":2}}],["lambdaλ",{"2":{"1184":1}}],["lambda|i",{"2":{"765":1}}],["lambdalr对象",{"2":{"385":1}}],["lambdalr",{"0":{"1233":1},"2":{"83":1,"402":2,"423":1,"424":1,"1233":1}}],["lambda",{"0":{"1881":1,"1906":1,"1907":2,"1924":1},"1":{"1882":1,"1883":1,"1884":1},"2":{"38":1,"39":2,"82":2,"344":4,"503":18,"523":1,"529":2,"533":2,"765":3,"843":1,"1099":1,"1184":1,"1233":2,"1234":1,"1603":2,"1904":3,"1906":5,"1907":3,"1914":1,"1920":1,"1924":4,"1932":1}}],["laion2b",{"2":{"1363":1}}],["launch",{"2":{"1308":1}}],["lable",{"2":{"1202":1,"1205":2,"1398":4}}],["labels",{"2":{"1244":3,"1250":5,"1283":1,"2086":2}}],["labelsmoothing类对象",{"2":{"398":1}}],["labelsmoothing",{"2":{"83":1,"399":4,"423":1,"424":1,"429":1}}],["labeling",{"2":{"906":2,"1309":2}}],["labeled",{"2":{"726":1,"1302":1}}],["label",{"0":{"399":1,"1016":1},"2":{"399":8,"1016":1,"1096":2,"1098":6,"1099":2,"1102":2,"1202":2,"1205":3,"1250":4,"1251":1,"1331":5,"1386":1,"1398":13,"1443":1}}],["label中已经给出的这个时刻之后的teacher",{"2":{"57":1}}],["layout分页布局中",{"2":{"986":1}}],["layout",{"2":{"820":1,"1082":2,"1086":2,"1087":18}}],["layer对应下面的decoderlayer",{"2":{"532":1}}],["layer前",{"2":{"334":1}}],["layer能够堆叠",{"2":{"116":1}}],["layer和decoder",{"2":{"116":1}}],["layers中的每一个编码器层",{"2":{"522":1}}],["layers中",{"2":{"522":1}}],["layers",{"0":{"154":1,"354":1,"1102":1},"2":{"83":2,"96":1,"126":1,"127":1,"151":1,"154":2,"156":3,"201":6,"293":1,"306":1,"334":3,"343":5,"522":4,"529":1,"532":2,"1210":1,"1216":6,"1217":4,"1218":9,"2086":1}}],["layer计算时",{"2":{"58":1}}],["layer是有很多层的",{"2":{"58":1}}],["layernorm等",{"2":{"976":1}}],["layernorm结束",{"2":{"976":1}}],["layernorm在每个transformer",{"2":{"468":1}}],["layernorm有助于稳定训练过程并提高收敛性",{"2":{"468":1}}],["layernorm需要在每个样本的特征维度上计算均值和标准差",{"2":{"348":1}}],["layernorm是沿着d做reduce",{"2":{"341":1}}],["layernorm是最常用的技术",{"2":{"311":1}}],["layernorm避开了batch",{"2":{"341":1}}],["layernorm的公式如下",{"2":{"332":1}}],["layernorm会突出残差分支",{"2":{"332":1}}],["layernorm会调整神经网络每一层的值",{"2":{"311":1}}],["layernorm和softmax都需要对输入进行多次传递才能计算非线性运算",{"2":{"327":1}}],["layernorm和batchnorm操作",{"2":{"327":1}}],["layernorm和gelu",{"2":{"327":1}}],["layernorm认为每个样本内的特征具有相同分布",{"2":{"326":1}}],["layernorm却可以将keys投影到同一超平面上",{"2":{"320":1}}],["layernorm所做的操作",{"2":{"318":1}}],["layernorm究竟应该放前面还是后面",{"2":{"294":1}}],["layernorm对秩崩溃的缓解作用",{"2":{"92":1}}],["layernorm",{"0":{"317":1,"343":1,"468":1,"808":1},"1":{"318":1,"319":1,"320":1,"321":1,"322":1,"323":1,"324":1,"325":1,"326":1,"327":1,"328":1,"329":1,"330":1,"331":1,"332":1,"333":1,"334":1,"335":1},"2":{"47":1,"83":1,"91":2,"95":1,"293":2,"311":1,"320":1,"322":1,"326":2,"330":5,"331":2,"341":1,"343":5,"344":4,"346":2,"351":1,"361":2,"420":1,"510":2,"520":1,"522":1,"523":4,"530":3,"532":1,"723":4,"808":2,"810":1,"1216":2,"1217":2,"1218":5}}],["layer",{"0":{"461":1,"1201":1,"1206":1,"1217":1},"1":{"462":1,"463":1,"464":1,"465":1,"466":1,"467":1,"468":1,"469":1,"470":1,"1202":1,"1203":1},"2":{"8":2,"83":6,"147":1,"151":1,"153":3,"160":1,"185":1,"201":7,"229":1,"231":1,"314":1,"320":2,"322":2,"326":6,"329":2,"334":2,"337":1,"343":12,"344":1,"346":3,"349":1,"361":5,"394":1,"436":2,"442":1,"443":1,"444":1,"499":1,"522":7,"529":2,"532":6,"621":2,"623":1,"640":1,"674":1,"723":1,"731":1,"735":1,"769":1,"785":2,"808":2,"812":4,"977":1,"1098":3,"1099":1,"1216":3,"1217":2,"1218":5,"1464":2}}],["latest",{"2":{"1308":1}}],["later",{"2":{"591":1}}],["latent",{"0":{"955":1,"1353":1},"1":{"956":1,"957":1},"2":{"240":1,"610":2,"638":1,"711":2,"731":1,"735":2,"1375":1}}],["lavit",{"2":{"429":1}}],["ladder",{"2":{"370":1}}],["langle",{"2":{"1342":1}}],["language",{"0":{"1379":1},"2":{"8":1,"47":2,"89":1,"95":1,"105":1,"122":3,"130":6,"136":1,"137":1,"141":1,"143":1,"147":1,"148":1,"156":8,"167":3,"204":1,"233":5,"259":2,"288":2,"292":2,"375":1,"429":3,"437":3,"483":1,"513":4,"542":2,"543":1,"557":2,"559":1,"560":1,"562":1,"567":1,"637":1,"638":6,"711":1,"716":1,"718":1,"732":1,"736":2,"740":7,"747":1,"751":1,"760":1,"764":1,"768":4,"894":1,"906":2,"908":2,"1312":3,"1315":4,"1316":1,"1360":1,"2009":2,"2078":1,"2079":1,"2081":1}}],["landvehicle",{"2":{"1664":3,"1665":7}}],["landmarks",{"2":{"1254":20}}],["landscape",{"2":{"314":1,"2087":1}}],["laws",{"0":{"1379":1},"2":{"561":1,"638":1}}],["law",{"2":{"147":1,"561":1,"1404":1}}],["las",{"2":{"638":1}}],["lasp+",{"2":{"217":1}}],["lastfive",{"2":{"1821":3,"1839":3}}],["last",{"2":{"130":1,"326":1,"590":1,"731":1,"735":3,"1082":1,"1217":1,"1233":1,"1235":1,"1236":1,"1237":1,"1238":1,"1239":1,"1240":1,"1328":2,"1330":2,"1556":1,"1713":1,"1756":1,"1821":1,"1839":1}}],["lashley",{"2":{"129":1}}],["laplaceactivation",{"2":{"110":1}}],["laplace",{"2":{"110":1}}],["largevalue",{"2":{"1910":2}}],["largenumber",{"2":{"1910":1}}],["largest",{"2":{"1087":1}}],["larger",{"2":{"334":1,"561":2,"638":2}}],["large",{"0":{"1380":1},"2":{"8":1,"89":1,"95":1,"141":1,"143":1,"150":1,"156":2,"233":3,"333":1,"429":2,"437":1,"562":1,"625":2,"638":4,"711":1,"736":1,"740":1,"768":1,"945":1,"965":1,"981":1,"1242":1,"1304":1,"1308":2,"1910":2,"2073":1,"2083":1}}],["l",{"2":{"5":2,"17":2,"113":4,"126":2,"145":2,"148":1,"161":3,"173":15,"217":1,"230":1,"315":1,"334":2,"341":3,"415":1,"485":1,"543":1,"571":3,"579":3,"580":1,"582":5,"583":3,"585":1,"614":4,"778":2,"967":1,"971":1,"986":1,"1003":60,"1004":17,"1188":1,"1189":1,"1190":2,"1191":2,"1192":2,"1193":3,"1308":2,"1363":1,"1388":2,"1509":1,"1513":1,"1547":1,"1557":1,"1589":3,"1624":2,"1664":1,"1713":6,"1715":2,"1774":5,"1789":2,"1791":2,"1792":2}}],["s7",{"2":{"1713":3}}],["s6",{"2":{"1713":3}}],["s5",{"2":{"1713":3}}],["s4",{"2":{"1713":3}}],["s2",{"2":{"1680":4,"1713":10,"1715":5}}],["sd3",{"2":{"1367":1}}],["sdxl",{"0":{"1364":1,"1366":1},"2":{"1364":3,"1365":1}}],["sd2",{"0":{"1363":1}}],["sd",{"0":{"1350":1,"1351":1,"1362":1,"1364":1},"1":{"1363":1,"1364":1},"2":{"1363":12,"1364":1,"1365":2}}],["s3",{"2":{"1309":1,"1680":5,"1713":3}}],["sgn",{"2":{"1087":2}}],["sgdr",{"2":{"1243":1,"1244":1}}],["sgd能够在多个局部最小值之间进行搜索",{"2":{"1026":1}}],["sgd的参数更新在每次迭代中都具有一定的随机性",{"2":{"1026":1}}],["sgd的参数更新速度比bgd更快",{"2":{"1026":1}}],["sgd每次仅使用单个样本或一小批样本的梯度进行参数更新",{"2":{"1026":1}}],["sgd",{"0":{"1026":1,"1028":1,"1037":1,"1188":1},"1":{"1029":1,"1030":1,"1031":1,"1032":1,"1038":1,"1039":1},"2":{"1026":1,"1027":2,"1034":1,"1035":1,"1039":1,"1130":3,"1202":1,"1205":2,"1215":1,"1221":1,"1222":1,"1231":2,"1239":1,"1241":1,"1242":1,"1243":1,"1245":1,"1296":1,"1440":1,"2086":1}}],["sglang",{"2":{"984":1,"985":2,"987":1}}],["s^",{"2":{"999":2}}],["s翻倍",{"2":{"973":1}}],["s的50",{"2":{"968":1}}],["s​ij​​−m​ij​​",{"2":{"944":1}}],["s​ij​​",{"2":{"944":1}}],["s​ij​​=q​i​​k​j​t​​∈r​b​r​​×b​c​​​​",{"2":{"944":1}}],["s​t​​=σ",{"2":{"856":1}}],["s​t​​",{"2":{"855":1}}],["s=qk​⊤​​∈r​n×n​​",{"2":{"941":1,"960":1}}],["s=qk⊤∈rn×n",{"2":{"941":1,"960":1}}],["s=qttks=xttwqtwkxs+bt",{"2":{"766":1}}],["s=qttks=xttwtqwkxs+bt",{"2":{"766":1}}],["sx",{"2":{"856":1}}],["s型神经元",{"2":{"845":1}}],["s型激活函数分析",{"2":{"839":1}}],["sn是一种覆盖特征图张量各个维度来计算统计信息的归一化方法",{"2":{"811":1}}],["snake",{"2":{"723":1}}],["ss3",{"2":{"1824":3,"1842":3}}],["ss2",{"2":{"1824":2,"1842":2}}],["ssh",{"2":{"1594":9}}],["ssh扩展",{"2":{"1583":1}}],["ssh工具",{"0":{"1583":1}}],["ssh连接",{"2":{"1561":1}}],["ssbashnetstat",{"2":{"1527":1}}],["sspaddmm",{"2":{"1087":1}}],["ss",{"2":{"766":1,"1824":3,"1825":2,"1842":3,"1843":2}}],["sstream>",{"2":{"1824":1,"1825":1,"1842":1,"1843":1,"1933":1}}],["sstream",{"2":{"1823":1,"1841":1}}],["sst",{"2":{"766":1}}],["sram",{"2":{"944":2,"963":1}}],["sr",{"2":{"761":1}}],["srt",{"2":{"761":1}}],["src里面是单词的索引",{"2":{"538":1}}],["src是",{"2":{"528":1}}],["src大于max",{"2":{"384":1}}],["src的形状是",{"2":{"450":1}}],["src的形状为",{"2":{"380":1}}],["src的长度小于max",{"2":{"65":1}}],["src要对句子中的填充词做mask",{"2":{"79":1}}],["src元素等于pad的位置为false",{"2":{"66":1}}],["src进行处理",{"2":{"65":1}}],["src",{"2":{"34":2,"36":1,"39":4,"66":8,"74":2,"77":1,"79":3,"82":18,"83":18,"84":11,"198":1,"343":2,"364":2,"371":1,"372":2,"374":4,"375":3,"380":7,"381":2,"382":2,"383":2,"384":16,"385":6,"399":2,"402":1,"410":2,"422":5,"423":3,"424":6,"428":8,"448":1,"449":5,"450":20,"451":4,"472":8,"503":6,"528":6,"529":15,"532":3,"533":11,"538":9,"557":4,"558":9,"703":21,"1087":15,"1200":1,"1217":2,"1218":19,"1332":1,"1997":1,"1999":4}}],["s改为rt−srt−sr",{"2":{"760":1}}],["s中",{"2":{"745":1}}],["s1",{"2":{"591":2,"1680":4,"1713":5,"1715":5,"1728":12}}],["s0",{"2":{"591":2}}],["s+",{"2":{"571":2}}],["s|",{"2":{"571":1}}],["sfinal",{"2":{"423":1}}],["skypile",{"2":{"387":1}}],["skywork",{"2":{"387":1}}],["skip",{"2":{"296":1,"302":1,"305":2,"446":1,"714":2,"1217":1,"1284":1}}],["s>",{"2":{"384":2,"557":4,"558":4,"679":2}}],["sln",{"2":{"1969":1}}],["sleep",{"2":{"1895":1,"2086":1}}],["sleeping",{"2":{"1674":2}}],["slerp",{"2":{"354":1}}],["slow",{"2":{"1214":1}}],["slogdet",{"2":{"1087":2}}],["slo",{"2":{"977":1}}],["sloss",{"2":{"398":3,"410":3,"423":2,"472":3}}],["slice",{"0":{"830":1,"832":1},"1":{"831":1,"832":1},"2":{"832":4,"1085":2,"1087":1}}],["slagle提出了一种简单而有效的改进",{"2":{"613":1}}],["slab",{"2":{"348":1,"361":2}}],["slm词汇表的变化趋势",{"2":{"559":1}}],["slm",{"2":{"513":1}}],["sweet",{"2":{"261":1}}],["swagger",{"2":{"2070":1}}],["swapbyreference",{"2":{"1650":2}}],["swapbypointer",{"2":{"1650":2}}],["swapbyvalue",{"2":{"1650":2}}],["swap指令",{"0":{"1423":1}}],["swapped",{"2":{"1254":1}}],["swap",{"2":{"1254":2,"2060":2}}],["swapdims",{"2":{"1087":2}}],["swapaxes",{"2":{"1087":2}}],["swaps",{"2":{"820":1}}],["swa",{"2":{"231":2}}],["switch",{"0":{"811":1,"1922":1},"2":{"1436":1,"1631":4,"1920":2,"1922":4,"1932":1}}],["swish为激活函数",{"2":{"109":1}}],["swish就变成了relu",{"2":{"108":1}}],["swish退化为一个线性函数",{"2":{"108":1}}],["swish",{"0":{"845":1},"2":{"107":1,"108":6,"109":5,"110":3,"845":7}}],["swish函数退化成relu函数",{"2":{"845":1}}],["swish函数退化成线性函数",{"2":{"845":1}}],["swish函数的数学表达式为",{"2":{"108":1}}],["swish函数的曲线是平滑的",{"2":{"108":1}}],["swish函数由google团队在2017年在论文",{"2":{"108":1}}],["swish函数",{"0":{"108":1},"2":{"96":1}}],["swiglu就是使用了nn",{"2":{"110":1}}],["swiglu也是全连接配合激活函数的形式",{"2":{"109":1}}],["swiglu本质上是对transformer的ffn前馈传播层的第一层全连接和relu进行替换",{"2":{"109":1}}],["swiglu的数学表达式为$",{"2":{"109":1}}],["swiglu可以提升模型的性能",{"2":{"107":1}}],["swiglu其实就是采用swish作为激活函数",{"2":{"107":1}}],["swiglu激活函数",{"0":{"109":1},"2":{"96":1}}],["swiglu",{"0":{"107":1},"1":{"108":1,"109":1,"110":1},"2":{"96":1,"107":3,"109":3,"356":1,"501":1}}],["sailing",{"2":{"1665":2}}],["sail",{"2":{"1664":3,"1665":3}}],["salesvolume",{"2":{"1657":1}}],["salesperson",{"2":{"1657":1}}],["say",{"2":{"1254":1}}],["sa",{"2":{"745":1,"765":1,"766":1}}],["saturday",{"2":{"1728":2}}],["saturated",{"2":{"995":1}}],["saturation",{"2":{"838":1}}],["sat",{"2":{"745":1,"765":1,"766":1}}],["savings",{"2":{"1874":4}}],["savingsaccount",{"2":{"1873":3,"1874":4}}],["saving",{"2":{"591":1,"1215":1}}],["savedmodels",{"2":{"1164":1}}],["saved",{"0":{"1114":1},"2":{"1100":1,"1111":4,"1114":8}}],["saves",{"2":{"591":1,"1330":2}}],["save",{"2":{"374":1,"423":2,"591":2,"592":2,"668":2,"1100":1,"1114":1,"1214":1,"1215":3,"1253":1,"1255":2,"1258":2,"1262":2,"1266":2,"1269":3,"1296":1,"1297":1,"1330":1,"1404":1}}],["safer",{"2":{"1254":1}}],["safe",{"2":{"573":1,"961":1}}],["sae",{"2":{"475":1}}],["sakana",{"2":{"222":1}}],["sana",{"0":{"232":1},"2":{"157":1,"232":2,"233":1}}],["sampling算法实现的",{"2":{"603":1}}],["sampling",{"0":{"897":1},"2":{"543":1,"726":1,"897":1,"983":1}}],["sample方法和大batch",{"2":{"726":1}}],["samples",{"2":{"385":1,"834":1,"983":1,"1087":1,"1254":1,"1350":10,"2086":1}}],["sampler=valid",{"2":{"375":1}}],["sampler=train",{"2":{"375":1}}],["sampler",{"2":{"90":2,"375":6,"423":2}}],["sample",{"0":{"87":1},"1":{"88":1,"89":1,"90":1},"2":{"49":1,"88":2,"89":10,"90":4,"95":1,"1244":3,"1254":18,"1329":2}}],["same",{"2":{"344":1,"503":1,"1087":1,"1254":2,"1350":1}}],["sam多吃青菜",{"2":{"156":1}}],["sv3",{"2":{"1929":2}}],["sv2",{"2":{"1929":2}}],["sv1",{"2":{"1929":2}}],["sv",{"2":{"1929":2}}],["svf利用rl在预定义的下游任务集上学习这些z向量",{"2":{"224":1}}],["svf利用rl训练这些组件的组合以应对不同任务",{"2":{"224":1}}],["svf学习一组z向量",{"2":{"224":1}}],["svf首先识别任务类型",{"2":{"224":1}}],["svf",{"0":{"224":1},"2":{"157":1,"224":2,"225":1}}],["svd通过识别llm权重矩阵中的主成分来实现这一目标",{"2":{"221":1}}],["svd",{"0":{"221":1},"2":{"157":1,"221":2,"1087":2}}],["smm",{"2":{"1087":1}}],["smi",{"2":{"792":1,"1489":2}}],["smart",{"2":{"567":1,"1647":1}}],["smaller",{"2":{"945":1,"965":1,"1254":1}}],["small",{"2":{"156":1,"559":1,"567":1,"1308":2,"1332":1}}],["smoothing的代码如下",{"2":{"399":1}}],["smoothing的原理如下图所示",{"2":{"399":1}}],["smoothing起到的作用实际上是抑制了feature",{"2":{"399":1}}],["smoothing主要针对的是softmax层",{"2":{"399":1}}],["smoothing",{"0":{"399":1,"1016":1},"2":{"399":11,"1016":1}}],["smoothing=0",{"2":{"83":1,"399":3,"423":1,"424":1}}],["smt的门槛也是很高的",{"2":{"908":1}}],["smt的主要思想就是从大量的数据中学习一个概率模型",{"2":{"908":1}}],["smt",{"0":{"908":1},"2":{"282":1,"907":1,"908":1}}],["sm",{"2":{"210":3,"373":6}}],["shdocker",{"2":{"2067":1}}],["sh",{"2":{"1302":1}}],["shufflenet",{"2":{"1308":4}}],["shuffle",{"2":{"1215":1}}],["shuffle=true",{"2":{"1251":1,"1283":1,"1296":1}}],["shuffle=",{"2":{"375":2}}],["shirt",{"2":{"370":1,"557":1}}],["shiftwidth=4",{"2":{"1559":1}}],["shifts",{"2":{"1087":1}}],["shifted",{"2":{"407":1,"408":1,"453":2,"528":2,"751":1}}],["shift从具体现象来看就是",{"2":{"309":1}}],["shift",{"2":{"148":2,"309":1,"310":1,"313":2,"314":2,"343":1,"361":2,"485":1,"1087":8}}],["showmigrations",{"2":{"2070":1}}],["showbalance",{"2":{"1873":1,"1874":8}}],["showderived",{"2":{"1857":2,"1861":2}}],["showinfo",{"2":{"1690":3,"1691":7}}],["showutilization",{"2":{"423":1}}],["show",{"2":{"399":1,"428":1,"1308":1,"1849":3,"1853":2,"1857":2,"1861":2,"2006":2}}],["shown",{"2":{"326":1}}],["shorter",{"2":{"1254":1}}],["shortcuts永远不会关闭",{"2":{"301":1}}],["shortcuts没有参数",{"2":{"301":1}}],["shortcuts",{"2":{"301":1}}],["shortcut",{"0":{"298":1},"2":{"293":1,"300":1,"301":3}}],["short",{"0":{"862":1},"1":{"863":1,"864":1,"865":1,"866":1,"867":1,"868":1,"869":1,"870":1,"871":1},"2":{"287":1,"288":1,"292":4,"437":1,"765":1,"768":1,"862":1,"1087":1,"1607":4}}],["shoes",{"2":{"239":3}}],["should",{"2":{"239":3,"591":1,"1086":1,"1245":1,"1305":1}}],["shot表现最好",{"2":{"542":1}}],["shot",{"2":{"222":1,"542":1,"629":1,"985":1,"1316":1}}],["shot场景下完成事实回忆任务的过程总结为以下几步",{"2":{"122":1}}],["shot情况下模型的工作机制如何",{"2":{"122":1}}],["shot或者few",{"2":{"122":1}}],["shot设定",{"2":{"122":1}}],["shader",{"2":{"2009":2}}],["shareresource",{"2":{"1891":2}}],["sharedptr",{"2":{"1695":2}}],["shared",{"2":{"970":1,"1083":2,"1671":1,"1695":12,"1890":1,"1891":14,"1902":4,"1911":2,"1913":2}}],["share",{"2":{"935":1,"951":1,"983":1,"1083":1,"1214":1}}],["sharing",{"2":{"772":1}}],["sharp",{"2":{"181":2,"233":1}}],["shallue",{"2":{"1124":1,"1131":1,"1133":1,"1137":1,"1186":1,"1195":1}}],["shall",{"2":{"504":1,"513":1}}],["shallower",{"2":{"301":2}}],["shazeer",{"2":{"434":1,"956":1}}],["shape2",{"2":{"1688":6}}],["shape1",{"2":{"1688":6}}],["shapes",{"2":{"1274":1}}],["shaped",{"2":{"930":1}}],["shape的基本思想是在训练过程中对绝对位置编码随机整体平移一段距离来实现泛化能力",{"2":{"751":1}}],["shape=",{"2":{"700":1}}],["shape",{"0":{"1274":1},"2":{"74":2,"76":4,"83":2,"84":2,"201":2,"325":1,"341":1,"382":2,"385":2,"424":1,"700":9,"723":5,"751":1,"773":1,"781":2,"807":1,"808":2,"809":1,"810":5,"1003":2,"1078":3,"1081":1,"1082":1,"1087":3,"1207":1,"1216":4,"1217":10,"1254":2,"1259":2,"1263":2,"1267":2,"1274":3,"1283":4,"1328":7,"1329":10,"1330":9,"1345":8,"1350":4,"1656":2,"1688":14,"1693":4}}],["shape不变",{"2":{"36":1}}],["shape指的是张量形状",{"2":{"23":1}}],["shell",{"2":{"2070":1}}],["shellavailable",{"2":{"2070":1}}],["shell脚本",{"2":{"1599":1}}],["shell01",{"2":{"1404":1}}],["shellgit",{"2":{"1309":1,"1332":1,"1347":1}}],["shecuder",{"2":{"1248":1}}],["shepherd",{"2":{"156":1}}],["she",{"2":{"131":1}}],["s代表头部实体",{"2":{"122":1}}],["sz",{"2":{"84":6}}],["sqlsequencereset",{"2":{"2070":1}}],["sqlsessionfactorybuilder",{"2":{"1481":2}}],["sqlsessionfactory",{"2":{"1481":5}}],["sqlsession",{"2":{"1481":3,"1485":1,"1486":1,"1487":1,"1488":1}}],["sqlmigrate",{"2":{"2070":1}}],["sqlflush",{"2":{"2070":1}}],["sql中取的值即可",{"2":{"1485":1}}],["sql语句编写的时候",{"2":{"1485":1}}],["sql语句返回值类型",{"2":{"1485":1}}],["sqlcreate",{"2":{"1481":1}}],["sql和代码的分离",{"2":{"1479":1}}],["sql写在xml里",{"2":{"1479":1}}],["squashmigrations",{"2":{"2070":1}}],["squashing",{"2":{"1460":1}}],["squad",{"2":{"1332":2}}],["squares",{"2":{"1883":1}}],["square",{"2":{"84":3,"346":1,"361":2,"801":3,"802":2,"812":2,"814":2,"815":2,"816":1,"1047":2,"1087":2,"1254":2,"1632":2,"1688":7,"1709":3,"1883":2,"1924":2}}],["squeezenet1",{"2":{"1308":2}}],["squeeze",{"2":{"399":1,"1087":10}}],["sqrt",{"2":{"57":1,"67":1,"71":1,"106":1,"173":2,"175":1,"186":2,"187":2,"194":2,"197":1,"199":2,"201":1,"332":2,"334":3,"394":2,"395":1,"510":1,"621":1,"640":1,"646":1,"701":1,"761":1,"807":1,"808":2,"809":1,"810":1,"829":2,"844":1,"918":1,"924":1,"933":1,"1000":5,"1007":4,"1087":2,"1115":3,"1191":1,"1192":2,"1193":2,"1211":1,"1216":1,"1218":1,"1345":1,"1350":14,"1361":1}}],["sound",{"2":{"1866":1}}],["sources",{"2":{"1917":2}}],["source端",{"2":{"649":1,"931":1}}],["source",{"2":{"84":7,"169":1,"170":7,"271":7,"450":2,"908":1,"1087":17,"1590":1,"1742":4,"1923":1,"1976":1,"1987":1,"2073":1}}],["so",{"2":{"1254":1,"1329":3,"1330":1,"2078":1}}],["some",{"2":{"1087":2,"1254":1,"1611":2}}],["something",{"2":{"736":2}}],["solution",{"2":{"1083":1,"1963":1}}],["solver",{"2":{"498":1}}],["solve",{"2":{"480":1,"513":2,"1083":3,"1087":4}}],["solving",{"2":{"122":1}}],["sonar文本嵌入空间使用编码器",{"2":{"631":1}}],["sonar嵌入空间",{"0":{"631":1}}],["sonar解码器和编码器",{"2":{"629":1}}],["sonar是一个强大的多语言",{"2":{"629":1}}],["sortnumbers",{"2":{"1645":4}}],["sort",{"0":{"1749":1,"1750":1,"1751":1},"2":{"557":1,"1087":9,"1284":1,"1645":1,"1732":3,"1749":2,"1750":3,"1751":2,"1758":2,"1883":1,"1933":1,"2153":2}}],["sortedcounts",{"2":{"1933":4}}],["sorted=true",{"2":{"1083":1}}],["sortedpacking",{"2":{"90":1}}],["sorted",{"2":{"89":1,"557":4,"1087":1,"1883":1}}],["sota",{"2":{"111":1}}],["softsign这三种激活函数",{"2":{"1000":1}}],["softmax模型和代码解析",{"2":{"233":1}}],["softmax与其变种",{"2":{"233":1}}],["softmax注意力",{"2":{"210":1}}],["softmax会让负无穷变为0",{"2":{"199":1}}],["softmax将几乎全部的概率分布都分配给了最大值对应的标签",{"2":{"191":1}}],["softmax将这种输出作了一定的平滑",{"2":{"180":1}}],["softmax中每个分量softmax",{"2":{"191":1}}],["softmax的层次模型被组织为",{"2":{"185":1}}],["softmax的解决方案是将huffman",{"2":{"184":1}}],["softmax也通过结合现代架构和矩阵乘积操作的特点",{"2":{"185":1}}],["softmax也存在一些固有限制",{"2":{"181":1}}],["softmax和一些变型",{"2":{"185":1}}],["softmax借鉴的是hierarchical",{"2":{"185":1}}],["softmax就给出了一些思路",{"2":{"185":1}}],["softmax就是word2vector中的hierarchical",{"2":{"184":1}}],["softmax就是用来解决这个问题的",{"2":{"184":1}}],["softmax后会有一定程度的增秩效果",{"2":{"180":1}}],["softmax先将这组数的差距拉大",{"2":{"180":1}}],["softmax操作实质上是在量化地衡量各个词的信息贡献度",{"2":{"180":1}}],["softmax操作的意义是归一化",{"2":{"178":1}}],["softmax在推理时可以将输出层的原始输出",{"2":{"180":1}}],["softmax函数用于将一组实数转换为范围在",{"2":{"847":1}}],["softmax函数在输入规模增大时",{"2":{"181":1}}],["softmax函数通过对每个分值应用指数函数",{"2":{"180":1}}],["softmax函数之所以在神经网络中得到广泛应用",{"2":{"180":1}}],["softmax函数将其转换为一个概率分布",{"2":{"178":1}}],["softmax关键性质如下",{"2":{"178":1}}],["softmax退化",{"0":{"191":1},"2":{"157":1}}],["softmax",{"0":{"177":1,"183":1,"184":2,"185":1,"847":1,"969":1},"1":{"178":1,"179":1,"180":1,"181":1,"182":1,"183":1,"184":1,"185":1},"2":{"54":1,"67":1,"71":1,"82":1,"125":1,"156":1,"157":5,"172":1,"173":4,"178":1,"181":1,"183":4,"184":3,"185":1,"186":1,"187":4,"188":1,"191":9,"193":1,"197":1,"199":1,"200":3,"201":1,"204":1,"209":1,"211":1,"212":4,"213":2,"216":1,"217":3,"233":4,"271":1,"284":1,"334":1,"352":1,"355":2,"357":1,"394":5,"464":1,"473":1,"501":1,"503":1,"510":1,"513":1,"515":1,"530":1,"542":1,"621":1,"642":1,"646":1,"651":1,"847":5,"933":2,"941":1,"943":10,"944":3,"960":1,"961":11,"963":3,"967":1,"969":2,"1016":1,"1087":4,"1215":2,"1216":1,"1217":1,"1218":1,"1257":1,"1315":1,"1339":1,"1345":1}}],["softmax无法严格为0",{"2":{"14":1}}],["soft",{"2":{"41":1}}],["sysconfig",{"2":{"2095":1}}],["syslog",{"2":{"1539":2}}],["systems",{"0":{"1431":1},"2":{"429":1,"508":1,"513":1,"906":1,"2082":1}}],["system",{"0":{"1930":1},"2":{"370":1,"373":2,"420":1,"498":3,"499":1,"513":1,"1411":1,"1436":1,"1481":1,"1485":1,"1486":1,"1487":1,"1488":1,"1920":1,"1930":1,"1991":2,"2076":1}}],["symfloat",{"2":{"1087":9}}],["symint",{"2":{"1087":54}}],["symbols",{"2":{"557":2}}],["symbolica首席科学家paul",{"2":{"505":1}}],["symbol",{"2":{"83":2}}],["symbol=0",{"2":{"83":1,"424":1}}],["synthesizer",{"2":{"233":1}}],["synthesis",{"2":{"232":1}}],["syntactic",{"2":{"20":1}}],["sc",{"2":{"1868":4}}],["scientific",{"2":{"1817":2,"1835":2}}],["scienceai",{"2":{"513":1}}],["science",{"2":{"513":1}}],["scripts",{"2":{"2095":1}}],["scripted",{"2":{"1296":6}}],["script装饰器标记模型的函数时",{"2":{"1291":1}}],["script通过静态分析和推断来捕获模型的计算图",{"2":{"1291":1}}],["script",{"0":{"1291":1,"1296":1},"2":{"1296":3}}],["sconcat等",{"2":{"785":1}}],["score评估自然语言",{"2":{"1360":1}}],["score较高时",{"2":{"1360":1}}],["score是指将文本和图像对输入到openai的clip",{"2":{"1360":1}}],["score往往就越小",{"2":{"904":1}}],["score加上之前层的attention",{"2":{"349":1}}],["score时",{"2":{"349":1}}],["score矩阵的第i行",{"2":{"71":1}}],["scores矩阵",{"2":{"349":1}}],["scores是一个方阵",{"2":{"199":1}}],["scores",{"2":{"67":4,"199":4,"201":8,"394":4,"933":4,"1216":4,"1218":4,"1329":2,"1331":4,"1345":5,"1623":6,"1825":3,"1843":3}}],["score",{"0":{"1331":1,"1360":1},"2":{"62":1,"173":1,"200":3,"204":3,"267":1,"270":2,"271":1,"349":1,"399":1,"765":1,"903":3,"923":1,"970":1,"1323":1,"1328":11,"1329":22,"1330":32,"1360":1,"1361":1,"1728":4}}],["scssdecode",{"2":{"528":1}}],["scada系统开发",{"2":{"1956":1}}],["scatter在底层转换为环拓扑中的点对点通信",{"2":{"976":1}}],["scatter到kv的激活梯度",{"2":{"976":1}}],["scatter通信操作来整合其他device上算出的结果",{"2":{"420":1}}],["scatter",{"2":{"399":1,"976":2,"1087":21,"1330":1,"1573":1,"1575":1}}],["scalars",{"2":{"1279":1,"1280":1}}],["scalar",{"0":{"1279":1},"2":{"1215":1,"1279":5}}],["scalinglaw可能就是对token序列进行修改",{"2":{"231":1}}],["scaling",{"0":{"1379":1},"2":{"8":1,"47":2,"156":2,"217":1,"320":1,"503":2,"561":1,"616":1,"623":2,"624":2,"638":2,"736":1,"1404":1}}],["scales",{"2":{"1087":1}}],["scaled",{"0":{"843":1,"916":1},"1":{"917":1,"918":1,"919":1,"920":1,"921":1,"922":1,"923":1,"924":1},"2":{"47":1,"67":1,"78":1,"173":1,"186":1,"199":1,"768":1,"933":1,"1216":1,"1218":1}}],["scale",{"0":{"154":1},"2":{"7":1,"96":1,"154":1,"156":2,"313":1,"343":1,"394":1,"610":1,"638":1,"702":6,"1087":1,"1254":2}}],["scholar",{"2":{"844":1}}],["scheduling",{"2":{"1229":1}}],["schedule=torch",{"2":{"1284":1}}],["schedule",{"2":{"1284":2,"1303":1}}],["scheduled",{"0":{"897":1},"2":{"543":1,"897":1}}],["schedulers=",{"2":{"1247":1}}],["scheduler2",{"2":{"1231":2,"1246":2,"1247":2}}],["scheduler1",{"2":{"1231":2,"1246":2,"1247":2}}],["scheduler",{"0":{"1230":1,"1232":1,"1233":1,"1234":1,"1235":1,"1236":1,"1237":1,"1238":1,"1239":1,"1240":1,"1241":1,"1242":1,"1243":1,"1244":1,"1246":1,"1247":1},"1":{"1231":1,"1232":1,"1233":1,"1234":1,"1235":1,"1236":1,"1237":1,"1238":1,"1239":1,"1240":1,"1241":1,"1242":1,"1243":1,"1244":1,"1245":1,"1246":1,"1247":1,"1248":1},"2":{"83":2,"364":1,"385":3,"399":2,"402":1,"423":2,"424":2,"667":1,"1215":3,"1230":2,"1231":5,"1233":2,"1234":2,"1235":2,"1236":2,"1237":2,"1238":2,"1239":4,"1240":2,"1241":3,"1242":3,"1243":5,"1244":6,"1245":2,"1246":2,"1247":2,"1404":1}}],["scheme",{"2":{"498":1}}],["schutzhelmen",{"2":{"370":1}}],["stl",{"2":{"1797":1,"1805":1,"1807":1}}],["stft",{"2":{"1083":1}}],["st=σ",{"2":{"856":1}}],["stick",{"2":{"723":1,"1254":1}}],["sts等任务",{"2":{"711":1}}],["stsscore",{"2":{"692":1}}],["sts",{"2":{"692":1,"855":1}}],["st",{"2":{"579":6,"766":1}}],["str4",{"2":{"1887":3}}],["strcmp",{"2":{"1715":2}}],["strcat",{"2":{"1715":2}}],["strcpy",{"2":{"1715":2,"1728":1,"1887":3}}],["strlen",{"2":{"1704":5,"1715":2,"1887":1}}],["str3现在应该指向nullptr",{"2":{"1887":1}}],["str3将不再拥有其资源",{"2":{"1887":1}}],["str3",{"2":{"1624":1,"1704":1,"1715":4,"1887":4}}],["str2",{"2":{"1624":9,"1704":5,"1715":7,"1824":2,"1842":2,"1887":1}}],["str1",{"2":{"1624":9,"1704":9,"1715":4,"1887":3}}],["stroustrup",{"2":{"1603":1}}],["streamsize>",{"2":{"1814":1,"1832":1}}],["streamsize",{"2":{"1813":6,"1831":6}}],["stream",{"0":{"1812":1,"1815":1,"1822":1,"1830":1,"1833":1,"1840":1},"1":{"1813":1,"1814":1,"1816":1,"1817":1,"1823":1,"1824":1,"1831":1,"1832":1,"1834":1,"1835":1,"1841":1,"1842":1},"2":{"1087":2,"1673":1,"1810":1,"1828":1}}],["stream=none",{"2":{"1083":1}}],["struct等函数创建适合具体应用的数据类型",{"2":{"1576":1}}],["struct",{"2":{"1078":1,"1110":1,"1726":1,"1727":1,"1728":14,"1750":1,"1923":1,"1925":1}}],["structures",{"0":{"1428":1}}],["structure",{"0":{"937":1,"953":1},"2":{"437":1,"513":1}}],["structured",{"0":{"1921":1},"2":{"289":1,"292":2,"1920":1,"1933":1}}],["strategy",{"0":{"986":1}}],["strang",{"2":{"498":2}}],["strict",{"2":{"1214":2}}],["strided",{"2":{"1087":5}}],["strides",{"2":{"1086":2}}],["stride=",{"2":{"801":2,"802":1,"814":1,"815":1}}],["stride=2",{"2":{"801":1,"802":3,"814":2,"815":2,"1283":1}}],["stride",{"2":{"801":3,"802":2,"1078":3,"1087":8,"1332":1}}],["stringvalue",{"2":{"1728":5}}],["string>",{"2":{"1624":1,"1665":1,"1673":1,"1674":1,"1675":1,"1677":1,"1680":1,"1683":1,"1685":2,"1691":1,"1713":4,"1715":1,"1728":1,"1729":2,"1763":1,"1803":1,"1805":2,"1807":2,"1813":1,"1820":1,"1824":1,"1831":1,"1838":1,"1842":1,"1902":1,"1906":1,"1912":2,"1914":1,"1921":4,"1923":1,"1926":4,"1927":2,"1928":2,"1929":1,"1933":1,"2062":1}}],["stringstream",{"2":{"1823":1,"1824":3,"1825":2,"1826":1,"1841":1,"1842":3,"1843":2,"1844":1}}],["strings",{"2":{"591":1}}],["string",{"0":{"1711":1,"1713":1,"1803":1,"1822":1,"1840":1,"1929":1},"1":{"1712":1,"1713":1,"1714":1,"1715":1,"1823":1,"1824":1,"1841":1,"1842":1},"2":{"572":5,"573":2,"591":2,"592":2,"700":1,"1436":1,"1481":3,"1485":5,"1624":10,"1654":3,"1665":5,"1673":1,"1674":4,"1675":2,"1683":1,"1685":8,"1691":5,"1702":1,"1708":1,"1711":2,"1713":48,"1715":16,"1725":9,"1728":2,"1729":5,"1750":1,"1761":2,"1763":4,"1795":1,"1803":2,"1813":3,"1820":1,"1821":1,"1824":6,"1825":4,"1831":3,"1838":1,"1839":1,"1842":6,"1843":4,"1887":1,"1902":6,"1906":2,"1914":2,"1920":1,"1921":1,"1923":1,"1926":3,"1928":4,"1929":20,"1932":1,"1933":10,"2062":3}}],["strip",{"2":{"557":1,"591":4}}],["stripaccents",{"2":{"552":2}}],["str=self",{"2":{"571":1}}],["str",{"2":{"552":1,"557":1,"558":8,"571":5,"572":6,"573":2,"591":2,"679":7,"1082":1,"1087":94,"1208":4,"1214":28,"1226":1,"1227":5,"1284":1,"1566":2,"1683":4,"1704":4,"1713":19,"1715":7,"1729":3,"1813":6,"1816":2,"1824":5,"1831":6,"1834":2,"1842":5,"1879":1,"1887":5,"1914":10,"1929":5,"2062":2}}],["students",{"2":{"1825":3,"1843":3}}],["student>",{"2":{"1825":1,"1843":1}}],["studentid",{"2":{"1661":6,"1662":6}}],["student",{"0":{"1680":1},"2":{"1607":1,"1661":5,"1662":4,"1680":5,"1728":7,"1805":3,"1807":12,"1825":18,"1843":18,"1921":3}}],["studio",{"2":{"1605":3,"1729":1,"1963":1,"1969":3}}],["studies",{"2":{"477":1,"513":1}}],["study",{"2":{"141":1,"156":1,"560":1,"638":1}}],["style",{"2":{"375":2,"731":1,"1715":1,"1929":1}}],["steal",{"2":{"1887":1}}],["stemming",{"2":{"552":1}}],["stehen",{"2":{"370":1}}],["steht",{"2":{"370":1}}],["step4",{"2":{"1215":1}}],["steplr",{"0":{"1235":1},"2":{"1215":2,"1235":1}}],["step3",{"2":{"963":1}}],["step2",{"2":{"963":1,"1215":1}}],["step1",{"2":{"963":1,"1215":2}}],["steps来解决这个问题",{"2":{"1183":1}}],["steps进行调优",{"2":{"1183":1}}],["steps的5",{"2":{"1183":1}}],["steps的值是10000",{"2":{"1183":1}}],["steps的值是必要的",{"2":{"1156":1}}],["steps的10",{"2":{"1183":1}}],["steps的两倍就可以了",{"2":{"1183":1}}],["steps的过程中",{"2":{"1183":2}}],["steps的数值",{"2":{"1155":1}}],["stepsd的起点n",{"2":{"1156":1}}],["steps应该有一个确定的值可以完美地拟合训练集",{"2":{"1156":1}}],["steps可能需要增加起始值",{"2":{"1155":1}}],["steps选择初始候选值",{"2":{"1155":1}}],["steps相关",{"2":{"1155":1}}],["steps−1",{"2":{"402":2}}],["steps是热启动阶段",{"2":{"402":1}}],["steps是预热部署",{"2":{"402":1}}],["steps为分界点的分段函数",{"2":{"402":1}}],["steps",{"0":{"1156":1},"2":{"399":2,"402":1,"1155":2,"1156":1,"1159":1,"1183":7,"1242":5}}],["step中",{"2":{"185":1}}],["step",{"0":{"290":1},"2":{"59":1,"83":2,"290":1,"385":4,"402":15,"423":2,"424":2,"428":1,"510":1,"832":2,"855":1,"934":1,"1039":1,"1087":2,"1202":1,"1205":2,"1215":14,"1218":1,"1223":5,"1226":3,"1227":13,"1231":6,"1233":1,"1234":1,"1235":3,"1236":1,"1237":1,"1238":1,"1239":1,"1240":1,"1241":2,"1242":3,"1243":1,"1244":6,"1245":2,"1246":1,"1247":1,"1284":4,"1295":1,"1646":1,"2075":2,"2086":3}}],["stores",{"2":{"1329":3,"1330":1}}],["store",{"2":{"1215":4,"2077":1}}],["storage",{"2":{"1078":1,"1083":4,"1085":2,"1086":1,"1087":15,"1867":7}}],["stormblafe",{"2":{"292":1}}],["stop",{"2":{"571":1}}],["stoi",{"2":{"375":1,"1683":2}}],["stove",{"2":{"370":1}}],["stosa",{"2":{"209":2}}],["stochastic",{"0":{"1026":1,"1188":1},"2":{"209":2,"429":2,"1026":1,"1046":1,"1059":1,"1243":1,"1244":1}}],["stale",{"2":{"2070":1}}],["stage3",{"2":{"1364":1}}],["stage2",{"2":{"1364":1}}],["stage",{"2":{"1364":3}}],["stan",{"2":{"1194":1}}],["stands",{"2":{"2087":1}}],["standard",{"2":{"450":1,"703":1,"960":1,"1433":1,"1982":2}}],["standing",{"2":{"370":1,"557":1}}],["star",{"2":{"579":2}}],["startproject",{"2":{"2070":1}}],["startapp",{"2":{"2070":1}}],["starts",{"2":{"1303":1}}],["started",{"2":{"1284":1,"1304":1}}],["starting",{"2":{"1284":1}}],["start",{"2":{"83":3,"201":15,"385":3,"424":1,"731":1,"828":1,"1087":7,"1238":1,"1284":1,"1308":2,"1328":2,"1329":2,"1330":2,"1566":1,"1594":5}}],["stable",{"0":{"1348":1,"1355":1,"1750":1},"1":{"1349":1,"1350":1,"1351":1,"1352":1,"1353":1,"1354":1,"1356":1,"1357":1,"1358":1},"2":{"361":2,"370":1,"429":1,"1087":3,"1254":1,"1367":1,"1404":1,"1732":1,"1750":2,"1758":1}}],["stability",{"2":{"8":1}}],["stack",{"0":{"824":1,"826":1,"1648":1},"1":{"825":1,"826":1,"827":1,"828":1},"2":{"343":1,"384":2,"513":1,"522":1,"826":1,"1646":1,"1648":3,"1668":1,"1726":12,"1762":2}}],["status",{"2":{"768":1,"1215":1,"1566":7,"1590":3,"1728":1,"1922":7}}],["stats",{"2":{"590":2,"592":7}}],["staticfiles",{"2":{"2070":1}}],["staticvar++",{"2":{"1649":1}}],["staticvar",{"2":{"1649":3,"1653":1}}],["static",{"0":{"1649":1,"1687":1},"2":{"1087":1,"1287":1,"1436":1,"1481":3,"1611":1,"1623":1,"1629":2,"1639":4,"1649":8,"1653":1,"1683":2,"1685":1,"1704":2,"1705":5,"1923":1,"1924":1,"1974":1,"1981":1,"2008":1}}],["staticmethod",{"2":{"74":1,"380":1,"1086":1,"1100":2,"1227":3}}],["statistics",{"2":{"316":1}}],["statistical",{"2":{"237":1,"282":1,"292":1,"907":1}}],["statement",{"2":{"1621":2,"1933":1}}],["state是一个将参数id映射到相应参数状态字典的字典",{"2":{"1227":1}}],["statedict",{"2":{"1227":6}}],["state可以在处理序列的过程中携带相关信息",{"2":{"863":1}}],["state充当传输高速公路",{"2":{"863":1}}],["state和其各种gates",{"2":{"863":1}}],["state==",{"2":{"666":1}}],["state=trainstate",{"2":{"385":1,"399":1}}],["state=train",{"2":{"364":1,"385":1,"423":1}}],["state的数据类型",{"2":{"346":2}}],["states的哪些信息应该被丢弃和保留",{"2":{"865":1}}],["states经过一个额外的映射器",{"2":{"731":1}}],["states用线性层变换一次",{"2":{"482":1}}],["states",{"2":{"346":8,"731":1}}],["states相同尺寸",{"2":{"346":1}}],["state",{"0":{"867":1},"2":{"165":1,"241":1,"287":2,"364":2,"372":1,"385":7,"423":5,"453":1,"495":1,"513":2,"649":1,"666":1,"668":1,"863":1,"864":1,"878":1,"931":1,"1083":1,"1208":11,"1211":1,"1214":27,"1226":9,"1227":32,"1258":1,"1259":1,"1266":4,"1267":4}}],["std=c++11",{"2":{"1917":1}}],["std=0",{"2":{"503":4}}],["stdexcept>",{"2":{"1713":1,"1762":1,"2063":1}}],["stdlib",{"2":{"1594":1}}],["stdio",{"2":{"1590":1,"1594":1,"1710":1}}],["std的形状为",{"2":{"343":1}}],["std",{"0":{"1673":2,"1678":1,"1736":1,"1737":1,"1738":1,"1739":1,"1741":1,"1742":1,"1743":1,"1744":1,"1746":1,"1747":1,"1749":1,"1750":1,"1751":1,"1752":1,"1754":1,"1755":1,"1756":1,"1911":1,"1912":1,"1926":1,"1927":1,"1928":1,"1929":1,"2062":1},"2":{"74":4,"79":1,"343":4,"380":2,"382":2,"395":2,"1087":7,"1606":1,"1607":1,"1608":1,"1611":8,"1615":2,"1616":6,"1619":1,"1620":1,"1621":2,"1623":1,"1624":1,"1625":2,"1633":10,"1634":7,"1645":27,"1646":12,"1647":9,"1648":10,"1649":10,"1650":20,"1659":6,"1660":6,"1661":4,"1662":2,"1663":10,"1665":33,"1666":2,"1667":8,"1668":6,"1670":4,"1671":2,"1672":2,"1673":18,"1674":16,"1675":8,"1676":4,"1677":4,"1678":13,"1680":8,"1683":6,"1684":2,"1685":2,"1687":2,"1688":2,"1691":1,"1693":6,"1694":12,"1695":26,"1698":4,"1699":4,"1700":4,"1701":2,"1702":1,"1704":12,"1705":12,"1706":8,"1707":12,"1708":10,"1709":4,"1711":2,"1712":6,"1713":58,"1714":4,"1715":12,"1718":5,"1719":31,"1720":24,"1721":24,"1722":24,"1723":4,"1724":16,"1725":29,"1726":2,"1728":20,"1729":21,"1732":17,"1736":5,"1737":5,"1738":5,"1739":6,"1741":7,"1742":7,"1743":6,"1744":6,"1746":5,"1747":6,"1749":6,"1750":7,"1751":6,"1752":9,"1754":8,"1755":5,"1756":9,"1758":12,"1761":7,"1762":20,"1763":16,"1764":2,"1772":2,"1774":4,"1778":2,"1779":4,"1784":8,"1788":2,"1789":9,"1791":1,"1792":1,"1797":15,"1799":5,"1800":5,"1801":5,"1802":5,"1803":3,"1805":6,"1806":10,"1807":12,"1811":6,"1813":16,"1814":17,"1816":6,"1817":41,"1820":18,"1821":17,"1824":5,"1825":1,"1829":6,"1831":16,"1832":17,"1834":6,"1835":41,"1838":18,"1839":17,"1842":5,"1843":1,"1849":2,"1853":2,"1857":2,"1861":2,"1866":6,"1867":2,"1868":3,"1869":10,"1874":8,"1879":1,"1883":17,"1887":18,"1890":3,"1891":16,"1895":5,"1897":7,"1898":1,"1902":26,"1904":2,"1905":2,"1906":8,"1907":6,"1908":4,"1909":4,"1910":4,"1911":16,"1912":29,"1913":4,"1914":24,"1920":4,"1921":12,"1922":13,"1923":3,"1924":5,"1925":15,"1926":25,"1927":20,"1928":15,"1929":20,"1930":20,"1931":2,"1932":4,"1933":28,"1999":1,"2003":1,"2004":1,"2005":1,"2006":5,"2007":1,"2008":1,"2059":11,"2060":6,"2061":4,"2062":33,"2063":6}}],["side",{"2":{"1254":1}}],["significant",{"2":{"2087":1}}],["signal",{"2":{"1284":1}}],["signbit",{"2":{"1087":1}}],["sign",{"2":{"1087":2}}],["signed",{"2":{"1087":1}}],["sigma相乘使得swish类似lstm中的门机制",{"2":{"108":1}}],["sigma为激活函数sigmoid",{"2":{"108":1}}],["sigma",{"2":{"105":2,"108":3,"109":3,"110":1,"313":2,"343":2,"844":1,"856":1,"1087":1,"1361":2}}],["sigmoid函数复杂",{"2":{"840":1}}],["sigmoid函数在",{"2":{"840":1}}],["sigmoid等",{"2":{"785":1}}],["sigmoid",{"2":{"103":2,"108":1,"110":2,"642":1,"838":1,"839":12,"845":1,"865":1,"866":3,"868":2,"995":1,"1087":2,"1092":1,"1098":3,"1099":1,"1102":2,"1202":2,"1205":1,"1398":8}}],["sij−mij",{"2":{"944":1}}],["sij",{"2":{"944":1}}],["sij=qikjt∈rbr×bcs",{"2":{"944":1}}],["sitting",{"2":{"557":1}}],["site",{"2":{"233":1,"1078":1}}],["sinusoidal",{"0":{"1336":1},"2":{"1336":1,"1344":3}}],["sinusoidal位置编码追根溯源",{"2":{"768":1}}],["sinh",{"2":{"1087":2}}],["sinc",{"2":{"1087":2}}],["since",{"2":{"201":1,"1299":1,"1305":1}}],["sin",{"2":{"1083":1,"1087":2,"1299":1,"1336":1,"1345":2}}],["singlenumber",{"2":{"2061":2}}],["single",{"0":{"1306":1},"2":{"381":1,"385":1,"592":1,"595":1,"810":1,"935":1,"951":1,"1215":1,"1308":1,"1616":1,"1669":2,"2061":1}}],["singularities",{"2":{"305":3}}],["singular",{"2":{"233":1}}],["sind",{"2":{"370":1,"557":1}}],["si",{"2":{"173":2,"271":2}}],["si=a",{"2":{"173":1,"271":1}}],["simsekl",{"2":{"1155":1}}],["simcse",{"2":{"740":1}}],["simplifies",{"2":{"2079":1}}],["simplified",{"2":{"348":1,"361":1}}],["simply",{"2":{"591":1}}],["simplenn",{"2":{"2086":3}}],["simplelosscompute对象",{"2":{"385":1}}],["simplelosscompute",{"2":{"83":2,"364":1,"385":1,"398":1,"399":1,"410":1,"423":2,"424":2,"472":1}}],["simple",{"2":{"83":1,"209":1,"292":1,"398":1,"424":1,"472":1,"736":1,"740":2,"1254":2,"2073":1,"2083":1,"2086":3}}],["sim",{"2":{"175":9,"1000":2,"1003":1,"1004":1,"1006":2,"1007":3}}],["similarity=→x⋅→y||→x||×||→y||similarity=x→⋅y→||x→||×||y→||similarity",{"2":{"692":1}}],["similarity",{"2":{"170":1,"271":1,"685":1,"692":2,"740":2}}],["silu成为激活函数的主要选择",{"2":{"103":1}}],["silu",{"2":{"103":2,"110":5,"114":1,"154":1}}],["sixteen",{"2":{"47":1}}],["size>",{"2":{"2063":2}}],["sizeof",{"0":{"1653":1,"1678":1},"2":{"1607":8,"1611":1,"1630":1,"1633":4,"1635":1,"1653":3,"1667":7,"1678":4,"1680":6,"1710":2,"1712":1,"1820":1,"1838":1,"1912":1,"2006":2}}],["size会影响最大可实现的验证性能",{"2":{"1186":1}}],["size会在训练算法中引入更多的不确定性",{"2":{"1186":1}}],["size进行单独调优的时候是最重要的",{"2":{"1186":1}}],["size影响最强烈的那些超参数",{"2":{"1186":1}}],["size单独调优",{"2":{"1186":1}}],["size单独调整它们",{"2":{"1135":1}}],["size重新调整所有内容可能会很困难",{"2":{"1135":1}}],["size交互最强烈的超参数是优化器超参数",{"2":{"1135":1}}],["size敏感",{"2":{"1135":1}}],["size需要重新调整大多数超参数",{"0":{"1135":1}}],["size需要升级硬件",{"2":{"1134":1}}],["size增加一倍",{"2":{"1134":1}}],["size可能更容易过度拟合",{"2":{"1186":1}}],["size可能会增加资源消耗",{"2":{"1134":1}}],["size可能会减少资源消耗",{"2":{"1134":1}}],["size可能不会改变资源消耗",{"2":{"1134":1}}],["size相同的硬件上运行",{"2":{"1134":1}}],["size有很大的前期成本",{"2":{"1134":1}}],["size有关",{"2":{"1134":1}}],["size以最小化资源消耗",{"0":{"1134":1}}],["size以最小化训练时间",{"0":{"1133":1}}],["size就没有意义了",{"2":{"1133":1}}],["size将小于临界batch",{"2":{"1133":1}}],["size仍可能通过减少所需的训练步数来提供有意义的加速",{"2":{"1133":1}}],["size与效果预算进行比较只会涉及到完美缩放的范围",{"2":{"1133":1}}],["size时请记住",{"2":{"1135":1}}],["size时",{"2":{"1133":1}}],["size时重新调整所有相关超参数",{"2":{"1133":1}}],["size取决于数据集",{"2":{"1133":1}}],["size通常需要重新开始调整过程",{"2":{"1135":1}}],["size通常可以减少总步骤数",{"2":{"1134":1}}],["size通常是最大的batch",{"2":{"1133":1}}],["size通常会产生一些开销",{"2":{"1133":1}}],["size通常会减少训练时间",{"2":{"1131":1}}],["size不会再使训练步数减少",{"2":{"1133":1}}],["size不应该被当作验证集性能的可调超参数",{"2":{"1131":1}}],["size在临界值之前",{"2":{"1133":1}}],["size翻倍可能会使训练步数减半",{"2":{"1133":1}}],["size之间的验证集性能差异通常会消失",{"2":{"1186":1}}],["size之间并没有明确的关系",{"2":{"1131":1}}],["size之后就不再增加",{"2":{"1132":1}}],["size加倍",{"2":{"1132":1}}],["size并估计训练吞吐量",{"0":{"1132":1}}],["size来直接提高验证集性能",{"0":{"1186":1},"2":{"1131":2}}],["size来进行训练",{"2":{"726":1}}],["size都能获得相同的最终性能",{"2":{"1131":1}}],["size让资源消耗增加",{"2":{"1131":1}}],["size决定训练速度",{"2":{"1131":1}}],["size较小时",{"2":{"807":1}}],["size的大小时",{"2":{"1186":1}}],["size的选择造成什么影响",{"0":{"1136":1}}],["size的每一步都可以在与小batch",{"2":{"1134":1}}],["size的所有好处都假定训练吞吐量增加",{"2":{"1132":1}}],["size的增加",{"2":{"1132":1}}],["size的同时对各个维度统计有很好的鲁棒性",{"2":{"811":1}}],["size的开方也会使",{"2":{"701":1}}],["size的开方",{"2":{"701":1}}],["size的概念",{"2":{"88":1}}],["size太小的话会增加i",{"2":{"665":1}}],["size迭代获取size条",{"2":{"665":1}}],["size为k",{"2":{"902":1}}],["size为总共的字库数量",{"2":{"709":1}}],["size为2",{"2":{"399":1}}],["size为1",{"2":{"343":1,"976":1}}],["sizes",{"2":{"395":10,"1083":2,"1087":6}}],["size等参数",{"2":{"372":1}}],["size是决定训练时间和计算资源消耗的关键因素",{"2":{"1131":1}}],["size是较为理想的数值",{"2":{"1131":1}}],["size是1",{"2":{"460":1}}],["size是d",{"2":{"344":1}}],["size是batch",{"2":{"23":1}}],["size维度",{"2":{"341":1}}],["size对模型的影响",{"2":{"338":1}}],["size样本内的每个特征维度上做归一化",{"2":{"322":1}}],["size这个维度使得bn在nlp场景下存在问题",{"2":{"316":1}}],["size=7",{"2":{"1283":1}}],["size=64",{"2":{"1283":1}}],["size=k",{"2":{"902":1}}],["size=32",{"2":{"1251":1}}],["size=30",{"2":{"1235":1}}],["size=3",{"2":{"814":1,"815":1}}],["size=input",{"2":{"802":1}}],["size=len",{"2":{"423":1}}],["size=ngpus",{"2":{"423":1}}],["size=",{"2":{"383":1,"976":1}}],["size=batch",{"2":{"375":2}}],["size=10",{"2":{"1296":1}}],["size=1",{"2":{"1215":1}}],["size=128",{"2":{"700":1}}],["size=12000",{"2":{"375":1}}],["size=1的卷积",{"2":{"101":1}}],["size=config",{"2":{"364":1,"423":1}}],["size=model",{"2":{"83":1,"402":1,"424":1}}],["size=v",{"2":{"83":1,"399":1,"424":1}}],["size上",{"2":{"32":1}}],["size越大",{"2":{"20":1,"1133":1}}],["size",{"2":{"5":1,"8":4,"17":4,"23":2,"24":1,"25":1,"28":2,"30":2,"31":1,"34":5,"35":3,"36":20,"66":3,"67":1,"74":8,"76":5,"79":3,"83":10,"84":3,"99":1,"101":1,"110":10,"113":1,"185":1,"198":6,"199":9,"201":12,"315":1,"316":1,"326":3,"338":3,"341":1,"343":2,"344":7,"346":4,"364":1,"372":1,"375":3,"380":7,"382":3,"383":3,"384":1,"385":1,"394":2,"398":1,"399":13,"402":5,"410":1,"420":3,"423":1,"424":3,"428":2,"450":2,"451":1,"460":2,"472":2,"473":2,"503":1,"510":1,"518":2,"520":11,"522":2,"523":5,"529":1,"530":15,"532":2,"533":6,"558":1,"591":4,"592":3,"624":1,"698":2,"700":14,"704":4,"709":1,"723":9,"749":1,"768":1,"781":2,"802":4,"804":1,"805":2,"816":3,"820":7,"822":2,"827":2,"834":1,"933":1,"944":1,"945":1,"965":1,"976":1,"986":1,"1082":1,"1083":2,"1085":2,"1086":5,"1087":75,"1132":9,"1133":4,"1135":1,"1136":2,"1149":1,"1154":1,"1162":1,"1164":3,"1186":1,"1215":8,"1216":11,"1217":24,"1218":25,"1235":1,"1254":32,"1295":1,"1306":1,"1307":2,"1308":7,"1328":13,"1329":17,"1330":23,"1332":1,"1345":7,"1532":1,"1575":1,"1590":4,"1594":8,"1607":3,"1634":2,"1667":2,"1696":4,"1700":5,"1706":3,"1713":5,"1714":5,"1719":5,"1741":1,"1742":1,"1797":1,"1821":1,"1839":1,"1867":6,"1887":17,"1902":1,"1912":3,"1914":1,"1924":2,"1929":2,"1930":5,"1933":1,"2086":1}}],["spt",{"2":{"1695":2}}],["sprintboot",{"0":{"1497":1}}],["spring",{"0":{"1495":1,"1496":1},"2":{"1501":1}}],["speech",{"2":{"2079":2}}],["speed",{"2":{"1304":1,"1664":1,"1665":14}}],["speaking",{"2":{"1685":2,"1693":2}}],["speak",{"2":{"1685":6,"1693":7,"1866":5}}],["spec",{"2":{"1083":1}}],["specifiers",{"2":{"1677":1}}],["specified",{"2":{"802":1,"1208":1,"2086":1}}],["specific",{"2":{"726":2}}],["specifically",{"2":{"572":1}}],["specialcounter",{"2":{"1868":3}}],["special`",{"2":{"572":2}}],["special=disallowed",{"2":{"572":1}}],["special=allowed",{"2":{"572":1}}],["special=",{"2":{"572":1}}],["special",{"2":{"557":5,"571":18,"572":10,"591":20,"731":1,"2077":1}}],["specials",{"2":{"557":6}}],["specials=specials",{"2":{"557":1}}],["specials=",{"2":{"557":3}}],["specialization",{"2":{"395":4}}],["specialized",{"2":{"20":1,"47":1,"2079":1}}],["speck",{"2":{"567":1}}],["speculative",{"2":{"361":1,"429":1}}],["sp仍然需要频繁的跨节点通信",{"2":{"977":1}}],["sp仍然导致mfu比仅使用单节点tp时更差",{"2":{"977":1}}],["sp仅拆分dropout和layernorm激活的序列",{"2":{"976":1}}],["sp",{"2":{"976":1,"977":2,"1557":1}}],["spin",{"2":{"508":1,"513":1}}],["spielhaus",{"2":{"370":1}}],["split",{"0":{"830":1,"831":1},"1":{"831":1,"832":1},"2":{"557":1,"572":1,"591":2,"831":2,"1082":1,"1083":4,"1087":13,"1216":1,"1217":1,"1218":1,"1557":1}}],["splitting",{"2":{"498":4}}],["splines",{"2":{"155":1}}],["sparrow",{"2":{"1654":4}}],["spark",{"2":{"1569":1}}],["sparsity和dropout",{"2":{"429":1}}],["sparsity",{"2":{"396":1,"429":1}}],["sparse",{"2":{"47":1,"111":1,"204":1,"361":1,"475":1,"477":1,"513":1,"702":6,"772":1,"1083":2,"1086":12,"1087":18}}],["span",{"2":{"485":1}}],["spans",{"2":{"288":1,"292":2}}],["spawn",{"2":{"422":1}}],["spawning",{"2":{"422":1}}],["spacy是一个文本预处理的python库",{"2":{"373":1}}],["spacy",{"2":{"364":4,"371":4,"372":2,"373":15,"374":4,"375":4,"422":10,"423":4,"557":5}}],["spaces",{"2":{"768":2}}],["space中来获取可解释性",{"2":{"475":1}}],["space",{"0":{"1353":1},"2":{"148":1,"483":1,"499":1,"513":3,"638":1,"713":1,"740":1,"981":1,"1015":1,"1329":1,"1375":1,"1395":20}}],["spatially",{"0":{"777":1}}],["spatial",{"2":{"326":1}}],["s",{"0":{"839":1},"2":{"18":1,"20":2,"47":1,"145":10,"173":2,"271":2,"285":2,"292":1,"423":1,"499":1,"503":2,"510":2,"513":1,"564":3,"567":1,"571":3,"572":4,"579":2,"580":1,"582":7,"583":7,"585":1,"591":6,"592":1,"745":1,"746":1,"760":2,"761":1,"763":3,"765":4,"766":5,"808":2,"839":1,"845":1,"856":3,"899":1,"941":6,"944":2,"946":1,"960":6,"966":1,"968":2,"975":1,"1086":2,"1087":3,"1215":1,"1254":6,"1302":1,"1304":1,"1520":1,"1554":5,"1594":1,"1696":2,"1713":29,"1725":1,"1784":1,"1806":1,"1824":2,"1842":2,"1867":2,"1929":4,"2073":1,"2077":1,"2078":1,"2079":1,"2086":2,"2149":2,"2152":2,"2153":4,"2154":2}}],["semaphore",{"2":{"1413":1}}],["semantics",{"2":{"740":1}}],["semantic",{"2":{"685":1,"711":1,"906":1}}],["sessions",{"2":{"2070":1}}],["session中",{"2":{"1298":1}}],["session",{"2":{"1273":6,"1298":3,"1481":7,"1485":3,"1486":4,"1487":4,"1488":4}}],["selectlike",{"2":{"1489":4}}],["selectlist",{"2":{"1481":1}}],["select语句有很多属性可以详细配置每一条sql语句",{"2":{"1485":1}}],["select标签是mybatis中最常用的标签之一",{"2":{"1485":1}}],["select>",{"2":{"1481":1,"1485":3,"1489":2}}],["selectuserbynp2",{"2":{"1485":3}}],["selectuserbynp",{"2":{"1485":2}}],["selectuserbyid",{"2":{"1485":3,"1487":1}}],["selectuser",{"2":{"1481":5,"1485":1}}],["select",{"0":{"1485":1},"2":{"1087":6,"1481":2,"1485":6,"1489":4}}],["selu",{"0":{"843":1},"2":{"843":3}}],["self",{"0":{"287":1,"289":1,"920":1,"925":1},"1":{"926":1,"927":1,"928":1,"929":1},"2":{"8":19,"16":1,"20":1,"23":7,"29":2,"36":20,"38":7,"39":9,"41":3,"47":1,"50":1,"66":3,"71":6,"76":1,"77":1,"78":2,"79":1,"82":18,"83":7,"97":1,"110":19,"113":10,"114":8,"117":1,"156":2,"160":1,"198":1,"201":79,"204":3,"209":1,"218":1,"233":4,"235":2,"289":1,"292":5,"330":2,"334":1,"343":23,"344":39,"346":14,"361":1,"380":12,"382":1,"394":10,"398":6,"399":19,"410":3,"442":1,"450":16,"472":6,"503":52,"512":1,"517":1,"522":7,"523":20,"528":2,"529":14,"532":7,"533":24,"535":2,"538":1,"558":2,"571":14,"572":5,"573":2,"591":27,"592":10,"649":2,"666":1,"701":7,"702":21,"703":16,"719":1,"723":21,"740":2,"759":1,"768":1,"911":1,"914":1,"931":2,"985":1,"1083":55,"1085":76,"1086":27,"1087":743,"1111":2,"1114":3,"1205":23,"1208":2,"1210":1,"1211":21,"1212":4,"1213":5,"1214":73,"1215":40,"1216":52,"1217":60,"1218":88,"1226":10,"1227":26,"1237":1,"1238":1,"1240":1,"1246":2,"1247":3,"1250":24,"1254":15,"1257":15,"1284":1,"1295":15,"1312":1,"1315":1,"1328":4,"1329":4,"1330":10,"1339":1,"1341":1,"1343":2,"1344":2,"1345":9,"1350":2,"2086":9}}],["segmentembeddings",{"2":{"722":1}}],["segment",{"2":{"722":1}}],["sebastian",{"2":{"638":1}}],["separate",{"2":{"810":2}}],["separately",{"2":{"99":1,"419":1}}],["separable",{"0":{"777":1}}],["sep",{"2":{"555":1,"721":1,"722":2}}],["securities",{"2":{"1949":1}}],["seconds",{"2":{"1895":1}}],["second",{"2":{"1096":1,"1284":1,"1714":1,"1756":1,"1805":1,"1807":1,"1921":1,"1925":1,"1933":2,"2062":1}}],["secondary",{"2":{"167":1,"259":1}}],["sections",{"2":{"1087":5}}],["section",{"2":{"543":1}}],["secretly",{"2":{"508":1,"513":1,"542":1,"543":1,"737":1,"740":2}}],["seamless",{"2":{"1086":1}}],["search获得较好的结果",{"0":{"1177":1}}],["search很简单",{"2":{"1175":1}}],["search遍历中的最佳点位于搜索空间的边界",{"2":{"1175":1}}],["search以一致且数据上可重现的方式运行",{"2":{"1175":1}}],["search可以被认为是",{"2":{"1175":1}}],["search而不是更复杂的黑盒优化算法",{"0":{"1175":1}}],["search的实现",{"0":{"1176":1}}],["search的非自适应性质使得我们可以基于最终验证误差",{"2":{"1175":1}}],["search的优势包括",{"2":{"1175":1}}],["search的优化方式不再适用",{"2":{"1153":1}}],["search的操作步骤如下",{"2":{"902":1}}],["search搜索",{"2":{"1152":1}}],["search算法不可用",{"2":{"1176":1}}],["search算法",{"2":{"1144":1}}],["search算法通过这种方式在保持一定搜索广度的同时",{"2":{"904":1}}],["search过程",{"2":{"904":1}}],["search会根据某种评估指标",{"2":{"904":1}}],["search方法",{"2":{"901":1}}],["search",{"0":{"900":1,"902":1,"904":1,"1755":1},"1":{"901":1,"902":1,"903":1,"904":1},"2":{"597":1,"638":1,"845":2,"902":1,"983":1,"985":1,"1150":1,"1175":3,"1732":1,"1755":2,"1758":1}}],["searching",{"2":{"108":1,"156":1}}],["season",{"2":{"513":1}}],["seas",{"2":{"432":1}}],["server",{"2":{"1569":1}}],["server的最新技术水平",{"2":{"980":1}}],["servlet",{"2":{"1493":1}}],["serving",{"2":{"429":1}}],["service",{"2":{"241":1,"1161":1}}],["serializetostring",{"2":{"1273":1}}],["serialized",{"2":{"723":1}}],["serial",{"2":{"480":1,"513":1}}],["series",{"2":{"429":2}}],["seekp",{"2":{"1821":3,"1839":3}}],["seekg",{"2":{"1821":7,"1839":7}}],["seed",{"2":{"1096":1,"1098":3,"1205":1,"1215":4,"1308":5}}],["see",{"2":{"428":1,"567":1,"1254":2}}],["setx",{"2":{"2006":2}}],["set等",{"2":{"1914":1}}],["set>",{"2":{"1724":3,"1806":2}}],["setter",{"2":{"1709":1}}],["settings",{"2":{"1215":1}}],["setting",{"2":{"572":3,"2062":1}}],["setw",{"2":{"1673":2,"1817":1,"1835":1}}],["setage",{"2":{"1655":2}}],["setattr",{"2":{"1210":2,"1214":1}}],["setvalue",{"2":{"1638":1}}],["setprecision",{"2":{"1608":2,"1673":2,"1817":2,"1835":2}}],["setpwd",{"2":{"1487":1}}],["setup",{"2":{"1309":2}}],["setitem",{"2":{"1085":1}}],["sets",{"2":{"1225":1}}],["setstate",{"2":{"1083":1,"1214":1,"1227":1}}],["sets求相关性矩阵",{"2":{"84":1}}],["set",{"0":{"1422":1,"1724":1,"1806":1},"2":{"385":1,"423":3,"503":3,"557":2,"572":1,"935":1,"951":1,"1087":3,"1095":2,"1116":1,"1208":2,"1214":3,"1215":1,"1227":1,"1284":1,"1308":2,"1329":1,"1330":1,"1481":1,"1487":1,"1559":3,"1700":3,"1724":9,"1795":1,"1806":10,"1982":2,"2062":1,"2063":2,"2086":1}}],["sendtestemail",{"2":{"2070":1}}],["send配合使用",{"2":{"1575":1}}],["send和mpi",{"2":{"1573":1}}],["send",{"2":{"1284":1,"1575":1,"1590":1}}],["sentiment",{"2":{"906":1,"2079":1}}],["sentencepiece的",{"2":{"595":1}}],["sentencepiece",{"2":{"591":1,"638":2}}],["sentence",{"2":{"289":1,"292":2,"326":2,"569":6,"628":1,"638":1,"721":1,"731":3,"736":5,"740":2,"808":2,"899":1,"1315":1}}],["sennrich",{"2":{"361":1,"638":1}}],["sensitive",{"2":{"204":1}}],["sensory",{"2":{"163":1}}],["several",{"2":{"167":1,"259":1,"370":1}}],["seq​t​​=t​i−person",{"2":{"1324":1}}],["seqt=ti−person",{"2":{"1324":1}}],["seq2seq结构呢",{"2":{"891":1}}],["seq2seq结构不再要求输入和输出序列有相同的时间长度",{"2":{"886":1}}],["seq2seq主要由一个编码器和一个解码器组成",{"2":{"885":1}}],["seq2seq将输入序列转换为输出序列",{"2":{"885":1}}],["seq2seq也能广泛地应用到各种不同的技术上",{"2":{"885":1}}],["seq2seq场景存在一种因果关系",{"2":{"536":1}}],["seq2seq中的两种attention机制",{"2":{"292":1}}],["seq2seq",{"0":{"237":1,"877":1,"884":1,"885":1,"886":1,"890":1,"891":1,"892":1,"893":1,"899":1},"1":{"878":1,"879":1,"880":1,"881":1,"882":1,"883":1,"887":1,"888":1,"889":1,"890":1,"892":1,"894":1,"895":1,"896":1,"897":1,"898":1},"2":{"237":1,"885":1,"888":1,"1312":1,"1317":3,"1332":2,"1404":1}}],["seq2seq角度",{"0":{"165":1},"2":{"157":1}}],["seqlen",{"2":{"201":31,"1345":3}}],["seqence",{"2":{"180":1}}],["sequenze",{"0":{"823":1}}],["sequentiallr",{"0":{"1247":1},"2":{"1247":1}}],["sequential对象",{"2":{"450":2}}],["sequential就是两个编码合并的结果",{"2":{"449":2}}],["sequential",{"2":{"160":1,"209":2,"233":1,"429":1,"449":5,"703":4,"1086":1,"1207":2,"1217":1}}],["sequence的函数逼近属性",{"2":{"93":1}}],["sequence函数通用逼近器",{"2":{"93":1}}],["sequences",{"0":{"1616":1},"2":{"82":1,"450":1,"591":1,"688":1,"703":1,"1086":1,"1329":2}}],["sequence",{"0":{"68":1,"283":2,"934":1,"1912":1},"1":{"69":1,"70":1,"71":1,"72":1,"73":1,"74":1,"75":1,"76":1},"2":{"17":1,"49":1,"50":2,"59":1,"69":2,"77":1,"78":2,"89":6,"90":4,"165":1,"201":1,"217":1,"233":1,"235":1,"237":2,"283":2,"290":4,"292":6,"420":4,"451":1,"460":3,"543":1,"552":1,"572":2,"573":2,"650":1,"747":2,"748":2,"853":1,"885":3,"906":1,"932":1,"934":1,"975":1,"977":1,"981":1,"1087":44,"1309":2,"1312":2,"1330":5,"1904":1,"1912":13}}],["seq",{"2":{"23":1,"25":1,"28":2,"30":2,"31":2,"34":6,"35":3,"36":20,"66":2,"70":2,"79":3,"84":8,"198":6,"199":7,"201":3,"326":2,"346":1,"380":2,"382":3,"450":3,"518":2,"520":11,"522":1,"530":16,"532":1,"533":1,"700":5,"704":2,"723":4,"945":2,"965":2,"974":1,"1207":2,"1216":7,"1217":3,"1218":11,"1328":7,"1329":5,"1330":12,"1331":1,"1332":1,"1345":12}}],["sudo",{"2":{"1530":1,"1537":3,"1584":6}}],["success",{"2":{"1728":1,"1922":1}}],["successful",{"2":{"1683":1}}],["successfully",{"2":{"1273":1,"1274":1,"1398":1,"1761":1}}],["succsessfully",{"2":{"1270":1}}],["such",{"2":{"167":1,"259":1,"688":1,"1300":1,"2079":1,"2081":1,"2082":1}}],["sutton",{"2":{"1194":1}}],["supports",{"2":{"2079":1}}],["support",{"2":{"1083":1,"2021":1,"2087":1}}],["supplied",{"2":{"557":1}}],["superproject",{"2":{"1980":1}}],["superposition",{"2":{"118":1,"137":1}}],["superglue",{"2":{"1315":1,"1317":1}}],["supervised",{"2":{"740":2,"1312":1}}],["super",{"2":{"8":1,"23":1,"83":1,"110":1,"113":1,"114":1,"201":3,"343":3,"344":2,"346":2,"399":1,"450":1,"503":1,"522":1,"523":1,"532":1,"533":1,"592":1,"701":1,"702":1,"703":1,"723":1,"1205":2,"1208":1,"1211":1,"1212":1,"1213":1,"1215":2,"1216":4,"1217":3,"1218":5,"1242":1,"1257":1,"1295":1,"1345":1,"2086":1}}],["sui",{"2":{"543":1}}],["sunday",{"2":{"564":2,"1728":2}}],["sun",{"2":{"543":1}}],["su",{"2":{"429":1}}],["sure",{"2":{"1350":1}}],["surprise",{"2":{"230":1}}],["surprising",{"2":{"136":1,"156":1}}],["surpassing",{"2":{"156":1}}],["survey",{"2":{"150":1,"263":1,"292":1,"429":3,"711":1,"740":1,"768":1}}],["sumofsquares",{"2":{"1883":4}}],["summary",{"2":{"1404":1}}],["summarywriter",{"0":{"1278":1},"2":{"1215":2,"1278":2,"1279":1,"1280":1,"1282":1,"1283":1}}],["summarization",{"2":{"906":1}}],["summarized",{"2":{"736":1}}],["sum为两个随机矩阵",{"2":{"209":1}}],["sum^",{"2":{"170":1,"189":4,"271":1}}],["sum",{"2":{"54":1,"71":2,"119":1,"173":1,"178":2,"183":2,"191":7,"270":2,"271":1,"380":1,"398":1,"399":1,"503":2,"692":3,"739":1,"833":3,"847":1,"899":1,"903":1,"943":1,"961":1,"970":1,"971":1,"1087":5,"1092":1,"1213":1,"1215":3,"1295":1,"1328":1,"1329":7,"1330":1,"1331":3,"1339":3,"1398":4,"1459":1,"1594":13,"1607":3,"1620":3,"1621":3,"1623":4,"1699":2,"1707":18,"1710":5,"1746":3,"1883":1,"1914":5}}],["subdirectory",{"2":{"1980":2,"1999":2}}],["subdirectory命令进行管理",{"2":{"1979":1}}],["subcommands",{"2":{"2070":1}}],["subcircle",{"2":{"1656":1}}],["subclass",{"2":{"1086":1,"1087":1}}],["subfolders",{"2":{"1302":1}}],["submodule",{"2":{"1214":1}}],["subtraction",{"2":{"1916":4,"1917":10}}],["subtract",{"2":{"1087":4,"1999":3}}],["subtle",{"2":{"592":1}}],["subgradient",{"2":{"838":1,"1046":1}}],["subword算法",{"2":{"638":2}}],["subwords",{"2":{"587":1,"605":1,"606":1,"638":1}}],["subword划分出来的词片段可以用来组成更大的词",{"2":{"567":1}}],["subword",{"2":{"564":1,"567":2,"574":1,"587":1,"601":2,"628":2,"638":5}}],["subln",{"2":{"503":2}}],["sublayer是attention层或feed",{"2":{"344":1}}],["sublayer是待执行的子层操作",{"2":{"344":1}}],["sublayerconnection则是",{"2":{"523":1}}],["sublayerconnection实例内部会做残差连接和层归一化",{"2":{"523":2}}],["sublayerconnection代码如下",{"2":{"344":1}}],["sublayerconnection",{"2":{"344":5,"394":1,"523":1,"533":1}}],["sublayerconnection类在其内部会构造",{"2":{"523":1}}],["sublayerconnection类",{"2":{"344":1}}],["sublayer表示经过的变换",{"2":{"330":1}}],["sublayer",{"2":{"38":2,"39":3,"82":3,"330":2,"344":16,"394":2,"523":9,"529":3,"533":6,"914":1}}],["sub",{"2":{"456":1,"563":1,"829":1,"1085":1,"1087":2,"1097":4,"1098":2,"1713":3}}],["subject",{"2":{"130":2,"145":1}}],["substring",{"2":{"1713":1}}],["substr",{"2":{"572":4,"1713":3}}],["substrs",{"2":{"572":2}}],["subsequent",{"2":{"74":10,"79":1,"83":2,"84":3,"380":2,"382":5,"428":1,"472":1,"529":1}}],["subspaces",{"2":{"5":1}}],["hpp",{"2":{"1628":1}}],["hpc",{"2":{"1569":1}}],["h>",{"2":{"1590":2,"1594":3,"1710":1}}],["h3",{"2":{"1388":1,"1389":1,"1393":1}}],["h2",{"2":{"1388":1,"1389":1,"1393":1}}],["hf",{"2":{"980":1}}],["hfinal=h+hknowℎfinal=ℎ+ℎknowℎ",{"2":{"141":1}}],["h表示组数等于头数",{"2":{"937":1,"953":1}}],["h表示注意力头的数量",{"2":{"335":1}}],["hd",{"2":{"927":1}}],["h​t​​",{"2":{"892":1,"899":1}}],["h可以对序列形的数据提取特征",{"2":{"878":1}}],["hh",{"2":{"856":1}}],["hw的均值",{"2":{"810":1}}],["hsplit",{"2":{"1087":3}}],["hs",{"2":{"738":1}}],["hstack",{"2":{"201":1}}],["h作为q",{"2":{"731":1}}],["hôw",{"2":{"552":1}}],["héllò",{"2":{"552":1}}],["h=8",{"2":{"449":1,"703":1}}],["h=64",{"2":{"7":1}}],["h=64d",{"2":{"7":1}}],["h=64dk=dv=dmodel",{"2":{"7":1}}],["hxw",{"2":{"325":2}}],["h100",{"2":{"799":1}}],["h1",{"2":{"325":1,"1388":2,"1389":1,"1392":2,"1393":2,"1394":15}}],["h×w",{"2":{"315":1,"340":1}}],["hugulas",{"2":{"513":1}}],["hugging",{"2":{"513":1}}],["huggingface详细教程之tokenizer库",{"2":{"638":1}}],["huggingface",{"2":{"346":1,"387":1,"638":2,"1309":1,"1332":1,"1347":1}}],["hurts",{"2":{"399":1}}],["huang",{"2":{"361":1}}],["hungry",{"2":{"261":1}}],["human",{"2":{"156":1,"591":2,"2073":1}}],["h4中包含最多信息的是当前的输入",{"2":{"253":1}}],["h0输入到decoder中",{"2":{"889":1}}],["h0是第一个隐状态",{"2":{"249":1}}],["h0",{"2":{"249":1}}],["hbm的访问次数是决定注意力运行时间的主要因素",{"2":{"945":1,"965":1}}],["hbm",{"2":{"206":1,"940":1,"941":2,"942":3,"944":2,"959":3,"960":2,"962":1}}],["h为head数",{"2":{"199":1}}],["h为head个数",{"2":{"7":1}}],["hypot",{"2":{"1087":2}}],["hypothesis",{"2":{"137":2,"513":1,"713":1,"715":1}}],["hyperband的论文也有类似的观点",{"2":{"1175":1}}],["hyperparameters",{"2":{"449":1}}],["hypersphere",{"2":{"350":1,"513":1}}],["hybrid",{"0":{"217":1},"2":{"157":1,"215":1,"217":2}}],["havok",{"2":{"1937":1}}],["have",{"2":{"167":1,"259":1,"402":1,"768":1,"1283":1,"1350":1,"2076":1}}],["halton",{"2":{"1176":1}}],["half",{"2":{"503":2,"1087":1,"1214":2}}],["hawkins",{"2":{"754":1}}],["hasattr",{"2":{"723":1}}],["has",{"2":{"591":1,"1087":1,"1233":1,"1284":1,"1308":1,"1329":2,"1737":2,"1927":2,"1931":1,"2076":1,"2079":1}}],["hashable",{"2":{"1227":1}}],["hash",{"2":{"1083":1}}],["hashmap",{"2":{"550":1,"1485":1}}],["hashing",{"2":{"204":1}}],["handler",{"2":{"1284":4}}],["handle",{"2":{"572":1,"1083":11,"1212":2,"1213":2}}],["handling",{"2":{"233":1,"1762":1}}],["haystack",{"2":{"477":1,"513":1}}],["harris",{"2":{"713":1}}],["harvardnlp",{"2":{"432":1}}],["harvard",{"2":{"432":1}}],["hardshrink",{"2":{"1087":1}}],["hardswish",{"0":{"845":1},"2":{"845":4}}],["hard",{"2":{"370":1,"845":1}}],["harnessing",{"2":{"233":1}}],["hats",{"2":{"370":1}}],["hat",{"2":{"263":1,"343":4,"484":1,"899":6,"1004":6,"1007":2,"1537":1}}],["hadoop",{"2":{"1569":1}}],["haddow",{"2":{"638":1}}],["had",{"2":{"167":1,"259":1,"592":1,"1254":1}}],["hadamard乘积",{"2":{"109":3}}],["haoc",{"2":{"233":1}}],["hao",{"2":{"156":4,"233":1,"543":1}}],["htsc",{"2":{"1949":1}}],["hth",{"2":{"892":1,"899":1}}],["htℎtℎ",{"2":{"285":1}}],["ht=g",{"2":{"240":1}}],["ht−1ℎt−1ℎ",{"2":{"240":1,"510":2}}],["ht−1",{"2":{"240":2}}],["http",{"2":{"156":1,"387":1,"432":1,"1195":1,"1302":1,"1309":1,"1476":1,"1481":2,"1528":1}}],["https",{"2":{"47":1,"95":2,"156":6,"233":3,"292":1,"361":5,"370":2,"373":2,"387":10,"429":9,"432":2,"503":1,"513":4,"543":2,"638":5,"713":1,"740":4,"768":5,"840":1,"1197":1,"1254":1,"1302":1,"1309":2,"1332":1,"1347":1,"1476":1,"1566":3,"1605":3,"1989":1,"1990":1}}],["html",{"2":{"137":1,"156":1,"361":2,"370":1,"387":1,"429":1,"432":1,"740":1,"1254":1,"1476":1,"1605":3}}],["horizontal",{"2":{"1616":1}}],["horace",{"2":{"95":1}}],["host2ip",{"2":{"1594":1}}],["host1ip",{"2":{"1594":1}}],["hostname",{"2":{"1594":1}}],["host",{"2":{"1583":1,"1594":1,"2086":1}}],["home",{"2":{"1506":1,"1509":1}}],["hop",{"2":{"1087":1}}],["hook来注册",{"2":{"1208":2}}],["hook",{"2":{"1083":5,"1098":2,"1099":6,"1208":8,"1212":4,"1213":6,"1214":20,"1227":20}}],["hooks",{"2":{"1082":1,"1114":1,"1208":11,"1214":4,"1226":6}}],["hooks是主要用二次开发等情况",{"2":{"664":1}}],["hoffmann",{"2":{"638":1}}],["holz",{"2":{"370":1}}],["holder",{"2":{"1676":2}}],["holding",{"2":{"380":1}}],["hold",{"2":{"136":1,"156":1}}],["honoria",{"2":{"292":1}}],["how",{"0":{"1380":1,"2084":1},"1":{"2085":1,"2086":1},"2":{"204":1,"233":2,"245":1,"314":1,"513":1,"545":1,"557":2,"722":1,"768":1,"1089":1,"1215":1,"1254":1,"2073":1,"2086":1}}],["hot型的矩阵运算简化为了查表操作",{"2":{"694":1}}],["hot型的矩阵和其它矩阵相乘",{"2":{"694":1}}],["hot的全连接层的参数",{"2":{"694":1}}],["hot的缺点",{"2":{"682":1}}],["hot为输入的单层全连接",{"2":{"722":1}}],["hot为输入",{"2":{"694":1}}],["hot编码不具备语义",{"2":{"685":1}}],["hot编码",{"2":{"684":1,"722":1}}],["hot编码形式",{"2":{"456":1}}],["hot",{"2":{"192":1,"458":1,"473":1,"676":1,"681":1,"700":9}}],["hot输出中最大值对应的1按输入元素值的大小分配给其他位置",{"2":{"180":1,"191":1}}],["hot向量",{"2":{"180":2,"545":1}}],["h⋅w",{"2":{"125":1}}],["h⋅w1",{"2":{"125":2}}],["hibernate",{"2":{"1492":1}}],["hints",{"2":{"1082":1}}],["hinton",{"2":{"840":1,"1048":1,"1049":1,"1151":1}}],["hinton在论文",{"2":{"104":1}}],["hi",{"2":{"986":2}}],["histogram",{"2":{"1087":4}}],["history",{"2":{"985":2,"1330":6}}],["histc",{"2":{"1087":1}}],["his",{"2":{"557":1}}],["hippocampal",{"2":{"490":2}}],["highest",{"2":{"577":1,"579":2,"588":1,"592":1}}],["highway",{"2":{"298":2,"301":3,"361":1}}],["high",{"2":{"232":1,"387":1,"588":1,"1303":1,"2009":1}}],["hierarchical",{"0":{"184":1},"2":{"157":1,"184":1}}],["hide",{"2":{"74":1,"79":1,"380":1}}],["hidden",{"2":{"5":1,"17":1,"110":6,"114":10,"165":1,"201":1,"241":1,"287":1,"346":11,"513":1,"649":1,"723":5,"749":1,"878":1,"931":1,"945":1,"965":1}}],["hex",{"2":{"1817":2,"1835":2}}],["hendricks",{"2":{"638":1}}],["hell",{"2":{"595":1}}],["helloworld",{"2":{"1436":1,"1966":1}}],["hello",{"2":{"545":1,"700":1,"986":2,"1436":1,"1510":1,"1590":1,"1606":1,"1615":1,"1624":2,"1704":2,"1708":1,"1713":15,"1715":7,"1728":1,"1729":1,"1803":1,"1816":2,"1820":1,"1824":1,"1834":2,"1838":1,"1842":1,"1887":1,"1902":1,"1906":2,"1912":3,"1914":1,"1925":1,"1926":2,"1928":2,"1929":2,"1930":1,"1966":2,"1973":1,"1975":1,"1987":1,"1993":1}}],["helps",{"2":{"2073":1}}],["help=",{"2":{"1215":11}}],["helpful",{"2":{"986":1}}],["helper",{"2":{"449":1}}],["help",{"2":{"314":1,"1198":1,"1308":2,"2070":1,"2079":1}}],["he初始化方法则特别适用于relu激活函数",{"2":{"403":1}}],["hemd",{"2":{"370":1}}],["height",{"2":{"315":1,"1083":2,"1607":5,"1774":4,"1789":5,"1791":3,"1792":5,"1921":3}}],["here",{"2":{"428":1,"573":1,"591":1,"2086":1}}],["herd",{"2":{"370":1}}],["her",{"2":{"131":1}}],["he",{"2":{"95":1,"768":1}}],["heapvar",{"2":{"1648":7}}],["heap",{"0":{"1648":1},"2":{"1647":1,"1648":3,"1668":1}}],["health",{"2":{"1227":1,"2070":2}}],["heaviside",{"2":{"1087":2}}],["heavy",{"2":{"20":1,"47":1}}],["head​h​​",{"2":{"927":1}}],["head​1​​",{"2":{"927":1}}],["headh",{"2":{"927":1}}],["head1",{"2":{"927":1}}],["header",{"2":{"571":2,"1923":2}}],["head提前加在每个中间层上",{"2":{"482":1}}],["head提前加在每个中间layer上",{"2":{"475":1}}],["head=8",{"2":{"448":1}}],["head=2",{"2":{"80":1}}],["head间信息交流可以为视觉",{"2":{"41":1}}],["head数来并行划分",{"2":{"36":1}}],["head数",{"2":{"36":4,"199":3}}],["head的作用就是为了保证这些pattern都能够被抽取出来",{"2":{"13":1}}],["heads时为多头注意力",{"2":{"201":1}}],["heads=1时为多查询注意力",{"2":{"201":1}}],["heads=n",{"2":{"119":1,"201":1}}],["heads=3",{"2":{"34":1}}],["heads",{"2":{"8":15,"20":1,"23":1,"28":1,"29":1,"34":4,"41":1,"47":3,"119":2,"201":39,"503":20,"935":1,"951":1,"1216":17,"1217":25,"1218":21}}],["head",{"0":{"649":1,"650":1,"925":1,"930":1,"931":1,"932":1,"955":1},"1":{"926":1,"927":1,"928":1,"929":1,"933":1,"934":1,"956":1,"957":1},"2":{"5":2,"7":1,"8":5,"17":1,"19":1,"20":4,"28":1,"35":1,"38":1,"39":2,"40":1,"41":3,"42":3,"43":2,"47":5,"71":3,"97":1,"119":2,"130":2,"135":2,"198":5,"199":3,"201":23,"330":1,"419":1,"420":6,"445":3,"449":2,"503":12,"517":1,"525":1,"535":3,"538":1,"650":1,"698":1,"765":1,"914":1,"915":1,"927":3,"928":2,"932":1,"935":1,"938":1,"941":1,"945":1,"951":1,"954":1,"960":1,"965":1,"1216":6,"1217":14,"1218":6,"1363":1,"1515":1}}],["h和",{"2":{"31":1}}],["h个小权重矩阵",{"2":{"26":2}}],["h个小权重矩阵来完成",{"2":{"25":1}}],["h是注意力头数",{"2":{"23":1}}],["h^v",{"2":{"9":1}}],["h^k",{"2":{"9":1}}],["h^q",{"2":{"9":1}}],["h维度的信息",{"2":{"9":1}}],["h",{"0":{"184":1},"2":{"5":1,"7":1,"17":1,"23":7,"27":1,"29":2,"30":1,"31":1,"34":2,"35":2,"36":23,"125":3,"157":1,"184":2,"199":1,"201":11,"240":3,"285":2,"301":6,"315":4,"325":5,"326":3,"337":1,"340":1,"341":9,"449":1,"503":4,"510":1,"614":1,"698":1,"703":1,"739":1,"802":3,"807":1,"808":1,"809":1,"810":1,"926":1,"927":1,"957":4,"986":1,"1082":1,"1102":4,"1110":1,"1254":20,"1308":2,"1363":1,"1388":2,"1398":8,"1547":1,"1557":1,"1624":2,"1628":5,"1704":1,"1713":5,"1715":2,"1774":5,"1789":2,"1791":2,"1792":2,"1916":5,"1917":2,"1923":6,"1974":1,"1981":1,"1999":6}}],["其特点是内存占用更低",{"2":{"1801":1}}],["其特点如下",{"2":{"281":1,"282":1,"283":1,"284":1,"517":1,"687":1}}],["其元素都是指针",{"2":{"1705":1}}],["其元素的下标范围是从",{"2":{"1623":1}}],["其名称与类名相同",{"2":{"1675":1,"1676":1}}],["其名称中的",{"2":{"406":1}}],["其指向的内存地址就不可改变",{"2":{"1612":1}}],["其基于",{"2":{"1315":1}}],["其声称",{"2":{"1242":1}}],["其叶节点是输入张量",{"2":{"1113":1}}],["其共轭转置为",{"2":{"1082":1}}],["其大小取决于序列长度",{"2":{"981":1}}],["其大小仅为",{"2":{"608":1}}],["其庞大的关键值",{"2":{"956":1}}],["其质量高于mqa但比mha快",{"2":{"937":1,"953":1}}],["其等价表示为",{"2":{"882":1}}],["其典型结构如下图所示",{"2":{"851":1,"1464":1}}],["其导数收敛于",{"2":{"839":1}}],["其梯度恒为",{"2":{"838":1}}],["其通道数",{"2":{"773":2}}],["其通过使用门控单元来学习管理信息",{"2":{"298":1}}],["其通过奇异值微调和权重自适应策略",{"2":{"218":1}}],["其通过将",{"2":{"147":1}}],["其bijbijb",{"2":{"766":1}}],["其对不同词的注意偏差都保持一致",{"2":{"760":1}}],["其对应了人类知识获取的三个不同阶段",{"2":{"141":1}}],["其改进思路值得借鉴",{"2":{"735":1}}],["其二是deocder部分自重建的交叉熵损失",{"2":{"727":1}}],["其二是训练数据需要被充分利用到",{"2":{"727":1}}],["其一是encoder部分的mlm损失",{"2":{"727":1}}],["其一是重建任务必须要对编码质量有足够的要求",{"2":{"727":1}}],["其目标是初步学习出通用embedding",{"2":{"726":1}}],["其目标是训练出更适合embedding任务的pre",{"2":{"726":1}}],["其目的就是为了让分布稳定下来",{"2":{"321":1}}],["其目的是学习输入",{"2":{"301":1}}],["其目的是取出数据库中对应的value值",{"2":{"164":1}}],["其背后逻辑是概念可以通过向量之间的相互关系和计算动态得出的",{"2":{"691":1}}],["其含义包含了视觉",{"2":{"689":1}}],["其意义是立即",{"2":{"683":1}}],["其意义是立即的",{"2":{"683":1}}],["其意义是瞬息",{"2":{"683":1}}],["其意义是输入编码",{"2":{"449":2}}],["其余位置是0",{"2":{"681":1}}],["其余注意力基本可以看成是0",{"2":{"14":1}}],["其满足唯一性和可区分性",{"2":{"678":1}}],["其任务是在给定句子嵌入和噪音输入的条件下预测下一个句子嵌入",{"2":{"634":1}}],["其组成一个巨大的知识库",{"2":{"623":1}}],["其",{"2":{"616":1,"623":1,"1704":1}}],["其功能是将字节表征池化为",{"2":{"614":1}}],["其功能与torch",{"2":{"343":1}}],["其稀有字符可能会不必要地占用词汇表并限制其紧凑性",{"2":{"606":1}}],["其损失会很大",{"2":{"602":1}}],["其结构如下",{"0":{"858":1}}],["其结论是大模型的词表大小同样适用于scaling",{"2":{"561":1}}],["其结果必须是布尔值",{"2":{"1619":1}}],["其结果对最终输出的影响越弱",{"2":{"333":1}}],["其结果不应该破坏数据之间的可比较性",{"2":{"316":1,"318":1}}],["其结果也只是value第一段的权重",{"2":{"33":1}}],["其学习通用表征的上限更高",{"2":{"542":1}}],["其细节如下",{"2":{"525":2}}],["其形状在解码器内部始终保持不变",{"2":{"530":1}}],["其形状在编码器内部始终保持不变",{"2":{"520":1}}],["其形状也是",{"2":{"518":1}}],["其形状是",{"2":{"66":1,"450":1,"704":1}}],["其表示数据从某个位置传递到另一个位置的最大长度",{"2":{"511":1}}],["其行为可以通过训练来塑造",{"2":{"508":1}}],["其时间演化如图右侧所示",{"2":{"507":1}}],["其要求在网络前向传播的过程中保留所有层的激活值",{"2":{"495":1}}],["其要学的参数其实就是三个矩阵",{"2":{"9":1}}],["其代表在transformer视角下的",{"2":{"472":1}}],["其代表从一个源序列生成一个目标序列的操作",{"2":{"237":1}}],["其本质上就是",{"2":{"714":1}}],["其本质上是用一个",{"2":{"681":1}}],["其本质思想是允许网络中的信息和梯度直接跨过一个或多个层进行传播",{"2":{"470":1}}],["其本质原因是因为qkv权重占比着大语言模型50",{"2":{"162":1}}],["其语义意义上越接近",{"2":{"692":1}}],["其语义完全不同",{"2":{"459":1}}],["其语言建模目标是根据给定的上下文来预测下一个单词",{"2":{"239":1}}],["其参数量为354",{"2":{"1363":1}}],["其参数都从batch类的实例中获取",{"2":{"451":1}}],["其参数是encoder",{"2":{"449":1}}],["其掩码是",{"2":{"450":1}}],["其内容是token在词表对应的编号",{"2":{"450":1}}],["其内部的masked",{"2":{"449":1}}],["其内部表示和权重需要更频繁的调整",{"2":{"144":1}}],["其成员变量如下",{"2":{"450":1}}],["其包括了linear和softmax",{"2":{"449":1}}],["其包含质量过滤",{"2":{"364":1}}],["其不仅仅代表对应token独立的含义",{"2":{"437":1}}],["其地址为",{"2":{"432":1}}],["其最先出现在nvidia",{"2":{"420":1}}],["其最终目的是输出一个新向量",{"2":{"166":1}}],["其期望数量级为1",{"2":{"403":1}}],["其思想是将误差信号",{"2":{"1443":1}}],["其思想最早起源于",{"2":{"50":1}}],["其思路如下",{"2":{"763":1}}],["其思路是",{"2":{"399":1}}],["其保留率通常设为更接近1的数",{"2":{"396":1}}],["其只能以串行方式进行预测",{"2":{"390":1}}],["其会对层的输入进行归一化处理",{"2":{"517":1}}],["其会调用传入的分词器参数对语句进行分词",{"2":{"557":1}}],["其会调用英语分词器对语句进行分词",{"2":{"375":1}}],["其会调用德语分词器对语句进行分词",{"2":{"375":1}}],["其会根据query和key的相似性来计算每个key的相似度或者匹配程度",{"2":{"164":1}}],["其提供了分词功能",{"2":{"373":1}}],["其定义为",{"2":{"359":1}}],["其λ",{"2":{"341":1}}],["其三维的表示如下图",{"2":{"341":1}}],["其作用有两种",{"2":{"450":1}}],["其作用就是遮掉输入矩阵的一部分",{"2":{"409":1}}],["其作用就是在",{"2":{"59":1}}],["其作用点依然如下",{"2":{"396":1}}],["其作用是将一个样本列表组合成一个张量的mini",{"2":{"384":1,"558":1}}],["其作者发现",{"2":{"337":1}}],["其训练效果是要优于post",{"2":{"334":1}}],["其训练过程中需要大量算力",{"2":{"138":1}}],["其原因是",{"2":{"500":1}}],["其原因也许是因为",{"2":{"326":1}}],["其原因就是这两个核心组件各有分工又彼此配合",{"2":{"120":1}}],["其identity",{"2":{"301":1}}],["其权重值为",{"2":{"1000":1}}],["其权重也不是永远为0",{"2":{"301":1}}],["其权重越大",{"2":{"169":1}}],["其具体操作如下",{"2":{"614":1}}],["其具体操作是norm",{"2":{"519":2}}],["其具体结构如下",{"2":{"301":1}}],["其具体特点如下",{"2":{"164":1,"510":1}}],["其来源于论文layer",{"2":{"294":1}}],["其来源于论文deep",{"2":{"294":1}}],["其可以有效解决梯度消失",{"2":{"286":1}}],["其也存在若干缺点",{"2":{"279":1}}],["其后续的输入也可能对输出产生影响",{"2":{"277":1}}],["其处理时间随上下文长度急剧增长",{"2":{"273":1}}],["其优势是既能在时间上保持固定大小",{"2":{"273":1}}],["其它超参数",{"2":{"1226":1}}],["其它参考",{"2":{"1078":1}}],["其它正则化方法",{"0":{"1020":1}}],["其它的是为了",{"2":{"976":1}}],["其它relu",{"0":{"842":1}}],["其它",{"0":{"751":1},"2":{"741":1,"922":1}}],["其它算法",{"0":{"596":1},"1":{"597":1,"598":1,"599":1,"600":1,"601":1,"602":1,"603":1,"604":1,"605":1,"606":1,"607":1,"608":1}}],["其它相关经典论文和精彩博客",{"2":{"432":1}}],["其它单词和它的位置距离都是固定常数",{"2":{"274":1}}],["其它单词回复关系密切与否",{"2":{"265":1}}],["其它元素赋值为0",{"2":{"191":1}}],["其英文原文如下",{"2":{"260":1}}],["其阻碍了训练时的并行计算",{"2":{"254":1}}],["其承载的信息可以跨时间步在rnn中进行传递",{"2":{"249":1}}],["其某一个阶段的方案可能解决",{"2":{"243":1}}],["其将",{"2":{"239":1}}],["其将语言模型在zero",{"2":{"122":1}}],["其并不是对transformer结构做了改变",{"2":{"207":1}}],["其效果也就被削减了",{"2":{"191":1}}],["其效果差不多",{"2":{"14":1}}],["其均值为零",{"2":{"189":1}}],["其能带来很客观的收益",{"2":{"183":1}}],["其允许模型在处理一个句子时",{"2":{"158":1}}],["其在编码和解码过程中共享相同的参数",{"2":{"541":1}}],["其在编码器和解码器两个模块中的需求如下",{"2":{"77":1}}],["其在学习的过程中自适应的给予输入的不同词赋予不同的注意力权重",{"2":{"260":1}}],["其在准确性和可解释性方面超越了传统的多层感知器",{"2":{"155":1}}],["其推导过程如下",{"2":{"153":1}}],["其核心思想是基于直线公式y=kx+by",{"2":{"2017":1}}],["其核心思想是通过哈希函数将高维数据投影到低维空间",{"2":{"153":1}}],["其核心在于能够动态调整权重矩阵中的关键组件",{"2":{"225":1}}],["其核心是",{"2":{"137":1}}],["其存储能力将恢复到与gpt",{"2":{"147":1}}],["其存储能力没有太明显的损失",{"2":{"147":1}}],["其存储能力大概会降低到",{"2":{"147":1}}],["其倾向于补全对应key的prefix的下一个词",{"2":{"128":1}}],["其输入为",{"2":{"1000":1}}],["其输入标记被分成小块",{"2":{"977":1}}],["其输入分布会改变得越明显",{"2":{"309":1}}],["其输入x∈rdinput×dmodelx∈rdinput×dmodelx∈r^",{"2":{"99":1}}],["其输出传递给前馈网络",{"2":{"975":1}}],["其输出可以作为下一个encoderlayer的输入",{"2":{"519":1}}],["其输出系数会趋于均匀分布",{"2":{"181":1}}],["其输出将残差流重定向到其预期答案的方向",{"2":{"122":1}}],["其产生原因主要与参数量有关",{"2":{"119":1}}],["其整体运算仍然是线性的",{"2":{"117":1}}],["其主要思路为通过模仿人类观察事物的行为来降低算法复杂度",{"2":{"257":1}}],["其主要作用是将输入字节序列",{"2":{"614":1}}],["其主要作用是还原维度",{"2":{"116":1}}],["其主要作用是拟合一个更高维的映射空间",{"2":{"116":1}}],["其主要起到两个作用",{"2":{"66":1}}],["其有三个w参数需要训练",{"2":{"114":1}}],["其使用columnparallellinear和rowparallellinear这样分布式线性层",{"2":{"114":1}}],["其公式为xt+1=xt+ft",{"2":{"334":1}}],["其公式和效果如下图所示",{"2":{"108":1}}],["其公式如下",{"2":{"105":1}}],["其计算是以token为颗粒度",{"2":{"101":1}}],["其数学表达为max",{"2":{"99":1}}],["其数据维度为",{"2":{"25":1}}],["其次所有模型层共享参数",{"2":{"1315":1}}],["其次在网络中间",{"2":{"807":1}}],["其次看串行操作的复杂度",{"2":{"511":1}}],["其次将掩码应用于注意力矩阵的上三角",{"2":{"464":1}}],["其次调用torch",{"2":{"384":2,"558":2}}],["其次依据所用信息把对齐函数分为基于内容的对齐和基于位置的对齐",{"2":{"285":1}}],["其次是没意义",{"2":{"316":1}}],["其次是",{"2":{"170":1}}],["其次",{"2":{"57":1,"154":1,"180":1,"198":1,"222":1,"245":1,"276":1,"301":1,"314":1,"405":1,"407":1,"426":1,"582":1,"613":1,"649":1,"691":1,"695":1,"712":1,"764":1,"765":1,"931":1,"975":1,"1059":1}}],["其步骤如下",{"2":{"35":1}}],["其实我们在日常生活中会不自觉的去使用",{"2":{"2121":1}}],["其实我在一年前对这些还没有太多的认识",{"2":{"2097":1}}],["其实贪心算法不是一种具体模板化的算法",{"2":{"2117":1}}],["其实所说的都是",{"2":{"770":1}}],["其实和t5类似",{"2":{"765":1}}],["其实相乘也是一种特征交叉的方式",{"2":{"751":1}}],["其实也有不同观点",{"2":{"698":1}}],["其实也被转换为输入",{"2":{"698":1}}],["其实本身的语义都是",{"2":{"576":1}}],["其实这块与上文",{"2":{"538":1}}],["其实这才是llm工作中最繁杂的一部分",{"2":{"364":1}}],["其实无论是什么结构的神经网络",{"2":{"495":1}}],["其实在整个过程中",{"2":{"343":1}}],["其实与torch",{"2":{"343":1}}],["其实际等效层数不如",{"2":{"334":1}}],["其实对于",{"2":{"333":1}}],["其实预测变换为两个子问题",{"2":{"240":1}}],["其实只做了一次线性变换",{"2":{"172":1}}],["其实已经开始有一些研究从网络架构或者注意力机制的角度探究知识的检索以及利用的问题",{"2":{"132":1}}],["其实就是不同的实现方式",{"2":{"2118":1}}],["其实就是linear",{"2":{"926":1}}],["其实就是计算受力",{"2":{"509":1}}],["其实就是淘宝商品数据库中与候选商品相关的关键字",{"2":{"463":1}}],["其实就是传统的注意力机制",{"2":{"444":1}}],["其实就是数据库中与候选商品相关的关键字",{"2":{"164":1}}],["其实就是relu的公式",{"2":{"113":1}}],["其实就是ββ",{"2":{"103":1}}],["其实是没什么意义的",{"2":{"933":1}}],["其实是最简单的decoding方式",{"2":{"901":1}}],["其实是cv里面的instancenorm",{"2":{"341":1}}],["其实是",{"2":{"82":1}}],["其实没什么意义",{"2":{"54":1}}],["其实",{"2":{"29":1,"84":1,"141":1,"172":1,"240":1,"260":1,"323":1,"326":1,"347":1,"498":1,"688":1,"844":1}}],["其实几乎每一个token在全句中的注意力都是稀疏的",{"2":{"14":1}}],["其他特性",{"0":{"1931":1},"2":{"1920":1,"1932":1}}],["其他规则",{"2":{"1917":2}}],["其他重要特性",{"0":{"1897":1},"1":{"1898":1,"1899":1}}],["其他cache类特有行为",{"2":{"1867":1}}],["其他常用的有序容器",{"0":{"1798":1},"1":{"1799":1,"1800":1,"1801":1,"1802":1,"1803":1}}],["其他常用有序容器介绍",{"2":{"1795":1}}],["其他文件操作",{"2":{"1763":1}}],["其他成员的值会变得无效",{"2":{"1728":2}}],["其他成分与动词或直接或间接的产生关系",{"2":{"20":1}}],["其他源文件无法访问",{"2":{"1649":1}}],["其他基本数据类型",{"2":{"1607":1}}],["其他开发环境",{"2":{"1605":1}}],["其他参数同send",{"2":{"1590":1}}],["其他工作进程",{"2":{"1578":1}}],["其他工作使用基于数百个数据集的全参数调整和",{"2":{"733":1}}],["其他目录和文件都是在此基础上扩展的",{"2":{"1505":1}}],["其他线程或是进程必须等待",{"2":{"1413":1}}],["其他",{"2":{"1222":1}}],["其他一些方案可能也不错",{"2":{"1172":1}}],["其他用途映射",{"2":{"733":1}}],["其他地方为",{"2":{"681":1}}],["其他都是",{"2":{"681":1}}],["其他都为",{"2":{"188":1}}],["其他流程和bpe是一样的",{"2":{"607":1}}],["其他值则为0",{"2":{"188":1}}],["其他物品对应的权重小",{"2":{"163":1}}],["其他说明",{"2":{"113":1}}],["其他部分为",{"2":{"89":1}}],["其他head中的信息也可以参与当前head的注意力计算",{"2":{"41":1}}],["其他头仍然可以提供有用的信息",{"2":{"21":1}}],["其pattern种类越丰富",{"2":{"20":1}}],["其没有加入bias",{"2":{"8":1}}],["其中比较有名有",{"2":{"1312":1}}],["其中计算图是根据实际输入数据的流动而动态生成的",{"2":{"1287":1}}],["其中所有的操作和数据流都在定义时确定",{"2":{"1287":1}}],["其中最简单的配置是在同一计算实例中执行训练和定期评估",{"2":{"1164":1}}],["其中最主要的部分是collate",{"2":{"375":1}}],["其中激活函数为relu",{"2":{"1003":1}}],["其中gpu的总数等于tpxcpxppxdp",{"2":{"976":1}}],["其中多个查询头关注相同的键头和值头",{"2":{"971":1}}],["其中l表示层数",{"2":{"957":1}}],["其中l是输入序列长度",{"2":{"173":1}}],["其中分配给每个value的权重通过query与相应key的兼容函数来计算",{"2":{"916":1}}],["其中sublayer",{"2":{"914":1}}],["其中si是解码器在时刻iii的隐状态",{"2":{"249":1}}],["其中x是源语言",{"2":{"908":1}}],["其中核心部分为三门",{"2":{"864":1}}],["其中隐藏层会重复执行",{"2":{"853":1}}],["其中β是一个常数或可学习的参数",{"2":{"845":1}}],["其中β是权衡超参数",{"2":{"42":1}}],["其中input使用",{"2":{"1086":1}}],["其中i",{"2":{"759":1}}],["其中两个",{"2":{"731":1}}],["其中包括山地针叶林",{"2":{"713":1}}],["其中包含多行英文文本",{"2":{"1933":1}}],["其中包含一个成员函数模板",{"2":{"1701":1}}],["其中包含模型的结构和计算逻辑",{"2":{"1291":1}}],["其中包含属于该组的参数列表",{"2":{"1222":1}}],["其中包含要优化的参数",{"2":{"1221":1}}],["其中包含输入项的内容",{"2":{"885":1}}],["其中包含如下单词",{"2":{"577":1}}],["其中包含了丰富的语言和知识",{"2":{"505":1}}],["其中包含主语",{"2":{"145":1}}],["其中patch表示作为查询",{"2":{"615":1}}],["其中1表示新patch的开始",{"2":{"613":1}}],["其中1的元素位置为其对应注意力头的映射矩阵在拼接后的整体矩阵中的位置",{"2":{"19":1}}],["其中相邻字母对的组合中",{"2":{"575":1}}],["其中键是词汇表中的单词",{"2":{"557":1}}],["其中大多数都是开源的",{"2":{"540":1}}],["其中bij=f",{"2":{"766":1}}],["其中batch",{"2":{"528":1}}],["其中b为batch",{"2":{"113":1}}],["其中一个系统的动态可以反映出另一个系统的动态",{"2":{"691":1}}],["其中一个自旋系统的磁化驱动下一个自旋系统",{"2":{"508":1}}],["其中一些相关论点如下",{"2":{"126":1}}],["其中一些洞察和观点如下",{"2":{"20":1}}],["其中query",{"2":{"916":1}}],["其中q",{"2":{"507":1}}],["其中qqq是查询向量",{"2":{"268":1}}],["其中使用了",{"2":{"503":1}}],["其中复合的次数其实等价于神经网络的层数",{"2":{"495":1}}],["其中特征表示为多语义神经元的稀疏线性组合",{"2":{"477":1}}],["其中词汇表中的每个token都有一个对应的值",{"2":{"473":1}}],["其中每一项的分别为",{"2":{"758":1}}],["其中每一行或向量表示一个独立的词元",{"2":{"463":1}}],["其中每个参数组是一个字典",{"2":{"1227":1}}],["其中每个试验训练",{"2":{"1156":1}}],["其中每个研究都会调整冗余超参数",{"2":{"1144":1}}],["其中每个单词基本上都有相似性0",{"2":{"694":1}}],["其中每个句子内容是原始语句中token对应的词典序号",{"2":{"380":1}}],["其中每个下游任务对应一个z向量",{"2":{"224":1}}],["其中每个token都是综合了源隐状态之中所有token的相关信息",{"2":{"200":1}}],["其中每个token都是综合了源序列之中所有token的相关信息",{"2":{"200":2}}],["其中每个",{"2":{"178":1}}],["其中0为bos",{"2":{"450":1,"451":1}}],["其中0表示",{"2":{"428":1}}],["其中0是",{"2":{"380":1}}],["其中dmodeldmodeld",{"2":{"402":1}}],["其中dvdvd",{"2":{"179":1}}],["其中α是一个可学习的参数",{"2":{"359":1}}],["其中涉及到注意力和",{"2":{"354":1}}],["其中注意力和",{"2":{"352":1}}],["其中调用self",{"2":{"344":1}}],["其中提到了sequence",{"2":{"235":1}}],["其中混合模型堆叠具有完整或滑动窗口注意力的循环模型",{"2":{"231":1}}],["其中各头的结构不同",{"2":{"231":1}}],["其中滑动窗口注意力",{"2":{"231":1}}],["其中技巧在于当前token与哪一部分",{"2":{"204":1}}],["其中𝑚是训练长度",{"2":{"194":1}}],["其中拼接相似度是将两个向量拼接起来",{"2":{"175":1}}],["其中aiaia",{"2":{"134":1}}],["其中函数f表示神经网络",{"2":{"134":1}}],["其中w是可训练的矩阵",{"2":{"209":1}}],["其中w1",{"2":{"109":1}}],["其中wowow^o由mha的输出投影矩阵切分得到",{"2":{"45":1}}],["其中σσ",{"2":{"108":1}}],["其中门控机制是一个sigmoid函数用来控制信息能够通过多少",{"2":{"105":1}}],["其中第i行第j列表示的是query的第i个词对key的第j个词的注意力是否无意义",{"2":{"66":1}}],["其中后两个矩阵由q矩阵和k矩阵动态生成",{"2":{"46":1}}],["其中的单例维度被扩展到更大的大小",{"2":{"827":1}}],["其中的clone函数的代码为",{"2":{"522":1}}],["其中的空白圈即表示补齐的零向量",{"2":{"316":1}}],["其中的",{"2":{"36":1}}],["其中关键参数及变量如下",{"2":{"23":1}}],["其中",{"2":{"5":1,"45":1,"99":1,"105":1,"106":1,"109":1,"119":1,"176":1,"210":1,"313":1,"325":1,"351":1,"360":1,"420":1,"501":1,"700":1,"739":1,"745":1,"748":1,"765":1,"766":1,"772":1,"840":1,"844":1,"855":1,"878":1,"890":1,"899":1,"903":1,"927":1,"941":1,"957":1,"960":1,"1003":1,"1004":1,"1023":1,"1335":1,"1336":1,"1339":1,"1343":1,"1344":1,"1361":1,"1386":1,"1394":1,"1574":1,"1648":1,"2155":1}}],["其就有语义逻辑",{"2":{"4":1}}],["最新研究内容和成果",{"0":{"2011":1}}],["最多",{"2":{"1813":2,"1831":2}}],["最强大的类型转换",{"2":{"1629":1}}],["最基本的是",{"2":{"1619":1}}],["最简单安装只要两个jar文件+配置几个sql映射文件就可以了",{"2":{"1479":1}}],["最简单的理解",{"2":{"2097":1}}],["最简单的",{"0":{"1966":1}}],["最简单的方法是将许多全连接层堆叠在一起",{"2":{"1464":1}}],["最简单的方法就是把encoder的最后一个隐状态赋值给context",{"2":{"888":1}}],["最简单的解决方案通常是以不同的批次大小",{"2":{"1132":1}}],["最快",{"2":{"1440":2}}],["最适合处理那些需要理解整个句子语义的任务",{"2":{"1315":1}}],["最适合用于早期训练不稳定的情况",{"2":{"1180":1}}],["最核心之处",{"2":{"1250":1}}],["最常用的方法已经得到支持",{"2":{"1219":1}}],["最优子结构性质",{"2":{"2119":1}}],["最优",{"2":{"2119":1}}],["最优学习率很容易随着下一次训练流程的改变而改变",{"2":{"1143":1}}],["最优的词表大小是有上限的",{"2":{"561":1}}],["最直接的实现方式是对参数进行",{"2":{"1228":1}}],["最直接的字节分组方法是固定大小的patch",{"2":{"613":1}}],["最直观的方法是将动量应用于缩放后的梯度",{"2":{"1059":1}}],["最小学习率",{"2":{"1243":1}}],["最小值",{"2":{"1087":1}}],["最小化训练时间的batch",{"2":{"1133":1}}],["最小化",{"2":{"1021":1}}],["最小熵原理",{"2":{"740":1}}],["最近最少使用",{"2":{"986":1}}],["最近",{"2":{"977":1}}],["最近的研究提出了弹性序列并行性",{"2":{"977":1}}],["最近的大型语言模型的可用上下文长度正在迅速增加",{"2":{"977":1}}],["最近的很多优化",{"2":{"206":1}}],["最早是出现在2019年谷歌的一篇论文",{"2":{"935":1,"951":1}}],["最早其实是一种数据压缩算法",{"2":{"575":1}}],["最上点是此时间步的输出",{"2":{"890":1}}],["最上层呈现语义特征",{"2":{"437":1}}],["最经典的例子是",{"2":{"838":1}}],["最先进的gpu",{"0":{"799":1}}],["最重要的一点",{"2":{"1479":1}}],["最重要的是无法改变patch大小",{"2":{"613":1}}],["最重要的信息",{"2":{"263":1}}],["最主要区别是bpe基于char粒度去执行合并的过程生成词表",{"2":{"607":1}}],["最主要的问题就是计算速度慢和存储占用高",{"2":{"279":1}}],["最高为2k",{"2":{"945":1,"965":1}}],["最高频的字符对是",{"2":{"583":1}}],["最高的",{"2":{"135":1}}],["最佳实践的约定",{"2":{"1728":1}}],["最佳学习率处于可行的边缘",{"2":{"1182":1}}],["最佳模型在验证集上的表现",{"2":{"1167":1}}],["最佳检查点并不一定是最后一个检查点",{"2":{"1166":1}}],["最佳",{"2":{"1130":1,"1139":1,"1149":2,"1154":1,"1155":1,"2101":1}}],["最佳的词表扩增策略会因特定任务和领域的需求而不同",{"2":{"560":1}}],["最佳主题论文奖",{"2":{"156":1}}],["最需要注意的",{"2":{"536":1}}],["最坏情况下",{"2":{"511":1}}],["最初用于图像的风格迁移",{"2":{"337":1}}],["最初的激活函数是下图左所示的阶跃函数",{"2":{"1460":1}}],["最初的",{"2":{"329":1}}],["最初的设计灵感来自",{"2":{"284":1}}],["最好检查指针是否为",{"2":{"1611":1}}],["最好复制生成衰减方案的算法",{"2":{"1173":1}}],["最好将您的基础策略设为自动生成尽可能多的图表",{"2":{"1151":1}}],["最好的办法是把它拆解成多个小问题",{"2":{"2104":1}}],["最好的学习率衰减方案是什么",{"0":{"1171":1}}],["最好的试验是否具有与有问题的过度拟合一致的训练曲线",{"2":{"1146":1}}],["最好的",{"2":{"1143":1}}],["最好的切入角度就是",{"2":{"892":1}}],["最好可以表示文本",{"2":{"682":1}}],["最好还是各个句子归一化自己的",{"2":{"323":1}}],["最好不做翻译",{"2":{"259":1}}],["最开始的",{"2":{"253":1}}],["最下层cnn使用滑动窗口作用于这个文本序列",{"2":{"247":1}}],["最大化了时间的利用率",{"2":{"2135":1}}],["最大化验证集指标",{"2":{"1141":1}}],["最大四位数的例子",{"0":{"2122":1},"1":{"2123":1,"2124":1,"2125":1,"2126":1}}],["最大值",{"2":{"1709":1}}],["最大速度和排水量并进行初始化",{"2":{"1664":1}}],["最大速度和轮子数量并进行初始化",{"2":{"1664":1}}],["最大速度",{"2":{"1664":2}}],["最大的搜索值不应超过max",{"2":{"1183":1}}],["最大的问题是如何调整学习率衰减计划",{"2":{"1157":1}}],["最大的那个序列作为最终结果",{"2":{"902":1}}],["最大的k个作为下一步的输入",{"2":{"902":1}}],["最大的聚类的与最不频繁的单词相关",{"2":{"185":1}}],["最大池化在每个池化窗口中选择最大的特征值作为输出",{"2":{"814":1}}],["最大路径长度",{"2":{"511":1}}],["最大概率词在每一层的残差向量所对应的预测分布里的概率值",{"2":{"306":1}}],["最大允许的序列长度为5",{"2":{"80":1}}],["最低的",{"2":{"135":1}}],["最传统的随机采样",{"2":{"90":1}}],["最朴素的训练方法应该是基于一个长为",{"2":{"57":1}}],["最朴素的实现都是在通道维度reshape成多头",{"2":{"24":1}}],["最终完成了",{"2":{"2135":1}}],["最终结果就是",{"2":{"2125":1}}],["最终你就会发现自己离大目标越来越近了",{"2":{"2102":1}}],["最终帮助你完成目标",{"2":{"2097":1}}],["最终效果与createsuperuser一模一样",{"2":{"2070":1}}],["最终效果变差了",{"2":{"334":1}}],["最终我放弃了这种策略",{"2":{"2070":1}}],["最终我们可以推出",{"2":{"1004":1}}],["最终我们将遍历所有tokens",{"2":{"587":1}}],["最终我们通过上下文语境可以推断出",{"2":{"259":1}}],["最终我们通过上下文语境",{"2":{"167":1}}],["最终派生类负责初始化虚基类",{"2":{"1665":1}}],["最终形式",{"2":{"1448":1}}],["最终形成带有位置信息的embedding编码矩阵",{"2":{"445":1}}],["最终产生输出",{"2":{"1438":1}}],["最终产生模型的预测结果",{"2":{"126":1}}],["最终输入data",{"2":{"1251":1}}],["最终输出的点乘结果ziziz",{"2":{"758":1}}],["最终输出依然是若干token对应的向量",{"2":{"472":1}}],["最终输出一个概率分布",{"2":{"431":1,"445":1}}],["最终输出ytyty",{"2":{"285":2}}],["最终输出向量o是用cicic",{"2":{"125":1}}],["最终输出包含有不同子空间中的编码表示信息",{"2":{"5":1}}],["最终训练出更好的模型",{"2":{"1131":1}}],["最终到达一个局部是凸碗的区域",{"2":{"1048":1}}],["最终retromae的损失由两部分相加得到",{"2":{"727":1}}],["最终利用a跟h1去重建原文本",{"2":{"727":1}}],["最终返回英语词典和德语词典",{"2":{"557":1}}],["最终返回相似度最高的若干商品v",{"2":{"164":1}}],["最终提高模型的训练效率和预测准确性",{"2":{"547":1}}],["最终提供的信息",{"2":{"265":1}}],["最终编码器层会调用业务逻辑",{"2":{"529":1}}],["最终编码器和解码器之间只通过这个固定长度的隐状态来传递信息",{"2":{"251":1}}],["最终获得输出xlxl",{"2":{"501":1}}],["最终选出其中最合适的单词推荐给用户",{"2":{"473":1}}],["最终依据某种策略输出一个最可能的单词",{"2":{"445":1,"515":1}}],["最终才能得到一件精美的瓷器",{"2":{"437":1}}],["最终每个token流经transformer最后一层之后得到的是一个代表语义的特征向量",{"2":{"431":1}}],["最终每个进程得到的向量长度是原来embedding长度的1",{"2":{"10":1}}],["最终导致模型崩盘",{"2":{"401":1}}],["最终导致梯度接近于零的现象",{"2":{"296":1}}],["最终使用的损失函数",{"2":{"399":1}}],["最终使得更新的参数很小",{"2":{"333":1}}],["最终的窗口大小为2k+1",{"2":{"759":1}}],["最终的token将是这些单词的一部分",{"2":{"553":1}}],["最终的网络可以近似看作集成了若干个不同网络的组合模型",{"2":{"393":1}}],["最终的现象是",{"2":{"296":1}}],["最终会调用",{"2":{"344":1}}],["最终不如同一深度的",{"2":{"334":1}}],["最终明确了一个核心结论",{"2":{"302":1}}],["最终把整本书的知识都处理一遍",{"2":{"216":1}}],["最终经过反向传播后",{"2":{"168":1}}],["最终两个transformer单词就通过和句子中其它单词的操作完成了对本身语义的重构",{"2":{"167":1}}],["最终可能会引发问题",{"2":{"1647":1}}],["最终可能导致",{"2":{"1671":1}}],["最终可能导致程序运行缓慢甚至崩溃",{"2":{"1647":1}}],["最终可能导致模型整体性能的显著提升",{"2":{"119":1}}],["最终可以捕捉到全局语义关系",{"2":{"216":1}}],["最终可以捕捉到某个特定的词和句子中其他每个词之间的一些互动",{"2":{"167":1}}],["最终计算单词之间的关联度",{"2":{"158":1}}],["最终将",{"2":{"727":1}}],["最终将多个记忆分片合并为一个记忆",{"2":{"143":1}}],["最终将各自的成果整合在一起",{"2":{"5":1}}],["最终造成灾难性遗忘",{"2":{"143":1}}],["最终一定会引起编辑知识冲突",{"2":{"143":1}}],["最终",{"2":{"99":1,"191":1,"265":1,"277":1,"291":1,"325":1,"330":1,"536":1,"1133":1,"1140":1,"1146":1,"1315":1,"1317":1,"1342":1}}],["最终进入注意力函数attention",{"2":{"82":1}}],["最终得到该词向量",{"2":{"714":1}}],["最终得到是",{"2":{"528":1}}],["最终得到的结果是4321",{"2":{"2126":1}}],["最终得到的子词词汇表如下",{"2":{"584":1}}],["最终得到的word",{"2":{"460":1}}],["最终得到的信息是所有书籍内容按照权重综合起来的结果",{"2":{"169":1}}],["最终得到整个句子的数学表示",{"2":{"445":1}}],["最终得到全局的答案",{"2":{"216":1}}],["最终得到zz",{"2":{"71":1}}],["最终得到返回一个",{"2":{"66":1}}],["最终得出来的翻译结果可能会更加准确",{"2":{"13":1}}],["最终src",{"2":{"66":1}}],["最终shape变为",{"2":{"36":1}}],["最终整体的concat计算相当于把来自多个小模型的结果进行了融合",{"2":{"13":1}}],["最终就得到是wowow^o方案",{"2":{"10":1}}],["最终有人将不同的意见和见解整合起来",{"2":{"5":1}}],["最后剩下的是",{"2":{"2125":1}}],["最后综合这些信息",{"2":{"2105":1}}],["最后综合考虑",{"2":{"901":1}}],["最后合起来看",{"2":{"2104":1}}],["最后出门去上班",{"2":{"2097":1}}],["最后我们直接用管理项目的账号去登录",{"2":{"2070":1}}],["最后问我们是哪个学校的",{"2":{"2051":1}}],["最后显示余额",{"2":{"1873":1}}],["最后构造成员",{"2":{"1653":1}}],["最后释放内存",{"2":{"1647":1}}],["最后计算上述公式得到",{"2":{"1361":1}}],["最后计算值向量的加权和",{"2":{"355":1}}],["最后单词转移分数",{"2":{"1328":1}}],["最后通过比较两个研究中的最优试验来比较这两个优化器",{"2":{"1144":1}}],["最后通过一个最终的线性变换得到输出",{"2":{"356":1}}],["最后n个序列选哪个",{"2":{"904":1}}],["最后得到结果",{"2":{"777":1}}],["最后得到的均值和方差向量都是",{"2":{"325":1}}],["最后得到的是一个代表这个",{"2":{"315":1}}],["最后得到的矩阵",{"2":{"168":1}}],["最后过一个mlp",{"2":{"722":1}}],["最后网络收敛停止迭代的时候",{"2":{"709":1}}],["最后会把新的embedding",{"2":{"676":1}}],["最后用bpe来压缩dct矩阵",{"2":{"637":1}}],["最后为了准备下一层的输出",{"2":{"620":1}}],["最后看看最大路径长度",{"2":{"511":1}}],["最后经logits层将计算结果映射至词表空间",{"2":{"510":1}}],["最后的权重",{"2":{"1396":1}}],["最后的生成的结果将会非常离谱",{"2":{"1374":1}}],["最后的生成结果接近数字0",{"2":{"1373":1}}],["最后的end",{"2":{"1330":1}}],["最后的task",{"2":{"726":1}}],["最后的输出层",{"2":{"471":1}}],["最后的携带整个输入序列信息的hm会作为解码器的输入s0",{"2":{"249":1}}],["最后再计算",{"2":{"1344":1}}],["最后再softmax分类",{"2":{"816":1}}],["最后再进行合并",{"2":{"775":1}}],["最后再利用无监督对比学习simcse",{"2":{"734":1}}],["最后再",{"2":{"460":1}}],["最后再导出transformer模型架构",{"2":{"235":1}}],["最后25",{"2":{"437":1}}],["最后将",{"2":{"944":1,"1315":1}}],["最后将结果合并组合",{"2":{"775":1}}],["最后将结果与原始输入x相加",{"2":{"344":1}}],["最后将瓷坯装入匣钵",{"2":{"437":1}}],["最后把这个批量传给解码器",{"2":{"408":1}}],["最后是残差连接",{"2":{"344":1}}],["最后是两个线形层",{"2":{"82":1}}],["最后返回归一化后的结果",{"2":{"343":1}}],["最后返回的是目标隐状态torch",{"2":{"200":2}}],["最后返回的是",{"2":{"200":1}}],["最后一维才是样本的维度",{"2":{"343":1}}],["最后一个访问",{"2":{"1670":1}}],["最后一个文件",{"2":{"1556":1}}],["最后一个具有",{"2":{"1330":1}}],["最后一个元素没有后继元素",{"2":{"1322":1}}],["最后一个令牌嵌入",{"2":{"735":1}}],["最后一个阶段则在保持通用的基础上",{"2":{"726":1}}],["最后一个encoderlayer的输出就是编码器的输出",{"2":{"518":1}}],["最后一个位置",{"2":{"122":1}}],["最后一个主语位置的表示",{"2":{"122":1}}],["最后输出",{"2":{"341":3}}],["最后标记",{"2":{"204":1}}],["最后其输出的值就是走某一条树分支的概率",{"2":{"184":1}}],["最后形成全新的分布",{"2":{"126":1}}],["最后填充词对应的掩码和未来词汇相关的掩码会做与操作",{"2":{"74":1,"382":1}}],["最后留下来的极少不相同的注意力头就是这个模型表达语义信息的注意力头",{"2":{"20":1}}],["最后",{"2":{"5":1,"93":1,"120":1,"145":1,"195":1,"274":1,"320":1,"427":1,"460":1,"478":1,"497":1,"582":1,"620":1,"628":1,"636":1,"868":1,"923":1,"937":1,"953":1,"1086":1,"1127":1,"1157":1,"1164":1,"1678":1}}],["才考虑使用友元",{"2":{"1793":1}}],["才需要重新考虑",{"2":{"1128":1}}],["才可以精确的表征人类概念",{"2":{"689":1}}],["才可以降低概率空间的信息熵到一定阈值",{"2":{"367":1}}],["才是一个好的解决方案",{"2":{"1412":1}}],["才是表达语义的最小单元",{"2":{"566":1}}],["才是多头背后的真正内在成因",{"2":{"12":1}}],["才是多头注意力背后的真正内在成因",{"2":{"4":1}}],["才会尝试使用模板生成匹配的函数",{"2":{"1698":1}}],["才会被加入词典",{"2":{"557":1}}],["才会与",{"2":{"394":1}}],["才有了正确输出下一个token的基础",{"2":{"536":1}}],["才决定某个注意力是交叉注意力还是掩码多头注意力",{"2":{"449":1}}],["才能顺利开始一天的工作",{"2":{"2097":1}}],["才能继续执行",{"2":{"1574":1}}],["才能防止出现以下情况",{"2":{"1412":1}}],["才能做出来对于序列的下一个输入的预测",{"2":{"754":1}}],["才能理解它们",{"2":{"715":1}}],["才能得到正结果",{"2":{"542":1}}],["才能通过学习生成正确的输出",{"2":{"536":1}}],["才能判断出来应该是",{"2":{"277":1}}],["才能确定是",{"2":{"516":1}}],["才能确定",{"2":{"277":1}}],["才能确定给这两个词建立关系所需的距离",{"2":{"256":1}}],["才能组成batch",{"2":{"53":1}}],["才能不仅抓住主旨",{"2":{"5":1}}],["另",{"2":{"1003":1,"1087":1}}],["另如果采用均匀分布初始化的话",{"2":{"1000":1}}],["另有一个基线输入x",{"2":{"134":1}}],["另一部分为前面累计下来的梯度值",{"2":{"1036":1}}],["另一部分专注于构建整个circuit",{"2":{"475":1}}],["另一组使用sp",{"2":{"977":1}}],["另一种思考方式是",{"2":{"968":1}}],["另一种基于anto",{"2":{"727":1}}],["另一种是操作对象",{"2":{"642":1}}],["另一种是先对数据进行学习量化",{"2":{"636":1}}],["另一种是tgt",{"2":{"198":1,"199":1}}],["另一个流行的开源mpi实现",{"2":{"1569":1}}],["另一个要访问该资源的进程必须等待",{"2":{"1409":1}}],["另一个包含非完整的反向传播钩子",{"2":{"1214":1}}],["另一个是第二项",{"2":{"1450":1}}],["另一个是hidden",{"2":{"287":1}}],["另一个是挑重点看",{"2":{"217":1}}],["另一个问题是如何根据当前的隐变量htℎtℎ",{"2":{"240":1}}],["另一个q去捕捉语义依赖",{"2":{"5":1}}],["另一方面在人工智能学的人工感知领域",{"2":{"1456":1}}],["另一方面作为",{"2":{"485":1}}],["另一方面",{"2":{"100":1,"180":1,"231":1,"709":1,"1127":1,"1134":1,"1137":1}}],["另外在效果上",{"2":{"935":1,"951":1}}],["另外独热向量",{"2":{"694":1}}],["另外两种生成式架构都引入了双向注意力",{"2":{"542":1}}],["另外还要加上shfited",{"2":{"526":1}}],["另外",{"2":{"3":1,"8":1,"23":1,"58":1,"99":1,"101":1,"118":1,"121":1,"122":1,"127":1,"131":1,"147":1,"172":1,"180":1,"185":1,"204":1,"222":2,"224":1,"274":1,"316":2,"326":1,"329":1,"333":1,"334":1,"341":1,"363":1,"393":1,"399":1,"411":1,"415":1,"420":1,"432":1,"437":1,"444":1,"473":1,"525":1,"547":1,"560":2,"566":1,"576":1,"579":1,"595":1,"678":1,"685":1,"692":1,"698":1,"701":1,"702":1,"746":1,"763":1,"839":1,"844":1,"1052":1}}],["多边形",{"2":{"2009":1}}],["多精度数值计算",{"2":{"1957":1}}],["多文件编程是大型",{"2":{"1918":1}}],["多文件编程允许将代码分解成独立的模块",{"2":{"1916":1}}],["多文件编程",{"0":{"1916":1}}],["多文件和",{"0":{"1915":1},"1":{"1916":1,"1917":1}}],["多文件和多窗口操作",{"0":{"1555":1},"1":{"1556":1,"1557":1}}],["多线程优化与内存管理技术",{"2":{"1937":1}}],["多线程支持",{"0":{"1893":1},"1":{"1894":1,"1895":1,"1896":1}}],["多线程编程",{"2":{"1566":1}}],["多继承与菱形问题",{"0":{"1869":1}}],["多类协作等特殊需求中尤其有用",{"2":{"1793":1}}],["多重继承",{"0":{"1659":1},"2":{"1658":1}}],["多重叠头自注意力",{"2":{"41":1}}],["多级指针主要用于更复杂的数据结构",{"2":{"1611":1}}],["多级指针",{"2":{"1611":1}}],["多机多处理机",{"2":{"1568":1}}],["多处理器系统和分布式系统",{"2":{"1566":1}}],["多处理机",{"2":{"1565":1}}],["多态性",{"2":{"1848":1}}],["多态等面向对象编程的特性",{"2":{"1728":1}}],["多态与虚函数",{"0":{"1681":1},"1":{"1682":1,"1683":1,"1684":1,"1685":1,"1686":1,"1687":1,"1688":1}}],["多态",{"0":{"1686":1},"1":{"1687":1,"1688":1,"1690":1,"1691":1},"2":{"1491":1,"1603":1,"1693":1,"1848":1}}],["多分类问题的解决",{"2":{"1465":1}}],["多次分割解决分线性",{"2":{"1463":1}}],["多次重复的主语",{"2":{"127":1}}],["多元函数",{"2":{"1440":1}}],["多张量",{"2":{"1228":1}}],["多主机训练非常容易引入错误",{"2":{"1169":1}}],["多主机管道的考虑因素",{"0":{"1169":1}}],["多主机工作流的注意事项",{"2":{"1125":1}}],["多tpu训练",{"2":{"1134":1}}],["多用于全连接层",{"2":{"1018":1}}],["多轮对话中的聊天历史",{"2":{"985":1}}],["多查询注意力",{"2":{"971":1}}],["多查询注意力和分组查询注意力",{"2":{"971":1}}],["多查询注意力的比较",{"2":{"937":1,"953":1}}],["多对多的",{"2":{"908":1}}],["多对一",{"2":{"908":1}}],["多选几个作为候选",{"2":{"901":1}}],["多轴交换",{"2":{"822":1}}],["多少维的",{"2":{"783":1}}],["多少个batch更新一次参数",{"2":{"385":1}}],["多维数组",{"2":{"1634":1}}],["多维tensor",{"0":{"1101":1}}],["多维离散的卷积公式演变如下",{"2":{"770":1}}],["多维的意义在于",{"2":{"683":1}}],["多阶段",{"2":{"733":1}}],["多年来一直致力于深度学习",{"2":{"1127":1}}],["多年来",{"2":{"729":1}}],["多语言支持",{"2":{"696":1}}],["多模态句子表征模型",{"2":{"629":1}}],["多余的空格",{"2":{"552":1}}],["多粒子的动态系统是在物理中常见的一种动态系统",{"2":{"498":1}}],["多粒子动态系统视角",{"0":{"498":1}}],["多层神经元解决非线性问题",{"2":{"1463":1}}],["多层网络的有效训练方法长时间以来一直未知",{"2":{"1443":1}}],["多层",{"0":{"437":1}}],["多注意力头",{"2":{"198":1}}],["多参数",{"2":{"155":1}}],["多了",{"2":{"90":1}}],["多个开发者可以独立开发不同的模块",{"2":{"1916":1}}],["多个参数之间用逗号分隔",{"2":{"1729":1}}],["多个",{"2":{"1695":1}}],["多个重载函数都可以被调用",{"2":{"1663":1}}],["多个源文件只需包含该头文件即可使用",{"2":{"1628":1}}],["多个进程参与的通信",{"2":{"1573":1}}],["多个进程同时访问同一块内存区域",{"2":{"1568":1}}],["多个进程同时处于临界区",{"2":{"1412":1}}],["多个进程或线程需要共享访问",{"2":{"1412":1}}],["多个进程或线程可能需要同时访问共享资源",{"2":{"1407":1}}],["多个输出序列从相同的提示中生成",{"2":{"983":1}}],["多个仿射变换叠加仍是一个仿射变换",{"2":{"838":1}}],["多个step的梯度累加",{"2":{"659":1}}],["多个卷积核可以得到多个特征图",{"2":{"325":1}}],["多个m专注于句子的不同部分",{"2":{"289":1}}],["多个lora联合使用是不具备数学意义的",{"2":{"222":1}}],["多个lora也不具备可组合型",{"2":{"222":1}}],["多个ffn",{"2":{"150":1}}],["多个头直接拼接的操作",{"2":{"10":1}}],["多个注意力头的结果会被拼接在一起",{"2":{"5":1}}],["多组q",{"2":{"5":1}}],["多头的实现细节展示",{"0":{"929":1}}],["多头的核心思想就是ensemble",{"2":{"13":1}}],["多头与batch本质上都是并行计算",{"2":{"36":1}}],["多头与",{"2":{"34":1}}],["多头",{"0":{"503":1},"2":{"13":1,"396":1}}],["多头就意味着需要把词嵌入分成若干的块",{"2":{"9":1}}],["多头注意力意味着模型有多组不同的注意力参数",{"2":{"517":1}}],["多头注意力层中的注意力头数",{"2":{"448":1}}],["多头注意力",{"2":{"445":1,"449":1}}],["多头注意力和位置表示",{"2":{"434":1}}],["多头注意力由类multiheadedattention来实现",{"2":{"23":1}}],["多头注意力的架构图",{"2":{"36":1}}],["多头注意力的架构及公式如下图",{"2":{"7":1}}],["多头注意力的优点如下",{"2":{"21":1}}],["多头注意力的计算流程就是把高维向量切分为若干份低维向量",{"2":{"16":1}}],["多头注意力可以理解为高维向量被拆分或者转化为h份低维向量",{"2":{"5":1}}],["多头注意力就是研究人员给出的解决方案",{"2":{"5":1}}],["多头注意力机制借鉴了cnn中multi",{"2":{"462":1}}],["多头注意力机制中",{"2":{"417":1}}],["多头注意力机制",{"2":{"42":1,"461":1}}],["多头注意力机制允许模型在不同的表示子空间上并行地学习",{"2":{"21":1}}],["多头注意力机制的并行化",{"2":{"417":1}}],["多头注意力机制的输入和输出的维度应该是一样的",{"2":{"10":1}}],["多头注意力机制的巧妙之处在于",{"2":{"1":1}}],["多头注意力机制通过更多的权重矩阵来增加了模型的容量",{"2":{"9":1}}],["多头注意力机制通过并行运行多个自注意力层并综合结果",{"2":{"1":1}}],["多头注意力机制是自注意力机制的变体",{"2":{"7":1}}],["多头注意力机制具体如下图所示",{"2":{"5":1}}],["多头注意力机制基于自注意力机制基础上进行扩展",{"2":{"5":1}}],["多头注意力机制就是蛋糕上的樱桃",{"2":{"1":1}}],["多头自注意力模块的实例化对象",{"2":{"523":1}}],["多头自注意力机制可以用标号4的公式来表示",{"2":{"498":1}}],["多头自注意力机制就是是diffusion的过程",{"2":{"498":1}}],["多头自注意力机制本质上是构造多个子空间",{"2":{"462":1}}],["多头自注意力机制",{"0":{"462":1},"1":{"463":1,"464":1,"465":1},"2":{"517":1}}],["多头自注意力",{"0":{"0":1},"1":{"1":1,"2":1,"3":1,"4":1,"5":1,"6":1,"7":1,"8":1,"9":1,"10":1,"11":1,"12":1,"13":1,"14":1,"15":1,"16":1,"17":1,"18":1,"19":1,"20":1,"21":1,"22":1,"23":1,"24":1,"25":1,"26":1,"27":1,"28":1,"29":1,"30":1,"31":1,"32":1,"33":1,"34":1,"35":1,"36":1,"37":1,"38":1,"39":1,"40":1,"41":1,"42":1,"43":1,"44":1,"45":1,"46":1,"47":1},"2":{"0":1,"1":1,"535":2}}],["来解决生活中的问题",{"2":{"2108":1}}],["来解决线性网络表达能力不足的问题",{"2":{"838":1}}],["来确定xstepxstepxstep和ystepystepystep的取值",{"2":{"2018":1}}],["来编译一个简单的",{"2":{"1918":1}}],["来编码元素之间的相对距离",{"2":{"757":1}}],["来创建",{"2":{"1911":1}}],["来创建类对象",{"2":{"1082":1}}],["来更安全地创建",{"2":{"1911":1}}],["来管理多个",{"2":{"1891":1}}],["来组合多种模式",{"2":{"1820":1,"1838":1}}],["来组合表示",{"2":{"170":1}}],["来存储用户输入的年份",{"2":{"1729":1}}],["来访问结构体变量的成员",{"2":{"1728":1}}],["来释放数组内存",{"2":{"1647":1}}],["来接收外卖送达的消息",{"2":{"1645":1}}],["来查看指针的大小",{"2":{"1611":1}}],["来估计",{"2":{"1377":1}}],["来估计xtxtx",{"2":{"240":1}}],["来使pytorch代码运行更快",{"2":{"1293":1}}],["来注册被禁用",{"2":{"1208":1}}],["来注册",{"2":{"1208":6}}],["来定义",{"2":{"1208":1}}],["来优化冗余超参数",{"2":{"1144":1}}],["来控制类成员的访问级别",{"2":{"1677":1}}],["来控制",{"2":{"1114":1}}],["来控制更新过程",{"2":{"354":1}}],["来排列",{"2":{"1087":1}}],["来触发",{"2":{"1083":1}}],["来",{"2":{"1083":1}}],["来启用cp",{"2":{"976":1}}],["来克服在次二次方级别的hbm访问中计算精确注意力的技术挑战",{"2":{"942":1,"959":1}}],["来得到了一个质量更高的",{"2":{"938":1,"954":1}}],["来得到计算u和mi之间的交互关系pipip",{"2":{"125":1}}],["来将源语言翻译成目标语言",{"2":{"907":1}}],["来将各个部分组合成最终的高级特征向量进行输出",{"2":{"270":1}}],["来改善或解决长度限制问题",{"2":{"892":1}}],["来改进transformer的ffn层",{"2":{"103":1}}],["来取代最后的全连接层",{"2":{"816":1}}],["来啦",{"2":{"768":1}}],["来偏置query",{"2":{"765":1}}],["来刻画位置关系",{"2":{"764":1}}],["来标明其绝对位置",{"2":{"744":1}}],["来看看绝对位置编码和相对位置编码的区别",{"2":{"743":1}}],["来在矩阵中查询相应的向量",{"2":{"700":1}}],["来度量两个词",{"2":{"691":1}}],["来实现迭代计算",{"2":{"974":1}}],["来实现的",{"2":{"674":1,"813":1}}],["来实现这一点",{"2":{"154":1}}],["来提前结束函数的执行",{"2":{"1729":1}}],["来提升模型性能",{"2":{"734":1}}],["来提升网络的长期记忆能力",{"2":{"118":1}}],["来提高vla模型的训练速度",{"2":{"637":1}}],["来学习如何从北京开到广州的",{"2":{"626":1}}],["来预处理数据",{"2":{"610":1}}],["来预测句子中被遮盖掉的词语",{"2":{"1312":1}}],["来预测这些单词",{"2":{"734":1}}],["来预测在此处某概念的概率",{"2":{"633":1}}],["来预测第二个词",{"2":{"406":1}}],["来预测下一个",{"2":{"378":2}}],["来预测掩码的单词应该是一个动词",{"2":{"131":1}}],["来预测",{"2":{"58":1}}],["来分和按子词粒度来分",{"2":{"564":1}}],["来分",{"2":{"564":1}}],["来构建德语词典",{"2":{"557":1}}],["来调用",{"2":{"538":2}}],["来生成新的文本",{"2":{"1015":1}}],["来生成位置向量",{"2":{"750":1}}],["来生成tgt的word",{"2":{"538":1}}],["来生成src的word",{"2":{"538":1}}],["来生成注意力的最终输出",{"2":{"201":1}}],["来自柳浩老师笔记",{"2":{"2049":1}}],["来自动管理内存",{"2":{"1671":1}}],["来自前一个隐藏状态和当前输入的信息通过",{"2":{"865":1}}],["来自于对正弦位置编码的简单修改",{"2":{"756":1}}],["来自于一篇发表于1994年的论文",{"2":{"575":1}}],["来自于编码器的输出",{"2":{"536":1}}],["来自于解码器的输出",{"2":{"536":1}}],["来自",{"2":{"533":1,"986":2}}],["来判断一个variable是否是leaf",{"2":{"1110":1}}],["来判断",{"2":{"516":1}}],["来判断它和其他三个词之间的关系",{"2":{"170":1}}],["来对最终结果进行非线性变换",{"2":{"1086":1}}],["来对一个新造的单词",{"2":{"587":1}}],["来对下一个时刻的输出进行解码预测",{"2":{"515":1}}],["来对这一结果进行一个线性变换得到最终结果",{"2":{"36":1}}],["来强调",{"2":{"503":1}}],["来训练resnets",{"2":{"497":1}}],["来训练网络",{"2":{"155":1}}],["来重新生成一个对应实例",{"2":{"449":1}}],["来讲",{"2":{"409":1}}],["来作为下一个状态的输入",{"2":{"406":1}}],["来拟合真正的目标序列",{"2":{"389":1}}],["来替换transformer中的query向量",{"2":{"760":1}}],["来替换transformer核心组件多头注意力模块",{"2":{"43":1}}],["来替代传统的",{"2":{"346":1}}],["来防止头文件被多次包含",{"2":{"1628":1}}],["来防止过拟合",{"2":{"344":1}}],["来防止同一个序列中的不同样本相互影响",{"2":{"88":1}}],["来进行一次仅包含500次训练的计划",{"2":{"1179":1}}],["来进行",{"2":{"941":1,"960":1}}],["来进行预测下一个单词",{"2":{"378":1}}],["来进行解决",{"2":{"287":1}}],["来进行高效计算",{"2":{"1":1}}],["来增强",{"2":{"739":1}}],["来增强或抑制不同",{"2":{"224":1}}],["来增加输入管道生成数据的进程的数量",{"2":{"1161":1}}],["来增加大模型的直接生成文本表征能力",{"2":{"736":1}}],["来增加信息含量",{"2":{"256":1}}],["来传递信息",{"2":{"241":1}}],["来了",{"2":{"233":1}}],["来投影输入",{"2":{"230":1}}],["来检索与查询相对应的记忆",{"2":{"228":1,"230":1}}],["来近似总体的均值和标准差",{"2":{"338":1}}],["来近似",{"2":{"210":1}}],["来赋予整个模型",{"2":{"204":1}}],["来表示一个简单的",{"2":{"1728":1}}],["来表示每个可能的位置偏移",{"2":{"762":1}}],["来表示这些概念",{"2":{"628":1}}],["来表示任何数字",{"2":{"595":1}}],["来表示终止符",{"2":{"579":1}}],["来表示",{"2":{"184":1}}],["来表示从根节点到",{"2":{"184":1}}],["来表示比维数更多的特征",{"2":{"137":1}}],["来决两个元素是否属于同一个类",{"2":{"174":1}}],["来回忆事实",{"2":{"145":1}}],["来选择主语",{"2":{"145":1}}],["来完成模型编辑",{"2":{"145":1}}],["来删除这些神经元",{"2":{"143":1}}],["来理解和处理复杂的数据和概念",{"2":{"137":1}}],["来获取知识神经元的平均激活",{"2":{"135":1}}],["来计算统计数据",{"2":{"1136":1}}],["来计算梯度",{"2":{"1114":1}}],["来计算相对于q",{"2":{"946":1,"966":1}}],["来计算偏移向量",{"2":{"745":1}}],["来计算ode的梯度",{"2":{"495":1}}],["来计算沿着测地线的插值",{"2":{"354":1}}],["来计算一些注意力分数",{"2":{"267":1}}],["来计算出",{"2":{"169":1}}],["来计算每一个特征对输出的归因",{"2":{"134":1}}],["来计算注意力权重",{"2":{"5":1}}],["来区别不同的",{"2":{"89":1}}],["来源提示",{"2":{"48":1}}],["来说",{"2":{"39":2,"74":1,"117":1,"334":1}}],["来折叠头部维度",{"2":{"35":1}}],["来运作了",{"2":{"33":1}}],["vmware",{"2":{"2089":1}}],["vc++",{"2":{"1938":1}}],["vr开发",{"2":{"1936":1}}],["vp",{"2":{"1611":5}}],["vpvpv",{"2":{"621":1}}],["v4",{"2":{"1332":1}}],["v0",{"2":{"1309":1}}],["v0=0m",{"2":{"1192":1,"1193":1}}],["v0=0v",{"2":{"1189":1,"1190":1}}],["v0=1",{"2":{"1191":1}}],["vgg19",{"2":{"1308":2}}],["vgg16",{"2":{"1308":2}}],["vgg13",{"2":{"1308":2}}],["vgg11",{"2":{"1308":2}}],["vgg",{"2":{"1300":1,"1303":2}}],["vtune",{"2":{"1961":1}}],["vt+1+ϵbt+1",{"2":{"1193":1}}],["vt+1=β2vt+",{"2":{"1192":1,"1193":1}}],["vt+1=ρvt+",{"2":{"1191":1}}],["vt+1=γvt+∇l",{"2":{"1189":1,"1190":1}}],["vtcv",{"2":{"944":1}}],["vdot",{"2":{"1087":1}}],["vd​v​​",{"2":{"926":1}}],["vulkan等图形api进行图形编程",{"2":{"2009":1}}],["vulkan",{"2":{"1087":1,"1937":1}}],["vllm的吞吐量最高提高了24倍",{"2":{"980":1}}],["vllm已经在加州大学伯克利分校开发",{"2":{"980":1}}],["vllm利用了我们的新注意力算法pagedattention",{"2":{"980":1}}],["vllm",{"0":{"979":1,"980":1},"1":{"980":1,"981":1,"982":1,"983":1}}],["vli⋅e",{"2":{"128":1}}],["vlivilv",{"2":{"128":3}}],["v已加载到sram中重新计算注意力矩阵s和p的值",{"2":{"964":1}}],["v块轻松地重新计算注意力矩阵s和p",{"2":{"946":1,"966":1}}],["vjk",{"2":{"944":1,"974":1}}],["vjp",{"2":{"148":4,"485":3}}],["v​0​​=1",{"2":{"1191":1}}],["v​0​​=0",{"2":{"1189":1,"1190":1,"1192":1,"1193":1}}],["v​t+1​​=β​2​​v​t​​+",{"2":{"1192":1,"1193":1}}],["v​t+1​​=ρv​t​​+",{"2":{"1191":1}}],["v​t+1​​=γv​t​​+∇l",{"2":{"1189":1,"1190":1}}],["v​t​c​​​​",{"2":{"944":1}}],["v​i​​",{"2":{"974":1}}],["v​j​​",{"2":{"944":1,"974":1}}],["v​1​​",{"2":{"944":1}}],["vwxyz",{"2":{"1821":1,"1839":1}}],["vw​i​v​​",{"2":{"927":1}}],["vwiv",{"2":{"927":1}}],["v中的位置编码",{"2":{"762":1}}],["v中的向量之间的余弦相似度",{"2":{"356":1}}],["v∈rdv∈rdv∈r^d",{"2":{"760":1}}],["v∈rl×dvv∈rl×dv",{"2":{"161":1}}],["v代表词表大小",{"2":{"698":1}}],["v代表相关向量构成的矩阵",{"2":{"265":1}}],["v吗还是只取出里面的k还是只在最后一个结束后才整体取一次",{"2":{"656":1}}],["vnewpvpnewv",{"2":{"623":1}}],["vehicles",{"2":{"2082":1}}],["vehicle",{"2":{"1664":3,"1665":14}}],["vedio",{"2":{"1090":1}}],["vec2",{"2":{"1087":2}}],["vec",{"2":{"692":6,"1087":1,"1714":14,"1736":3,"1737":3,"1738":3,"1739":4,"1741":4,"1743":4,"1744":4,"1746":3,"1747":4,"1749":4,"1751":5,"1752":6,"1754":5,"1755":3,"1756":5,"1797":1,"1879":1,"1897":3,"1898":2}}],["vector2",{"2":{"1788":14}}],["vector>",{"2":{"1645":1,"1691":1,"1714":1,"1718":1,"1719":6,"1736":1,"1737":1,"1738":1,"1739":1,"1741":1,"1742":1,"1743":1,"1744":1,"1746":1,"1747":1,"1749":1,"1750":1,"1751":1,"1752":1,"1754":1,"1755":1,"1756":1,"1797":2,"1825":1,"1843":1,"1883":1,"1891":1,"1897":1,"1914":1,"1922":1,"1925":1,"1928":1,"1933":1,"2061":1}}],["vector就好了",{"2":{"898":1}}],["vector来进行文本的生成的",{"2":{"894":1}}],["vector再解码",{"2":{"891":1}}],["vectorization",{"2":{"676":2,"680":1}}],["vector",{"0":{"1719":1,"1797":1},"2":{"173":1,"271":1,"395":1,"495":1,"713":1,"740":1,"834":1,"888":5,"889":3,"898":1,"916":1,"1615":2,"1645":2,"1670":1,"1714":7,"1715":1,"1718":4,"1719":14,"1726":1,"1736":1,"1737":1,"1738":1,"1739":1,"1741":2,"1742":2,"1743":1,"1744":1,"1746":1,"1747":1,"1749":1,"1750":1,"1751":1,"1752":1,"1754":1,"1755":1,"1756":1,"1795":1,"1797":14,"1798":1,"1800":1,"1825":1,"1843":1,"1879":1,"1883":2,"1891":3,"1897":3,"1898":1,"1914":2,"1922":1,"1925":6,"1928":1,"1933":2,"2061":2}}],["vector的方向",{"2":{"122":1}}],["vector全部的k和v",{"2":{"72":1}}],["vectors",{"2":{"36":1,"688":2}}],["versus",{"0":{"881":1}}],["version=",{"2":{"1481":2}}],["version>8",{"2":{"1481":1}}],["version>",{"2":{"1481":2}}],["version>3",{"2":{"1481":1}}],["version",{"2":{"591":5,"1082":1,"1110":2,"1195":1,"1208":1,"1214":1,"1584":1,"1923":3,"1966":1,"1971":1,"1972":1,"1980":1,"1999":1,"2004":2}}],["verbose",{"2":{"592":1}}],["verbose=false",{"2":{"591":1,"592":1}}],["very",{"2":{"590":1,"1242":1}}],["ve|",{"2":{"571":1}}],["v2从以下三个方面做了改进",{"2":{"972":1}}],["v2设计了一种创新的注意力机制",{"2":{"956":1}}],["v2来自论文",{"2":{"735":1}}],["v2则有一个类似的单位数",{"2":{"595":1}}],["v2",{"0":{"969":1,"972":1,"1382":1,"1399":1},"2":{"553":1,"735":1,"740":1,"955":1,"956":1,"1097":1,"1098":1,"1308":5,"1332":1,"1788":2}}],["v3",{"0":{"1383":1,"1401":1},"2":{"553":2,"569":1,"1098":1,"1308":3,"1990":1}}],["v到底是怎么来的",{"2":{"535":1}}],["v均来自编码器的同一个输出",{"2":{"526":1}}],["v均来自编码器的输出",{"2":{"526":1}}],["v均来自同一个部分",{"2":{"71":1}}],["v就像是指针一样",{"2":{"510":1}}],["v就是分母",{"2":{"179":1}}],["v来自如下",{"2":{"444":1}}],["v有如下可能",{"2":{"442":1,"443":1}}],["v生成可以并行化",{"2":{"417":1}}],["vz",{"2":{"394":1}}],["vz=attention",{"2":{"394":1}}],["v1",{"0":{"969":1,"972":1,"1381":1},"2":{"361":1,"591":2,"944":1,"1098":1,"1262":1,"1263":1,"1363":9,"1788":2}}],["v1|v2|",{"2":{"145":1}}],["v沿着嵌入维度进行归一化",{"2":{"356":1}}],["v沿着n",{"2":{"180":1}}],["v和wowow",{"2":{"355":1}}],["v和xq位于同一设备",{"2":{"201":1}}],["v进行深入分析",{"2":{"265":1}}],["v这三个术语",{"2":{"265":1}}],["v这三者的关系就是",{"2":{"164":1}}],["v形",{"2":{"204":1}}],["v矩阵来自编码器的输出",{"2":{"525":1}}],["v矩阵通俗理解",{"2":{"233":1}}],["v矩阵",{"2":{"199":1}}],["v是把输入映射成高维空间的点",{"2":{"173":1}}],["v三个矩阵",{"2":{"172":1}}],["v可能更好理解",{"2":{"164":1}}],["v会被用来计算注意力得分",{"2":{"161":1}}],["v都来自解码器中前一层的输出",{"2":{"443":1}}],["v都来自编码器中前一层的输出",{"2":{"442":1}}],["v都来自输入x",{"2":{"3":1}}],["v都是x",{"2":{"533":1}}],["v都是解码器的输入序列",{"2":{"443":1}}],["v都是输入序列",{"2":{"442":1}}],["v都是来自于这个序列",{"2":{"158":1}}],["v∗",{"2":{"145":8}}],["v为swiglu模块的两个权重矩阵",{"2":{"109":1}}],["vscode",{"2":{"1583":1}}],["vsp",{"2":{"1557":1}}],["vsplit",{"2":{"1087":3,"1557":1}}],["vsoftmax",{"2":{"519":2}}],["vs",{"0":{"307":1,"328":1,"880":1,"882":1,"883":1,"969":1},"1":{"329":1,"330":1,"331":1,"332":1,"333":1,"334":1,"335":1},"2":{"93":2,"293":2,"661":1,"845":1,"882":1,"883":1,"1104":1,"1280":1,"1461":1,"1605":1,"1705":1}}],["vvv",{"2":{"105":1,"502":1}}],["vv",{"2":{"71":1}}],["volume",{"2":{"1774":4}}],["volutional",{"2":{"769":1}}],["voldpvpoldv",{"2":{"623":1}}],["volitional",{"2":{"163":1}}],["voice",{"2":{"597":1,"638":1}}],["void",{"2":{"47":1,"395":1,"1436":1,"1481":1,"1485":1,"1486":1,"1487":1,"1488":1,"1590":2,"1607":3,"1611":9,"1633":2,"1638":2,"1639":1,"1645":7,"1648":1,"1649":1,"1650":3,"1653":1,"1654":2,"1655":1,"1656":2,"1659":3,"1660":3,"1663":3,"1665":6,"1667":1,"1671":1,"1674":2,"1675":2,"1677":2,"1685":4,"1688":6,"1690":2,"1691":8,"1693":3,"1700":1,"1701":1,"1704":3,"1705":5,"1706":1,"1708":1,"1726":2,"1729":7,"1761":1,"1770":1,"1772":2,"1774":2,"1778":1,"1779":1,"1784":4,"1788":1,"1791":2,"1792":2,"1825":1,"1843":1,"1849":1,"1853":2,"1857":3,"1861":3,"1866":3,"1867":2,"1868":2,"1869":1,"1874":5,"1887":1,"1891":2,"1895":1,"1902":2,"1909":3,"1912":3,"1914":1,"1928":1,"1929":2,"2004":2,"2005":1,"2006":4,"2007":1,"2008":1,"2060":2,"2063":1}}],["voc",{"2":{"399":1,"530":2}}],["vocb",{"2":{"363":1}}],["vocab是词典大小",{"2":{"703":1}}],["vocabularies",{"2":{"561":1,"638":1}}],["vocabulary",{"2":{"83":1,"148":1,"374":1,"483":1,"485":1,"513":1,"557":6,"561":1,"565":1,"591":1,"638":1,"688":1,"700":2}}],["vocab把句子转换为词表index的序列",{"2":{"384":2,"558":2}}],["vocab对象有8185个单词",{"2":{"679":1}}],["vocab对象",{"2":{"384":2,"558":2}}],["vocabparallelembedding",{"2":{"201":1}}],["vocab",{"2":{"65":1,"201":5,"364":4,"371":3,"372":4,"374":14,"375":4,"384":6,"422":10,"423":8,"448":2,"449":7,"473":1,"510":1,"557":27,"558":11,"571":1,"580":1,"591":25,"592":11,"679":2,"699":1,"700":3,"701":3,"702":2,"703":9,"709":2,"723":2,"1217":2,"1218":13}}],["v所影响",{"2":{"41":1}}],["v也可以被其相邻head的",{"2":{"41":1}}],["video",{"2":{"2079":1}}],["vim编辑器",{"0":{"1584":1}}],["vim操作命令",{"0":{"1540":1},"1":{"1541":1,"1542":1,"1543":1,"1544":1,"1545":1,"1546":1,"1547":1,"1548":1,"1549":1,"1550":1,"1551":1,"1552":1,"1553":1,"1554":1,"1555":1,"1556":1,"1557":1,"1558":1,"1559":1,"1560":1}}],["vim",{"0":{"1517":1,"1541":1,"1542":1},"1":{"1518":1,"1519":1,"1520":1,"1543":1,"1544":1},"2":{"1518":3,"1520":1,"1541":1,"1554":2,"1555":1,"1556":1,"1559":1,"1561":1,"1584":6}}],["vitepress解析展示latex数学公式",{"2":{"2045":1}}],["viterbi",{"0":{"1330":1},"2":{"1330":2}}],["vit",{"2":{"1308":4,"1363":2}}],["virtualbox安装linux虚拟机",{"2":{"1582":1}}],["virtual",{"2":{"981":1,"1435":1,"1656":1,"1662":3,"1665":2,"1685":2,"1688":1,"1691":3,"1693":5,"1866":1,"1869":3,"2006":1}}],["viq",{"2":{"974":1}}],["vincent",{"2":{"429":1}}],["vincentlee",{"2":{"429":1}}],["via",{"2":{"429":1,"638":2,"740":2,"768":1,"1784":1,"1785":1}}],["vieler",{"2":{"370":1,"557":1}}],["view>",{"2":{"1929":2,"1933":1}}],["view更加安全",{"2":{"658":1}}],["view和reshape的区别",{"2":{"658":1}}],["view",{"0":{"818":1,"820":1,"1929":1},"1":{"819":1,"820":1,"821":1,"822":1},"2":{"36":3,"76":1,"201":4,"398":2,"410":2,"472":2,"498":1,"503":4,"513":1,"658":1,"820":3,"833":1,"1079":1,"1080":1,"1082":2,"1083":1,"1086":2,"1087":4,"1097":1,"1110":1,"1215":1,"1216":4,"1218":6,"1330":7,"1345":7,"1920":1,"1929":11,"1932":1,"1933":2}}],["vicle",{"2":{"156":1}}],["vil⋅𝐸",{"2":{"128":1}}],["viv𝑖v",{"2":{"125":1}}],["visit",{"2":{"1926":2}}],["vision",{"2":{"41":2,"156":1,"211":1,"233":1,"429":1,"446":1,"513":1,"637":1,"638":1}}],["visualize",{"2":{"431":2}}],["visualization",{"2":{"47":1,"399":2,"431":1,"455":1,"460":1,"698":1}}],["visual",{"2":{"131":1,"260":1,"1605":3,"1729":1,"1963":1,"1969":3}}],["v的梯度",{"2":{"946":1,"966":1}}],["v的维度是",{"2":{"173":1}}],["v的独立使得模型可以同时计算整个序列中所有位置的注意力分数",{"2":{"172":1}}],["v的逻辑",{"2":{"172":1}}],["v的来源有两种",{"2":{"161":1}}],["v的每个头进行一一对应的点积",{"2":{"36":1}}],["v的shape为",{"2":{"36":1}}],["v后",{"2":{"36":1}}],["v向量进行线性变换",{"2":{"23":1}}],["vae做分类任务的基础",{"2":{"1375":1}}],["vae做生成任务的基础",{"2":{"1375":1}}],["vae框架已经形成",{"2":{"1375":1}}],["vae",{"0":{"1357":1,"1368":1,"1369":1,"1375":1},"1":{"1370":1,"1371":1,"1372":1,"1373":1,"1374":1,"1375":1},"2":{"1378":3}}],["vaswani等人",{"2":{"956":1}}],["vaswani",{"2":{"429":1}}],["valgrind",{"2":{"1648":1,"1671":1}}],["val2",{"2":{"1641":4}}],["val1",{"2":{"1641":4}}],["vale",{"2":{"614":1}}],["val",{"2":{"557":4,"1085":1,"1245":2,"1280":1,"1303":2,"1306":1,"1307":2,"1630":2,"1640":2,"2006":4}}],["validate",{"2":{"1231":1,"1233":1,"1234":1,"1235":1,"1236":1,"1237":1,"1238":1,"1240":1,"1245":2,"1246":1,"1247":1}}],["validation",{"2":{"370":1,"423":1,"1280":2,"1302":1,"1308":1}}],["valid",{"2":{"364":1,"375":9,"423":3,"591":1,"1328":2,"1329":1,"1330":1,"1695":1,"1927":4}}],["value2",{"2":{"1641":4}}],["value1",{"2":{"1641":4}}],["valueerror",{"2":{"1226":1}}],["value头",{"2":{"938":1,"954":1}}],["value则是编码器最后一层的输出",{"2":{"533":1}}],["value这三个参数传递给mha",{"2":{"517":1}}],["value设置成相同的东西",{"2":{"442":1}}],["value有两种可能的形状",{"2":{"199":1}}],["value是李宁鞋商品本身",{"2":{"463":1}}],["value是对应的信息",{"2":{"265":1}}],["value是输入的向量组",{"2":{"198":1}}],["value是与地址key相关联的值",{"2":{"164":1}}],["value=",{"2":{"173":1,"271":1,"1481":4}}],["value=∑iaiviattention",{"2":{"173":2,"271":2}}],["value=pad",{"2":{"65":1,"384":2}}],["value不应该是人工指定的",{"2":{"172":1}}],["value应该不仅仅只和该单词本身有关",{"2":{"172":1}}],["value的角色互换",{"2":{"614":1}}],["value的权重就越大",{"2":{"265":1}}],["value的形状只会是第二种",{"2":{"199":1}}],["value的形状有两种可能",{"2":{"198":1}}],["value的shape只会是第二种",{"2":{"198":1}}],["value的变换过程",{"2":{"172":1}}],["value的影响力就越大",{"2":{"168":1}}],["value的名称也暗示了整个注意力计算的思路",{"2":{"164":1}}],["value的维度是",{"2":{"36":1}}],["value向量包含序列中每个token的实际内容或特征",{"2":{"162":1}}],["value对",{"2":{"916":1}}],["value对于transformers",{"2":{"162":1}}],["value对应的key所关联的模式句子的下一个词会以高概率值出现在该分布中",{"2":{"126":1}}],["value对的信息",{"2":{"70":1}}],["values也是",{"2":{"666":1}}],["values",{"2":{"76":6,"201":6,"230":1,"384":2,"723":3,"935":1,"945":1,"951":1,"965":1,"1086":3,"1087":5,"1217":10,"1481":1,"1486":1}}],["value和",{"2":{"916":1}}],["value和predict",{"2":{"288":1}}],["value和key的shape从",{"2":{"36":1}}],["value和词嵌入的向量维度",{"2":{"23":1}}],["value分组",{"2":{"33":1}}],["value按照各自的维度分割为若干段",{"2":{"33":1}}],["value做物理切分",{"2":{"29":1}}],["value拆分成多头",{"2":{"29":1}}],["value",{"0":{"126":1},"2":{"16":1,"29":2,"36":9,"38":3,"39":2,"63":1,"67":2,"84":1,"96":1,"117":3,"125":3,"126":4,"127":1,"151":1,"156":2,"158":1,"162":1,"163":1,"164":4,"169":2,"170":1,"198":2,"199":3,"200":6,"201":3,"209":1,"265":3,"271":1,"288":3,"306":1,"347":1,"394":2,"399":1,"463":1,"517":1,"525":1,"533":2,"535":1,"621":1,"623":4,"756":1,"923":1,"933":2,"935":2,"951":2,"957":1,"975":14,"980":1,"981":2,"1087":26,"1214":1,"1216":2,"1217":5,"1218":2,"1227":3,"1330":1,"1489":2,"1613":1,"1633":3,"1638":4,"1640":5,"1650":1,"1684":1,"1694":4,"1700":4,"1701":5,"1719":2,"1720":5,"1721":4,"1722":3,"1723":1,"1724":5,"1725":2,"1729":1,"1807":1,"1868":1,"1898":2,"1907":2,"1910":4,"1911":4,"1917":1,"1922":4,"1925":2,"1926":4,"1927":6,"1928":8}}],["vanishing",{"2":{"296":1}}],["vanilla",{"2":{"19":1,"32":1,"193":1,"210":1,"330":1,"403":1,"624":1,"750":1}}],["varchar",{"2":{"1481":2}}],["var2",{"2":{"1221":1}}],["var1",{"2":{"1221":1}}],["vars=false",{"2":{"1214":1}}],["vars",{"2":{"1214":3}}],["vars进行调用",{"2":{"1214":1}}],["varun",{"2":{"1124":1,"1195":1}}],["various",{"2":{"1086":1}}],["variable",{"0":{"1908":1},"2":{"621":1,"700":1,"723":1,"1082":1,"1086":1,"1110":3,"1613":1,"1649":3,"1904":1,"1917":2}}],["variables",{"0":{"1613":1,"1923":1},"2":{"592":1,"1920":1}}],["variant>",{"2":{"1926":2}}],["variant",{"0":{"1926":1},"2":{"1920":1,"1926":5,"1932":1}}],["variants",{"2":{"103":1,"105":1}}],["variance并不见得是最好的选择",{"2":{"313":1}}],["variance",{"2":{"313":1,"346":4,"1211":2}}],["var",{"2":{"189":5,"640":2,"807":10,"808":4,"809":3,"810":3,"833":2,"999":6,"1000":2,"1002":10,"1003":42,"1004":1,"1087":5,"1097":1,"1098":3,"1099":1,"1102":1,"1211":15,"1506":1,"1539":3}}],["vattention",{"2":{"57":2,"194":4,"918":1}}],["v权重可以在参数量相同的情况提升模型的表达能力",{"2":{"9":1}}],["v=d",{"2":{"7":1}}],["v",{"0":{"653":1},"2":{"5":1,"9":2,"10":1,"16":1,"17":2,"23":3,"26":1,"28":4,"30":1,"31":1,"36":3,"54":1,"57":4,"71":5,"83":5,"105":6,"109":1,"128":2,"161":5,"162":1,"164":1,"172":1,"173":6,"175":8,"185":4,"186":3,"194":8,"197":2,"198":1,"199":1,"200":1,"201":6,"213":1,"231":1,"265":1,"270":1,"356":1,"383":3,"394":5,"399":3,"424":5,"503":8,"510":4,"519":1,"535":1,"536":1,"542":3,"561":1,"646":2,"652":1,"656":1,"700":3,"745":1,"760":1,"918":5,"923":1,"926":1,"927":8,"941":2,"942":2,"944":5,"948":1,"959":2,"960":2,"963":2,"974":2,"978":1,"1097":2,"1189":3,"1190":3,"1191":2,"1192":3,"1193":3,"1216":4,"1218":4,"1339":11,"1360":6,"1541":3,"1551":6,"1741":5,"1742":2,"1743":2,"1744":2,"1747":2,"1749":2,"1751":2,"1752":2,"1925":4,"1926":8}}],["和饼干数组",{"2":{"2152":1}}],["和截距bbb",{"2":{"2018":1}}],["和信用账户",{"2":{"1873":1}}],["和电话",{"2":{"1808":1}}],["和成绩",{"2":{"1728":1}}],["和成员函数",{"2":{"1678":1}}],["和联合体",{"2":{"1727":1}}],["和公有成员函数",{"2":{"1678":1}}],["和操作这些数据的函数",{"2":{"1674":1}}],["和提成比例",{"2":{"1657":1}}],["和等于运算符",{"2":{"1630":1}}],["和右值",{"2":{"1629":1}}],["和实现分离",{"2":{"1628":1}}],["和引用是什么",{"2":{"1616":1}}],["和大家熟悉的",{"2":{"1602":1}}],["和链接选项",{"2":{"1589":1}}],["和集体通信",{"2":{"1573":1}}],["和从数据库中取数据",{"2":{"1479":1}}],["和运行",{"2":{"1435":1}}],["和y0",{"2":{"1377":1}}],["和ytyt",{"2":{"510":2}}],["和协方差矩阵",{"2":{"1361":1}}],["和图像对之间的匹配度和相关性",{"2":{"1360":1}}],["和它们之间的相对位置",{"2":{"1342":1}}],["和自定义数据类型",{"2":{"1576":1}}],["和自然语言生成",{"2":{"1317":1}}],["和自注意力机制至关重要",{"2":{"162":1}}],["和多语言",{"2":{"1315":1}}],["和多层感知器",{"2":{"354":1}}],["和多层感知机",{"2":{"155":1}}],["和动态图",{"2":{"1287":1}}],["和label",{"2":{"1283":1}}],["和lbfgs",{"2":{"1223":1}}],["和对字典值进行迭代的迭代器",{"2":{"1225":1}}],["和缓冲区",{"2":{"1214":1}}],["和不同的种子",{"2":{"1169":1}}],["和冗余超参数为",{"2":{"1144":2}}],["和正则化超参数",{"2":{"1135":1,"1186":1}}],["和步数预算",{"2":{"1133":1}}],["和推断模式",{"2":{"1118":1}}],["和存储偏移",{"2":{"1087":1}}],["和具有一些重要区别的动量的变种",{"2":{"1059":1}}],["和0",{"2":{"1029":1}}],["和随机梯度下降",{"2":{"1027":1}}],["和随机的注意力",{"2":{"204":1}}],["和1",{"2":{"1016":1}}],["和偏置",{"2":{"988":1}}],["和反向传播中的",{"2":{"976":2}}],["和分组查询注意力",{"2":{"971":1}}],["和分组查询注意力机制",{"2":{"956":1}}],["和卷积",{"2":{"911":1}}],["和relu",{"2":{"846":1}}],["和resnet",{"2":{"467":1}}],["和中央处理器",{"2":{"795":1}}],["和填充",{"2":{"779":1}}],["和transformer",{"2":{"976":1}}],["和transformer层的hidden",{"2":{"698":1}}],["和t5恰恰相反",{"2":{"763":1}}],["和weight",{"2":{"783":1}}],["和wk",{"2":{"760":1}}],["和w2∈rdinput×dmodelw2∈rdinput×dmodelw",{"2":{"101":1}}],["和加权求和ziziz",{"2":{"759":1}}],["和直接的embedding相加是等价的",{"2":{"722":1}}],["和gpt类似",{"2":{"721":1}}],["和switch",{"2":{"1922":1}}],["和skip",{"2":{"714":1}}],["和self",{"2":{"523":1,"533":1}}],["和sequence",{"2":{"77":1}}],["和通常具有较小参数计数的神经网络",{"2":{"711":1}}],["和康奈尔大学的一项研究挑战了我们对这种流行方法的理解",{"2":{"692":1}}],["和b=",{"2":{"692":1}}],["和embedding",{"2":{"689":1}}],["和eps",{"2":{"343":1}}],["和方向的量",{"2":{"680":1}}],["和向量化",{"2":{"676":1}}],["和以前的动作tokenization方法相比",{"2":{"637":1}}],["和概念",{"2":{"628":1}}],["和模型参数的交互中",{"2":{"624":1}}],["和涉及模型参数的计算",{"2":{"624":1}}],["和merge",{"2":{"590":1}}],["和mlp",{"2":{"446":1}}],["和更细粒度的token切分",{"2":{"562":1}}],["和`",{"2":{"557":1}}],["和训练数据进行了专门的调整",{"2":{"553":1}}],["和词干提取",{"2":{"552":1}}],["和额外空白",{"2":{"552":1}}],["和英文句子",{"2":{"528":1}}],["和u",{"2":{"507":1}}],["和位置细胞",{"2":{"490":1}}],["和位置编码器",{"2":{"454":2}}],["和cot长度t",{"2":{"480":1}}],["和局部模式",{"2":{"477":1}}],["和局部注意力",{"2":{"285":1}}],["和掩码",{"2":{"464":1}}],["和一些高级编程技巧中",{"2":{"1611":1}}],["和一个泛型lambda表达式作为参数",{"2":{"1914":1}}],["和一个更新门",{"2":{"874":1}}],["和一个高维向量相关联",{"2":{"458":1}}],["和一个ffn",{"2":{"97":1}}],["和一起作为解码器的输入",{"2":{"445":1}}],["和执行过程都相同",{"2":{"426":1}}],["和ate",{"2":{"409":1}}],["和he初始化",{"2":{"403":1}}],["和hidden",{"2":{"346":1}}],["和dp",{"2":{"976":1}}],["和d=",{"2":{"692":1}}],["和decay",{"2":{"402":1}}],["和dropout",{"2":{"344":1}}],["和真实分布",{"2":{"398":1}}],["和真实物理环境相关",{"2":{"259":1}}],["和ββ",{"2":{"343":1}}],["和β",{"2":{"341":1}}],["和β都是维度等于通道数",{"2":{"341":1}}],["和β的区别",{"2":{"341":1}}],["和在",{"2":{"334":1}}],["和尺度缩放",{"2":{"320":1}}],["和平移不变性",{"2":{"320":1}}],["和空白没有比较的意义",{"2":{"318":1}}],["和空白",{"2":{"316":1}}],["和输出流",{"2":{"1826":1,"1844":1}}],["和输出序列",{"2":{"1322":1}}],["和输出",{"2":{"773":1}}],["和输出h",{"2":{"301":1}}],["和输出词表的embedding矩阵e",{"2":{"128":1}}],["和残差流状态",{"2":{"295":1}}],["和解码器当前步的隐状态htℎtℎ",{"2":{"285":1}}],["和其它单词之间的相似度",{"2":{"265":1}}],["和其他所有的",{"2":{"204":1}}],["和知识",{"2":{"246":1}}],["和之前的输入信息xt−1xt−1x",{"2":{"240":1}}],["和可组合型",{"2":{"222":1}}],["和序列长度",{"2":{"217":1}}],["和传统的机制相比",{"2":{"217":1}}],["和块间计算",{"2":{"216":1}}],["和线性注意力",{"2":{"210":1}}],["和最近的",{"2":{"204":1}}],["和v",{"2":{"201":2}}],["和value",{"2":{"164":1,"981":1}}],["和p1",{"2":{"2018":1}}],["和prelu",{"2":{"843":1}}],["和p",{"2":{"200":2}}],["和相对位置编码相比",{"2":{"1341":1}}],["和相对位置编码",{"2":{"742":1}}],["和相加",{"2":{"175":1}}],["和相应键之间的相似性进行加权的",{"2":{"154":1}}],["和相应的向量值",{"2":{"145":1}}],["和物理环境相关",{"2":{"167":1}}],["和感官输入",{"2":{"163":1}}],["和权重",{"2":{"148":1,"485":1}}],["和定位",{"2":{"142":1}}],["和f",{"2":{"134":1,"265":1}}],["和人脑类似",{"2":{"129":1}}],["和很多方案一样",{"2":{"122":1}}],["和单层变化没有本质区别",{"2":{"117":1}}],["和门限",{"2":{"98":1}}],["和前面padding",{"2":{"75":1}}],["和未来时刻的key的内积值为负无穷大",{"2":{"70":1}}],["和变换",{"2":{"45":1}}],["和每个头部分配一个",{"2":{"34":1}}],["和",{"0":{"772":1,"823":1,"824":1,"830":1,"843":1,"873":1,"1203":1,"1647":1,"1668":1,"1669":1,"1673":1,"1922":1},"1":{"825":1,"826":1,"827":1,"828":1,"831":1,"832":1},"2":{"9":2,"16":2,"17":2,"28":2,"30":1,"31":1,"34":1,"36":3,"38":1,"39":1,"66":1,"78":1,"89":1,"105":2,"109":1,"119":2,"126":1,"130":2,"131":1,"135":2,"145":1,"167":2,"169":1,"172":1,"189":1,"198":1,"213":2,"231":1,"246":5,"247":2,"259":2,"277":2,"292":2,"311":1,"313":2,"329":1,"330":1,"334":2,"338":1,"341":1,"343":1,"344":3,"347":1,"349":1,"351":1,"360":1,"381":1,"382":1,"396":1,"398":1,"402":1,"423":1,"437":1,"449":1,"456":1,"457":1,"501":1,"513":1,"517":2,"523":1,"533":3,"541":1,"542":4,"560":1,"567":3,"577":1,"579":2,"595":2,"616":1,"620":1,"623":3,"624":1,"632":1,"647":1,"650":1,"691":1,"701":1,"709":1,"713":1,"714":1,"721":1,"746":1,"760":1,"765":1,"773":2,"775":1,"808":3,"838":1,"839":1,"861":1,"864":1,"865":1,"866":2,"924":1,"926":1,"932":1,"935":3,"941":1,"948":1,"951":3,"960":1,"970":1,"971":1,"978":1,"980":1,"981":1,"986":3,"999":1,"1004":1,"1016":1,"1018":1,"1046":1,"1052":1,"1059":1,"1073":1,"1081":1,"1087":1,"1099":1,"1105":2,"1111":1,"1122":2,"1130":1,"1143":1,"1144":1,"1163":1,"1175":1,"1214":3,"1226":1,"1227":2,"1228":1,"1242":1,"1275":1,"1312":1,"1315":1,"1316":1,"1317":2,"1328":1,"1331":1,"1342":2,"1344":3,"1350":1,"1361":2,"1364":2,"1373":1,"1386":3,"1396":1,"1441":1,"1485":1,"1493":1,"1509":1,"1510":1,"1589":1,"1594":2,"1605":2,"1611":1,"1619":1,"1624":1,"1627":1,"1630":2,"1635":3,"1647":1,"1648":1,"1652":1,"1664":4,"1666":3,"1668":3,"1672":1,"1674":1,"1678":3,"1690":1,"1695":2,"1698":2,"1700":1,"1704":1,"1715":1,"1728":3,"1729":3,"1766":1,"1789":1,"1800":1,"1813":1,"1821":3,"1825":1,"1831":1,"1839":3,"1843":1,"1851":1,"1855":1,"1859":1,"1869":1,"1871":1,"1874":1,"1911":1,"1913":1,"1920":1,"1922":1,"1932":2,"2018":1,"2054":4}}],["和值",{"2":{"5":1,"158":1,"230":1,"535":1,"542":1,"957":1}}],["和强大功能",{"2":{"1":1}}],["键开头",{"2":{"1917":1}}],["键盘还是网络",{"2":{"1810":1,"1828":1}}],["键盘",{"2":{"1810":1,"1828":1}}],["键是唯一的",{"2":{"1725":1}}],["键是相应的参数和缓冲区名称",{"2":{"1214":1}}],["键进入普通模式",{"2":{"1541":1}}],["键进入",{"2":{"1519":2}}],["键值缓存共享示例",{"2":{"985":1}}],["键值缓存",{"2":{"985":1}}],["键值对存储",{"2":{"1807":1}}],["键值对",{"2":{"123":1}}],["键值对形式",{"0":{"124":1},"1":{"125":1,"126":1,"127":1,"128":1,"129":1},"2":{"96":1}}],["键和值矩阵来计算掩码自注意力",{"2":{"464":1}}],["键和值矩阵",{"0":{"463":1}}],["键和值",{"2":{"355":1,"502":1}}],["键矩阵和值矩阵",{"2":{"507":1}}],["键矩阵",{"2":{"265":1}}],["键向量",{"2":{"194":2}}],["键向量和值向量",{"2":{"162":1,"172":1}}],["键",{"2":{"5":1,"118":1,"154":3,"158":1,"535":1}}],["qm⊤kn=",{"2":{"1343":1}}],["qmq",{"2":{"1342":1}}],["qa",{"2":{"1332":1}}],["qscheme",{"2":{"1087":2}}],["qr",{"2":{"1087":2}}],["q​m​⊤​​k​n​​=",{"2":{"1343":1}}],["q​m​​",{"2":{"1342":1}}],["q​i​​k​j​⊤​​=x​i​​w​q​​w​k​⊤​​x​j​⊤​​+x​i​​w​q​​w​k​⊤​​p​j​⊤​​+p​i​​w​q​​w​k​⊤​​x​j​⊤​​+p​i​​w​q​​w​k​⊤​​p​j​⊤​​",{"2":{"1340":1}}],["q​i​​k​j​⊤​​",{"2":{"1339":1,"1340":1}}],["q​i​​",{"2":{"974":1}}],["q​t​r​​​​",{"2":{"944":1}}],["q​1​​",{"2":{"944":1}}],["qw​i​q​​",{"2":{"927":1}}],["qwiq",{"2":{"927":1}}],["q^tw",{"2":{"766":1}}],["q^t",{"2":{"760":1,"766":1}}],["q^tk",{"2":{"175":2,"210":1}}],["qtrq",{"2":{"944":1}}],["qtqtq",{"2":{"746":1,"765":2,"766":1}}],["qtk",{"2":{"210":2}}],["q与k计算得到了一个注意力分数矩阵",{"2":{"537":1}}],["q来自掩码多头注意力的输出",{"2":{"525":1}}],["q来自前一个解码器层",{"2":{"444":1}}],["q2",{"2":{"503":2}}],["q2q2q",{"2":{"502":1}}],["q1",{"2":{"503":2,"944":1}}],["q1q1q",{"2":{"502":1}}],["q表示对信息的请求",{"2":{"264":1}}],["q是译文",{"2":{"536":1}}],["q是序列中当前位置的词向量",{"2":{"442":1,"443":1}}],["q是从",{"2":{"263":1}}],["q是由查询模型所产生的查询向量",{"2":{"263":1}}],["q可以被解释为一个一般问题",{"2":{"263":1}}],["q×kt√dkq×ktdk",{"2":{"197":1}}],["q⋅k⊤q⋅k⊤q",{"2":{"188":1}}],["qikj⊤=xiwqwk⊤xj⊤+xiwqwk⊤pj⊤+piwqwk⊤xj⊤+piwqwk⊤pj⊤q",{"2":{"1340":1}}],["qikj⊤q",{"2":{"1339":1,"1340":1}}],["qi×ki",{"2":{"189":2}}],["qi⋅kit",{"2":{"189":1}}],["qi⋅kti",{"2":{"189":1}}],["qi⋅kj=∥qi∥∥kj∥cos",{"2":{"176":2}}],["qi",{"2":{"176":8,"189":2,"210":2,"974":1}}],["qi=xiwqki=xiwkvi=xiwvqi=xiwqki=xiwkvi=xiwvq",{"2":{"161":1}}],["qi=wqi×qki=wki×kvi=wvi×vqi=wiq×qki=wik×kvi=wiv×vq",{"2":{"28":1}}],["q和k的维度均是",{"2":{"173":1}}],["q=d",{"2":{"173":1}}],["q=xwqk=xwkv=xwvq=xwqk=xwkv=xwv",{"2":{"161":1}}],["q代表了我们要查询的信息或者说当前位置希望获得的信息",{"2":{"172":1}}],["q∗ktq∗ktq",{"2":{"172":1}}],["q就能逐渐学习到v的特征",{"2":{"168":1}}],["q就是任务相关的查询向量",{"2":{"164":1}}],["qqq和kkk的的点积会容易出现较大数值",{"2":{"187":1}}],["qq",{"2":{"71":1,"768":4}}],["qk​t​​−l",{"2":{"971":1}}],["qk^",{"2":{"186":1,"971":1}}],["qk^t",{"2":{"34":1,"57":1,"71":4,"173":1,"187":1,"194":2,"199":1,"394":1,"510":1,"519":1}}],["qkv均来自同一个序列",{"2":{"535":4}}],["qkv",{"0":{"265":1,"288":1},"2":{"292":1,"463":1}}],["qkv存储量还会随着上下文长度的增长而线性增长",{"2":{"162":1}}],["qkv解析",{"0":{"162":1},"1":{"163":1,"164":1,"165":1,"166":1,"167":1,"168":1,"169":1},"2":{"157":1}}],["qk",{"2":{"93":2}}],["qk⊤d",{"2":{"186":1}}],["qk⊤√d",{"2":{"186":1}}],["qk⊤",{"2":{"71":2}}],["qkt−l",{"2":{"971":1}}],["qktd",{"2":{"194":1}}],["qktdk",{"2":{"57":1,"394":1,"510":1,"918":1}}],["qkt√d",{"2":{"194":1}}],["qkt√dk",{"2":{"57":1,"394":1,"510":1}}],["qkt",{"2":{"173":2,"187":2,"519":2}}],["qktmasked",{"2":{"71":4}}],["qktqkt",{"2":{"71":1}}],["qktqktqk^t",{"2":{"70":1,"169":1,"394":1,"536":1}}],["qktqktqk^t之后对角线元素过大的问题",{"2":{"20":1}}],["queue>",{"2":{"1723":1}}],["queue",{"0":{"1723":1},"2":{"1723":3}}],["questions",{"2":{"985":1}}],["question",{"2":{"906":1,"1332":3,"1616":1}}],["queries",{"2":{"41":2,"1217":7}}],["query越需要吸收value的信息",{"2":{"265":1}}],["query和key",{"2":{"614":1}}],["query和key的点积被用来来判断两个向量之间的相似性",{"2":{"463":1}}],["query和key越相似就代表value对query的影响力越大",{"2":{"265":1}}],["query和所有的key进行相似度计算",{"2":{"173":1,"271":1}}],["query的相似度进行内插值",{"2":{"173":1,"271":1}}],["query的相似度进行内插值的结果",{"2":{"168":1}}],["query可以定义成解码器中某一步的隐状态",{"2":{"165":1}}],["query张量和与key有对应关系的value张量",{"2":{"163":1}}],["query向量代表当前正在处理的token或位置",{"2":{"162":1}}],["query也被编码成一个内部状态u",{"2":{"125":1}}],["query是你在搜索栏输入的查询内容",{"2":{"463":1}}],["query是你要找的内容",{"2":{"265":1}}],["query是查询信息",{"2":{"164":1}}],["query是",{"2":{"36":1}}],["query",{"0":{"935":1,"936":1,"951":1,"952":1},"1":{"937":1,"953":1},"2":{"1":1,"23":1,"29":4,"36":12,"38":3,"39":2,"41":1,"67":3,"82":1,"158":1,"162":1,"163":1,"164":3,"169":1,"170":6,"198":3,"199":5,"201":1,"204":1,"213":1,"265":2,"271":6,"394":3,"463":1,"525":1,"533":2,"535":1,"621":1,"727":1,"765":1,"933":3,"935":3,"938":1,"951":3,"954":1,"975":1,"1216":3,"1217":14,"1218":3,"1342":1,"1344":3}}],["quasi",{"2":{"1153":1,"1175":4}}],["quad",{"2":{"941":2,"943":4,"960":2,"961":4}}],["quantile",{"2":{"1087":2}}],["quantized",{"2":{"154":1,"1087":1}}],["quant",{"0":{"636":1}}],["quality这三个维度",{"2":{"725":1}}],["quality",{"2":{"364":1}}],["quickly",{"2":{"1215":1,"1304":1,"2078":1}}],["quickgeluactivation",{"2":{"110":1}}],["quick",{"2":{"110":1}}],["quote",{"2":{"1616":2}}],["quotient",{"2":{"1607":4}}],["quot",{"2":{"19":1,"20":4,"29":2,"58":4,"78":2,"101":5,"104":2,"105":2,"115":2,"118":2,"122":8,"126":2,"127":4,"130":2,"131":2,"134":2,"136":2,"137":8,"143":2,"148":2,"151":2,"155":2,"161":6,"170":2,"181":2,"185":1,"209":1,"210":1,"218":2,"239":2,"245":11,"267":1,"277":2,"280":2,"282":2,"298":3,"301":2,"302":2,"306":2,"316":1,"318":1,"320":2,"325":2,"326":2,"348":2,"350":2,"381":8,"394":2,"398":4,"405":10,"407":10,"408":4,"409":2,"420":2,"427":10,"437":3,"446":2,"449":2,"453":1,"456":4,"473":2,"477":2,"480":1,"483":2,"487":2,"493":2,"498":2,"499":2,"500":2,"504":2,"507":2,"508":2,"516":2,"528":7,"536":4,"537":9,"542":2,"547":8,"561":2,"562":2,"567":8,"575":10,"576":9,"579":6,"587":2,"588":2,"601":2,"605":2,"610":2,"616":2,"625":2,"637":2,"638":2,"689":2,"691":2,"694":2,"713":16,"714":3,"724":2,"735":2,"736":9,"744":3,"747":4,"748":2,"757":1,"764":2,"770":2,"840":2,"933":2,"943":2,"961":2,"986":28,"1134":2,"1143":12,"1144":4,"1149":6,"1150":2,"1154":4,"1157":2,"1158":2,"1222":2,"1242":4,"1304":2,"1324":8,"1460":6,"1574":4,"1607":6,"1616":6,"1628":4,"1642":2,"1664":4,"1690":6,"1695":2,"1704":5,"1713":50,"1725":4,"1729":2,"1999":2,"2045":2,"2093":2}}],["q",{"2":{"5":1,"9":2,"10":1,"12":1,"17":2,"26":1,"28":3,"30":1,"31":2,"41":2,"57":3,"67":1,"71":4,"154":1,"158":1,"161":4,"164":2,"169":1,"172":2,"173":7,"175":23,"176":5,"186":3,"189":5,"194":6,"197":2,"200":1,"201":1,"210":4,"211":1,"212":3,"213":6,"230":3,"233":1,"265":2,"270":6,"271":3,"355":1,"394":3,"417":1,"442":3,"443":3,"444":1,"503":13,"510":4,"535":1,"536":1,"620":1,"646":2,"764":2,"765":2,"918":4,"923":1,"926":1,"927":6,"941":2,"942":2,"944":5,"957":1,"959":2,"960":2,"963":2,"975":1,"976":1,"1087":9,"1216":4,"1218":4,"1339":1,"1340":6,"1342":1,"1343":4,"1361":9,"1377":6,"1518":1,"1544":2,"1557":1,"1699":3}}],["你能够毛慢感受到这里的",{"2":{"2140":1}}],["你能够保证每次都省下更多的钱",{"2":{"2130":1}}],["你优先处理紧急任务",{"2":{"2135":1}}],["你今天有很多任务要做",{"2":{"2133":1}}],["你今年",{"2":{"1673":1}}],["你有",{"2":{"2131":1}}],["你是要选择时间最短的最先安排",{"2":{"2118":1}}],["你最终到达了这座山的最高点",{"2":{"2115":1}}],["你在生活中已经不自觉地应用了这种算法思维",{"2":{"2105":1}}],["你在选择工作时",{"2":{"2105":1}}],["你在哪里",{"2":{"245":1}}],["你都会选当前看起来最好的那个选项",{"2":{"2101":1}}],["你早上要上班",{"2":{"2097":1}}],["你现在得过且过",{"2":{"2056":1}}],["你唯一能做的",{"2":{"2056":1}}],["你去阻止",{"2":{"2054":1}}],["你内心深处依旧是愿意",{"2":{"2054":1}}],["你自然而然会感受到",{"2":{"2054":1}}],["你感到反感或者厌恶",{"2":{"2054":1}}],["你看到别人随地吐痰",{"2":{"2054":1}}],["你只能当做理解过程中的一点参考",{"2":{"2052":1}}],["你只能使用一组查询",{"2":{"5":1}}],["你说说",{"2":{"2051":1}}],["你觉得他们之间的有何种关系",{"2":{"2031":1}}],["你给它一些",{"2":{"1729":1}}],["你需要按顺序完成这些任务",{"2":{"2097":1}}],["你需要维护当前有效成员的信息",{"2":{"1728":1}}],["你需要清楚地知道当前联合体中哪个成员存储了有效的值",{"2":{"1728":1}}],["你要将饼干分给这些小孩",{"2":{"2147":1}}],["你要描述一个学生的信息",{"2":{"1728":1}}],["你要用python爬取三个网页的数据",{"2":{"1562":1}}],["你无需手动分配和释放内存",{"2":{"1714":1}}],["你好",{"2":{"1673":1,"1729":1}}],["你好吗",{"2":{"245":1}}],["你传递给它的函数",{"2":{"1645":1}}],["你可能会发现更高的山峰",{"2":{"2115":1}}],["你可能会回忆上次用钥匙的地点",{"2":{"2100":1}}],["你可能不会先计划好所有要买的东西",{"2":{"2101":1}}],["你可能需要先显示菜单",{"2":{"1620":1}}],["你可以每次选择最便宜的出行方式",{"2":{"2130":1}}],["你可以创建一个函数来计算两个数的和",{"2":{"1729":1}}],["你可以把联合体想象成一个",{"2":{"1728":1}}],["你可以把它想象成学习如何写作",{"2":{"542":1}}],["你可以根据需要自定义名称",{"2":{"1728":1}}],["你可以修改地址指向的房子",{"2":{"1612":1}}],["你可以使用",{"2":{"1611":1}}],["你可以使用多组不同的q",{"2":{"5":1}}],["你可以在不重新运行实验的情况下",{"2":{"1175":1}}],["你可以将其视为网络的",{"2":{"863":1}}],["你可以想到某个物体有数千个不同的语义属性",{"2":{"708":1}}],["你就不能再把它给其他人了",{"2":{"1612":1}}],["你才能真正掌握",{"2":{"1610":1}}],["你应该按照以下方式编写代码",{"2":{"1231":1}}],["你应该选择更简单的观点",{"2":{"542":1}}],["你想要更改评估指标",{"2":{"1175":1}}],["你想要的物品",{"2":{"163":1}}],["你找到了一组超参数",{"2":{"1175":1}}],["你使用随机搜索来寻找最优的超参数组合",{"2":{"1175":1}}],["你会根据重要性或紧急程度来决定先做什么",{"2":{"2099":1}}],["你会不自觉地心怀感恩",{"2":{"2054":1}}],["你会看到一个无限延伸的影像",{"2":{"1646":1}}],["你会一个一个爬吗",{"2":{"1562":1}}],["你会如何设计呢",{"2":{"1077":1}}],["你会用注意力机制组装出一个transformer吗",{"2":{"407":1,"429":1,"543":1}}],["你将",{"2":{"866":1}}],["你还需要将隐藏状态和当前输入传递到",{"2":{"866":1}}],["你还会让它与随机的",{"2":{"709":1}}],["你的",{"2":{"1914":1}}],["你的专家组合llm是秘密的免费嵌入模型",{"2":{"740":1}}],["你的大脑能自动关注到书的内容",{"2":{"3":1}}],["你浏览语料库中的每个单词",{"2":{"709":1}}],["你到底会如何设置不同属性的值",{"2":{"708":1}}],["你并不是基于学习在每个路口如何打方向盘",{"2":{"626":1}}],["你不知道它指向哪里",{"2":{"1611":1}}],["你不仅会关注这句话本身",{"2":{"167":1}}],["你不能创建一个指向",{"2":{"1612":1}}],["你不能",{"2":{"59":1}}],["你注意力都被变形金刚",{"2":{"163":1}}],["你",{"2":{"80":2,"381":1,"385":1,"709":1}}],["dhcp服务未开启",{"2":{"2091":1}}],["dhcp",{"2":{"2090":1}}],["django",{"2":{"2070":1}}],["dbshell",{"2":{"2070":1}}],["dbstorage",{"2":{"1867":1}}],["db",{"2":{"2070":1}}],["dpu加速编程需求增长",{"2":{"1960":1}}],["dpdk",{"2":{"1952":1}}],["dptr",{"2":{"1633":4}}],["d$",{"2":{"1548":2}}],["d0",{"2":{"1548":1}}],["dw1",{"2":{"1398":2}}],["dw2",{"2":{"1398":2}}],["dw",{"2":{"1398":3,"1548":1}}],["ds",{"0":{"1428":1}}],["dst",{"2":{"1214":2}}],["dsplit",{"2":{"1087":3}}],["dsigmoid",{"2":{"839":1}}],["dsigmoiddx=sigmoid",{"2":{"839":1}}],["d表示的输出通道的数量",{"2":{"1003":1}}],["d×nd",{"2":{"1003":1}}],["dnf",{"2":{"1584":1}}],["dn",{"2":{"941":2,"960":2}}],["dnn",{"2":{"499":1}}],["dnns",{"2":{"492":1}}],["d​i+1​​的递推关系式求解出来即可",{"2":{"2023":1}}],["d​​",{"2":{"1343":1,"1344":4}}],["d​​​​k​​",{"2":{"1336":2}}],["d​c​​≪d​h​​n​h​​",{"2":{"957":1}}],["d​model​​",{"2":{"926":1}}],["d​v​​",{"2":{"926":1}}],["d​q​​",{"2":{"926":1}}],["d​k​​",{"2":{"924":3,"926":1}}],["dqd",{"2":{"926":1}}],["dtd",{"2":{"1481":6}}],["dtanh",{"2":{"839":1}}],["dtanhdx=1−tanh2",{"2":{"839":1}}],["dtype=original",{"2":{"1350":1}}],["dtype=none",{"2":{"702":1,"1083":2}}],["dtype=dtype",{"2":{"76":1}}],["dtype=torch",{"2":{"65":1,"384":2,"503":5,"558":2,"723":2,"1086":1,"1216":1,"1218":1,"1330":5}}],["dtype",{"2":{"76":6,"346":3,"702":2,"1078":3,"1082":2,"1086":2,"1087":79,"1214":5,"1227":2,"1350":2}}],["dx",{"2":{"839":2,"1398":1}}],["dda",{"0":{"2017":1},"2":{"2017":2}}],["dda算法",{"0":{"2016":1},"1":{"2017":1,"2018":1,"2019":1,"2020":1}}],["dd",{"2":{"1520":1,"1548":1,"1642":1}}],["ddxzzx",{"2":{"768":1}}],["ddp",{"2":{"423":1}}],["dfun",{"2":{"1440":4}}],["dfires",{"2":{"740":1}}],["dffndffnd",{"2":{"99":1}}],["dloss",{"2":{"1398":1}}],["dlpack",{"2":{"1083":5}}],["dl",{"0":{"639":1},"1":{"640":1,"641":1,"642":1,"643":1,"644":1,"645":1,"646":1,"647":1,"648":1,"649":1,"650":1,"651":1,"652":1,"653":1,"654":1}}],["dla应用未嵌入矩阵来模拟内部激活",{"2":{"479":1}}],["dla",{"2":{"479":1}}],["dave",{"2":{"1750":1}}],["dangling",{"2":{"1647":4,"1672":1}}],["danae",{"2":{"513":1}}],["date",{"2":{"1513":1,"1642":1,"2051":1,"2053":2,"2056":1}}],["datatype",{"2":{"1590":2}}],["datasource>",{"2":{"1481":1}}],["datasource",{"2":{"1481":1}}],["dataset2",{"2":{"1215":2}}],["dataset1",{"2":{"1215":2}}],["dataset会迭代的将所有数据加载到内存吗",{"2":{"665":2}}],["datasets",{"2":{"370":1,"375":1,"387":1,"557":2,"1215":3,"1253":2,"1278":1,"1283":1,"2079":3}}],["dataset",{"0":{"669":1,"1249":1},"2":{"370":1,"375":3,"1161":1,"1215":4,"1223":2,"1231":2,"1250":2,"1253":1,"1254":3,"1295":4,"1296":2,"1300":1,"1302":1,"1303":1,"1304":1,"1308":1,"1332":1,"2086":1}}],["database",{"2":{"1481":1}}],["data0",{"2":{"1071":4,"1092":2}}],["data4",{"2":{"1070":3}}],["data1",{"2":{"1070":1,"1071":1,"1097":4}}],["data3",{"2":{"1069":1,"1070":1,"1071":1,"1092":2}}],["data2",{"2":{"1069":3,"1070":2,"1071":2,"1092":2}}],["data上微调",{"2":{"726":1}}],["data数据",{"2":{"658":1}}],["data的stride和shape等属性就变了",{"2":{"658":1}}],["data不变",{"2":{"658":1}}],["dataframe",{"2":{"399":1}}],["data是",{"2":{"399":1}}],["data增加一维",{"2":{"399":1}}],["dataloader内部会将",{"2":{"384":1,"558":1}}],["dataloaders",{"2":{"364":1,"375":2,"423":1}}],["dataloader",{"0":{"669":1,"1251":1},"2":{"364":3,"375":6,"385":1,"423":6,"1215":2,"1241":1,"1242":1,"1244":2,"1251":4,"1283":1,"1296":3,"1299":2,"1404":1}}],["data",{"0":{"1304":1,"1305":1,"1428":1},"1":{"1306":1,"1307":1},"2":{"74":1,"76":1,"79":1,"83":2,"156":1,"217":1,"315":6,"380":3,"381":1,"383":5,"385":2,"387":2,"398":1,"399":13,"410":2,"424":2,"428":3,"431":1,"455":1,"456":1,"460":1,"472":4,"529":3,"557":2,"575":1,"620":1,"621":1,"623":1,"624":1,"698":1,"1069":3,"1070":2,"1071":1,"1072":11,"1078":22,"1082":1,"1086":2,"1087":3,"1161":2,"1211":2,"1213":2,"1214":1,"1215":13,"1241":3,"1242":3,"1250":5,"1251":5,"1253":5,"1254":4,"1273":8,"1283":1,"1295":10,"1296":1,"1297":4,"1304":1,"1308":5,"1330":1,"1331":2,"1478":1,"1566":5,"1623":1,"1663":7,"1694":20,"1696":4,"1700":5,"1728":12,"1763":1,"1820":4,"1838":4,"1867":1,"1887":27,"1925":3,"2073":1,"2077":2,"2079":1,"2086":1}}],["dao",{"2":{"1478":1,"1481":2}}],["dao层",{"2":{"1478":1}}],["dahl",{"2":{"1124":1,"1195":1}}],["day",{"2":{"564":2}}],["daily",{"2":{"1309":1}}],["dai",{"2":{"543":1}}],["d=64",{"2":{"941":1,"960":1}}],["d=512",{"2":{"445":1}}],["d=x+3",{"2":{"399":1}}],["dynamo",{"0":{"1299":1},"2":{"1227":4,"1299":2}}],["dynamicholder",{"2":{"1676":2}}],["dynamic",{"0":{"1688":1,"2075":1},"2":{"359":1,"498":3,"513":1,"981":1,"1090":1,"1287":1,"1629":1,"1683":2,"2075":1}}],["dynamically",{"2":{"43":2,"47":1}}],["dy​l−1​​",{"2":{"1003":1}}],["dy",{"2":{"1003":1,"1398":3}}],["dyl−1e",{"2":{"1003":1}}],["dyt并不是一种新的归一化层",{"2":{"360":1}}],["dyt层的定义如下图标号1所示",{"2":{"360":1}}],["dyt无需计算激活统计量即可实现这两种效果",{"2":{"359":1}}],["dyt",{"0":{"358":1},"1":{"359":1,"360":1},"2":{"293":1,"358":1,"359":2,"360":1}}],["dzmitry",{"2":{"284":4}}],["d≥n",{"2":{"210":1}}],["duplicate",{"2":{"1214":4}}],["duplex",{"2":{"740":1}}],["dumpdata",{"2":{"2070":1}}],["dumps",{"2":{"1214":1}}],["dump",{"2":{"1208":1,"1214":1,"1275":1}}],["dummy",{"0":{"1304":1},"2":{"1304":3,"1308":2,"2086":1}}],["dummyscheduler",{"2":{"83":1,"423":1,"424":1}}],["dummyoptimizer",{"2":{"83":1,"423":1,"424":1}}],["dubois以及charles",{"2":{"1194":1}}],["duchi",{"2":{"1042":1}}],["ducharme",{"2":{"429":1}}],["due",{"2":{"428":1,"2087":1}}],["dutim",{"2":{"387":1}}],["during",{"2":{"380":1,"399":1,"2076":1}}],["dual",{"2":{"204":1}}],["dcg",{"2":{"1090":1}}],["dc∗ld",{"2":{"957":1}}],["dc≪dhnhd",{"2":{"957":1}}],["dct是一种频域变换",{"2":{"637":1}}],["dct",{"2":{"637":3}}],["dca由三个组件组成",{"2":{"204":1}}],["dca的创新点也在于它与flash",{"2":{"204":1}}],["dca没有使用线性缩放位置索引或增加rope的基频",{"2":{"204":1}}],["dca",{"2":{"204":3}}],["dcmha",{"0":{"43":1},"1":{"44":1,"45":1,"46":1},"2":{"0":1,"43":1}}],["dvd",{"2":{"926":2}}],["dvlab",{"2":{"768":1}}],["dv",{"2":{"173":4}}],["d是hidden",{"2":{"185":1}}],["d是特征维度",{"2":{"113":1}}],["d是embedding",{"2":{"99":1}}],["drf",{"2":{"2070":2}}],["drawing",{"2":{"1688":6}}],["draw",{"2":{"1656":2,"1688":10}}],["drank",{"2":{"261":2}}],["dry",{"2":{"1215":2}}],["droupout",{"0":{"1017":1}}],["dropblock",{"0":{"1019":1},"2":{"1018":1}}],["dropconnect的思想也很简单",{"2":{"1018":1}}],["dropconnet",{"0":{"1018":1}}],["drop",{"2":{"533":1,"1481":1}}],["dropout2",{"2":{"1215":4,"1257":2}}],["dropout1",{"2":{"1215":4,"1257":2}}],["dropout掩码",{"2":{"1152":1}}],["dropout实现方法很简单",{"2":{"1017":1}}],["dropout指在训练神经网络过程中随机丢掉一部分神经元来减少神经网络复杂度",{"2":{"1017":1}}],["dropout的实例",{"2":{"523":1}}],["dropout的子网络的平均",{"2":{"393":1}}],["dropout来达到更好的效果",{"2":{"396":1}}],["dropout对于每个batch的step所优化的参数都不同",{"2":{"393":1}}],["dropout通过随机丢弃的神经元来削弱节点彼此之间的依赖",{"2":{"393":1}}],["dropout概念是hilton在论文",{"2":{"393":1}}],["dropout概率为传入的dropout参数",{"2":{"113":1}}],["dropout是为了随机停止一些网络中神经元的作用",{"2":{"344":1}}],["dropout率",{"2":{"198":1,"344":1}}],["dropout=dropout",{"2":{"1217":1}}],["dropout=none",{"2":{"67":1,"199":1,"394":1,"933":1}}],["dropout=self",{"2":{"36":1}}],["dropout=0",{"2":{"23":1,"113":1,"448":1,"449":1,"703":1,"1216":2,"1218":3}}],["dropout",{"0":{"392":1,"469":1,"835":1},"1":{"393":1,"394":1,"395":1,"396":1},"2":{"23":2,"36":1,"67":2,"99":1,"113":5,"198":1,"199":2,"330":1,"344":11,"392":1,"393":3,"394":11,"395":11,"396":1,"429":1,"449":5,"461":2,"469":1,"523":2,"533":2,"703":4,"723":4,"835":1,"844":1,"933":2,"1018":1,"1122":1,"1143":6,"1149":1,"1154":1,"1215":4,"1216":8,"1217":12,"1218":20,"1257":2}}],["driver",{"2":{"792":1,"794":1,"1481":2}}],["driss",{"2":{"95":1}}],["drelu公式和效果如下",{"2":{"111":1}}],["drelu",{"0":{"111":1},"2":{"96":1}}],["dkv",{"2":{"957":1}}],["dkd",{"2":{"647":3,"924":3,"926":1}}],["dkdkd",{"2":{"173":2,"188":1}}],["dk∑i=1qi×ki",{"2":{"189":1}}],["dk1",{"2":{"187":1}}],["dk",{"2":{"71":1,"173":2}}],["doe",{"2":{"1805":2}}],["doesn",{"2":{"573":1}}],["does",{"2":{"18":1,"20":2,"47":1,"136":1,"156":1,"314":1,"437":1,"513":1,"820":1,"1093":2}}],["domain",{"2":{"1762":1}}],["domains",{"0":{"2080":1},"1":{"2081":1,"2082":1,"2083":1},"2":{"1086":1}}],["dout",{"2":{"1398":2}}],["double类型",{"2":{"1873":3}}],["doublecontainer",{"2":{"1701":3}}],["doublearray",{"2":{"1700":3}}],["double>",{"2":{"1623":1,"1629":1,"1700":1,"1701":1,"1908":1,"1925":2,"1926":1}}],["double",{"2":{"395":3,"1087":1,"1214":2,"1436":1,"1607":3,"1608":2,"1615":1,"1616":1,"1622":1,"1623":4,"1629":5,"1633":2,"1664":1,"1665":3,"1673":1,"1677":4,"1680":1,"1687":6,"1693":4,"1698":3,"1699":1,"1700":2,"1701":1,"1702":1,"1707":6,"1709":3,"1712":8,"1772":2,"1774":12,"1779":4,"1789":7,"1791":4,"1792":3,"1817":1,"1825":1,"1835":1,"1843":1,"1873":2,"1874":11,"1879":1,"1905":1,"1908":2,"1910":1,"1912":2,"1921":1,"1925":1,"1926":5,"2004":2}}],["doubly",{"2":{"115":1}}],["dog>",{"2":{"1685":1}}],["dogage",{"2":{"1674":4}}],["dogname",{"2":{"1674":4}}],["dog",{"2":{"557":1,"1655":2,"1674":9,"1685":14,"1690":1,"1691":6,"1693":8,"1866":3}}],["don",{"2":{"2076":1}}],["done",{"2":{"499":1,"1895":1}}],["dong",{"2":{"95":1,"543":1}}],["doccano",{"2":{"2066":1,"2067":1}}],["docker下的doccano添加普通用户",{"0":{"2064":1},"1":{"2065":1,"2066":1,"2067":1,"2068":1,"2069":1,"2070":1}}],["docker技术",{"2":{"1599":1}}],["docker",{"2":{"1499":1,"2043":1,"2066":2}}],["doctype",{"2":{"1481":2}}],["doc",{"2":{"1214":2,"1332":1}}],["docstr",{"2":{"1083":2}}],["docs",{"2":{"361":2,"429":1,"1254":1}}],["documents",{"2":{"768":1}}],["document",{"2":{"89":7}}],["downcasting",{"2":{"1683":2}}],["downblock",{"2":{"1364":1}}],["downsample",{"2":{"802":2}}],["download=true",{"2":{"1215":1,"1253":1,"1283":1}}],["download",{"2":{"373":2,"795":1,"796":1,"1253":1,"1302":1,"1304":1,"1605":6}}],["down",{"2":{"110":4,"167":1,"259":1}}],["do",{"0":{"497":1},"2":{"20":1,"36":1,"47":1,"204":1,"233":2,"497":1,"573":1,"944":2,"981":1,"1254":1,"1332":2,"1436":1,"1620":4,"1631":1,"2076":1}}],["dots",{"2":{"879":2,"912":3,"1000":2}}],["dot",{"0":{"916":1},"1":{"917":1,"918":1,"919":1,"920":1,"921":1,"922":1,"923":1,"924":1},"2":{"7":2,"9":1,"16":1,"67":1,"78":1,"173":1,"186":1,"199":1,"233":1,"434":1,"513":2,"647":1,"921":1,"924":1,"933":1,"1087":1,"1216":1,"1218":1}}],["deel",{"2":{"2049":1}}],["deepwise",{"0":{"776":1}}],["deepseek",{"0":{"1399":1,"1400":1,"1401":1,"1402":1},"2":{"553":3,"569":1,"595":1,"955":1,"956":2,"1404":1}}],["deepspeed",{"2":{"89":1,"387":1}}],["deepcopy是深度拷贝函数",{"2":{"449":1}}],["deepcopy",{"2":{"449":4,"522":1,"703":1,"1072":1,"1083":1}}],["deepnet",{"2":{"361":1}}],["deep",{"0":{"347":1,"1404":1},"2":{"47":1,"134":1,"156":4,"263":1,"292":1,"293":1,"302":1,"347":4,"361":2,"429":1,"449":1,"498":1,"543":1,"717":1,"747":1,"796":1,"1036":1,"1078":1,"1083":1,"1195":1,"1196":1,"1404":3,"1455":1,"1457":1,"2073":1,"2078":1,"2083":1,"2087":2}}],["deque>",{"2":{"1722":4,"1800":1}}],["deque",{"0":{"1722":1,"1800":1},"2":{"1722":17,"1795":1,"1800":3}}],["dequantize",{"2":{"1087":1}}],["deduction",{"0":{"1615":1,"1925":1},"2":{"1920":1}}],["debian",{"2":{"1537":1,"1539":1}}],["debug",{"2":{"1332":1}}],["deberta提供了使用相对位置和绝对位置编码的一个新视角",{"2":{"763":1}}],["deberta去掉了分解后的第四项",{"2":{"763":1}}],["deberta",{"2":{"763":1,"766":1,"768":1,"1315":3}}],["deberta出自",{"2":{"763":1}}],["deberta式",{"0":{"763":1},"2":{"741":1}}],["dev",{"2":{"1729":1}}],["developed",{"2":{"2073":1}}],["developers",{"2":{"1197":1}}],["development",{"2":{"1435":1,"1481":2}}],["devil",{"2":{"210":1}}],["device设置",{"2":{"1215":1}}],["device类型",{"2":{"1214":1}}],["device=original",{"2":{"1350":1}}],["device=freqs",{"2":{"1345":1}}],["device=self",{"2":{"723":1}}],["device=none",{"2":{"702":1}}],["device=tokens",{"2":{"201":2}}],["device=device",{"2":{"65":1,"76":3,"84":3,"384":4,"558":4,"1330":5}}],["device",{"0":{"1076":1},"2":{"76":5,"201":3,"375":2,"384":1,"422":1,"423":2,"558":1,"702":2,"723":1,"795":1,"1076":6,"1078":3,"1082":2,"1083":1,"1085":4,"1086":3,"1087":33,"1214":10,"1215":17,"1217":4,"1227":2,"1255":3,"1263":2,"1330":2,"1332":1,"1345":1,"1350":3}}],["deg2rad",{"2":{"1087":2}}],["degradation",{"2":{"334":1,"768":1}}],["demo",{"0":{"1218":1},"2":{"980":1,"1067":1,"1075":1,"1076":1,"1086":1,"1092":2,"1097":2,"1098":3,"1102":1,"1202":1,"1205":1,"1207":1,"1258":1,"1259":1,"1262":1,"1263":1,"1266":1,"1267":1,"1296":1,"1297":1,"1298":1,"1398":4,"1404":1,"2069":1,"2070":1}}],["dead",{"2":{"840":1}}],["decimal",{"2":{"2059":2}}],["decimals",{"2":{"1087":2}}],["declare",{"2":{"1990":1}}],["declaration",{"2":{"1611":1}}],["decltype",{"2":{"1897":2,"1905":2}}],["decrement",{"2":{"1621":1}}],["decays",{"2":{"1303":1}}],["decay或cosine",{"2":{"1172":1}}],["decay",{"2":{"1046":2,"1064":1,"1172":1,"1221":1,"1308":2}}],["decay阶段",{"2":{"402":1}}],["dec",{"2":{"727":1,"1218":4,"1817":3,"1835":3}}],["decoupling",{"2":{"1645":1}}],["decoupled",{"2":{"1064":1}}],["deconvolution",{"0":{"779":1}}],["decomposition",{"2":{"233":1,"298":1,"1083":1}}],["decomposing",{"2":{"137":1}}],["decoding",{"0":{"900":1},"1":{"901":1,"902":1,"903":1,"904":1},"2":{"156":1,"361":1,"429":2,"571":1,"591":1,"727":3,"763":1,"768":1,"901":1,"908":1,"935":1,"951":1}}],["decoded",{"2":{"573":2,"591":1}}],["decodes",{"2":{"573":1}}],["decode",{"2":{"69":1,"82":2,"83":4,"424":1,"428":1,"450":2,"472":1,"529":3,"538":2,"573":2,"591":2,"592":3,"703":2,"1330":1}}],["decoder其实是一种条件语言模型",{"2":{"898":1}}],["decoder即使遇到了end标识也不会结束",{"2":{"897":1}}],["decoder就可以快速步入正轨",{"2":{"896":1}}],["decoder如何进行解码",{"2":{"543":1}}],["decoder结构中",{"2":{"891":1}}],["decoder结构中的掩码mask介绍",{"2":{"95":1}}],["decoder结构的最大潜力",{"2":{"542":1}}],["decoder模型",{"2":{"883":1}}],["decoder模型因为可以看到双向",{"2":{"542":1}}],["decoder模型和decoder",{"2":{"541":1}}],["decoder架构中主要使用t5",{"2":{"729":1}}],["decoder架构",{"2":{"541":1,"542":1}}],["decoder架构和prefix",{"2":{"541":1}}],["decoder中的masked",{"2":{"535":1}}],["decoder中使用方式如下",{"2":{"343":1}}],["decoder交叉注意力模块",{"2":{"532":1}}],["decoder类的代码具体如下",{"2":{"532":1}}],["decoder类依次会根据当前翻译过的第i个单词",{"2":{"532":1}}],["decoder类是解码器的实现",{"2":{"532":1}}],["decoder类实例由n个decoderlayer类实例构建而成",{"2":{"461":1}}],["decoder类实例",{"2":{"450":1}}],["decoder则需要在一定量的标注数据上做multitask",{"2":{"542":1}}],["decoder则包括外面的n参数",{"2":{"461":1}}],["decoder则包括外面的n",{"2":{"449":1}}],["decoder则会看src",{"2":{"85":1}}],["decoder进行改进的案例",{"2":{"280":1}}],["decoder使用两种注意力结构",{"2":{"200":1}}],["decoder会看src",{"2":{"79":1}}],["decoder是不知道下文信息的",{"2":{"57":1}}],["decoder数据流",{"2":{"49":1}}],["decoder",{"0":{"532":1,"539":1,"541":1,"887":1,"889":1,"1216":1,"1316":1,"1317":1},"1":{"540":1,"541":1,"542":1},"2":{"39":1,"59":1,"74":1,"78":4,"82":3,"83":5,"84":1,"89":1,"165":1,"237":1,"241":2,"282":1,"292":1,"343":3,"344":1,"414":1,"436":2,"449":2,"450":6,"461":1,"501":1,"503":3,"529":2,"530":1,"532":3,"533":1,"535":1,"538":1,"539":1,"540":2,"541":7,"542":14,"543":1,"703":6,"729":1,"732":1,"886":2,"887":1,"889":3,"890":2,"898":1,"909":1,"934":2,"935":2,"951":2,"977":1,"1216":2,"1218":5,"1312":2,"1316":7,"1317":8}}],["decoderlayer主要成员变量如下",{"2":{"533":1}}],["decoderlayer和encoderlayer的内部非常相似",{"2":{"533":1}}],["decoderlayer类是解码器层的实现",{"2":{"533":1}}],["decoderlayer包含两个注意力层",{"2":{"461":1}}],["decoderlayer包含两个attention层",{"2":{"449":1}}],["decoderlayer",{"0":{"533":1},"2":{"39":1,"82":1,"449":1,"461":1,"529":1,"530":2,"533":2,"538":2,"703":1,"1216":3,"1218":3}}],["decoder的预训练",{"0":{"898":1}}],["decoder的并行化仅在训练阶段",{"2":{"426":1}}],["decoder的输入",{"2":{"39":1}}],["decoder的目的是",{"2":{"39":1}}],["decoder层用到两处",{"2":{"37":1}}],["de的封装器",{"2":{"384":1,"558":1}}],["dereference",{"2":{"1611":1}}],["derivatives",{"2":{"1088":1}}],["derived类对外部隐藏了base的接口",{"2":{"1861":1}}],["derived",{"2":{"591":1,"1663":5,"1683":8,"1849":2,"1853":2,"1857":2,"1861":2,"2006":3}}],["der",{"2":{"370":1,"557":1}}],["de",{"2":{"364":3,"370":2,"371":2,"372":1,"373":6,"374":2,"375":5,"422":5,"423":2,"557":7,"638":1}}],["desktop",{"2":{"2089":1}}],["desktop安装linux镜像",{"2":{"1582":1}}],["destroyed",{"2":{"1891":1}}],["destructor",{"2":{"1665":4}}],["dest",{"2":{"1590":2}}],["destination=none",{"2":{"1214":1}}],["destination",{"2":{"1087":4,"1214":6,"1742":3,"1987":2,"1999":2}}],["designed",{"2":{"2078":1}}],["desired",{"2":{"1254":2,"1303":1}}],["desiderata",{"2":{"160":1}}],["description=",{"2":{"1215":1}}],["descend",{"0":{"1022":1,"1025":1,"1026":1},"1":{"1023":1,"1024":1,"1025":1,"1026":1,"1027":1},"2":{"1398":1}}],["descending",{"2":{"557":1,"1087":7,"1883":1}}],["descent在内存使用方面更有效率",{"2":{"1027":1}}],["descent在大规模数据集上具有一定的优势",{"2":{"1027":1}}],["descent的学习率可以根据需要进行调整",{"2":{"1027":1}}],["descent的梯度估计具有更小的抖动",{"2":{"1027":1}}],["descent的参数更新速度较慢",{"2":{"1027":1}}],["descent的参数更新速度更快",{"2":{"1027":1}}],["descent",{"0":{"1188":1},"2":{"298":1,"542":1,"543":1,"1023":1,"1025":1,"1026":1,"1027":1,"1067":1,"1243":1,"1244":1}}],["deserve",{"2":{"561":1,"638":1}}],["denied",{"2":{"1874":1,"2060":1}}],["density",{"2":{"1087":2}}],["densenet201",{"2":{"1308":1}}],["densenet169",{"2":{"1308":1}}],["densenet161",{"2":{"1308":1}}],["densenet121",{"2":{"1308":1}}],["dense",{"2":{"8":1,"113":2,"344":4,"488":1,"688":1,"1083":1,"1086":7,"1087":9}}],["denoiser",{"0":{"1354":1},"2":{"635":1}}],["dengbocong",{"2":{"233":1,"361":1}}],["depends",{"2":{"1993":1,"1994":1}}],["dependent",{"2":{"758":1,"760":1}}],["dependency>",{"2":{"1481":4}}],["dependency",{"2":{"429":1}}],["dependencies",{"2":{"160":1,"1917":2}}],["deprecated",{"0":{"1909":1},"2":{"1904":1,"1909":5,"1913":1}}],["deprecation",{"2":{"1083":1}}],["deposit",{"2":{"1677":2,"1766":2,"1873":1,"1874":3}}],["deploying",{"2":{"513":1}}],["deplication",{"2":{"364":1}}],["depth=vocab",{"2":{"700":1}}],["depth",{"2":{"28":2,"47":1,"115":1,"503":2,"1090":1}}],["det",{"2":{"1087":1}}],["deterministically",{"2":{"591":1}}],["detection",{"2":{"2079":1}}],["detecting",{"2":{"562":1}}],["detect",{"2":{"592":1}}],["detected",{"2":{"422":1}}],["detectors",{"2":{"393":1}}],["detached",{"2":{"1083":1}}],["detach",{"0":{"1094":1},"2":{"383":2,"399":1,"1083":4,"1087":2,"1094":1,"1211":6}}],["details",{"2":{"95":1,"768":2}}],["detoxification",{"2":{"123":1}}],["delim",{"2":{"1813":6,"1831":6}}],["delete>",{"2":{"1488":1}}],["deleteuser",{"2":{"1488":3}}],["delete",{"0":{"1488":1,"1647":2,"1669":2},"2":{"1488":2,"1630":2,"1647":15,"1648":3,"1666":2,"1668":4,"1669":8,"1671":2,"1672":4,"1676":3,"1678":1,"1680":1,"1683":2,"1685":1,"1688":4,"1691":1,"1694":4,"1695":1,"1696":1,"1700":1,"1706":1,"1714":2,"1887":3,"1911":1}}],["delattr",{"2":{"1214":1}}],["delving",{"2":{"156":1}}],["deltaδ",{"2":{"1004":1,"1443":1}}],["delta",{"2":{"148":3,"191":12,"485":6,"763":1,"1004":11,"1023":1,"1052":1,"1395":1,"2018":10}}],["del",{"2":{"36":3,"385":2}}],["definitions",{"2":{"1985":2}}],["define",{"2":{"1039":1,"1604":1,"1628":1,"1632":4,"1916":2,"1923":1,"1999":1,"2086":1}}],["defined",{"2":{"38":1,"39":1,"82":1,"344":1,"523":1,"533":1,"1082":1}}],["deformable",{"0":{"780":1},"1":{"781":1,"782":1}}],["defaultdict",{"2":{"1226":2}}],["defaults",{"2":{"1224":1,"1225":1,"1226":2,"1227":2}}],["default=",{"2":{"1481":1}}],["default=false",{"2":{"1215":4}}],["default=0",{"2":{"1215":1}}],["default=10",{"2":{"1215":1}}],["default=1000",{"2":{"1215":1}}],["default=1",{"2":{"1215":3}}],["default=64",{"2":{"1215":1}}],["default",{"2":{"76":1,"402":1,"557":2,"572":1,"591":1,"1106":1,"1215":6,"1283":1,"1284":1,"1303":1,"1308":7,"1481":3,"1922":1,"1927":2,"2006":1}}],["def",{"2":{"8":1,"23":1,"36":1,"38":1,"39":1,"66":1,"74":1,"76":1,"82":4,"83":2,"84":3,"110":2,"113":2,"114":2,"119":1,"201":6,"343":6,"344":4,"346":5,"373":1,"375":3,"380":2,"394":3,"398":2,"399":4,"410":1,"422":1,"428":2,"450":4,"472":2,"503":2,"522":2,"523":2,"529":3,"532":2,"533":2,"557":3,"558":1,"571":1,"590":2,"591":7,"592":4,"666":1,"701":2,"702":2,"703":5,"723":2,"807":1,"808":1,"1072":1,"1083":55,"1085":75,"1086":27,"1087":731,"1097":1,"1098":2,"1099":1,"1100":2,"1205":6,"1211":2,"1212":3,"1213":3,"1214":72,"1215":7,"1216":8,"1217":6,"1218":11,"1223":1,"1227":23,"1250":6,"1253":1,"1254":5,"1255":1,"1257":2,"1262":1,"1273":1,"1284":1,"1295":6,"1296":1,"1298":1,"1299":3,"1328":1,"1345":4,"1398":6,"1440":3,"1566":1,"1616":1,"2086":2}}],["digamma",{"2":{"1087":2}}],["digit",{"2":{"595":1}}],["digital",{"2":{"156":1,"2017":1}}],["dialogue",{"2":{"906":1}}],["diagflat",{"2":{"1087":1}}],["diagonal",{"2":{"89":2,"90":1,"1087":9}}],["diagonal=1",{"2":{"74":1,"84":1,"201":1,"382":1}}],["diagonal=1意为不包含主对角线",{"2":{"74":1}}],["diag",{"2":{"46":2,"192":1,"1087":2}}],["dilation=",{"2":{"801":1}}],["dilation",{"2":{"801":1}}],["dilated",{"0":{"778":1}}],["dir=",{"2":{"1254":1}}],["dir",{"2":{"1083":1,"1214":1,"1250":4,"1281":1,"1308":2,"1332":1,"1509":5,"1976":1,"1987":1,"1989":1}}],["dirichlet",{"2":{"711":1}}],["directories",{"0":{"1976":1},"2":{"1976":1}}],["directory>",{"2":{"1482":2}}],["directory>src",{"2":{"1482":2}}],["directory",{"2":{"1283":1,"1930":10}}],["directx",{"2":{"1937":1,"2009":1}}],["directed",{"2":{"1089":1}}],["directly",{"2":{"764":1}}],["direct",{"2":{"479":1}}],["directional",{"2":{"76":1,"1315":1}}],["diego",{"2":{"638":1}}],["divergence",{"2":{"1377":1}}],["diversity",{"2":{"725":1}}],["division",{"2":{"1762":1}}],["divisible",{"2":{"1216":1,"1217":1,"1218":1}}],["divide",{"2":{"1087":12,"1762":3,"1999":3}}],["div",{"2":{"395":1,"829":1,"1085":1,"1087":2,"1330":1}}],["dict调用该钩子",{"2":{"1227":1}}],["dict调用之前执行预处理操作",{"2":{"1214":1,"1227":1}}],["dict函数返回的键完全匹配",{"2":{"1214":1}}],["dict的形式返回优化器的状态",{"2":{"1227":1}}],["dict的键必须与该模块的torch",{"2":{"1214":1}}],["dict的时候只有参数",{"2":{"668":1}}],["dict复制到当前模块及其子模块",{"2":{"1214":1}}],["dict复制到当前模块",{"2":{"1214":1}}],["dict方法之后运行",{"2":{"1214":1}}],["dict将返回相同的对象",{"2":{"1214":1}}],["dict之后",{"2":{"1227":1}}],["dict之后被调用",{"2":{"1227":1}}],["dict之后调用",{"2":{"1214":1}}],["dict之前对其进行后处理",{"2":{"1227":1}}],["dict之前被调用",{"2":{"1227":1}}],["dict之前",{"2":{"1214":1,"1227":1}}],["dict中对应的张量",{"2":{"1214":1}}],["dict中保存的元数据作为local",{"2":{"1214":1}}],["dict中的每个子模块上调用",{"2":{"1214":1}}],["dict中的每个子模块上调用此方法",{"2":{"1214":1}}],["dict中",{"2":{"1214":1}}],["dict",{"2":{"372":1,"423":2,"557":3,"571":1,"591":1,"666":1,"1082":1,"1208":25,"1211":1,"1214":26,"1222":2,"1225":1,"1226":12,"1227":36,"1258":1,"1259":1,"1266":4,"1267":4,"1273":4}}],["dictionary",{"2":{"137":1,"590":2}}],["diffsettings",{"2":{"2070":1}}],["diffusers",{"2":{"1347":1}}],["diffusion中u",{"2":{"1365":1}}],["diffusion",{"0":{"633":1,"634":1,"635":1,"1348":1,"1355":1},"1":{"1349":1,"1350":1,"1351":1,"1352":1,"1353":1,"1354":1,"1356":1,"1357":1,"1358":1},"2":{"232":1,"498":1,"634":1,"1365":1,"1367":1,"1404":1}}],["diff",{"2":{"503":2,"1087":1,"1633":3}}],["difficulty",{"2":{"361":1,"449":1}}],["difference",{"2":{"1086":1,"1443":1,"1607":3}}],["differences",{"2":{"136":1,"156":1}}],["differentiable",{"2":{"1228":1}}],["differential",{"0":{"497":1,"502":1},"2":{"493":1,"497":1,"498":1,"500":1,"513":4,"2017":1}}],["differentical",{"2":{"500":1,"513":1}}],["different",{"2":{"5":2,"764":1,"935":1,"951":1,"1086":1,"1217":1,"1284":1,"1299":1,"2079":2}}],["displacement",{"2":{"1664":1,"1665":10}}],["displayprivatemember",{"2":{"1778":2}}],["display",{"2":{"1664":5,"1665":8,"1788":2}}],["dispatch",{"2":{"1083":2,"1086":2}}],["disables",{"2":{"1215":2}}],["disable",{"2":{"1214":2,"1227":4}}],["disabled",{"2":{"1083":1}}],["disallowed",{"2":{"572":4}}],["disentangled",{"2":{"763":1,"768":1}}],["discrete",{"2":{"688":1}}],["discretize",{"0":{"497":1},"2":{"497":1}}],["distillation",{"2":{"1315":1}}],["distilbert",{"2":{"569":1,"1315":2}}],["distance",{"2":{"745":1,"1361":1,"1633":1,"1719":1,"1754":1,"1756":2,"1922":1}}],["distance=n∑i=1|xi−yi|distance=n∑i=1|xi−yi|distance=∑i=1n|xi−yi|distance=∑i=1n|xi−yi|distance",{"2":{"692":1}}],["dist执行过scatter后变为",{"2":{"399":1}}],["dist上设置为self",{"2":{"399":1}}],["dist的第一个1维度上与target",{"2":{"399":1}}],["dist为",{"2":{"399":1}}],["dist全部填充为",{"2":{"399":2}}],["dist",{"2":{"399":9,"423":1,"1087":1,"1306":2,"1307":4,"1308":8}}],["distribute",{"2":{"1404":1}}],["distributedsampler",{"2":{"375":3}}],["distributed=true",{"2":{"375":1}}],["distributed=is",{"2":{"364":1,"423":1}}],["distributed=false",{"2":{"364":1,"423":1}}],["distributed",{"0":{"1305":1},"1":{"1306":1,"1307":1},"2":{"90":1,"364":1,"372":1,"375":2,"422":3,"423":3,"740":1,"1087":1,"1305":2,"1306":1,"1307":2,"1308":8}}],["distribution",{"2":{"181":2,"233":1,"411":1,"713":1,"715":1}}],["distributor",{"2":{"167":1,"259":1}}],["dissecting",{"2":{"122":1,"156":1}}],["dinputdinputd",{"2":{"99":1,"419":1}}],["dim为64",{"2":{"1363":1}}],["dim0",{"2":{"1087":5}}],["dim2",{"2":{"1087":4}}],["dim1",{"2":{"1087":9}}],["dimv",{"2":{"1086":1}}],["dimi",{"2":{"1086":1}}],["dim结尾的维度被扁平化",{"2":{"828":1}}],["dim开头且以end",{"2":{"828":1}}],["dim或end",{"2":{"828":1}}],["dims",{"2":{"700":1,"1087":9}}],["dimension从原来的768变成了1024",{"2":{"1363":1}}],["dimension为字向量的维度",{"2":{"709":1}}],["dimensionality",{"2":{"692":1}}],["dimension是512",{"2":{"460":1}}],["dimension",{"2":{"460":2,"701":1,"709":1,"820":1,"827":1,"941":1,"960":1,"1087":1,"1344":1}}],["dimensions",{"2":{"326":2,"820":1,"1082":1,"1217":1}}],["dim进行concat",{"2":{"201":1}}],["dim等同于将每头的head",{"2":{"201":1}}],["dim是单个头的注意力维度",{"2":{"198":1}}],["dim=3",{"2":{"1217":1}}],["dim=0",{"2":{"1083":1}}],["dim=none",{"2":{"1083":3}}],["dim=4",{"2":{"201":1}}],["dim=args",{"2":{"201":1}}],["dim=d",{"2":{"119":1}}],["dim=1",{"2":{"83":1,"428":2,"472":2,"529":2,"826":1,"828":1,"833":1,"847":1,"1215":3,"1257":1,"1328":1,"1329":2,"1330":3}}],["dim=",{"2":{"67":1,"76":1,"199":1,"201":1,"394":1,"503":3,"1216":1,"1218":1,"1345":1}}],["dim",{"2":{"8":10,"83":1,"114":18,"198":1,"201":44,"326":3,"346":2,"395":2,"399":1,"420":4,"449":1,"503":24,"518":2,"522":1,"702":6,"808":3,"828":1,"933":1,"1078":1,"1083":2,"1086":6,"1087":171,"1216":6,"1217":17,"1218":6,"1345":14}}],["dmodeld",{"2":{"926":1}}],["dmodeldmodeld",{"2":{"7":1,"99":1,"316":1,"335":1,"419":1}}],["dmodel=512dmodel=512d",{"2":{"7":1}}],["d",{"2":{"5":1,"17":1,"23":13,"25":1,"28":7,"29":2,"30":4,"31":1,"34":2,"35":5,"36":28,"57":1,"67":2,"71":1,"83":1,"99":2,"101":4,"113":12,"119":4,"161":10,"173":5,"175":2,"186":2,"187":2,"189":5,"194":2,"197":1,"198":2,"199":9,"210":1,"217":1,"224":1,"315":1,"320":1,"341":4,"394":3,"399":5,"402":2,"419":2,"423":2,"424":1,"445":2,"448":2,"449":13,"461":2,"479":1,"485":1,"499":1,"510":1,"520":10,"530":14,"532":1,"533":1,"543":1,"564":2,"571":1,"579":1,"580":1,"582":6,"583":6,"585":1,"598":1,"614":2,"646":1,"699":2,"700":3,"701":8,"702":2,"703":13,"704":1,"761":1,"762":1,"768":1,"918":1,"924":1,"927":7,"933":2,"941":3,"944":10,"945":1,"957":4,"960":3,"963":1,"965":1,"986":1,"1082":1,"1216":38,"1218":51,"1336":5,"1343":10,"1344":10,"1443":1,"1547":1,"1548":1,"1551":1,"1590":2,"1594":6,"1629":5,"1663":5,"1683":5,"1698":3,"1710":2,"1825":2,"1843":2,"1849":2,"1853":3,"1857":4,"1861":4,"1879":1,"2006":2,"2027":2}}],["fgf",{"2":{"1342":1}}],["f​q",{"2":{"1343":1}}],["f​q​​",{"2":{"1342":1}}],["f​g​​",{"2":{"1342":1}}],["f​k​​",{"2":{"1342":2}}],["f​2​​",{"2":{"1003":2}}],["fkf",{"2":{"1342":1}}],["fk",{"2":{"1342":1}}],["fq",{"2":{"1342":1,"1343":1}}],["f1",{"0":{"1331":1},"2":{"1331":2}}],["fx图是一种中间表示形式",{"2":{"1290":1}}],["fx图是一种基于符号执行的图表示",{"2":{"1290":1}}],["fx图本身并不能直接运行",{"2":{"1290":1}}],["fx图",{"2":{"1290":1}}],["fx",{"0":{"1290":1},"2":{"1290":1}}],["fm",{"2":{"1902":6}}],["fmod",{"2":{"1087":4}}],["fmin",{"2":{"1087":1}}],["fmax",{"2":{"1087":1}}],["fweights",{"2":{"1087":1}}],["fw=pt",{"2":{"638":1}}],["f^",{"2":{"1003":2}}],["f2",{"2":{"1003":2,"1331":2}}],["fpga",{"2":{"1942":1}}],["fp",{"2":{"612":1}}],["ft+1",{"2":{"334":3}}],["fea",{"2":{"1957":1}}],["feature",{"2":{"321":1,"326":1,"337":1,"393":1,"395":6,"718":1,"770":1,"773":1,"1455":1}}],["features",{"0":{"2074":1},"1":{"2075":1,"2076":1,"2077":1,"2078":1,"2079":1},"2":{"318":2,"323":2,"326":4,"341":2,"343":4,"1211":1}}],["fetchcontent",{"0":{"1990":1},"2":{"1990":3}}],["fetching",{"2":{"1566":1}}],["fetched",{"2":{"1566":1}}],["fetch",{"2":{"1566":1}}],["few",{"2":{"985":1,"1316":1}}],["fenster",{"2":{"370":1}}],["feedforward",{"0":{"1387":1,"1451":1},"1":{"1388":1,"1389":1},"2":{"114":1,"201":1,"330":2,"449":1,"1216":3,"1218":4,"1404":1,"1438":1,"1457":2,"1464":2}}],["feed",{"2":{"38":2,"39":2,"82":2,"97":1,"101":1,"126":1,"127":1,"156":2,"201":2,"306":1,"330":2,"344":7,"419":1,"445":1,"449":1,"466":1,"517":1,"523":7,"529":1,"533":6,"1216":3,"1217":2,"1218":6,"1273":1}}],["f又分别转换为k和v",{"2":{"265":1}}],["f是从",{"2":{"263":1}}],["fc3",{"2":{"2086":2}}],["fc2",{"2":{"1215":4,"1216":2,"1218":2,"1257":2,"1295":2,"2086":2}}],["fc1",{"2":{"1215":4,"1216":2,"1218":2,"1257":2,"1295":2,"2086":2}}],["fc",{"2":{"145":2,"1216":2,"1217":2,"1218":4}}],["fc𝑊𝑓𝑐",{"2":{"126":1}}],["fft",{"2":{"1087":1}}],["ff=2048",{"2":{"448":1,"449":1,"703":1}}],["ff2ff2ff",{"2":{"148":3,"485":5}}],["ff1ff1ff",{"2":{"148":1,"485":6}}],["fff",{"2":{"125":1}}],["ff",{"2":{"113":6,"449":5,"461":2,"703":4,"1216":11,"1218":16}}],["ffn模块",{"2":{"533":1}}],["ffn进行运算",{"2":{"523":1}}],["ffn输出是一个更具抽象性",{"2":{"517":1}}],["ffn和esnet",{"2":{"510":1}}],["ffn可以用标号5的公式来表示",{"2":{"498":1}}],["ffn可以类比为一种键值对存储结构",{"2":{"118":1}}],["ffn则是独立应用于句子中每个位置的单词",{"2":{"498":1}}],["ffn则允许模型利用注意力机制生成的上下文信息",{"2":{"101":1}}],["ffn块的dla和",{"2":{"479":1}}],["ffn之后",{"2":{"470":1}}],["ffn对序列中每个元素都独立计算",{"2":{"466":1}}],["ffn对输入序列的每个位置执行相同的操作",{"2":{"417":1}}],["ffn不单是激活一个key及其value",{"2":{"129":1}}],["ffn中两个全连接层之间会施加dropout",{"2":{"394":1}}],["ffn中对于某个知识也是分布存储的",{"2":{"129":1}}],["ffn中的激活函数是一个主要的能提供非线性变换的单元",{"2":{"117":1}}],["ffn是convection的过程",{"2":{"498":1}}],["ffn是transformer模型中的无名英雄",{"2":{"120":1}}],["ffn是一个包含两个线性变换和一个激活函数的简单网络",{"2":{"97":1}}],["ffn这种存储是分布式的",{"2":{"118":1}}],["ffn并非简单的直接在输入维度这个嵌入空间上进行建模",{"2":{"116":1}}],["ffn在把输入的词向量映射到输出的词向量的过程中",{"2":{"116":1}}],["ffn从2个权重矩阵变成3个权重矩阵",{"2":{"109":1}}],["ffn等价于kernel",{"2":{"101":1}}],["ffn所做的是在注意力层进行元素间的信息交换之后",{"2":{"101":1}}],["ffn层的实例化对象",{"2":{"523":1}}],["ffn层的输入矩阵为",{"2":{"419":1}}],["ffn层由两个线性变换组成",{"2":{"466":1}}],["ffn层将自注意力表示投影到更高的维度",{"2":{"466":1}}],["ffn层",{"0":{"466":1},"2":{"461":1}}],["ffn层和dropout",{"2":{"449":1}}],["ffn层对输入矩阵每行",{"2":{"101":1}}],["ffn层是一个两层的全连接层",{"2":{"99":1}}],["ffn通常设置中间比率为4",{"2":{"100":1}}],["ffn的革新",{"0":{"622":1}}],["ffn的并行化",{"2":{"417":1}}],["ffn的第一层可以认为是key",{"2":{"126":1}}],["ffn的采用",{"2":{"125":1}}],["ffn的本质就是一个position",{"2":{"101":1}}],["ffn的中间比率是指中间层维数与隐含层维数之间的比值",{"2":{"100":1}}],["ffn的权重体现在这两个线性层上",{"2":{"99":1}}],["ffn的作用",{"0":{"115":1},"1":{"116":1,"117":1,"118":1,"119":1,"120":1},"2":{"96":1}}],["ffn最终得到的输出矩阵维度与输入x的维度一致",{"2":{"99":1}}],["ffn",{"0":{"96":1,"143":1,"419":1},"1":{"97":1,"98":1,"99":1,"100":1,"101":1,"102":1,"103":1,"104":1,"105":1,"106":1,"107":1,"108":1,"109":1,"110":1,"111":1,"112":1,"113":1,"114":1,"115":1,"116":1,"117":1,"118":1,"119":1,"120":1,"121":1,"122":1,"123":1,"124":1,"125":1,"126":1,"127":1,"128":1,"129":1,"130":1,"131":1,"132":1,"133":1,"134":1,"135":1,"136":1,"137":1,"138":1,"139":1,"140":1,"141":1,"142":1,"143":1,"144":1,"145":1,"146":1,"147":1,"148":1,"149":1,"150":1,"151":1,"152":1,"153":1,"154":1,"155":1,"156":1},"2":{"17":1,"96":2,"98":7,"99":2,"100":1,"103":2,"113":2,"114":3,"115":1,"117":2,"118":1,"119":2,"120":1,"126":2,"144":3,"154":1,"201":4,"334":1,"394":2,"448":1,"498":1,"510":1,"517":1,"520":1,"525":1,"530":2,"620":1}}],["ffn需要的输入是一个矩阵而不是多个矩阵",{"2":{"10":1}}],["fn是否为空",{"2":{"1110":1}}],["fn=collate",{"2":{"375":2}}],["fn=",{"2":{"315":3}}],["fn",{"2":{"110":2,"375":4,"384":1,"399":1,"503":1,"558":1,"661":2,"1039":1,"1082":2,"1096":2,"1104":2,"1107":2,"1110":2,"1111":1,"1113":1,"1114":8,"1117":2,"1212":2,"1213":2,"1214":6,"1223":2,"1231":2}}],["fnn",{"2":{"101":1}}],["f",{"2":{"109":1,"114":1,"134":2,"201":1,"270":3,"300":1,"301":11,"302":2,"304":1,"329":2,"332":3,"334":7,"422":1,"423":3,"498":1,"503":1,"543":1,"557":2,"571":1,"591":21,"592":1,"612":1,"621":1,"702":1,"933":1,"943":13,"961":13,"1000":2,"1003":2,"1093":2,"1096":4,"1101":3,"1115":1,"1180":3,"1205":1,"1211":2,"1215":13,"1216":3,"1217":1,"1218":4,"1243":1,"1255":1,"1257":6,"1259":1,"1263":1,"1340":1,"1342":3,"1345":1,"1398":2,"1440":1,"1442":3,"1532":1,"1539":1,"1566":6,"1607":1,"1611":5,"1651":5,"1917":2,"2086":1}}],["flying",{"2":{"1654":1,"1659":1}}],["fly",{"2":{"1654":2,"1659":2}}],["flyer",{"2":{"387":1}}],["flush",{"2":{"1282":1,"2070":1}}],["flush=true",{"2":{"423":3}}],["flipud",{"2":{"1087":1}}],["fliplr",{"2":{"1087":1}}],["flip",{"2":{"1083":1,"1087":2,"2063":3}}],["flitter",{"2":{"292":1}}],["flag",{"2":{"1817":3,"1835":3}}],["flatten",{"0":{"824":1,"828":1},"1":{"825":1,"826":1,"827":1,"828":1},"2":{"828":2,"1087":4,"1215":2,"1217":1,"1257":1,"1345":2,"1350":2}}],["flat",{"2":{"700":2}}],["flase",{"2":{"660":1}}],["flashattention3",{"0":{"973":1}}],["flashattention在节省内存方面可以达到10",{"2":{"964":1}}],["flashattention避免了需要存储大型中间值",{"2":{"964":1}}],["flashattention1",{"0":{"963":1,"964":1,"967":1},"2":{"958":1,"964":1,"969":1}}],["flashattention2",{"2":{"947":1,"968":1,"969":1}}],["flashattention不需要将大型的𝑁×𝑁注意力矩阵读取和写入hbm",{"2":{"940":1,"962":1}}],["flashattention获得了加速",{"2":{"940":1,"962":1}}],["flashattention通过q矩阵的块循环",{"2":{"940":1,"962":1}}],["flashattention通过k和v矩阵的块循环",{"2":{"940":1,"962":1}}],["flashattention使用切片技术",{"2":{"940":1,"962":1}}],["flashattention",{"0":{"939":1,"958":1,"968":1},"1":{"940":1,"941":1,"942":1,"943":1,"944":1,"945":1,"946":1,"959":1,"960":1,"961":1,"962":1,"963":1,"964":1,"965":1,"966":1,"967":1,"969":1,"970":1,"971":1,"972":1},"2":{"90":1,"95":1,"941":1,"944":1,"946":1,"947":1,"960":1,"963":2,"973":1}}],["flash",{"0":{"942":1,"945":1,"947":1,"962":1,"965":1},"2":{"90":1,"206":1,"945":2,"965":2,"974":2}}],["flan",{"2":{"569":1}}],["flavius",{"2":{"292":1}}],["floor",{"2":{"1087":4,"1330":1}}],["floordiv",{"2":{"1083":1,"1085":1}}],["flow",{"2":{"499":1}}],["flops",{"2":{"217":1,"561":2,"970":1}}],["flop",{"2":{"154":1}}],["float>",{"2":{"1908":1}}],["floatvalue",{"2":{"1728":5}}],["float等",{"2":{"1576":1}}],["floating",{"2":{"1087":1,"1155":1,"1607":1}}],["float16",{"2":{"1075":1}}],["floattensor",{"2":{"399":1,"723":1,"834":2,"1330":1,"1350":3}}],["float32",{"2":{"346":2,"503":5,"1086":1,"1102":1,"1202":1,"1205":1,"1216":1,"1218":1}}],["float32数据类型",{"2":{"346":1}}],["float",{"2":{"84":4,"114":1,"201":3,"346":2,"399":1,"503":4,"592":1,"700":2,"702":4,"1085":3,"1087":35,"1102":2,"1205":1,"1214":2,"1216":1,"1217":1,"1218":1,"1227":4,"1328":1,"1345":6,"1436":1,"1607":6,"1611":4,"1728":6,"1729":1,"1908":2}}],["flexible",{"2":{"2073":1}}],["flexibility",{"2":{"95":1,"2079":1,"2081":1,"2087":1}}],["flexattention",{"2":{"95":1}}],["favored",{"2":{"2087":1}}],["far",{"2":{"1329":2,"1330":1}}],["faq",{"2":{"1254":1}}],["fake",{"2":{"1250":1,"1295":1,"1308":1}}],["failure",{"2":{"1922":1}}],["failed",{"2":{"1683":1,"1761":1,"1763":1}}],["fail",{"2":{"1069":1,"1814":2,"1832":2}}],["facebook",{"2":{"2073":1,"2082":1}}],["faces",{"2":{"1254":2}}],["facelandmarksdataset",{"2":{"1254":1}}],["face",{"2":{"513":1,"1254":1}}],["fact",{"2":{"428":1}}],["factual",{"2":{"122":2,"145":1,"156":4,"437":1}}],["factorial",{"2":{"1621":3,"1646":3}}],["factorized",{"2":{"698":1}}],["factor=0",{"2":{"1237":1,"1238":1,"1246":1,"1247":1}}],["factor=1",{"2":{"83":1,"402":1,"423":1,"424":1}}],["factory",{"2":{"702":2}}],["factor",{"2":{"114":1,"298":1,"402":2,"621":1,"1303":1}}],["fan",{"2":{"449":1,"1006":2}}],["fallen",{"2":{"167":3,"259":2}}],["false是需要mask掉的位置",{"2":{"66":1}}],["false",{"2":{"62":1,"66":2,"74":35,"135":4,"372":1,"382":27,"383":2,"394":1,"395":4,"422":1,"450":26,"661":1,"700":1,"702":2,"1086":5,"1087":102,"1095":2,"1104":1,"1106":1,"1107":1,"1114":1,"1116":2,"1117":2,"1119":1,"1122":1,"1208":4,"1211":1,"1214":8,"1227":5,"1283":1,"1331":1,"1607":1,"1619":1,"1620":1,"1630":1,"1674":1,"1729":1,"1817":1,"1835":1,"1927":1}}],["fastest",{"2":{"1308":1}}],["faster",{"0":{"968":1},"1":{"969":1,"970":1,"971":1,"972":1},"2":{"233":1,"947":1,"2077":1}}],["fastai",{"2":{"1242":1}}],["fasttext",{"2":{"709":1}}],["fast首先对输入的动作进行归一化",{"2":{"637":1}}],["fast使用了一种基于离散余弦变换",{"2":{"637":1}}],["fastgeluactivation",{"2":{"110":1}}],["fast",{"0":{"973":1},"2":{"90":1,"110":1,"156":1,"210":1,"233":1,"429":1,"637":1,"638":1,"935":1,"951":1,"1242":1,"1312":1,"2073":1}}],["fused",{"2":{"1228":5}}],["fuse",{"2":{"944":1,"963":1}}],["funds",{"2":{"1874":1}}],["fun",{"0":{"1111":1},"2":{"1274":1,"1440":2,"1566":1}}],["funcptr",{"2":{"1706":4}}],["func",{"2":{"1083":1,"1227":1,"1649":4,"1710":2,"2004":5,"2006":2,"2007":3,"2008":4}}],["functiononstack",{"2":{"1648":2}}],["functional>",{"2":{"1883":1}}],["functional",{"2":{"1096":1,"1215":1,"1216":1,"1217":1,"1218":1,"1257":1,"1295":1}}],["function的值在其最小值附近震荡",{"2":{"991":1}}],["function相对于权值参数的梯度值很大",{"2":{"991":1}}],["function",{"0":{"644":1,"1646":1},"2":{"156":1,"173":3,"270":1,"271":3,"302":1,"394":1,"398":1,"402":1,"428":1,"472":1,"513":1,"700":1,"770":1,"838":1,"840":1,"1083":12,"1096":2,"1100":1,"1107":1,"1114":1,"1205":2,"1208":1,"1212":1,"1213":1,"1215":1,"1227":2,"1257":1,"1459":1,"1460":1,"1769":1,"1909":3,"2086":1}}],["functions",{"0":{"817":1,"1645":1},"2":{"108":1,"156":1,"845":1,"1084":1,"1254":1,"1404":1}}],["fune",{"2":{"733":1}}],["fu",{"2":{"429":1,"513":1}}],["fullconnect",{"2":{"1205":3}}],["fully",{"2":{"151":1,"543":1}}],["full",{"2":{"76":1,"84":1,"201":1,"503":2,"513":1,"832":2,"1086":1,"1087":1,"1205":7,"1208":3,"1214":6,"1330":1,"2086":1}}],["future",{"2":{"74":1,"79":1,"380":1,"625":1,"1215":1,"1257":1,"1894":1}}],["five",{"2":{"1821":1,"1839":1}}],["fifo",{"2":{"1723":1}}],["fibnacci",{"2":{"1651":1}}],["fibonacci",{"2":{"1646":4}}],["fid值",{"2":{"1361":1}}],["fid首先用",{"2":{"1361":1}}],["fidelity",{"2":{"1361":1}}],["fid可以衡量生成图像的逼真度",{"2":{"1361":1}}],["fid",{"0":{"1361":1},"2":{"1361":5}}],["fix协议开发经验",{"2":{"1947":1}}],["fix",{"2":{"1087":2}}],["fixed",{"2":{"760":1,"768":1,"1254":1,"1608":2,"1817":2,"1835":2}}],["fixedlengthpacking",{"2":{"90":4}}],["fishing",{"2":{"562":1}}],["filtered",{"2":{"1883":4}}],["filtering>",{"2":{"1482":2}}],["filtering>false",{"2":{"1482":2}}],["filtering",{"2":{"364":1}}],["filepath",{"2":{"1930":5,"1933":5}}],["filemanager",{"2":{"1902":6}}],["fileopenerror",{"2":{"1763":4}}],["filename",{"2":{"1543":2,"1557":4,"1761":5,"1763":9,"1902":6}}],["file2",{"2":{"1510":2,"1556":1,"1917":1}}],["file1",{"2":{"1510":5,"1556":1,"1917":1}}],["file=",{"2":{"1254":1}}],["filesystem>",{"2":{"1930":2,"1933":1}}],["filesystem",{"2":{"1930":4,"1933":2,"1991":2}}],["filestream",{"2":{"1902":16}}],["filesize",{"2":{"1821":3,"1839":3}}],["files",{"2":{"591":1,"1930":1,"1987":1}}],["file",{"0":{"1930":1},"2":{"372":1,"423":6,"557":4,"571":1,"591":16,"723":1,"1227":1,"1250":2,"1513":4,"1515":3,"1516":2,"1518":1,"1532":2,"1761":5,"1763":2,"1814":1,"1820":1,"1821":2,"1825":2,"1832":1,"1838":1,"1839":2,"1843":2,"1902":3,"1920":1,"1930":15,"1933":6}}],["fill",{"0":{"1744":1},"2":{"67":1,"76":1,"79":1,"83":1,"84":2,"198":1,"199":2,"394":1,"399":2,"428":1,"472":1,"503":1,"529":1,"933":1,"1087":18,"1216":1,"1217":1,"1218":1,"1732":1,"1744":2,"1758":1}}],["fields",{"2":{"2081":1}}],["field",{"0":{"1319":1},"1":{"1320":1,"1321":1,"1322":1,"1323":1,"1324":1,"1325":1,"1326":1},"2":{"274":1,"495":1,"499":1,"2087":1}}],["first=special",{"2":{"557":1}}],["first为true",{"2":{"557":1}}],["first",{"2":{"235":1,"557":4,"591":2,"592":1,"731":1,"977":1,"1096":1,"1101":1,"1284":1,"1328":1,"1329":2,"1330":1,"1556":1,"1713":4,"1714":1,"1756":2,"1805":1,"1807":1,"1921":1,"1925":1,"1929":1}}],["figure",{"2":{"82":1,"344":1,"523":1,"529":1,"533":1,"976":1,"1147":1,"1177":1,"1178":1,"1179":1,"1181":1,"1182":4,"1183":1,"1184":1}}],["finished",{"2":{"1092":1}}],["findcontentchildren",{"2":{"2153":1,"2154":1}}],["findstatic",{"2":{"2070":1}}],["findbashfind",{"2":{"1532":1}}],["find",{"0":{"1754":1,"1977":1,"1991":1},"2":{"591":1,"592":2,"1330":1,"1532":1,"1713":5,"1719":1,"1720":1,"1721":1,"1722":1,"1724":2,"1725":2,"1732":1,"1754":2,"1758":2,"1806":1,"1807":5,"1922":1,"2075":1}}],["finding",{"2":{"477":1,"513":1}}],["finetune",{"2":{"333":4}}],["finetuning才能激发最佳性能",{"2":{"542":1}}],["finetuning",{"2":{"233":1,"938":1,"954":1}}],["fine",{"2":{"139":1,"393":1,"429":1,"718":1,"726":5,"768":1,"938":1,"954":1,"1312":2}}],["finfo",{"2":{"76":1}}],["finalize",{"2":{"1575":1,"1590":1,"1594":1}}],["finally",{"2":{"1217":1}}],["final",{"0":{"1656":1},"2":{"36":1,"141":1,"372":2,"1216":1,"1218":1,"1652":1,"1656":6}}],["fréchet",{"2":{"1361":1}}],["front",{"2":{"1713":2,"1720":3,"1721":4,"1722":4,"1723":2,"1799":1,"1800":1,"1801":1}}],["frostig",{"2":{"1194":1}}],["fro",{"2":{"1083":1}}],["from",{"2":{"5":1,"36":1,"76":1,"167":1,"259":1,"420":1,"422":1,"428":1,"429":2,"449":2,"487":1,"498":1,"513":1,"543":1,"552":1,"557":13,"591":2,"688":1,"716":1,"718":1,"719":1,"723":1,"768":1,"820":1,"834":1,"1072":9,"1078":1,"1083":1,"1086":1,"1087":2,"1214":1,"1215":3,"1239":1,"1253":4,"1254":5,"1278":3,"1302":1,"1312":1,"1329":1,"1330":2,"1481":1,"1485":3,"1488":1,"1489":2,"1594":2,"1715":2,"1797":2,"1849":1,"2073":1}}],["friday",{"2":{"1728":3}}],["friendliness",{"2":{"2087":1}}],["friendly",{"2":{"567":1}}],["friendfunction",{"2":{"1784":4}}],["friend",{"0":{"1770":1},"2":{"1712":1,"1768":1,"1769":3,"1770":3,"1772":3,"1774":2,"1778":1,"1779":1,"1784":2,"1788":1,"1789":2,"1793":1}}],["friston",{"2":{"363":1}}],["frustratingly",{"2":{"288":1,"292":2}}],["frexp",{"2":{"1087":2}}],["freq=min",{"2":{"557":1}}],["freq=2",{"2":{"557":2}}],["frequencies",{"2":{"1933":1}}],["frequency",{"2":{"557":2,"1308":1}}],["frequent",{"2":{"557":1}}],["freq",{"2":{"557":7,"702":6,"1308":1}}],["freqs",{"2":{"201":11,"1345":17}}],["freien",{"2":{"370":1,"557":1}}],["fred",{"2":{"156":3}}],["french",{"2":{"130":4}}],["freeport",{"2":{"1306":1,"1307":2}}],["free",{"2":{"90":1,"233":1,"361":1,"406":2,"497":1,"513":1,"737":1,"740":2,"895":1,"1668":1,"1947":1,"2073":1}}],["frasincar",{"2":{"292":1}}],["framework进行增强",{"2":{"975":1}}],["framework",{"0":{"2086":1},"2":{"233":1,"429":1,"513":2,"1501":1,"2070":1,"2087":1}}],["franch",{"2":{"130":1}}],["france",{"2":{"122":1,"130":5}}],["frac",{"2":{"54":1,"57":1,"106":2,"108":1,"109":1,"148":1,"175":2,"178":1,"183":1,"186":1,"191":8,"192":1,"194":2,"270":1,"334":3,"343":1,"394":1,"485":1,"510":1,"640":1,"646":1,"692":1,"765":4,"839":4,"844":2,"845":2,"847":1,"899":1,"908":2,"918":1,"924":1,"943":2,"944":4,"961":2,"999":2,"1000":3,"1003":6,"1004":2,"1006":2,"1007":5,"1016":2,"1087":2,"1184":1,"1191":1,"1192":2,"1193":2,"1240":1,"1243":2,"1244":2,"1336":2,"1377":1,"1388":2,"1389":4,"1392":5,"1393":11,"1394":19,"1395":1,"1442":1,"2018":4}}],["folders",{"2":{"1303":2,"1306":1,"1307":2}}],["folder",{"2":{"1250":1,"1303":2,"1306":1,"1307":2}}],["follow",{"2":{"82":1,"344":1,"523":1,"529":1,"533":1}}],["four",{"2":{"513":1}}],["found",{"2":{"8":1,"1713":2,"1719":2,"1720":2,"1721":2,"1722":2,"1724":2,"1725":1,"1754":2,"1755":4,"1806":3,"1807":1,"1922":2,"1933":1}}],["foo",{"2":{"1489":2}}],["foo2",{"2":{"1299":2}}],["football",{"2":{"405":2}}],["food",{"2":{"370":1}}],["focused",{"2":{"156":1}}],["fortran",{"2":{"1589":1}}],["forth",{"2":{"1284":1}}],["foreach",{"2":{"1228":8}}],["force",{"2":{"1087":1}}],["forcing进行详细分析",{"2":{"528":1}}],["forcing机制",{"2":{"525":1}}],["forcing机制保证了",{"2":{"406":1}}],["forcing和掩码",{"2":{"412":1}}],["forcing开始",{"2":{"411":1}}],["forcing也存在一定的问题",{"2":{"411":1}}],["forcing的优势是因为模型是在",{"2":{"411":1}}],["forcing的实现相对简单",{"2":{"410":1}}],["forcing本时刻的输入是上一时刻的真值标签",{"2":{"407":1}}],["forcing模型下",{"2":{"416":1}}],["forcing模型在训练中纠正错误",{"2":{"407":1}}],["forcing模式下",{"2":{"528":1}}],["forcing模式的全靠真值均不可取",{"2":{"411":1}}],["forcing模式训练出来的模型在训练环节和预测环节存在行为差异",{"2":{"411":1}}],["forcing模式中",{"2":{"407":1}}],["forcing模式",{"2":{"81":1,"382":1,"528":1}}],["forcing就是有老师带着做训练",{"2":{"406":1}}],["forcing就是每次推理给解码器输入时",{"2":{"406":1}}],["forcing结合掩码来满足这个需求",{"2":{"390":1}}],["forcing",{"0":{"404":1,"896":1},"1":{"405":1,"406":1,"407":1,"408":1,"409":1,"410":1,"411":1,"412":1},"2":{"57":1,"70":1,"404":1,"406":2,"409":1,"543":2,"896":2}}],["forall",{"2":{"999":2,"1000":2}}],["forget",{"2":{"865":1}}],["forgetting",{"2":{"230":1}}],["format=torch",{"2":{"1087":2}}],["formation",{"2":{"490":1}}],["format",{"2":{"395":1,"1083":3,"1086":1,"1087":12,"1215":2,"1295":1,"1398":1}}],["form",{"2":{"156":1}}],["foring示例",{"2":{"409":1}}],["foring是在训练过程中去掉了每次推理的序贯关系",{"2":{"408":1}}],["foring",{"2":{"57":1}}],["for循环一个一个计算头",{"2":{"32":1}}],["for",{"0":{"1379":1,"1621":1,"1739":1,"1898":1},"2":{"8":1,"36":1,"47":1,"76":1,"82":1,"83":3,"89":1,"90":1,"95":1,"108":1,"119":1,"122":1,"134":1,"141":1,"143":1,"150":1,"156":6,"181":2,"185":1,"201":4,"209":2,"211":1,"233":8,"237":1,"282":1,"287":1,"288":1,"292":3,"294":1,"343":2,"344":1,"364":2,"380":1,"381":1,"383":1,"384":2,"385":2,"395":2,"396":1,"399":3,"402":1,"423":4,"424":1,"428":2,"429":4,"446":1,"449":1,"450":1,"472":1,"513":4,"522":2,"523":1,"529":3,"532":1,"533":1,"542":1,"543":3,"557":4,"558":1,"562":1,"571":2,"572":3,"575":1,"587":1,"590":1,"591":14,"592":4,"637":1,"638":1,"688":1,"700":2,"703":1,"735":1,"736":1,"737":1,"740":6,"747":1,"751":1,"768":2,"834":1,"944":4,"945":1,"965":1,"982":1,"983":1,"1046":1,"1059":1,"1083":1,"1086":2,"1096":4,"1098":3,"1100":1,"1102":1,"1114":1,"1202":1,"1205":2,"1211":1,"1214":2,"1215":6,"1216":2,"1217":2,"1218":5,"1226":1,"1228":5,"1231":5,"1233":1,"1234":1,"1235":2,"1236":2,"1237":2,"1238":2,"1239":1,"1240":2,"1241":3,"1242":2,"1243":1,"1244":3,"1245":1,"1246":2,"1247":2,"1251":1,"1254":4,"1273":2,"1279":1,"1284":2,"1295":2,"1299":1,"1303":3,"1304":1,"1305":1,"1308":4,"1312":1,"1328":2,"1329":6,"1330":5,"1331":1,"1398":3,"1436":1,"1440":1,"1481":1,"1566":3,"1590":1,"1594":1,"1621":9,"1623":2,"1625":2,"1631":2,"1634":1,"1645":3,"1647":2,"1665":8,"1667":2,"1668":2,"1670":1,"1671":1,"1684":1,"1691":2,"1696":1,"1706":2,"1708":1,"1710":2,"1713":5,"1714":2,"1718":1,"1719":5,"1720":3,"1721":3,"1722":3,"1724":2,"1725":3,"1729":1,"1732":1,"1739":3,"1741":1,"1742":1,"1743":1,"1744":1,"1747":1,"1749":1,"1750":1,"1751":1,"1752":1,"1758":1,"1797":5,"1799":1,"1800":1,"1801":1,"1802":1,"1806":1,"1807":1,"1825":1,"1843":1,"1883":4,"1887":1,"1891":2,"1895":1,"1897":1,"1898":1,"1914":1,"1925":1,"1930":1,"1933":2,"2061":1,"2070":1,"2073":3,"2079":4,"2082":1,"2086":3,"2087":1}}],["forward函数",{"2":{"1206":1}}],["forward并没有",{"2":{"668":1}}],["forward和add",{"2":{"449":1}}],["forward层",{"2":{"344":1}}],["forward→addnorm",{"2":{"344":1}}],["forward→addnorm→attention→add→norm→feed",{"2":{"344":1}}],["forward→add",{"2":{"330":2}}],["forward",{"0":{"36":1,"963":1,"970":1,"1721":1,"1801":1},"2":{"0":1,"36":1,"38":5,"39":4,"82":4,"83":1,"97":1,"101":1,"110":1,"113":1,"114":1,"126":1,"127":1,"156":2,"201":5,"306":1,"330":2,"343":3,"344":12,"346":2,"381":1,"385":1,"394":3,"399":2,"410":1,"419":1,"423":1,"428":1,"445":1,"450":1,"451":2,"466":1,"503":1,"517":1,"522":1,"523":8,"528":1,"529":3,"532":1,"533":8,"538":12,"558":1,"701":1,"702":1,"703":1,"723":1,"963":1,"1100":1,"1105":1,"1205":2,"1208":7,"1211":1,"1212":2,"1213":1,"1214":8,"1215":2,"1216":7,"1217":12,"1218":11,"1257":1,"1295":1,"1345":1,"1438":1,"1713":1,"1721":19,"1795":1,"1801":4,"1821":1,"1839":1,"2086":2}}],["fstream对象",{"2":{"1902":1}}],["fstream",{"2":{"1819":2,"1825":1,"1826":1,"1837":2,"1843":1,"1844":1}}],["fstream>",{"2":{"1761":1,"1820":2,"1821":1,"1825":1,"1838":2,"1839":1,"1843":1,"1902":4,"1930":1,"1933":1}}],["fs",{"2":{"8":1,"201":1,"1930":28,"1933":3}}],["tn>",{"2":{"1699":1,"1700":1}}],["t2",{"2":{"1699":1,"1700":1}}],["t1",{"2":{"1699":1,"1700":1,"2006":2}}],["t30",{"2":{"1616":1}}],["tjohn",{"2":{"1616":1}}],["tsetselectuserbyid",{"2":{"1485":1}}],["tsl指令",{"0":{"1422":1}}],["ts指令",{"0":{"1422":1}}],["tsfm",{"2":{"1254":2}}],["tqdm",{"2":{"1331":1}}],["t02t",{"2":{"1323":1}}],["t01t",{"2":{"1323":1}}],["t00t",{"2":{"1323":1}}],["tb",{"2":{"1315":1}}],["tbt",{"2":{"977":1}}],["tlm",{"2":{"1315":2}}],["tloss",{"2":{"1215":1}}],["txtcalculator",{"2":{"1997":1}}],["txt",{"0":{"1966":1,"1980":1,"1981":1},"2":{"1301":1,"1510":8,"1513":4,"1515":3,"1516":2,"1518":1,"1532":2,"1761":1,"1763":1,"1820":3,"1821":2,"1825":7,"1838":3,"1839":2,"1843":7,"1902":2,"1930":2,"1933":2,"1965":1,"1981":1,"1987":1,"1993":3,"1994":1,"1997":3,"1999":7}}],["tgi",{"2":{"980":1}}],["tgt里面是单词的索引",{"2":{"538":1}}],["tgt是",{"2":{"528":1}}],["tgt大于max",{"2":{"384":1}}],["tgt可以为空",{"2":{"381":1}}],["tgt和tgt",{"2":{"381":1}}],["tgt就应该是",{"2":{"380":1}}],["tgt需要斜着进行mask",{"2":{"79":1}}],["tgt=none",{"2":{"66":1,"380":1}}],["tgt加入到tgt",{"2":{"65":1}}],["tgt",{"2":{"34":2,"39":2,"65":8,"74":10,"76":7,"77":1,"79":9,"82":11,"83":2,"84":10,"198":1,"343":2,"364":2,"371":1,"372":2,"374":4,"375":2,"380":21,"381":4,"382":1,"383":2,"384":16,"385":6,"398":2,"399":3,"410":3,"422":5,"423":5,"448":1,"449":6,"450":16,"451":4,"503":7,"528":5,"529":9,"532":3,"533":4,"538":8,"557":4,"558":9,"703":17,"1218":23}}],["ttft",{"2":{"977":3}}],["ttt",{"2":{"273":1,"1283":1}}],["tpu",{"2":{"1132":1}}],["tp和cp的组合可以通过消除重新计算开销",{"2":{"976":1}}],["tp",{"2":{"976":2,"977":1}}],["tp2cp2",{"2":{"976":1}}],["t​02​​",{"2":{"1323":1}}],["t​01​​",{"2":{"1323":1}}],["t​00​​",{"2":{"1323":1}}],["t​c​​",{"2":{"944":1}}],["t​c​​=​b​c​​​​n​​",{"2":{"944":1}}],["t​r​​",{"2":{"944":4}}],["t​r​​=​b​r​​​​n​​",{"2":{"944":1}}],["tcp",{"2":{"1306":1,"1307":2,"1589":1}}],["tcurtiπ",{"2":{"1244":1}}],["tcurtmaxπ",{"2":{"1243":1}}],["tct",{"2":{"944":1}}],["tc=nbct",{"2":{"944":1}}],["tc0tc0tc^0",{"2":{"480":1}}],["tmp",{"2":{"1284":1,"1332":1,"1506":1}}],["tma",{"2":{"973":1}}],["tm则需要通过",{"2":{"908":1}}],["tm",{"2":{"908":1}}],["t代表decoder有多少步",{"2":{"899":1}}],["t=1",{"2":{"855":1}}],["t^tw",{"2":{"766":1}}],["tk",{"2":{"766":1}}],["t−s",{"2":{"763":1}}],["t−1",{"2":{"240":1,"510":2}}],["t的相对位置编码",{"2":{"760":1}}],["t为锚点",{"2":{"759":1,"760":1}}],["tflops",{"2":{"968":2}}],["tf",{"2":{"676":1,"700":7,"1161":2}}],["t|",{"2":{"571":1}}],["t5模型出自文章",{"2":{"1340":1}}],["t5直接将绝对位置公式的后三项换成一个可学习的bias",{"2":{"762":1}}],["t5去除了位置信息和语义信息的交互",{"2":{"762":1}}],["t5采用了一种简单的相对位置编码方案",{"2":{"762":1}}],["t5",{"0":{"762":1,"1333":1,"1340":1},"2":{"569":2,"741":1,"765":1,"768":1,"1312":1,"1317":4,"1332":1,"1333":2,"1340":1,"1404":1}}],["t5layernorm",{"2":{"346":1}}],["t时刻的输入必然依赖t",{"2":{"426":1}}],["t>",{"2":{"395":2,"1698":1,"1699":1,"1700":1,"1701":1,"1726":2,"1908":4,"1925":5}}],["t也还要加个normalization",{"2":{"334":1}}],["t方差将会很大",{"2":{"334":1}}],["t拼接",{"2":{"285":1}}],["t和xt+1xt+1x",{"2":{"334":1}}],["t和hthth",{"2":{"285":1}}],["t和ht−1ht−1h",{"2":{"285":1}}],["t和之前的输入xt−1xt−1x",{"2":{"240":1}}],["t来计算输出ytyty",{"2":{"285":1}}],["t之后",{"2":{"240":1}}],["t就是隐变量",{"2":{"240":1}}],["t从未被观测到",{"2":{"240":1}}],["tweet",{"2":{"1690":1,"1691":1}}],["twitter",{"2":{"513":1,"768":1}}],["tw",{"2":{"230":1}}],["two",{"0":{"635":1,"1102":1},"2":{"101":1,"370":1,"429":1,"557":2,"591":1,"1082":1,"1217":1,"1233":1}}],["tϕ",{"2":{"210":2}}],["tidb团队",{"2":{"1954":1}}],["tip",{"2":{"1414":1,"1605":1,"1680":1,"2019":1}}],["title",{"2":{"1195":1}}],["titans可能就是这样一个自我验证纠错机制的雏形",{"2":{"231":1}}],["titans架构",{"0":{"229":1},"2":{"157":1}}],["titans",{"0":{"226":1},"1":{"227":1,"228":1,"229":1,"230":1,"231":1},"2":{"157":1,"226":2,"231":4,"233":2}}],["tiny",{"2":{"1098":1,"1308":1}}],["tiktoken",{"2":{"571":5,"572":4,"573":1}}],["tiktokenizer",{"2":{"569":1}}],["tile",{"2":{"1087":2}}],["tilde",{"2":{"285":3,"1014":1}}],["tiling",{"2":{"180":1}}],["timed",{"2":{"1913":1}}],["time时候",{"2":{"668":1}}],["time2",{"2":{"504":1}}],["time",{"2":{"59":1,"226":1,"231":1,"233":1,"273":1,"385":6,"402":1,"504":6,"768":1,"934":1,"977":2,"1096":2,"1284":2,"1304":1,"2053":1,"2086":2}}],["timesteps",{"2":{"1350":5}}],["timestep",{"2":{"1328":2,"1329":2,"1330":1,"1350":1}}],["times所有",{"2":{"325":1}}],["times+1",{"2":{"83":1}}],["times",{"2":{"25":1,"26":2,"28":3,"83":3,"99":1,"101":2,"161":6,"170":4,"189":3,"197":1,"289":1,"325":1,"419":1,"436":1,"510":1,"592":1,"614":1,"692":2,"844":1,"846":1,"927":4,"941":3,"944":2,"945":1,"946":1,"960":3,"965":1,"966":1,"1003":10,"1004":4,"1184":1,"1388":5,"1389":7,"1392":5,"1393":5,"1394":17,"1708":2}}],["tuesday",{"2":{"1728":2}}],["tuln",{"2":{"1527":1}}],["tune",{"2":{"938":1,"954":1,"1312":1}}],["tunin",{"2":{"638":1}}],["tuningplaybookgithub",{"2":{"1195":1}}],["tuning和hard",{"2":{"726":1}}],["tuning阶段",{"2":{"726":1}}],["tuning阶段在c",{"2":{"726":1}}],["tuning",{"2":{"139":1,"393":1,"429":1,"638":1,"718":1,"726":2,"733":1,"768":1,"1195":2,"1196":1,"1312":1,"1404":1}}],["tup",{"2":{"1912":4}}],["tupe架构如下图所示",{"2":{"764":1}}],["tupe解耦了token和position的投影矩阵",{"2":{"764":1}}],["tupe移除了二三项",{"2":{"764":1}}],["tupe将语义信息和位置信息同等看待",{"2":{"764":1}}],["tupe位置编码去掉绝对位置编码的公式的第二三项",{"2":{"764":1}}],["tupe其实可以看作是t5和deberta的位置编码的结合",{"2":{"764":1}}],["tupe有ape和rpe两个版本",{"2":{"764":1}}],["tupe出自论文",{"2":{"764":1}}],["tupe",{"0":{"764":1},"2":{"741":1,"764":1,"766":1,"768":1}}],["tuple>",{"2":{"1912":1,"1921":1}}],["tuple",{"2":{"557":2,"1083":1,"1085":2,"1087":7,"1214":8,"1227":4,"1254":5,"1345":1,"1912":8,"1921":2,"1925":1}}],["tuple为",{"2":{"557":1}}],["tuples",{"2":{"557":3}}],["tutorials",{"0":{"2085":1},"2":{"740":1}}],["tutorial",{"2":{"428":1,"740":1,"1090":1}}],["turing",{"2":{"504":2,"513":1}}],["turn",{"2":{"343":1,"522":1,"571":1,"985":1,"1284":1}}],["turbo采用了dca来通过分块处理长序列",{"2":{"204":1}}],["turbo",{"2":{"111":1,"361":1}}],["ta是跨越时空的",{"2":{"2054":1}}],["ta并非文字所能解释清楚",{"2":{"2054":1}}],["ta",{"2":{"1661":3,"1662":2}}],["tab>rm",{"2":{"1917":2}}],["tab>$",{"2":{"1917":4}}],["tab>g++",{"2":{"1917":4}}],["tab>commands",{"2":{"1917":1}}],["tab",{"2":{"1616":1,"1917":1}}],["tabstop=4",{"2":{"1559":1}}],["table自然地实现了内存共享",{"2":{"983":1}}],["table",{"2":{"700":5,"1284":1,"1481":2}}],["tage",{"2":{"1616":1}}],["tag",{"2":{"1328":2,"1329":8,"1330":16,"1590":3,"1990":1,"2038":1}}],["tags",{"2":{"1328":12,"1329":16,"1330":26}}],["tan",{"2":{"1087":2}}],["tanh和softsign还有个很好的性质",{"2":{"1000":1}}],["tanh^",{"2":{"839":1}}],["tanh的原因",{"2":{"360":1}}],["tanh",{"2":{"103":1,"110":3,"175":3,"359":2,"642":1,"839":7,"844":1,"846":1,"866":3,"868":2,"1000":1,"1087":2,"1143":1}}],["taku",{"2":{"638":1}}],["take",{"2":{"23":1,"82":1,"450":1,"703":1,"1087":2,"1283":1}}],["tain",{"2":{"588":1}}],["tai",{"2":{"513":1}}],["tail",{"2":{"135":2,"1515":1}}],["tarbashtar",{"2":{"1535":1}}],["tar",{"2":{"370":4,"1309":1,"1535":5}}],["target=fetch",{"2":{"1566":1}}],["target=http",{"2":{"713":1}}],["target​o2​​−out​o2​​",{"2":{"1394":1}}],["target​o1​​−out​o1​​",{"2":{"1393":2,"1394":1}}],["targeto2−outo2",{"2":{"1394":1}}],["targeto1−outo1",{"2":{"1393":2,"1394":1}}],["target−out​o1​​",{"2":{"1392":1}}],["target−outo1",{"2":{"1392":1}}],["target端",{"2":{"649":1,"931":1}}],["targets",{"2":{"398":1,"1987":1,"1999":2}}],["target",{"0":{"1975":1},"2":{"82":1,"83":1,"84":7,"130":2,"170":3,"271":3,"381":2,"399":5,"450":3,"703":1,"816":3,"908":1,"1039":1,"1213":1,"1214":3,"1215":9,"1223":4,"1231":4,"1250":7,"1251":2,"1295":8,"1392":1,"1393":2,"1394":2,"1917":5,"1980":1,"1984":1,"1985":2,"1991":1,"1994":2,"1999":1}}],["tape计算出某个词与其之前词之间的相关程度",{"2":{"287":1}}],["tape",{"2":{"287":1}}],["task1",{"2":{"370":1}}],["tasks",{"2":{"122":1,"1318":1,"2079":5,"2082":1}}],["task",{"2":{"79":1,"85":1,"726":2,"1309":1,"1895":2}}],["typora",{"2":{"2021":1}}],["typology",{"2":{"513":1}}],["typical",{"2":{"945":1,"965":1}}],["typically",{"2":{"76":1}}],["typeinfo>",{"2":{"1897":1}}],["typeid",{"2":{"1630":1,"1712":1,"1897":2}}],["type=",{"2":{"1481":2}}],["type=float",{"2":{"1215":2}}],["type=int",{"2":{"1215":5}}],["typevar",{"2":{"1214":1}}],["typedstorage",{"2":{"1087":2}}],["typed",{"2":{"1083":1}}],["typecast",{"2":{"573":1}}],["types",{"2":{"513":1,"1083":1,"1087":30,"2077":1,"2079":1}}],["typename",{"2":{"395":1,"1698":3,"1699":4,"1700":4,"1701":2,"1726":1,"1908":3,"1912":3,"1914":2,"1925":2}}],["type",{"0":{"1615":1,"1682":1},"1":{"1683":1,"1684":1},"2":{"74":2,"76":1,"79":1,"83":4,"84":1,"201":2,"346":1,"380":1,"382":1,"428":3,"472":3,"503":4,"529":3,"702":6,"723":21,"1070":2,"1072":1,"1081":1,"1083":12,"1085":2,"1086":1,"1087":5,"1205":1,"1210":1,"1214":3,"1345":2,"1532":1,"1576":1,"1590":3,"1629":5,"1635":1,"1684":1,"1728":1,"1897":2,"2073":1}}],["tellp",{"2":{"1821":2,"1839":2}}],["tellg",{"2":{"1821":3,"1839":3}}],["texmpi的实现",{"2":{"1569":1}}],["text|>",{"2":{"571":5}}],["text",{"2":{"71":1,"104":1,"106":3,"108":2,"109":3,"110":1,"186":2,"233":1,"370":1,"375":4,"402":1,"429":1,"513":1,"557":8,"571":1,"572":3,"591":3,"592":14,"710":2,"711":1,"740":1,"768":2,"906":4,"1086":2,"1191":1,"1192":1,"1193":1,"1312":1,"1340":2,"1520":2,"1553":4,"1566":3,"2079":4}}],["techniques",{"2":{"735":1,"736":1,"740":1}}],["tesla",{"2":{"2082":1}}],["tesnor",{"2":{"661":1,"1104":1}}],["testdeleteuser",{"2":{"1488":1}}],["testupdateuser",{"2":{"1487":1}}],["testadduser",{"2":{"1486":1}}],["test1",{"2":{"1279":1}}],["testing",{"2":{"1215":1,"1986":1}}],["testserver",{"2":{"2070":1}}],["tests",{"2":{"428":2,"1986":2}}],["test",{"0":{"1422":1},"2":{"226":1,"231":1,"233":1,"273":1,"370":1,"375":1,"428":7,"472":6,"529":7,"557":4,"765":1,"768":1,"1215":18,"1279":1,"1284":1,"1481":1,"1485":1,"1486":1,"1487":1,"1488":1,"1986":3,"2006":11,"2062":1,"2063":1,"2070":1,"2078":1,"2081":1,"2086":1}}],["tem",{"2":{"490":1}}],["temporary",{"2":{"1887":1}}],["temporal",{"2":{"315":1}}],["temp",{"2":{"1650":6}}],["templates",{"0":{"1908":1},"2":{"1904":1}}],["template",{"0":{"1925":1},"2":{"1087":2,"1698":1,"1699":2,"1700":2,"1701":3,"1908":2,"1912":2,"1914":1,"1920":1,"1925":1,"2063":1}}],["temperature",{"2":{"187":2,"1608":5}}],["teach",{"2":{"2083":1}}],["teachingassistant",{"2":{"1661":3,"1662":3}}],["teacherid",{"2":{"1661":6,"1662":6}}],["teacher",{"0":{"404":1,"896":1},"1":{"405":1,"406":1,"407":1,"408":1,"409":1,"410":1,"411":1,"412":1},"2":{"404":1,"406":5,"408":1,"410":1,"411":1,"543":1,"896":1,"1661":5,"1662":4}}],["team",{"2":{"95":1}}],["tener其实揭示了目前位置编码的一些弊病",{"2":{"761":1}}],["tener还发现",{"2":{"761":1}}],["tener的位置编码和transformer",{"2":{"761":1}}],["tener作者提出了将相对方向和相对距离都纳入到位置编码当中",{"2":{"761":1}}],["tener作者发现了传统三角式位置编码在实践中不具备单调性",{"2":{"761":1}}],["tener",{"0":{"761":1},"2":{"741":1,"768":1}}],["tend",{"2":{"334":1}}],["tensorlistlist",{"2":{"1227":4}}],["tensorboard会自动安装",{"2":{"1276":1}}],["tensorboardx",{"2":{"1275":1}}],["tensorboard",{"0":{"1275":1},"2":{"1215":1,"1275":1,"1276":1,"1278":2,"1281":3,"1284":2,"1318":1,"1404":1}}],["tensorbase",{"0":{"1084":1},"1":{"1085":1,"1086":1,"1087":1},"2":{"1082":4,"1083":8,"1084":1}}],["tensormeta",{"2":{"1082":1}}],["tensors",{"0":{"1114":1,"2077":1},"2":{"834":1,"1100":1,"1114":1,"1227":1,"1254":1,"2077":2}}],["tensor2",{"2":{"805":2,"1078":5,"1087":5}}],["tensor1",{"2":{"805":2,"1087":5}}],["tensor类和重要属性",{"2":{"664":1}}],["tensorflow",{"2":{"661":1,"688":1,"723":2,"1104":1,"1161":2,"1275":1,"1276":1,"1602":1}}],["tensor中",{"2":{"661":1,"1104":1}}],["tensor",{"0":{"658":1,"1068":1,"1069":1,"1074":1,"1077":1,"1078":1,"1080":1,"1081":1,"1082":1,"1083":1,"1087":1,"1099":1,"1106":1,"1203":1,"1255":1},"1":{"1069":1,"1070":1,"1071":1,"1072":1,"1075":1,"1076":1,"1078":1,"1079":1,"1080":1,"1081":1},"2":{"65":1,"76":3,"201":7,"315":5,"344":1,"384":4,"395":3,"558":4,"661":4,"700":4,"702":5,"723":1,"773":1,"785":1,"808":1,"819":1,"820":1,"822":1,"827":1,"828":1,"829":1,"832":14,"833":1,"981":1,"1069":3,"1071":1,"1072":14,"1073":4,"1075":9,"1076":11,"1078":20,"1082":16,"1083":26,"1085":84,"1086":41,"1087":948,"1094":1,"1095":1,"1096":1,"1098":5,"1099":4,"1102":4,"1104":4,"1113":1,"1114":1,"1116":3,"1202":2,"1205":4,"1208":1,"1212":2,"1214":8,"1216":1,"1218":1,"1225":1,"1226":1,"1227":5,"1255":7,"1266":1,"1272":2,"1295":2,"1328":2,"1329":3,"1330":1,"1345":6,"1404":1}}],["terminate",{"2":{"1762":1,"1764":2}}],["terminating",{"2":{"592":1}}],["termius",{"2":{"1583":1}}],["term",{"0":{"862":1},"1":{"863":1,"864":1,"865":1,"866":1,"867":1,"868":1,"869":1,"870":1,"871":1},"2":{"229":1,"287":1,"292":2,"862":1}}],["tr",{"2":{"1361":2}}],["trt",{"2":{"944":4}}],["tr=nbrt",{"2":{"944":1}}],["trevor",{"2":{"638":1}}],["treat",{"2":{"572":1}}],["tree的优势在于其能够高效地存储和检索大量的键值对",{"2":{"986":1}}],["tree的边不仅可以用单个元素标记",{"2":{"986":1}}],["tree来管理token序列与其对应的kv",{"2":{"986":1}}],["tree",{"2":{"429":1,"503":1,"985":2,"986":2,"1309":1,"1952":1}}],["tree融入进来",{"2":{"184":1}}],["trotter",{"2":{"498":1}}],["trotter法解上面的常微分方程",{"2":{"498":1}}],["trunc",{"2":{"1087":2,"1820":1,"1838":1}}],["truth未知",{"2":{"413":1}}],["truth已知",{"2":{"412":1}}],["truth",{"2":{"399":1,"406":1,"528":1}}],["true时",{"2":{"1208":1}}],["truediv",{"2":{"1085":1}}],["true的位置是不需要动的",{"2":{"66":1}}],["true",{"2":{"66":3,"74":46,"110":1,"135":1,"136":1,"156":1,"382":33,"395":4,"399":9,"422":1,"423":1,"450":30,"557":1,"661":3,"700":1,"833":1,"1082":1,"1083":1,"1086":2,"1087":22,"1092":1,"1097":1,"1098":1,"1104":3,"1106":1,"1114":3,"1120":1,"1214":14,"1215":6,"1227":1,"1242":1,"1331":4,"1607":4,"1619":1,"1620":1,"1630":1,"1729":1,"1763":1,"1814":4,"1817":3,"1832":4,"1835":3,"1927":2,"1982":1}}],["try",{"2":{"373":2,"428":2,"1481":1,"1491":1,"1566":1,"1647":1,"1713":1,"1762":8,"1763":1,"1765":1,"1766":1,"1902":1,"2062":1,"2063":1,"2075":1}}],["trivial",{"2":{"1728":1}}],["tril",{"2":{"1087":2,"1216":1}}],["triangular2",{"2":{"1241":1}}],["triangular",{"2":{"1087":2,"1241":1}}],["tries",{"2":{"2073":1}}],["trie",{"2":{"986":1}}],["trick",{"0":{"969":1},"2":{"210":1,"969":2}}],["triu",{"2":{"74":2,"84":2,"201":1,"382":1,"503":1,"1087":2}}],["trg",{"2":{"83":1,"423":1}}],["track",{"2":{"2086":1}}],["trace时",{"2":{"1292":1}}],["trace是将pytorch模型转换为跟踪形式的工具",{"2":{"1292":1}}],["traced",{"0":{"1297":1},"2":{"1269":4,"1270":1,"1297":6}}],["trace",{"0":{"1292":1},"2":{"1087":1,"1269":3,"1270":1,"1284":10,"1292":1,"1297":1,"1330":1}}],["tracing和circuit",{"2":{"475":1}}],["tracing",{"0":{"476":1},"1":{"477":1,"478":1,"479":1},"2":{"475":1,"1214":1,"2009":2}}],["trainloader",{"2":{"1283":2}}],["trainset",{"2":{"1283":2}}],["trainstate",{"2":{"423":1}}],["trainstate对象",{"2":{"385":1}}],["train=false",{"2":{"1215":1}}],["train=true",{"2":{"1215":1,"1253":1,"1283":1}}],["trainning",{"2":{"1215":1}}],["trainable",{"2":{"731":1}}],["train模式和veal模式不会对grad的情况做修改",{"2":{"662":1}}],["train+log",{"2":{"364":1,"385":3,"423":1}}],["trained",{"2":{"172":1,"371":1,"372":2,"428":1,"562":2,"721":1,"726":1,"727":1,"1308":1,"2079":3}}],["train",{"0":{"1102":1,"1156":1},"2":{"83":2,"364":5,"370":4,"372":1,"375":9,"381":1,"385":14,"395":2,"398":1,"399":4,"422":7,"423":9,"424":2,"428":1,"472":1,"557":4,"591":2,"592":1,"612":1,"768":1,"938":1,"954":1,"1102":1,"1122":3,"1155":6,"1156":4,"1159":1,"1183":4,"1205":2,"1214":1,"1215":14,"1218":2,"1231":1,"1233":1,"1234":1,"1235":1,"1236":1,"1237":1,"1238":1,"1239":1,"1240":1,"1241":1,"1242":1,"1245":1,"1246":1,"1247":1,"1251":3,"1279":2,"1280":1,"1283":1,"1295":1,"1296":1,"1299":1,"1303":3,"1306":1,"1307":2,"1332":3,"2073":1,"2077":1}}],["training阶段的目标是为了学习出更适合embedding的pre",{"2":{"727":1}}],["training阶段在wudao",{"2":{"726":1}}],["training",{"0":{"1300":1,"1303":1,"1305":1,"1380":1},"1":{"1304":1,"1305":1,"1306":2,"1307":2,"1308":1},"2":{"8":1,"90":2,"141":1,"156":1,"273":1,"361":3,"370":1,"380":1,"399":1,"420":1,"422":1,"423":2,"428":1,"449":1,"543":1,"638":1,"664":1,"726":1,"735":1,"740":3,"747":1,"764":1,"768":2,"1208":1,"1211":1,"1214":1,"1215":5,"1241":1,"1242":1,"1253":2,"1280":3,"1284":1,"1300":1,"1302":1,"1304":1,"1305":2,"1308":7,"1360":1,"1404":3,"2076":2,"2086":4}}],["transitive",{"2":{"1781":1}}],["transitioned",{"2":{"1330":2}}],["transitioning",{"2":{"1329":1}}],["transitions",{"2":{"1328":3,"1329":3,"1330":4}}],["transition",{"2":{"1328":3,"1329":2,"1330":2}}],["transactionmanager",{"2":{"1481":1}}],["trans",{"2":{"1331":2}}],["transfomer",{"2":{"1472":2}}],["transform=totensor",{"2":{"1253":1}}],["transform=transform",{"2":{"1215":2,"1283":1}}],["transform=transforms",{"2":{"1215":1,"1254":1}}],["transform=none",{"2":{"1250":1}}],["transformsers",{"2":{"1332":1}}],["transforms",{"0":{"1254":1},"2":{"1215":3,"1253":1,"1254":8,"1278":1,"1283":3}}],["transformed",{"2":{"1254":4}}],["transforme",{"2":{"975":1}}],["transformerdecoder",{"2":{"1216":3}}],["transformer遵循这种整体架构",{"2":{"912":1}}],["transformer允许进行更多的并行化",{"2":{"911":1}}],["transformer嵌入层实现",{"0":{"697":1},"1":{"698":1,"699":1,"700":1,"701":1,"702":1,"703":1,"704":1,"705":1,"706":1,"707":1,"708":1,"709":1}}],["transformer其实是构建了一个高维的语言体系",{"2":{"676":1}}],["transformer就将其传递给自注意力层",{"2":{"676":1}}],["transformer就是用点积来表征两个向量之间的相似度",{"2":{"692":1}}],["transformer就是通过增加模型的层数来加大模型可学习的参数量",{"2":{"437":1}}],["transformer就是这样一个融汇贯通的集大成者",{"2":{"291":1}}],["transformer实际上是把人类的语义通过向量化",{"2":{"676":1}}],["transformer实际上是通过三重注意力机制建立起了序列内部以及序列之间的全局联系",{"2":{"441":1}}],["transformer相同的自注意力组件",{"2":{"620":1}}],["transformer接受的处理是向量",{"2":{"547":1}}],["transformer接受的是高维向量",{"2":{"545":1}}],["transformer架构最初作为机器翻译任务的编码器",{"2":{"540":1}}],["transformer架构中的非线性特征对transformer模型的能力有重大影响",{"2":{"117":1}}],["transformer完整的前向计算过程如下图所示",{"2":{"515":1}}],["transformer和大脑新皮质的一致性",{"2":{"513":1}}],["transformer还有其他优点",{"2":{"512":1}}],["transformer优化的过程",{"2":{"509":1}}],["transformer机制本质上是描述一个运动轨迹",{"2":{"509":1}}],["transformer层堆叠而成",{"2":{"501":1}}],["transformer结构容易往往会过度关注不相关的上下文",{"2":{"500":1}}],["transformer结构的含义",{"2":{"498":1}}],["transformer块由一个前馈网络近似",{"2":{"489":1}}],["transformer通过固定的数学公式",{"2":{"750":1}}],["transformer通过一个线性层达到了这个目的",{"2":{"473":1}}],["transformer通过teacher",{"2":{"390":1}}],["transformer处理的核心在于transformer块",{"2":{"461":1}}],["transformer会一次性接收整个输入句子的嵌入矩阵",{"2":{"457":1}}],["transformer会一次输出多个概率分布",{"2":{"426":1}}],["transformer总体架构是一个有机整体",{"2":{"446":1}}],["transformer在生物学上是否合理",{"2":{"513":1}}],["transformer在计算任意一个词的特征时",{"2":{"415":1}}],["transformer在构建句内关系时使用的自注意力机制",{"2":{"318":1}}],["transformer训练的目的是通过对输入源序列和模型输出序列的学习",{"2":{"389":1}}],["transformer为什么使用layernorm而不是batchnorm",{"2":{"361":1}}],["transformer为什么要引入mha",{"2":{"233":1}}],["transformer每一层都在不断调整每个词在空间中的位置",{"2":{"326":1}}],["transformer采用的是层归一化",{"2":{"294":1}}],["transformer可以组合得到从简单到复杂逻辑的嵌套结构",{"2":{"510":1}}],["transformer可以学习到输入序列里所有词的相关性",{"2":{"291":1}}],["transformer可以持续不断的优化",{"2":{"1":1}}],["transformer整个网络结构完全是由注意力机制组成",{"2":{"291":1}}],["transformer所包括或者说所需要的组件一步步的被实现出来",{"2":{"291":1}}],["transformer等网络结构都可以看作是在mlp上添加的约束条件",{"2":{"242":1}}],["transformer^2",{"2":{"233":1}}],["transformer时刻",{"2":{"233":1}}],["transformer升级之路",{"2":{"233":1,"361":1,"768":4}}],["transformer使用了6层堆叠来进行学习",{"2":{"436":1}}],["transformer使用的就是xavier初始化",{"2":{"403":1}}],["transformer使用softmax注意力",{"2":{"210":1}}],["transformer使用大矩阵方式",{"2":{"32":1}}],["transformerblock",{"2":{"201":2,"1217":3}}],["transformerblock层",{"2":{"201":1}}],["transformer提出的方案就是缩放矩阵以获得与之前相同的方差",{"2":{"193":1}}],["transformer模型‌",{"2":{"513":2}}],["transformer模型中还应用了layernorm",{"2":{"467":1}}],["transformer模型",{"2":{"385":1}}],["transformer模型结构中每层都包含着残差结构",{"2":{"329":1}}],["transformer模型已成为自然语言处理和图像分类等应用中使用最广泛的架构",{"2":{"259":1}}],["transformer模型的设计允许在处理序列时进行高效的并行计算",{"2":{"172":1}}],["transformer模型成功的主要原因是不同",{"2":{"41":1}}],["transformer是第一个完全依靠self",{"2":{"911":1}}],["transformer是自回归模型",{"2":{"443":1}}],["transformer是建立在很多巨人肩膀之上的集大成者",{"2":{"280":1}}],["transformer是整个模型的主体",{"2":{"201":1}}],["transformer是把每个",{"2":{"172":1}}],["transformer是否真正理解了自然语言的语义信息",{"2":{"156":1}}],["transformer只有一个输入序列",{"2":{"161":1}}],["transformer如何做到",{"2":{"160":1}}],["transformer²要做",{"2":{"233":1}}],["transformer²框架通过组合",{"2":{"225":1}}],["transformer²通过以下三种适配方法之一来识别任务的特征",{"2":{"225":1}}],["transformer²这个名称也反映了它的两步过程",{"2":{"225":1}}],["transformer²采用两阶段适应策略",{"2":{"225":1}}],["transformer²使用reinforce方式而非sft",{"2":{"224":1}}],["transformer²",{"0":{"218":1},"1":{"219":1,"220":1,"221":1,"222":1,"223":1,"224":1,"225":1},"2":{"157":1,"218":2,"224":3,"225":1,"233":1}}],["transformer内部是如何调整或者修改的",{"2":{"146":1}}],["transformer用embedding解决无法定义的概念",{"2":{"120":1}}],["transformer并没有外接显式的数据库",{"2":{"118":1}}],["transformer作者提出了正弦位置嵌入",{"2":{"756":1}}],["transformer作者之一",{"2":{"434":2}}],["transformer作者彻底摒弃了rnn和cnn",{"2":{"291":1}}],["transformer作者初创重磅发布transformer²",{"2":{"233":1}}],["transformer作者认为可以将其理解为",{"2":{"101":1}}],["transformer作者通过增加两层网络来增强模型加模型的容量和非线性",{"2":{"97":1}}],["transformer已经利用注意力机制来考虑单词在不同位置的语义和依赖关系",{"2":{"101":1}}],["transformer抽取",{"2":{"97":1}}],["transformer系列",{"0":{"48":1},"2":{"47":2,"95":1,"543":1}}],["transformer多头自注意力机制的本质洞察",{"2":{"47":1,"233":2}}],["transformer自下而上理解",{"2":{"47":1,"292":1}}],["transformers高出多达24倍",{"2":{"980":1}}],["transformers基本原理",{"2":{"543":1}}],["transformers",{"2":{"41":2,"43":1,"47":3,"76":1,"91":1,"95":1,"115":1,"126":1,"130":1,"141":1,"143":2,"156":3,"167":1,"210":2,"211":1,"232":1,"233":3,"259":1,"280":1,"292":1,"316":1,"320":1,"326":1,"348":1,"358":1,"361":4,"429":1,"480":1,"487":1,"490":1,"499":2,"507":1,"508":1,"513":5,"719":1,"740":1,"747":1,"751":1,"768":1,"980":1,"1312":1,"1332":3}}],["transformer中也可以使用多头注意力机制",{"2":{"503":1}}],["transformer中normalization的二三事",{"2":{"361":1}}],["transformer中layer",{"2":{"334":1}}],["transformer中的编码器详解",{"2":{"543":1}}],["transformer中的解码器详解",{"2":{"543":1}}],["transformer中的encoder与rnn中的encoder作用一样",{"2":{"510":1}}],["transformer中的attention为什么scaled",{"2":{"233":1}}],["transformer中的mask机制超详细讲解",{"2":{"95":1}}],["transformer中包含了很多层transformerblock",{"2":{"201":1}}],["transformer中使用的是多头注意力机制",{"2":{"198":1}}],["transformer中ffn的记忆功能",{"2":{"156":1}}],["transformer中为2048",{"2":{"99":1}}],["transformer中为512",{"2":{"99":1}}],["transformer中三种mask的使用",{"2":{"95":1}}],["transformer中",{"2":{"19":1}}],["transformer论文是针对输入嵌入序列",{"2":{"745":1}}],["transformer论文原图中只画了一个encoderlayer",{"2":{"517":1}}],["transformer论文从三个维度比较了当时特征提取的主流框架",{"2":{"511":1}}],["transformer论文将每个token表示为512维向量",{"2":{"458":1}}],["transformer论文",{"2":{"432":1,"1336":1}}],["transformer论文使用了一种特殊的自适应学习率调整策略",{"2":{"402":1}}],["transformer论文使用了乘法函数或者说点积注意力函数来计算相似度",{"2":{"174":1}}],["transformer论文中也使用了label",{"2":{"399":1}}],["transformer论文中对于多注意力机制的论述如下",{"2":{"5":1}}],["transformer论文末尾给出了多头注意力机制中两个头的attention可视化结果",{"2":{"18":1}}],["transformer的位置编码",{"2":{"768":1}}],["transformer的目标就是在训练中逐步调整这些嵌入",{"2":{"709":1}}],["transformer的输入层首先要把输入文本的每个词",{"2":{"676":1}}],["transformer的输出是最有可能放在输入序列末尾的单词",{"2":{"473":1}}],["transformer的词嵌入式用三角位置嵌入",{"2":{"671":1}}],["transformer的主要计算成本并非注意力机制",{"2":{"612":1}}],["transformer的编码器模块结构如下图紫色方框所示",{"2":{"517":1}}],["transformer的核心",{"2":{"513":1}}],["transformer的核心所在或者说与其他架构的关键区别之处是自注意力机制",{"2":{"158":1}}],["transformer的物理原理",{"2":{"513":1}}],["transformer的缺点也同样明显",{"2":{"512":1}}],["transformer的架构并非一成不变",{"2":{"539":1}}],["transformer的架构包含了堆叠的编解码器",{"2":{"512":1}}],["transformer的架构却和目前神经科学中的海马体模型极其相似",{"2":{"490":1}}],["transformer的处理流程就是token流转的过程",{"2":{"510":1}}],["transformer的处理流程可以看作是",{"2":{"498":1}}],["transformer的新颖之处在于它是一个完全基于注意力机制实现的序列转换架构",{"2":{"434":1}}],["transformer的并行化主要体现在训练阶段",{"2":{"413":1}}],["transformer的细枝末节",{"2":{"361":1}}],["transformer的对称性",{"2":{"232":1}}],["transformer的设计",{"2":{"169":1}}],["transformer的自注意力使用了缩放点积注意力评分函数",{"2":{"160":1}}],["transformer的原始拼接方式",{"2":{"19":2}}],["transformer的角色定位是特征抽取器或者万能函数逼近器",{"2":{"12":1}}],["transformer的多头注意力应该也借鉴了cnn中同一卷积层内使用多个卷积核的思想",{"2":{"12":1}}],["transformer本质上是一个通用的可微计算机",{"2":{"1":1}}],["transformer",{"0":{"76":1,"461":1,"912":1,"1218":1,"1311":1,"1312":1,"1314":1},"1":{"462":1,"463":1,"464":1,"465":1,"466":1,"467":1,"468":1,"469":1,"470":1,"1312":1,"1313":1,"1314":1,"1315":1,"1316":1,"1317":1,"1318":1},"2":{"1":2,"12":1,"37":1,"41":2,"47":2,"49":1,"82":1,"83":2,"84":1,"95":1,"103":1,"105":1,"121":2,"122":2,"126":2,"127":1,"131":1,"144":1,"151":3,"152":1,"154":1,"156":10,"161":1,"167":10,"172":1,"201":1,"204":1,"210":2,"217":1,"230":1,"231":1,"232":1,"233":6,"235":2,"259":8,"260":1,"273":1,"294":1,"306":1,"311":1,"320":1,"326":3,"329":1,"330":1,"334":3,"346":1,"347":1,"349":2,"350":2,"351":2,"352":3,"354":3,"355":4,"356":2,"357":2,"361":4,"394":1,"402":2,"406":1,"420":1,"422":1,"428":1,"432":4,"436":1,"437":1,"446":1,"470":1,"472":1,"487":2,"488":3,"489":4,"490":1,"498":3,"499":2,"500":2,"501":2,"503":3,"504":8,"512":2,"513":13,"515":2,"535":1,"541":1,"543":1,"610":3,"614":8,"616":2,"620":1,"621":2,"624":2,"638":2,"650":1,"721":1,"742":1,"746":1,"756":1,"760":1,"768":4,"911":1,"932":1,"935":1,"951":1,"974":1,"976":1,"977":1,"1218":5,"1312":11,"1314":1,"1315":2,"1316":5,"1317":2,"1340":1,"1341":2,"1364":2,"2049":1}}],["transformation",{"2":{"838":1,"1216":1,"1218":1,"2079":1}}],["transformations",{"2":{"494":1,"1015":2,"2079":1}}],["transform",{"0":{"1741":1},"2":{"233":1,"1250":9,"1254":4,"1283":1,"1732":1,"1741":2,"1758":1,"1914":1}}],["transfer",{"2":{"513":1,"706":1,"768":1,"1312":1,"1340":1}}],["translate",{"2":{"257":1,"284":1,"292":2}}],["translation",{"2":{"175":1,"237":1,"257":1,"281":1,"282":1,"284":1,"285":1,"292":7,"574":1,"601":1,"605":1,"638":5,"698":1,"906":1,"907":2,"1315":1}}],["transduction",{"2":{"235":1}}],["transnormerllm",{"2":{"233":1}}],["transnormer",{"2":{"210":2,"216":1,"233":1}}],["transpose的区别",{"2":{"1080":1}}],["transposed",{"0":{"779":1},"2":{"1082":2}}],["transpose会让raw",{"2":{"658":1}}],["transpose",{"0":{"818":1,"821":1},"1":{"819":1,"820":1,"821":1,"822":1},"2":{"36":3,"67":1,"84":3,"199":2,"201":5,"394":1,"503":5,"658":1,"820":1,"821":1,"933":1,"1087":5,"1216":5,"1218":5,"1254":1,"1330":2,"1345":1}}],["t+1",{"2":{"58":1,"329":2,"332":1,"334":6,"1188":1,"1189":3,"1190":3,"1191":5,"1192":9,"1193":9}}],["t",{"2":{"58":2,"59":4,"69":1,"70":1,"135":1,"230":6,"240":13,"241":2,"245":1,"267":2,"285":8,"329":6,"332":3,"334":11,"395":1,"504":17,"507":3,"510":2,"564":1,"572":5,"573":4,"579":2,"580":1,"582":7,"583":7,"585":1,"591":1,"658":1,"745":2,"746":1,"760":2,"761":1,"763":3,"765":4,"766":3,"768":1,"828":2,"855":4,"856":5,"886":2,"892":1,"899":3,"903":2,"918":1,"934":4,"944":7,"957":2,"971":1,"1082":1,"1087":2,"1101":1,"1188":3,"1189":4,"1190":5,"1191":6,"1192":6,"1193":7,"1214":52,"1227":1,"1243":4,"1244":7,"1255":2,"1324":1,"1345":3,"1398":6,"1594":1,"1616":1,"1698":4,"1699":3,"1700":4,"1701":2,"1713":4,"1726":4,"1887":1,"1895":2,"1908":6,"1912":7,"1914":1,"1925":3,"1933":1,"2006":5,"2076":1}}],["thursday",{"2":{"1728":2}}],["thus",{"2":{"201":1}}],["th",{"2":{"1329":1}}],["those",{"2":{"688":1}}],["thought和knowledge",{"2":{"736":1}}],["thought",{"2":{"480":1,"513":1,"985":1}}],["think",{"2":{"513":1}}],["third",{"2":{"160":1,"1284":1}}],["this",{"0":{"1638":1},"2":{"8":1,"399":1,"449":1,"450":1,"591":7,"592":1,"700":1,"703":1,"736":3,"1082":1,"1158":1,"1254":6,"1284":1,"1300":1,"1303":1,"1304":1,"1308":3,"1329":1,"1330":2,"1638":16,"1639":1,"1640":1,"1642":1,"1691":3,"1712":2,"1820":1,"1838":1,"1887":4,"1895":1,"1909":3,"1929":1,"2062":1,"2075":1,"2076":1,"2086":1}}],["that",{"2":{"160":1,"428":1,"572":1,"591":3,"700":1,"827":1,"935":1,"951":1,"983":1,"1086":1,"1245":1,"1254":2,"1284":1,"1304":1,"1329":3,"1330":2,"2073":2,"2075":1,"2076":1,"2079":1}}],["thanks",{"2":{"1605":3}}],["thang",{"2":{"292":2}}],["than",{"0":{"610":1},"1":{"611":1,"612":1,"613":1,"614":1,"615":1},"2":{"47":1,"334":1,"610":1,"945":1,"965":1,"1283":1,"1883":1}}],["throw",{"2":{"1762":3,"1763":1,"1765":1,"1902":1,"2062":1}}],["through",{"2":{"156":1,"343":1,"437":1,"499":1,"522":1,"1217":1,"2086":1}}],["thread>",{"2":{"1895":1}}],["thread",{"2":{"1566":6,"1894":1,"1895":3}}],["threads",{"2":{"1566":3}}],["threading",{"2":{"1566":3}}],["threshold",{"2":{"1461":1,"1883":4}}],["three",{"2":{"160":1,"326":1,"1242":1,"1254":1,"2086":1}}],["thery的章节结构整改",{"2":{"2049":1}}],["theroy",{"0":{"1404":1}}],["therefore",{"2":{"1254":1}}],["there",{"2":{"592":1,"1737":1,"1738":1}}],["these",{"2":{"1254":1,"2079":1}}],["thesis",{"2":{"292":1}}],["theory",{"2":{"713":1,"1404":1,"2049":1}}],["theorem",{"2":{"296":1}}],["their",{"2":{"449":1,"558":1,"740":1}}],["then",{"2":{"277":1,"516":1,"557":1,"591":1,"1217":1,"1254":2,"1302":1}}],["theta=",{"2":{"1343":1}}],["theta",{"2":{"201":1,"945":2,"965":2,"1052":1,"1188":3,"1189":3,"1190":4,"1191":4,"1192":4,"1193":5,"1343":6,"1344":8,"1345":4,"1377":2,"1438":1}}],["they",{"2":{"181":1,"246":1,"2077":1}}],["them",{"2":{"47":1,"591":2,"1254":1,"2081":1}}],["the",{"0":{"938":1,"954":1,"2080":1},"1":{"2081":1,"2082":1,"2083":1},"2":{"5":1,"8":1,"20":2,"36":2,"47":4,"76":7,"91":1,"95":3,"130":3,"143":1,"148":1,"156":3,"160":6,"167":2,"201":4,"210":1,"233":2,"235":1,"257":1,"259":2,"261":4,"320":2,"326":3,"334":2,"343":1,"344":1,"350":1,"361":4,"370":1,"393":1,"394":1,"399":1,"402":1,"422":1,"428":8,"429":5,"432":3,"437":2,"449":1,"483":1,"490":1,"499":4,"507":1,"513":9,"522":1,"557":8,"558":2,"571":4,"572":4,"573":3,"588":2,"590":4,"591":18,"592":14,"625":2,"688":1,"691":2,"700":5,"723":1,"731":2,"735":1,"764":1,"768":2,"810":1,"820":1,"827":1,"935":1,"951":1,"1036":1,"1082":1,"1083":2,"1086":4,"1087":2,"1100":1,"1214":1,"1215":3,"1217":1,"1254":15,"1280":1,"1284":10,"1300":1,"1302":2,"1303":5,"1304":1,"1305":2,"1308":3,"1312":1,"1329":7,"1330":14,"1340":1,"1633":1,"1695":1,"1738":1,"1806":3,"1821":1,"1839":1,"1902":1,"1909":2,"1933":1,"2061":1,"2062":1,"2073":1,"2075":4,"2076":3,"2077":1,"2086":7,"2087":2}}],["toupper",{"2":{"1914":1}}],["touch",{"2":{"1510":1}}],["tom",{"2":{"1675":1}}],["tomcat",{"2":{"1493":1}}],["tostring",{"2":{"1481":1}}],["tongjilibo",{"2":{"1309":1}}],["too",{"2":{"1303":1}}],["tools",{"2":{"2079":3}}],["toolarge",{"2":{"2062":2}}],["tool",{"2":{"131":1,"2073":1}}],["totensor",{"2":{"1215":1,"1253":1,"1254":2,"1283":1}}],["totalwords++",{"2":{"1933":1}}],["totalwords",{"2":{"1933":2}}],["totallength",{"2":{"1914":4}}],["total",{"2":{"160":1,"385":6,"1237":2,"1238":2,"1240":2,"1242":2,"1243":3,"1246":1,"1247":1,"1284":1,"1295":3,"1308":2,"1389":1,"1392":4,"1393":3,"1394":3,"1395":1,"1594":4,"1710":3,"1914":1,"1933":1}}],["tolower",{"2":{"1933":1}}],["tolist",{"2":{"1087":1}}],["tolman",{"2":{"490":1}}],["together",{"2":{"638":1,"2079":1}}],["tower",{"0":{"634":1,"635":1}}],["towards",{"2":{"137":1,"156":1,"429":1}}],["today",{"2":{"545":1,"564":2,"1728":3}}],["tok",{"2":{"201":2,"557":2}}],["token粒度",{"2":{"1331":1}}],["token和position之间用了相同的矩阵做qkv变化",{"2":{"764":1}}],["token对应的输出作为整句的embedding",{"2":{"735":1}}],["token序号可以认为是独热编码",{"2":{"700":1}}],["token序列",{"2":{"510":1}}],["tokenformer是一个完全由注意力驱动的架构",{"2":{"620":1}}],["tokenformer则不同",{"2":{"620":1}}],["tokenformer消除了在增加模型规模时需要从头开始重新训练模型的需求",{"2":{"617":1}}],["tokenformer来统一",{"2":{"616":1}}],["tokenformer",{"0":{"616":1},"1":{"617":1,"618":1,"619":1,"620":1,"621":1,"622":1,"623":1,"624":1},"2":{"616":1,"620":2,"621":1,"623":1,"624":1,"638":1}}],["token化之所以重要",{"2":{"612":1}}],["token拼接在之前的输入序列上",{"2":{"529":1}}],["token从从一个初始位置经过一段时间在高维度空间中的另一位置",{"2":{"510":1}}],["token可以是单词或子词",{"2":{"456":1}}],["token是常微分方程在不同时间",{"2":{"510":1}}],["token是模型词汇表的一部分",{"2":{"456":1}}],["token是分词的结果",{"2":{"363":1,"548":1}}],["tokenmixer",{"2":{"446":1}}],["token的数量也会明显增多",{"2":{"562":1}}],["token的方向",{"2":{"437":1}}],["token的逻辑关联",{"2":{"224":1}}],["tokenizing",{"2":{"571":1,"595":1}}],["tokenization不存在了",{"2":{"638":1}}],["tokenization的基础上",{"2":{"554":1}}],["tokenization",{"2":{"364":1,"545":1,"564":1,"567":1,"568":2,"577":1,"595":1,"610":1,"637":1,"638":4}}],["tokenized",{"2":{"616":1,"638":1}}],["tokenizer是基类",{"2":{"591":1}}],["tokenizer的效果越好",{"2":{"584":1}}],["tokenizer类封装了tiktoken",{"2":{"571":1}}],["tokenizer将字符串切分为子词",{"2":{"563":1}}],["tokenizer总体上做两件事情",{"2":{"563":1}}],["tokenizer会把token映射成不同的整数",{"2":{"456":1}}],["tokenizer会将输入文本切分为更小",{"2":{"456":1}}],["tokenizer",{"0":{"549":1,"563":1,"591":1,"592":1},"1":{"564":1,"565":1,"566":1,"567":1,"568":1,"569":1,"570":1,"571":1,"572":1,"573":1},"2":{"454":1,"456":1,"549":1,"557":5,"569":1,"571":4,"572":1,"591":4,"592":1,"595":2,"610":2,"638":1}}],["tokenizer先把序列转换成token序列",{"2":{"445":1}}],["tokenizers",{"2":{"371":1,"373":2,"552":2,"591":1,"638":2}}],["tokenize",{"2":{"363":1,"375":6,"431":1,"547":1,"557":10}}],["token既可以是一个单词",{"2":{"363":1,"548":1}}],["token输入的原因",{"2":{"279":1}}],["token位置和最后一个token位置上",{"2":{"130":1}}],["token数量",{"2":{"126":1}}],["token会收敛到一个均衡状态",{"2":{"94":1}}],["tokens=self",{"2":{"571":1}}],["tokens从一个初始位置经过一段时间的处理之后",{"2":{"498":1}}],["tokens来定位模型中存储的信息",{"2":{"147":1}}],["tokens",{"0":{"610":1},"1":{"611":1,"612":1,"613":1,"614":1,"615":1},"2":{"50":1,"87":1,"201":3,"204":1,"230":1,"318":1,"323":1,"326":1,"385":7,"420":1,"463":1,"557":18,"558":4,"562":1,"571":14,"572":8,"575":1,"591":12,"592":1,"610":1,"620":2,"621":3,"623":1,"624":2,"638":1,"977":1}}],["token",{"0":{"458":1,"544":1,"548":1},"1":{"545":1,"546":1,"547":1,"548":1,"549":1,"550":1,"551":1,"552":1,"553":1,"554":1,"555":1,"556":1,"557":1,"558":1,"559":1,"560":1,"561":1,"562":1,"563":1,"564":1,"565":1,"566":1,"567":1,"568":1,"569":1,"570":1,"571":1,"572":1,"573":1,"574":1,"575":1,"576":1,"577":1,"578":1,"579":1,"580":1,"581":1,"582":1,"583":1,"584":1,"585":1,"586":1,"587":1,"588":1,"589":1,"590":1,"591":1,"592":1,"593":1,"594":1,"595":1,"596":1,"597":1,"598":1,"599":1,"600":1,"601":1,"602":1,"603":1,"604":1,"605":1,"606":1,"607":1,"608":1,"609":1,"610":1,"611":1,"612":1,"613":1,"614":1,"615":1,"616":1,"617":1,"618":1,"619":1,"620":1,"621":1,"622":1,"623":1,"624":1,"625":1,"626":1,"627":1,"628":1,"629":1,"630":1,"631":1,"632":1,"633":1,"634":1,"635":1,"636":1,"637":1,"638":1},"2":{"5":1,"12":1,"17":1,"41":6,"45":1,"50":3,"55":1,"57":1,"58":1,"84":1,"87":1,"88":1,"89":3,"101":4,"122":1,"130":1,"172":2,"201":1,"204":10,"230":1,"231":2,"239":4,"318":1,"323":1,"326":3,"363":1,"378":3,"379":2,"381":2,"382":1,"384":2,"394":1,"420":1,"458":2,"461":2,"463":1,"515":3,"520":1,"525":1,"530":1,"536":3,"542":2,"545":1,"548":1,"553":5,"557":1,"558":2,"563":1,"567":2,"571":9,"572":4,"573":2,"587":1,"590":1,"591":8,"592":2,"595":4,"616":4,"620":3,"621":3,"624":12,"628":1,"674":1,"700":2,"702":3,"722":1,"723":18,"731":2,"732":1,"734":3,"735":3,"765":1,"899":1,"922":3,"935":1,"951":1,"977":1,"1331":2,"1341":2,"1344":3,"1345":1,"2070":1}}],["topk",{"2":{"733":1,"1087":2,"1330":2}}],["topology",{"2":{"513":1}}],["top",{"2":{"71":1,"128":1,"135":2,"148":1,"186":1,"188":1,"192":1,"204":1,"334":1,"485":2,"621":1,"636":1,"941":1,"960":1,"1254":4,"1330":1,"1339":2,"1340":14,"1343":3,"1522":1,"1594":1,"1726":2}}],["torchaudio",{"2":{"2079":2}}],["torchtitan",{"2":{"1384":1}}],["torchtext",{"2":{"370":1,"557":2,"558":2,"679":2,"2079":2}}],["torchvision",{"0":{"1252":1,"1253":1,"1254":1},"1":{"1253":1,"1254":1},"2":{"1215":1,"1253":3,"1272":2,"1278":2,"1282":1,"1283":3,"2079":2}}],["torchdynamo",{"2":{"1208":1}}],["torch会做额外的一些事情",{"2":{"1106":1}}],["torch",{"0":{"662":1,"666":1,"1200":1,"1201":1,"1202":1,"1209":1,"1219":1,"1224":1,"1225":1,"1226":1,"1227":1,"1230":1,"1251":1,"1291":1,"1292":1,"1293":1},"1":{"1202":1,"1203":1,"1210":1,"1211":1,"1212":1,"1213":1,"1214":1,"1225":1,"1226":1,"1227":1,"1231":1,"1232":1,"1233":1,"1234":1,"1235":1,"1236":1,"1237":1,"1238":1,"1239":1,"1240":1,"1241":1,"1242":1,"1243":1,"1244":1,"1245":1,"1246":1,"1247":1,"1248":1},"2":{"65":2,"67":2,"74":3,"76":12,"83":6,"84":6,"119":1,"198":1,"199":4,"201":17,"315":4,"326":2,"343":2,"346":8,"361":3,"372":1,"374":2,"382":3,"383":1,"384":8,"394":2,"399":4,"402":1,"422":1,"423":6,"424":3,"428":6,"429":1,"472":6,"503":13,"529":6,"558":7,"661":1,"665":1,"667":1,"702":3,"704":2,"723":8,"781":2,"801":3,"802":4,"804":3,"805":10,"807":1,"808":3,"809":1,"810":1,"814":2,"815":2,"816":5,"819":4,"820":3,"821":2,"822":2,"825":3,"826":3,"827":1,"828":3,"829":2,"831":3,"832":2,"833":13,"834":6,"835":1,"839":3,"840":3,"841":1,"842":3,"843":2,"844":1,"845":1,"846":1,"847":1,"933":2,"1039":2,"1069":4,"1070":6,"1071":5,"1072":9,"1075":3,"1076":4,"1078":3,"1082":3,"1083":19,"1085":1,"1086":7,"1087":51,"1088":1,"1092":6,"1093":3,"1094":2,"1095":10,"1096":7,"1097":2,"1098":23,"1099":4,"1101":4,"1102":11,"1104":1,"1110":1,"1113":1,"1114":3,"1116":5,"1120":1,"1122":2,"1202":9,"1205":18,"1207":1,"1208":2,"1211":9,"1212":3,"1213":4,"1214":1,"1215":20,"1216":10,"1217":7,"1218":12,"1219":2,"1221":1,"1225":1,"1226":1,"1227":13,"1230":2,"1231":1,"1239":3,"1241":3,"1242":3,"1243":3,"1245":1,"1250":2,"1251":1,"1253":1,"1254":4,"1255":6,"1257":5,"1258":1,"1259":2,"1262":2,"1263":4,"1266":2,"1267":2,"1269":3,"1270":2,"1272":3,"1275":1,"1278":5,"1282":1,"1283":4,"1284":3,"1293":2,"1295":5,"1296":3,"1297":4,"1298":3,"1299":9,"1328":7,"1329":6,"1330":12,"1345":17,"1350":4,"2086":7}}],["to",{"0":{"283":1,"1074":1,"1298":1,"1299":1,"1380":1,"2072":1,"2078":1,"2084":1},"1":{"1075":1,"1076":1,"2073":1,"2074":1,"2075":1,"2076":1,"2077":1,"2078":1,"2079":1,"2080":1,"2081":1,"2082":1,"2083":1,"2084":1,"2085":2,"2086":2,"2087":1},"2":{"5":2,"8":1,"47":1,"74":4,"76":1,"79":1,"93":2,"125":1,"131":2,"156":2,"201":4,"209":2,"226":1,"233":2,"235":1,"237":1,"257":1,"283":1,"284":1,"285":1,"290":2,"292":6,"334":1,"344":1,"346":3,"375":2,"380":1,"385":1,"399":1,"402":4,"428":7,"429":1,"431":2,"455":1,"460":1,"480":1,"490":1,"503":4,"513":2,"557":8,"558":1,"564":1,"571":1,"572":9,"573":1,"590":1,"591":4,"592":1,"688":3,"698":1,"700":1,"723":2,"724":1,"747":1,"748":1,"768":3,"885":1,"977":1,"981":1,"1075":4,"1076":3,"1083":16,"1086":11,"1087":27,"1102":1,"1202":1,"1205":2,"1214":9,"1215":8,"1217":2,"1227":2,"1254":18,"1255":1,"1283":1,"1284":3,"1302":1,"1303":2,"1304":2,"1308":8,"1312":1,"1328":1,"1329":2,"1330":2,"1340":1,"1350":2,"1589":2,"1594":2,"1614":2,"1633":1,"1715":2,"1761":1,"1763":1,"1784":1,"1807":4,"1825":1,"1843":1,"1933":1,"2043":1,"2062":3,"2073":3,"2075":1,"2076":2,"2077":2,"2078":2,"2079":1,"2081":2,"2083":2,"2085":1,"2086":3,"2087":4}}],["a++",{"2":{"1630":2}}],["azure",{"2":{"1499":1}}],["a​i",{"2":{"1339":1}}],["axes",{"2":{"1254":2}}],["axis1",{"2":{"1087":2}}],["axis0",{"2":{"1087":2}}],["axis=",{"2":{"700":1,"807":2,"808":2,"809":2,"810":2}}],["axis",{"2":{"623":1,"1087":1,"1254":2}}],["axiomatic",{"2":{"134":1,"156":2}}],["a的词是跟b的词怎么对应的",{"2":{"908":1}}],["a∣y",{"2":{"908":2}}],["apt",{"2":{"1537":3,"1584":4}}],["apache",{"2":{"1481":4}}],["api处于原型阶段",{"2":{"1086":1}}],["api",{"2":{"1084":1,"1086":1,"1110":1,"1433":1,"1500":1,"2070":1}}],["ape和rpe两者最终统一于旋转位置编码",{"2":{"767":1}}],["ape只在第一层之前出现",{"2":{"745":1}}],["ape",{"2":{"742":1,"756":1}}],["app",{"2":{"1820":1,"1838":1,"1902":1}}],["appears",{"2":{"592":1}}],["append",{"2":{"65":1,"201":1,"384":2,"572":2,"590":2,"1087":1,"1099":1,"1273":1,"1566":1,"1713":2}}],["appropriate",{"2":{"1303":1}}],["approach",{"2":{"513":1,"1086":1}}],["approaches",{"2":{"40":1,"285":1,"292":1}}],["approx",{"2":{"191":2,"843":2,"844":2}}],["approximation",{"2":{"156":1,"185":1,"233":1,"296":1}}],["applied",{"2":{"1254":1}}],["application",{"0":{"2080":1},"1":{"2081":1,"2082":1,"2083":1},"2":{"1086":1,"2021":1}}],["applications",{"2":{"167":1,"259":1,"2073":1,"2087":1}}],["apples",{"2":{"713":2}}],["apple",{"2":{"122":3,"277":1,"391":1,"398":1,"405":1,"407":4,"408":8,"409":2,"427":2,"445":1,"453":2,"516":1,"536":1,"537":1,"713":2,"1715":1}}],["applyinterest",{"2":{"1873":1,"1874":2}}],["apply",{"2":{"36":2,"201":1,"344":1,"503":2,"1087":1,"1100":2,"1113":1,"1214":2,"1254":1,"1345":2}}],["a部分给出了神经元",{"2":{"489":1}}],["agi",{"2":{"1318":1}}],["agarwal为其他联合研究制作的几个分析图的一些实验数据",{"2":{"1194":1}}],["agagrad会累加所有历史梯度的平方",{"2":{"1052":1}}],["ag",{"2":{"976":11,"1530":1}}],["agwave的博客",{"2":{"429":1}}],["ages",{"2":{"1725":22}}],["age",{"2":{"156":1,"1436":1,"1607":13,"1613":1,"1655":3,"1661":15,"1662":14,"1673":3,"1674":6,"1675":4,"1725":1,"1750":6,"1811":4,"1829":4,"1921":3}}],["av",{"2":{"1665":3}}],["averages",{"2":{"1284":1}}],["averaged",{"2":{"1280":1}}],["average",{"0":{"816":1},"2":{"816":1,"1215":1,"1623":2,"1680":2,"1825":5,"1843":5}}],["averagepooling",{"0":{"815":1}}],["averaging",{"2":{"429":2}}],["available",{"2":{"592":2,"1076":1,"1215":2,"1255":1,"1284":2,"1927":2}}],["avgpool2d",{"2":{"815":2}}],["avg",{"2":{"449":1,"1215":2,"1280":2}}],["avoid",{"2":{"402":1,"1083":1,"2086":1}}],["a7",{"2":{"429":1}}],["aesthetics",{"2":{"1363":5}}],["ae",{"2":{"429":1}}],["akaihaoshuai",{"2":{"361":1}}],["a3窗口获取到了",{"2":{"247":1}}],["a10g",{"2":{"980":1}}],["a100",{"2":{"799":1,"968":1,"980":1}}],["a1窗口获取到了",{"2":{"247":1}}],["a1a1a",{"2":{"209":1}}],["abd",{"2":{"1713":1}}],["abcdecdef",{"2":{"1713":1}}],["abcdefghijklmnopqrstuvwxyz",{"2":{"1821":1,"1839":1}}],["abcdefg",{"2":{"1713":5}}],["abcdef",{"2":{"1481":1}}],["abc",{"2":{"1616":1,"1713":2}}],["above",{"2":{"1254":2}}],["about",{"2":{"233":1,"437":1,"513":2,"692":1,"740":1}}],["able",{"2":{"723":1}}],["ability",{"2":{"679":1}}],["abide",{"2":{"679":1}}],["abeyance",{"2":{"679":1}}],["ab",{"2":{"429":1}}],["absolute",{"2":{"723":2,"742":1,"749":1,"751":1,"1087":2}}],["abstractset",{"2":{"572":1}}],["abs",{"2":{"156":1,"233":2,"387":3,"429":3,"432":1,"543":1,"840":1,"1083":2,"1085":1,"1087":4,"1176":1}}],["aws",{"2":{"1499":1}}],["aweights",{"2":{"1087":1}}],["aware",{"2":{"211":1,"233":1,"513":1,"768":1}}],["awrsr",{"2":{"209":1}}],["adopt",{"2":{"2083":1}}],["admin需要添加名称和密码",{"2":{"2070":1}}],["admin",{"2":{"2070":5}}],["advance",{"2":{"724":1,"1720":1,"1721":1,"1722":1,"2087":1}}],["adjoint",{"2":{"495":1,"497":1,"1087":1}}],["adddouble",{"2":{"1687":2}}],["addint",{"2":{"1687":2}}],["adding",{"2":{"1633":1,"1797":2,"1902":1}}],["additional",{"0":{"733":1}}],["addition",{"2":{"302":1,"1916":9,"1917":10}}],["additive",{"2":{"209":1,"647":2,"713":1,"921":1,"924":2}}],["adduser",{"2":{"1486":3}}],["added",{"2":{"1328":2,"2086":1}}],["adds",{"2":{"1214":1}}],["addcuboid",{"2":{"1792":3}}],["addcmul",{"2":{"1087":2}}],["addcdiv",{"2":{"1087":2}}],["addmm",{"2":{"1086":1}}],["address",{"2":{"1594":3,"1611":1}}],["addressing",{"2":{"758":1}}],["addr",{"2":{"422":1,"1078":2,"1526":1}}],["add之中用到dropout",{"2":{"394":1}}],["add指x+multiheadattention",{"2":{"294":1}}],["add",{"0":{"1973":1,"1974":1,"1989":1},"2":{"294":2,"330":6,"334":2,"343":1,"344":2,"394":1,"395":3,"517":1,"522":1,"557":1,"785":1,"829":1,"1083":2,"1085":1,"1087":7,"1214":3,"1215":12,"1217":1,"1226":1,"1227":1,"1279":5,"1280":1,"1282":2,"1283":3,"1350":1,"1603":1,"1687":4,"1699":5,"1702":1,"1706":7,"1905":2,"1906":2,"1916":4,"1966":1,"1980":3,"1981":1,"1986":3,"1989":1,"1993":1,"1994":1,"1999":5,"2070":1}}],["adadelta",{"0":{"1051":1},"1":{"1052":1,"1053":1,"1054":1},"2":{"1052":1,"1054":1,"1057":1,"1205":1,"1215":1}}],["adagrad",{"0":{"1040":1,"1042":1,"1043":1},"1":{"1041":1,"1042":1,"1043":1,"1044":1,"1045":1,"1046":1},"2":{"400":1,"1042":3,"1048":2,"1052":1,"1057":1,"1205":1}}],["adapting",{"2":{"768":1}}],["adaptiveavgpool2d",{"2":{"816":3,"1215":1}}],["adaptive",{"0":{"185":1},"2":{"157":1,"185":3,"218":1,"233":5,"1046":1,"1054":1,"1059":1}}],["adaptively",{"2":{"47":1}}],["adaptation",{"2":{"233":1,"393":1,"560":1,"638":1}}],["adamw",{"0":{"1063":1},"1":{"1064":1,"1065":1},"2":{"1205":1}}],["adam优化器的beta等",{"2":{"1143":1}}],["adam优化器",{"2":{"385":1}}],["adam",{"0":{"1058":1,"1174":1,"1192":1},"2":{"83":1,"333":2,"400":1,"402":1,"423":1,"424":1,"1059":5,"1130":5,"1143":4,"1144":2,"1174":1,"1180":1,"1205":1,"1215":1,"1218":1,"1221":1,"1266":1,"1267":1}}],["adaloglou",{"2":{"47":1,"233":2}}],["a爱b",{"2":{"172":1}}],["a对于b的重要性",{"2":{"172":1}}],["among",{"2":{"2081":1}}],["amounts",{"2":{"2073":1}}],["amount",{"2":{"160":1,"1677":5,"1873":3,"1874":6}}],["aminmax",{"2":{"1087":2}}],["amin",{"2":{"1087":1}}],["ami和nmi在0",{"2":{"739":1}}],["amarpreet",{"2":{"692":1}}],["am",{"2":{"370":1,"1590":1}}],["amphibious",{"2":{"1665":1}}],["amphibiousvehicle",{"2":{"1664":2,"1665":5}}],["amp",{"0":{"12":1,"13":1,"198":1,"199":1,"351":1,"388":1,"514":1,"698":1,"766":1},"1":{"389":1,"390":1,"391":1,"392":1,"393":1,"394":1,"395":1,"396":1,"397":1,"398":1,"399":1,"400":1,"401":1,"402":1,"403":1,"404":1,"405":1,"406":1,"407":1,"408":1,"409":1,"410":1,"411":1,"412":1,"413":1,"414":1,"415":1,"416":1,"417":1,"418":1,"419":1,"420":1,"421":1,"422":1,"423":1,"424":1,"425":1,"426":1,"427":1,"428":1,"429":1,"515":1,"516":1,"517":1,"518":1,"519":1,"520":1,"521":1,"522":1,"523":1,"524":1,"525":1,"526":1,"527":1,"528":1,"529":1,"530":1,"531":1,"532":1,"533":1,"534":1,"535":1,"536":1,"537":1,"538":1,"539":1,"540":1,"541":1,"542":1,"543":1},"2":{"0":2,"156":1,"157":2,"289":1,"292":2,"293":1,"294":2,"330":10,"394":1,"449":1,"475":1,"543":1,"638":4,"694":24,"698":2,"740":1,"741":1,"943":2,"1176":1,"1481":2,"1611":3,"1612":2,"1619":2,"1629":2,"1630":8,"1635":5,"1674":1,"1729":5,"1762":1,"1813":1,"1824":1,"1831":1,"1842":1,"1886":2,"1921":3,"2043":1,"2059":1}}],["affine",{"2":{"838":1}}],["affine=false",{"2":{"807":1}}],["affine=true",{"2":{"503":1,"809":1}}],["after",{"2":{"302":1,"1101":1,"1217":1,"1245":1,"1284":1,"1633":2,"1713":1,"1714":1,"1715":1,"1721":2,"1752":1,"1762":1,"1797":2,"2062":1,"2086":1}}],["af",{"2":{"134":1}}],["audio",{"2":{"2079":4}}],["authtoken",{"2":{"2070":1}}],["auth",{"2":{"2070":1}}],["author",{"2":{"1195":1}}],["autonomous",{"2":{"2082":1}}],["autoindent",{"2":{"1559":1}}],["autogard模块才会填充一个新的图",{"2":{"1108":1}}],["autogradmeta",{"2":{"1110":1}}],["autograd是什么",{"0":{"1110":1}}],["autograd",{"0":{"659":1,"1088":1,"1092":1,"1098":1,"1112":1,"1123":1,"2076":1},"1":{"1089":1,"1090":1,"1113":1,"1114":1,"1115":1,"1116":1,"1117":1,"1118":1,"1119":1,"1120":1,"1121":1,"1122":1,"1123":1},"2":{"664":1,"1082":1,"1088":3,"1089":1,"1090":1,"1092":2,"1098":3,"1105":3,"1112":1,"1118":1,"1123":1,"1404":1,"2076":1}}],["autocast",{"2":{"1086":1}}],["automatic",{"0":{"1615":1,"2076":1}}],["automatically",{"2":{"562":1,"2076":1}}],["automated",{"2":{"845":1}}],["autoencoder",{"2":{"475":1}}],["autoencoder分解后的特征数量多于神经元数量",{"2":{"137":1}}],["autoregressive",{"2":{"210":1,"233":1,"239":1,"240":1}}],["auto",{"0":{"661":1,"1104":1,"1109":1,"1615":1,"1877":1},"1":{"1878":1,"1879":1,"1880":1},"2":{"122":1,"156":1,"395":3,"740":3,"1312":2,"1315":1,"1316":1,"1615":11,"1691":2,"1695":1,"1739":1,"1741":1,"1742":1,"1743":1,"1744":1,"1747":1,"1749":1,"1750":1,"1751":1,"1752":1,"1754":1,"1756":1,"1797":1,"1807":2,"1879":3,"1883":1,"1891":2,"1897":1,"1898":1,"1905":6,"1906":8,"1907":4,"1911":4,"1914":10,"1921":4,"1922":4,"1924":1,"1926":4,"1927":3,"1928":2,"1930":1,"1933":4}}],["auf",{"2":{"370":1}}],["aus",{"2":{"370":1}}],["augmentation",{"2":{"139":1,"1254":1}}],["aux",{"2":{"131":1,"1522":1,"1524":1}}],["academic",{"2":{"2087":1}}],["across",{"2":{"2079":1}}],["acquired",{"2":{"1891":1}}],["acquisition",{"2":{"1671":1,"1764":1}}],["acylic",{"2":{"1089":1}}],["acosh",{"2":{"1087":2}}],["acos",{"2":{"1087":2}}],["acoustic",{"2":{"156":1}}],["achieve",{"2":{"768":1}}],["achieving",{"2":{"111":1}}],["ackrb8wdpdan6v6ru",{"2":{"513":1}}],["account",{"2":{"1873":2,"1874":8,"2070":2}}],["according",{"2":{"1227":1}}],["accessbasemembers",{"2":{"1853":2,"1857":2,"1861":2}}],["accessing",{"2":{"1797":2}}],["access",{"2":{"1478":1,"1677":1,"1784":2,"1785":1,"1926":1}}],["accelerating",{"2":{"361":2}}],["accelerated",{"0":{"1033":1},"1":{"1034":1,"1035":1,"1036":1},"2":{"298":1}}],["accuracy",{"0":{"1280":1},"2":{"399":1,"1215":1,"1279":2,"1295":1,"1304":1}}],["accurategeluactivation",{"2":{"110":1}}],["accurate",{"0":{"973":1},"2":{"110":1,"156":1}}],["accumulation",{"2":{"2086":1}}],["accumulator",{"2":{"1110":1}}],["accumulate",{"0":{"1746":1},"2":{"1083":1,"1086":1,"1087":4,"1096":1,"1732":1,"1746":2,"1758":1}}],["accum",{"2":{"364":2,"372":1,"385":8,"399":1,"423":2}}],["acl",{"2":{"156":1,"513":1}}],["active",{"2":{"2087":1}}],["active=2",{"2":{"1284":2}}],["activities=",{"2":{"1284":1}}],["activating",{"2":{"810":1}}],["activation2",{"2":{"1205":2}}],["activation1",{"2":{"1205":2}}],["activation的就会丢弃",{"2":{"665":1}}],["activations",{"0":{"642":1}}],["activation",{"0":{"817":1,"838":1,"1099":1},"1":{"839":1,"840":1},"2":{"108":1,"135":2,"156":2,"302":1,"661":1,"837":1,"838":1,"840":1,"845":1,"849":1,"1086":1,"1104":1,"1404":1,"1441":1,"1459":1}}],["activate",{"2":{"326":1,"808":1}}],["activated",{"2":{"111":1}}],["action=",{"2":{"1215":4}}],["action",{"2":{"637":2,"638":2}}],["actually",{"2":{"446":1,"513":1}}],["act2cls",{"2":{"110":1}}],["act2fn",{"2":{"110":2}}],["act",{"2":{"110":3,"1202":4,"1205":4}}],["ai图像生成",{"2":{"2010":1}}],["ai自学之路",{"2":{"1403":1}}],["ai时代的算法学习",{"0":{"1403":1}}],["ainslie等人",{"2":{"956":1}}],["ai探索时代",{"2":{"740":1}}],["aidan",{"2":{"638":1}}],["ai的无限游戏",{"2":{"638":1}}],["ai的研究人员设计了多种lcm架构变体",{"2":{"631":1}}],["ai研究员提出全新语言建模新范式lcm",{"2":{"627":1}}],["ai研究的复杂理论",{"2":{"284":1}}],["ai不惑境",{"2":{"361":1}}],["aiwithgary",{"2":{"292":1}}],["ai寒武纪",{"2":{"292":1}}],["ai算法之道",{"2":{"233":1,"361":1,"543":1}}],["ai模型活了",{"2":{"233":1}}],["ai帝国",{"2":{"233":1}}],["ai",{"2":{"220":1,"222":1,"387":2,"625":1,"768":1,"1214":1,"1312":1,"1339":1,"1602":1,"2048":1,"2049":1,"2073":2,"2081":1}}],["ai修猫prompt",{"2":{"156":1,"513":1}}],["aigc小白入门记",{"2":{"95":1}}],["ai闲谈",{"2":{"47":2,"95":1,"387":1}}],["aaron",{"2":{"543":1}}],["aaa",{"2":{"109":1}}],["aa",{"2":{"71":1,"1082":2,"1099":2}}],["a=softmax",{"2":{"71":1,"173":1,"271":1}}],["arm架构体系与指令集优化",{"2":{"1942":1}}],["armup",{"2":{"402":1}}],["ar",{"2":{"1936":1}}],["arr3",{"2":{"1634":1,"1714":1}}],["arr2",{"2":{"1634":4,"1714":1}}],["arr1",{"2":{"1634":1,"1714":1}}],["arr",{"2":{"1330":3,"1633":14,"1634":7,"1647":5,"1667":13,"1668":5,"1670":3,"1704":1,"1705":2,"1706":3,"1710":11,"1715":4,"1924":2,"2006":3}}],["arrays",{"2":{"2077":1}}],["array>",{"2":{"1802":1,"1924":1}}],["array",{"0":{"1802":1},"2":{"735":1,"943":2,"961":2,"1072":6,"1078":6,"1083":5,"1398":11,"1715":1,"1752":1,"1795":1,"1802":3,"1924":2}}],["archive",{"2":{"1535":4}}],["architecture中提到",{"2":{"334":1}}],["architectures",{"2":{"175":1,"698":1,"1300":1}}],["architecture",{"0":{"912":1},"2":{"167":1,"259":1,"301":2,"361":1,"429":1,"450":1,"542":1,"703":1,"795":1,"1303":1,"1308":1}}],["arch",{"2":{"1308":4}}],["arctanh",{"2":{"1087":2}}],["arctan2",{"2":{"1087":2}}],["arctan",{"2":{"1087":2}}],["arcsinh",{"2":{"1087":2}}],["arcsin",{"2":{"1087":3}}],["arccosh",{"2":{"1087":2}}],["arccos",{"2":{"1087":2}}],["around",{"2":{"945":1,"965":1}}],["arbitrarily",{"2":{"592":1}}],["artifactid>mysql",{"2":{"1481":1}}],["artifactid>mybatis",{"2":{"1481":1}}],["artifactid>",{"2":{"1481":2}}],["artificial",{"2":{"1456":1,"2073":1}}],["articoder",{"2":{"95":1}}],["article",{"2":{"95":1,"156":1,"768":2}}],["arthur",{"2":{"638":1}}],["art",{"2":{"233":2}}],["arxiv",{"2":{"156":3,"233":2,"361":2,"387":3,"429":3,"432":1,"513":1,"543":1,"638":5,"713":1,"740":1,"768":3,"840":1,"1176":1,"1313":1}}],["arnold表示定理推广到了任意宽度和深度",{"2":{"155":1}}],["arnold表示定理来构建神经网络的研究",{"2":{"155":1}}],["arnold表示定理来构建神经网络",{"2":{"155":1}}],["arnold表示定理",{"2":{"155":1}}],["arnold",{"2":{"155":2,"156":1}}],["argc",{"2":{"1594":2}}],["argv",{"2":{"1590":2,"1594":2,"1650":1}}],["argparse",{"2":{"1215":2,"1257":1}}],["argwhere",{"2":{"1087":1}}],["argmin",{"2":{"1087":1}}],["argmax​y​​p",{"2":{"908":1}}],["argmaxyp",{"2":{"908":1}}],["argmax",{"2":{"128":1,"188":1,"191":1,"908":2,"1087":1,"1215":1}}],["arguments",{"2":{"1308":2}}],["argumentparser",{"2":{"1215":1}}],["argument",{"0":{"1925":1},"2":{"802":1,"1215":11,"1920":1}}],["arg",{"2":{"503":2,"1926":4}}],["args>",{"2":{"1912":1}}],["argsort",{"2":{"1087":3}}],["args=",{"2":{"422":1,"1083":1,"1566":1}}],["args",{"2":{"8":13,"76":1,"201":26,"503":5,"557":1,"558":1,"571":1,"572":1,"573":1,"700":1,"1085":1,"1087":1,"1208":1,"1214":6,"1215":12,"1227":4,"1254":2,"1345":1,"1436":1,"1590":2,"1912":4}}],["arange",{"2":{"76":1,"723":1,"819":1,"831":1,"833":1,"1217":1,"1328":3,"1330":1,"1345":2}}],["arena和vicuna",{"2":{"980":1}}],["area",{"2":{"141":1,"1693":1,"1779":4}}],["are",{"2":{"47":1,"126":1,"127":1,"156":2,"201":1,"210":1,"233":1,"245":2,"306":1,"370":3,"508":1,"513":1,"545":1,"552":1,"557":4,"592":1,"688":2,"722":1,"757":3,"981":1,"986":1,"1089":1,"1254":3,"1299":1,"1715":1,"1736":2,"1737":1,"1738":1,"2076":1,"2077":3,"2079":1,"2086":1}}],["asia",{"2":{"2011":1}}],["asinh",{"2":{"1087":2}}],["asin",{"2":{"1087":2}}],["ascii",{"2":{"1607":1,"1704":2}}],["ascii是一种有效的方式",{"2":{"678":1}}],["asdfgh",{"2":{"1487":1}}],["aspect",{"2":{"1254":1}}],["asarray",{"2":{"1083":1}}],["asynchrony",{"0":{"973":1}}],["asymptotic",{"2":{"507":2,"513":1}}],["ask",{"2":{"504":1,"513":1}}],["ast",{"2":{"1290":1}}],["astrocyte",{"2":{"487":1}}],["astrocytes",{"2":{"487":1}}],["astype",{"2":{"399":1}}],["as",{"0":{"728":1},"1":{"729":1,"730":1,"731":1,"732":1,"733":1},"2":{"42":1,"47":1,"74":1,"79":1,"83":4,"119":1,"160":1,"167":2,"201":2,"229":3,"231":3,"259":2,"315":1,"326":1,"346":1,"380":1,"396":1,"399":1,"428":4,"429":1,"472":3,"503":5,"529":3,"542":1,"543":1,"557":1,"572":2,"591":4,"688":1,"735":1,"736":1,"740":1,"801":1,"802":2,"804":1,"807":1,"808":1,"814":1,"815":1,"816":1,"839":1,"1027":1,"1083":1,"1087":14,"1211":1,"1212":1,"1213":1,"1215":4,"1216":2,"1217":2,"1218":3,"1239":1,"1243":2,"1253":2,"1254":1,"1257":3,"1278":3,"1282":1,"1284":1,"1298":1,"1300":1,"1303":1,"1345":6,"1350":1,"1398":1,"1566":1,"2062":1,"2075":1,"2079":1,"2081":1,"2082":1,"2086":2,"2087":2}}],["assembly",{"2":{"1604":1}}],["assert",{"2":{"23":1,"399":1,"591":2,"592":1,"702":1,"1216":1,"1217":1,"1218":1,"1254":3,"1273":1,"1924":1}}],["assuming",{"2":{"1233":1,"1235":1,"1236":1,"1237":1,"1238":1,"1240":1,"1246":1,"1247":1}}],["assumes",{"2":{"700":1}}],["assume",{"2":{"23":1}}],["assistant",{"2":{"986":1}}],["assignment",{"2":{"1887":2}}],["assign",{"2":{"592":1,"1214":1}}],["associated",{"2":{"488":1,"558":1}}],["associative",{"2":{"145":1,"154":1,"230":1}}],["associations",{"2":{"122":1,"145":1,"156":3}}],["along",{"2":{"1087":1}}],["aloha",{"2":{"156":1}}],["alexnet",{"2":{"775":1,"1300":1,"1303":3,"1308":1}}],["alexandra",{"2":{"638":1}}],["alex",{"2":{"638":1,"740":1}}],["also",{"2":{"591":1,"802":1,"1027":1,"1304":1,"2087":1}}],["albert没有直接用独热编码去映射到大小为",{"2":{"698":1}}],["albert的作者就认为没有必要",{"2":{"698":1}}],["albert",{"2":{"569":1,"700":1,"740":2,"844":1,"1315":2}}],["algorithm>",{"2":{"1645":1,"1719":1,"1720":1,"1721":1,"1722":1,"1736":1,"1737":1,"1738":1,"1739":1,"1741":1,"1742":1,"1743":1,"1744":1,"1749":1,"1750":1,"1751":1,"1752":1,"1754":1,"1755":1,"1756":1,"1883":1,"1914":1,"1922":1,"1933":1}}],["algorithm",{"0":{"1734":1},"2":{"543":2,"575":1,"1330":1,"1731":1,"1734":1}}],["algorithms",{"2":{"429":1,"1067":2}}],["algebraic",{"2":{"513":1}}],["alammar",{"2":{"429":1}}],["alt",{"2":{"399":1}}],["alice",{"2":{"1725":6,"1728":1,"1750":1,"1807":2,"1921":2,"1927":1}}],["alibi为代表的位置编码",{"2":{"766":1}}],["alibi都会对其给予严格的惩罚",{"2":{"765":1}}],["alibi的偏置项更像是在计算注意力分数时通过一个带坡度的滑动窗口或者掩码来直接实现注意力计算过程中的远程衰减",{"2":{"765":1}}],["alibi的动机是",{"2":{"765":1}}],["alibi通过线性偏置项",{"2":{"765":1}}],["alibi通过加入位置i",{"2":{"765":1}}],["alibi是一个很朴素",{"2":{"765":1}}],["alibi编码不是给词向量加入位置嵌入向量",{"2":{"765":1}}],["alibi编码出自论文",{"2":{"765":1}}],["alibi",{"0":{"765":1},"2":{"741":1,"765":4,"768":1}}],["alias",{"2":{"395":4}}],["alignof",{"2":{"1635":1}}],["aligned",{"2":{"235":1,"698":1}}],["alignment",{"2":{"89":1,"95":1,"173":1,"271":1,"908":1}}],["align",{"2":{"47":1,"257":1,"284":1,"292":2,"1083":2,"1087":3}}],["alphas",{"2":{"1350":5}}],["alpha是一个可学习的标量参数",{"2":{"360":1}}],["alpha",{"2":{"172":1,"347":1,"395":11,"843":1,"1086":2,"1087":13,"1180":1,"1192":1,"1193":1,"1350":14}}],["al",{"2":{"122":1,"429":4,"513":1,"638":1,"1042":1,"1131":1,"1133":1,"1137":1,"1155":1,"1186":1}}],["allclose",{"2":{"1087":1}}],["allreduce",{"2":{"1081":1}}],["alloc",{"2":{"1647":1,"1762":1}}],["allocation",{"2":{"711":1}}],["allow",{"2":{"1086":1}}],["allowed",{"2":{"572":4}}],["allows",{"2":{"5":1,"590":1,"768":1,"1254":1,"1284":1,"2073":1}}],["all",{"0":{"911":1,"1736":1},"2":{"36":2,"115":1,"233":1,"235":1,"292":1,"429":1,"432":1,"446":1,"572":7,"590":1,"591":2,"592":1,"661":1,"734":1,"747":1,"810":1,"935":1,"951":1,"976":3,"1010":1,"1086":1,"1087":3,"1104":1,"1235":1,"1236":1,"1237":1,"1238":1,"1240":1,"1246":1,"1247":1,"1308":1,"1312":1,"1329":4,"1330":1,"1732":1,"1736":6,"1737":1,"1758":1,"1917":4,"1994":1}}],["always",{"2":{"23":1,"1208":1,"1214":1,"1305":1}}],["a",{"0":{"1102":1},"2":{"19":1,"36":2,"71":2,"74":1,"76":1,"79":1,"89":1,"95":1,"109":6,"122":1,"131":2,"134":1,"141":1,"144":1,"145":2,"150":1,"156":4,"173":2,"209":3,"224":1,"229":3,"230":1,"231":3,"233":3,"257":1,"263":1,"271":2,"277":4,"289":1,"292":3,"343":4,"370":7,"375":1,"380":2,"381":1,"385":1,"395":4,"396":1,"398":1,"428":2,"429":5,"449":1,"450":1,"472":1,"477":1,"479":1,"498":1,"499":1,"513":7,"516":3,"522":1,"540":1,"543":2,"557":5,"558":2,"560":1,"564":2,"571":1,"572":4,"573":2,"575":1,"590":2,"591":8,"592":3,"638":2,"688":1,"700":1,"703":1,"711":1,"731":1,"740":3,"745":1,"760":1,"764":1,"768":3,"804":1,"810":1,"819":1,"820":4,"826":1,"829":1,"831":2,"833":6,"834":1,"908":1,"935":1,"951":1,"982":1,"983":1,"986":2,"1007":4,"1010":1,"1059":1,"1069":1,"1082":8,"1083":2,"1086":4,"1087":1,"1093":2,"1098":2,"1099":3,"1214":1,"1215":1,"1253":2,"1254":8,"1279":1,"1284":2,"1299":3,"1303":4,"1304":1,"1306":1,"1307":2,"1308":2,"1329":2,"1330":1,"1339":4,"1340":1,"1398":5,"1450":1,"1509":1,"1541":1,"1545":2,"1607":9,"1611":11,"1612":11,"1614":8,"1619":2,"1629":6,"1630":9,"1645":4,"1648":3,"1650":21,"1655":2,"1660":4,"1663":6,"1683":6,"1685":2,"1687":8,"1688":6,"1691":3,"1698":6,"1699":2,"1702":1,"1704":1,"1706":4,"1707":6,"1709":3,"1713":2,"1729":4,"1750":2,"1762":2,"1778":4,"1786":2,"1788":4,"1789":5,"1816":2,"1820":1,"1825":3,"1834":2,"1838":1,"1843":3,"1883":2,"1891":1,"1905":5,"1906":4,"1916":6,"1928":6,"1929":2,"1931":2,"1933":3,"1999":21,"2004":1,"2005":4,"2007":2,"2021":1,"2059":6,"2060":6,"2062":1,"2073":2,"2076":1,"2077":1,"2078":1,"2083":1,"2086":6,"2087":2}}],["answers",{"2":{"1332":1}}],["answer",{"2":{"1332":1}}],["answering",{"2":{"906":1,"1332":1}}],["animal",{"2":{"1654":4,"1655":2,"1659":3,"1685":22,"1690":3,"1691":18,"1693":6,"1866":7,"1869":8}}],["anil",{"2":{"1194":1}}],["anisotropic",{"2":{"711":1}}],["angle",{"2":{"1087":1}}],["anns",{"2":{"1456":1}}],["annotations",{"2":{"1250":2}}],["annotated",{"2":{"422":1,"432":4}}],["anne",{"2":{"638":1}}],["anthropic",{"2":{"513":1}}],["anti",{"2":{"437":1}}],["antriebsradsystem",{"2":{"370":1}}],["another",{"2":{"160":1}}],["andrej",{"2":{"279":1,"284":1,"292":1}}],["and",{"0":{"354":1,"669":1,"938":1,"954":1,"968":1,"973":2,"979":1,"1280":1,"1354":1,"1422":1,"1449":1,"1613":1,"1614":1,"1648":1},"1":{"969":1,"970":1,"971":1,"972":1,"980":1,"981":1,"982":1,"983":1},"2":{"23":1,"36":1,"38":1,"39":1,"47":2,"74":1,"79":1,"82":3,"91":1,"95":1,"99":1,"136":1,"145":1,"148":1,"156":5,"167":3,"201":1,"230":1,"233":1,"235":1,"257":1,"259":3,"277":1,"284":1,"292":3,"293":1,"313":3,"326":1,"334":2,"343":1,"344":1,"348":1,"361":2,"380":1,"385":1,"398":1,"399":1,"419":1,"428":1,"429":2,"450":3,"472":1,"485":1,"487":1,"490":1,"498":1,"504":1,"513":3,"516":1,"522":1,"523":1,"528":1,"533":1,"542":1,"543":1,"557":1,"567":3,"571":1,"590":3,"591":4,"592":1,"597":1,"624":1,"638":2,"703":3,"723":3,"740":2,"768":4,"801":6,"802":3,"820":1,"935":1,"944":1,"945":1,"947":1,"951":1,"965":1,"981":2,"1036":1,"1046":1,"1059":1,"1067":1,"1082":1,"1083":11,"1085":3,"1087":7,"1130":1,"1195":4,"1208":1,"1215":2,"1217":1,"1227":3,"1254":7,"1284":3,"1300":1,"1302":2,"1303":6,"1304":1,"1306":1,"1307":2,"1308":1,"1328":1,"1329":3,"1330":2,"1350":2,"1459":1,"1462":1,"1485":2,"1619":1,"1698":2,"1699":2,"1715":1,"1807":2,"1825":1,"1843":1,"1920":1,"1930":1,"1933":1,"2073":3,"2075":1,"2077":1,"2078":2,"2079":9,"2081":1,"2082":2,"2083":2,"2086":5,"2087":2,"2153":1}}],["analyzer",{"2":{"2017":1}}],["analyzing",{"2":{"20":1,"47":1,"156":1,"437":1,"513":1}}],["analysis为例来看看有关研究思路",{"2":{"475":1}}],["analysis",{"0":{"480":1},"2":{"18":1,"20":2,"47":1,"131":1,"475":2,"711":1,"906":1,"2079":2}}],["an",{"2":{"18":1,"20":2,"47":2,"170":1,"277":8,"398":1,"405":1,"407":6,"408":8,"409":2,"427":3,"445":1,"453":2,"513":1,"516":3,"537":2,"557":2,"572":2,"590":1,"592":1,"688":1,"737":1,"740":2,"802":1,"834":1,"840":1,"1054":1,"1067":1,"1226":1,"1254":1,"1713":1,"1762":2}}],["any>",{"2":{"1928":2}}],["anymore",{"2":{"592":1}}],["anything",{"2":{"573":1}}],["any",{"0":{"1737":1,"1928":1},"2":{"8":1,"344":1,"591":1,"723":1,"768":1,"1082":1,"1085":41,"1086":1,"1087":6,"1214":21,"1226":2,"1227":6,"1732":1,"1737":2,"1920":1,"1928":7,"1932":1}}],["atanh",{"2":{"1087":2}}],["atan2",{"2":{"1087":2}}],["atan",{"2":{"1087":2}}],["atol",{"2":{"1087":3}}],["ate",{"2":{"277":1,"398":1,"405":1,"407":7,"408":8,"409":2,"427":4,"445":2,"453":2,"516":3,"537":3,"1820":1,"1838":1}}],["atrous",{"0":{"778":1},"2":{"204":1}}],["attetion",{"2":{"1343":1}}],["attenion",{"2":{"344":1,"923":1}}],["attentive",{"2":{"289":1,"292":2,"760":1,"768":1}}],["attentions",{"2":{"1404":1}}],["attention或striped",{"2":{"977":1}}],["attention或前馈神经网络",{"2":{"344":1}}],["attention允许模型的不同表示子空间联合关注不同位置的信息",{"2":{"928":1}}],["attention可以描述为将query和一组",{"2":{"916":1}}],["attention可能会降低下游任务性能",{"2":{"732":1}}],["attention子层",{"2":{"915":1}}],["attention堆叠和point",{"2":{"912":1}}],["attention来计算输入和输出表示而不使用序列对齐rnn或卷积的转导模型",{"2":{"911":1}}],["attention来更新状态",{"2":{"287":1}}],["attention矩阵的基础上加一个可训练的偏置项",{"2":{"762":1}}],["attention计算原理一样",{"2":{"650":1,"932":1}}],["attention计算过程的不同之处在于score矩阵送入到softmax计算weight矩阵先进行一步mask操作",{"2":{"71":1}}],["attention加入到target端得到的attention中",{"2":{"649":1,"931":1}}],["attention首先分别在source端和target端进行自身的attention",{"2":{"649":1,"931":1}}],["attention与传统的attention机制非常的不同",{"2":{"649":1,"931":1}}],["attention与上面描述的multi",{"2":{"71":1}}],["attention结果的分布表明了该模型学习到了一些语法和语义信息",{"2":{"512":1}}],["attention结构中",{"2":{"71":1}}],["attention模型更可解释",{"2":{"512":1}}],["attention模块中",{"2":{"445":1}}],["attention模块",{"2":{"445":1}}],["attention做信息的提取和聚合",{"2":{"510":1}}],["attention后",{"2":{"419":1}}],["attention后面的cross",{"2":{"72":1}}],["attention层中",{"2":{"419":1}}],["attention层前放置了7层线性注意力",{"2":{"214":1}}],["attention→add",{"2":{"330":1}}],["attention和transformer",{"2":{"361":1}}],["attention和intra",{"2":{"287":1}}],["attention和bahdanau",{"2":{"285":1}}],["attention进行对齐",{"2":{"287":1}}],["attention用的是双向rnn单元",{"2":{"285":1}}],["attention使用lstm",{"2":{"285":1}}],["attention使用先前的隐状态",{"2":{"285":1}}],["attention使用当前解码器的隐状态来计算对齐向量",{"2":{"285":1}}],["attention对注意力机制的计算方式进行多样性探索",{"2":{"285":1}}],["attention是message",{"2":{"509":1}}],["attention是attention的开创者之一",{"2":{"284":1}}],["attention是通过进行tiling",{"2":{"216":1}}],["attention等概念",{"2":{"235":1}}],["attention提出了一种",{"2":{"217":1}}],["attention将q",{"2":{"216":1}}],["attention机制",{"2":{"914":1}}],["attention机制已经成为序列建模和转导模型不可或缺的一部分",{"2":{"911":1}}],["attention机制提供了一种直观的方式来可视化和理解模型是如何关注序列中不同部分的",{"2":{"512":1}}],["attention机制的设计应该使得ai",{"2":{"194":1}}],["attention机制中的相关性计算和加权求和步骤可以看作是对输入向量空间的一种动态变换",{"2":{"173":1}}],["attention就退化成一个point",{"2":{"172":1}}],["attention里面使用到的",{"2":{"78":1}}],["attention里面会用到sequence",{"2":{"78":1}}],["attention不需要sequence",{"2":{"78":1}}],["attention需要也加一个attention",{"2":{"72":1}}],["attention的实现过程中每层至少需要一次跨节点通信",{"2":{"977":1}}],["attention的输出",{"2":{"920":1}}],["attention的输入是ctctc",{"2":{"285":2}}],["attention的不可或缺的作用",{"2":{"446":1}}],["attention的具体区别如下",{"2":{"285":1}}],["attention的具体实现",{"2":{"233":1}}],["attention的兼容性",{"2":{"204":1}}],["attention的目的是让源序列与目标序列对齐",{"2":{"200":1}}],["attention的目的是找到目标序列自身的关系",{"2":{"200":1}}],["attention的设计动机",{"2":{"71":1}}],["attention的q",{"2":{"71":1}}],["attention的作用到底是什么",{"2":{"47":1}}],["attention网络结构和代码解析",{"2":{"47":1}}],["attention",{"0":{"89":1,"216":1,"284":1,"285":1,"287":1,"288":1,"289":1,"290":1,"502":1,"649":1,"650":1,"891":1,"892":1,"911":1,"916":1,"920":1,"925":1,"930":1,"931":1,"932":1,"935":1,"936":1,"942":1,"945":1,"947":1,"949":1,"951":1,"952":1,"955":1,"962":1,"965":1,"968":1,"973":1,"976":1,"979":1},"1":{"892":1,"917":1,"918":1,"919":1,"920":1,"921":1,"922":1,"923":1,"924":1,"926":1,"927":1,"928":1,"929":1,"933":1,"934":1,"937":1,"953":1,"956":1,"957":1,"969":1,"970":1,"971":1,"972":1,"980":1,"981":1,"982":1,"983":1},"2":{"5":1,"7":3,"8":1,"9":1,"12":1,"16":2,"17":2,"18":1,"19":1,"20":3,"33":1,"35":1,"36":2,"41":3,"42":2,"43":2,"47":9,"49":1,"50":1,"57":1,"62":1,"67":3,"71":2,"76":1,"77":3,"78":3,"79":1,"83":1,"84":3,"89":6,"90":1,"91":1,"95":2,"97":1,"115":3,"117":2,"119":4,"130":1,"157":1,"160":1,"170":1,"173":4,"175":7,"186":4,"194":2,"197":2,"198":3,"199":2,"201":7,"204":10,"206":2,"209":6,"210":4,"211":1,"212":1,"214":1,"216":3,"217":5,"233":11,"235":2,"257":1,"260":1,"263":1,"267":1,"271":1,"285":3,"287":2,"288":5,"289":2,"290":1,"292":14,"320":1,"330":1,"334":1,"344":6,"348":1,"349":2,"361":2,"382":1,"394":2,"409":1,"420":2,"429":1,"432":2,"434":1,"442":1,"443":1,"444":1,"445":1,"446":1,"449":2,"500":2,"503":3,"507":1,"510":2,"513":2,"517":1,"525":1,"533":1,"535":7,"538":1,"542":1,"616":1,"620":1,"621":1,"624":1,"646":1,"647":1,"649":2,"698":6,"719":1,"731":1,"732":1,"734":2,"735":2,"747":1,"759":1,"763":1,"765":3,"768":3,"892":3,"910":2,"911":2,"915":1,"918":2,"923":1,"924":1,"927":1,"931":2,"933":2,"935":2,"938":2,"945":2,"947":1,"949":1,"951":2,"954":2,"960":1,"965":2,"970":1,"974":7,"975":1,"976":2,"979":1,"987":2,"1216":2,"1217":7,"1218":4,"1312":1,"1315":1,"1331":2,"1339":1,"1341":1,"1343":1,"1344":2,"1345":2,"1363":1,"1364":1}}],["attenton",{"2":{"287":1}}],["attentyion",{"2":{"38":1,"39":2}}],["attend",{"2":{"5":1,"74":2,"79":1,"198":1}}],["attributions",{"2":{"479":1}}],["attribution",{"0":{"134":1},"2":{"96":1,"133":1,"134":1,"156":3,"437":1}}],["attn和feed",{"2":{"533":1}}],["attn和src",{"2":{"533":1}}],["attn和self",{"2":{"344":1}}],["attn成员变量",{"2":{"533":1}}],["attn如何转换为z",{"2":{"344":1}}],["attn把目标隐状态转换成新的目标隐状态",{"2":{"200":1}}],["attn把目标序列转换为目标隐状态",{"2":{"200":1}}],["attn对value向量进行加权求和",{"2":{"199":1}}],["attn进行dropout操作",{"2":{"199":1}}],["attn是多头注意力",{"2":{"199":1}}],["attn是自注意力",{"2":{"199":1}}],["attn的实现完全一样",{"2":{"533":1}}],["attn的功能",{"2":{"344":1}}],["attn的形状如下",{"2":{"199":1}}],["attn的shape为",{"2":{"36":1}}],["attn",{"2":{"23":1,"36":1,"38":2,"39":4,"66":1,"67":5,"74":2,"78":1,"82":4,"198":3,"199":6,"200":13,"344":8,"354":1,"382":2,"394":5,"449":5,"461":3,"503":28,"523":7,"529":2,"533":15,"538":2,"703":4,"933":5,"1216":10,"1218":18}}],["at",{"0":{"154":1},"2":{"5":1,"18":1,"20":2,"47":1,"96":1,"130":2,"154":1,"156":2,"226":1,"233":1,"334":2,"370":1,"395":3,"557":3,"558":1,"590":1,"591":1,"679":1,"765":1,"766":1,"1196":1,"1303":1,"1329":1,"1330":3,"1700":2,"1713":7,"1714":1,"1719":1,"1737":1,"1738":1,"1752":1,"1754":1,"1756":2,"1797":6,"1821":1,"1839":1,"1922":1,"2063":1,"2077":1}}],["势必导致虽然最终生成的向量可以在该空间上有效将人类概念进行映射",{"2":{"4":1}}],["概率和统计",{"2":{"2009":1}}],["概率响应曲面模型拥有它自己的真实超参数",{"2":{"1185":1}}],["概率",{"2":{"1016":1}}],["概率或1",{"2":{"1016":1}}],["概率未归一化的",{"2":{"762":1}}],["概率输出",{"0":{"471":1},"1":{"472":1,"473":1}}],["概率的使用",{"2":{"183":1}}],["概要",{"0":{"363":1,"674":1}}],["概念约束等进阶技能",{"2":{"1961":1}}],["概念和作用",{"2":{"1619":1,"1620":1,"1621":1}}],["概念理解",{"0":{"1437":1},"1":{"1438":1,"1439":1}}],["概念在高维向量空间中得到了充分表征",{"2":{"691":1}}],["概念不仅相互关联",{"2":{"689":1}}],["概念信息分布在整个向量上",{"2":{"683":1}}],["概念的形成过程始于输入嵌入层",{"2":{"674":1}}],["概念嵌入空间中推理的可视化",{"2":{"629":1}}],["概念",{"0":{"329":1,"406":1,"676":1,"769":1,"2114":1},"2":{"293":1,"627":3,"628":3,"632":1,"689":1,"840":1,"841":1,"938":1,"954":1,"1478":1,"1611":1,"1645":1,"1646":1,"1647":1,"1648":1,"1649":1,"1650":1}}],["概念最早由bengio在2014年的论文",{"2":{"237":1}}],["概念等的认知和理解",{"2":{"121":1}}],["概念是被配置为能够跨任务工作的向量",{"2":{"4":1}}],["概述",{"0":{"1":1,"50":1,"97":1,"158":1,"235":1,"294":1,"389":1,"430":1,"545":1,"742":1,"1021":1,"1052":1,"1312":1},"1":{"431":1,"432":1},"2":{"0":1,"49":1,"96":1,"157":1,"293":1,"741":1}}],["ecosystem",{"0":{"2079":1}}],["ecs",{"2":{"1937":1}}],["echo",{"2":{"731":1,"1510":1,"1993":1}}],["ehtan",{"2":{"1481":1}}],["eof",{"2":{"1814":2,"1832":2}}],["eo1=12",{"2":{"1393":1}}],["eos>`",{"2":{"385":2,"398":1,"557":1}}],["eos>在词典中的index",{"2":{"384":1,"558":1}}],["eos>这些也算是句子的token",{"2":{"380":1}}],["eos>",{"2":{"380":5,"384":2,"398":1,"428":1,"558":2}}],["eos",{"2":{"65":1,"80":2,"241":1,"384":3,"431":1,"515":1,"558":3,"571":2,"572":4,"899":3}}],["eq",{"2":{"1085":1,"1087":4,"1215":1,"1331":1}}],["equivariance",{"2":{"772":1}}],["equivariant",{"2":{"772":2}}],["equivalent",{"2":{"346":1,"591":1,"810":2,"1087":2}}],["equal",{"0":{"1756":1},"2":{"801":1,"802":1,"820":1,"1087":15,"1114":2,"1715":1,"1732":1,"1756":2}}],["equals",{"2":{"23":1}}],["equation",{"2":{"498":2}}],["equations",{"0":{"497":1},"2":{"493":1,"497":1,"513":2}}],["eye",{"2":{"1070":1,"1101":1}}],["ey",{"2":{"1002":2}}],["e​o1​​=​2​​1​​",{"2":{"1393":1}}],["e​total​​=e​o1​​+e​o2​​",{"2":{"1393":1}}],["e​total​​=e​o1​​+e​o2​​=​2​​1​​",{"2":{"1389":1}}],["e​total​​",{"2":{"1392":1}}],["e​x​b​​−m",{"2":{"961":1}}],["e​m",{"2":{"943":2,"961":2}}],["ew​l​​",{"2":{"1003":1}}],["ewl",{"2":{"1003":1}}],["ew",{"2":{"760":1}}],["ewk",{"2":{"760":1}}],["eg",{"2":{"642":2,"781":1,"840":1}}],["e下计算下一个字节的熵",{"2":{"613":1}}],["especially",{"2":{"2073":1,"2077":1}}],["escape",{"0":{"1616":1}}],["esc",{"2":{"1519":1,"1541":1}}],["es",{"2":{"576":1}}],["estimation",{"2":{"713":1,"740":1}}],["est",{"2":{"567":1,"584":1,"585":1,"587":1,"588":1}}],["essen",{"2":{"370":1}}],["edge",{"2":{"2081":1}}],["edges",{"2":{"1254":1}}],["eda工具开发专家",{"2":{"1958":1}}],["eda工具开发",{"2":{"1956":1}}],["ed",{"2":{"567":2,"576":1,"598":1}}],["eddie",{"2":{"513":1}}],["education",{"0":{"2083":1}}],["edu",{"2":{"432":1,"740":1}}],["edition",{"2":{"1433":1}}],["editing",{"2":{"123":1,"141":1,"143":1,"144":1,"145":1,"156":6}}],["editor",{"2":{"141":1}}],["edit",{"2":{"141":2}}],["e6",{"2":{"429":2}}],["e9",{"2":{"429":1}}],["e7",{"2":{"429":2}}],["e8",{"2":{"429":1}}],["e501",{"2":{"571":1}}],["e5",{"2":{"429":1}}],["etc",{"2":{"1506":1,"1507":1,"1987":1,"2095":1}}],["ethan",{"2":{"1481":5,"1485":3,"1486":1,"1487":1,"2021":1,"2034":1,"2111":1}}],["etotal=eo1+eo2e",{"2":{"1393":1}}],["etotal=eo1+eo2=12",{"2":{"1389":1}}],["etotale",{"2":{"1392":1}}],["eta",{"2":{"1188":1,"1189":1,"1190":1,"1191":1,"1243":5,"1244":4,"1395":1}}],["et",{"2":{"429":4,"513":1,"638":1,"1042":1,"1131":1,"1133":1,"1137":1,"1155":1,"1186":1}}],["either",{"2":{"1308":1}}],["eigenvectors=false",{"2":{"1083":1}}],["eig",{"2":{"1083":1}}],["eichenbaum",{"2":{"490":1}}],["einsum",{"2":{"1217":2}}],["einer",{"2":{"370":1}}],["einem",{"2":{"370":1,"558":1,"679":1}}],["ein",{"2":{"370":5,"558":1,"679":1}}],["eieie",{"2":{"326":2}}],["easier",{"2":{"2076":1,"2079":1}}],["easy",{"0":{"2078":1},"2":{"688":1,"2075":1,"2078":1,"2083":1}}],["ea",{"2":{"1939":1}}],["eat",{"2":{"1654":2,"1659":2}}],["eating",{"2":{"257":1,"1654":1,"1659":1}}],["each",{"0":{"1739":1},"2":{"343":1,"522":1,"591":1,"834":1,"1329":3,"1330":2,"1732":1,"1739":2,"1758":1,"1883":4,"2086":3}}],["e",{"2":{"224":1,"320":1,"326":1,"579":3,"580":1,"582":14,"583":14,"585":1,"591":1,"598":1,"640":1,"698":1,"760":1,"899":1,"986":1,"1002":3,"1003":17,"1004":1,"1124":1,"1195":1,"1254":2,"1308":2,"1330":1,"1392":3,"1393":2,"1394":11,"1395":1,"1398":1,"1481":2,"1566":3,"1624":3,"1713":5,"1715":1,"1762":3,"1763":5,"1825":3,"1843":3,"1902":2,"1993":1,"2062":2,"2063":2}}],["e−δie−δie^",{"2":{"191":1}}],["evolve",{"2":{"2087":1}}],["evict",{"2":{"986":1}}],["everytime",{"2":{"1254":1}}],["everything",{"2":{"259":1}}],["every",{"2":{"592":2,"1284":1,"1303":1,"1329":2,"1330":2}}],["even",{"2":{"181":1,"1736":4,"1737":3,"1738":4}}],["evaluation",{"2":{"1315":2}}],["evaluate",{"2":{"1308":2,"1331":1}}],["eval时候用",{"2":{"665":1}}],["eval",{"0":{"1122":1},"2":{"83":3,"423":2,"424":3,"428":1,"472":1,"529":1,"1116":1,"1122":4,"1214":2,"1215":1,"1269":1,"1332":1}}],["eln",{"2":{"974":1}}],["elliot",{"2":{"1090":1}}],["ellipsis",{"2":{"1087":63}}],["ell",{"2":{"943":7,"961":7}}],["ello",{"2":{"595":1}}],["elu",{"0":{"843":2},"2":{"843":5}}],["elus",{"2":{"156":1}}],["elmo是分别以𝑃",{"2":{"721":1}}],["elmo使用独立训练的从左到右和从右到左lstm的连接来为下游任务生成特征",{"2":{"721":1}}],["elmo在拼接正反向lstm输出时",{"2":{"720":1}}],["elmo模型通过将这多层双向lstm的隐藏状态和字符嵌入以某种方式组合在一起",{"2":{"718":1}}],["elmo模型的双向语言模型",{"2":{"717":1}}],["elmo不仅学会了单词的word",{"2":{"718":1}}],["elmo不是对每个单词使用固定的嵌入",{"2":{"717":1}}],["elmo用了一个典型的两阶段过程来做预训练",{"2":{"718":1}}],["elmo再依据单词的上下文来调整单词的word",{"2":{"717":1}}],["elmo首先使用语言模型学好一个单词的word",{"2":{"717":1}}],["elmo的全名就是embeddings",{"2":{"718":1}}],["elmo的本质思想是",{"2":{"717":1}}],["elmo的作者认为在理想情况下",{"2":{"689":1}}],["elmo",{"0":{"716":1},"1":{"717":1,"718":1},"2":{"716":1}}],["elision",{"2":{"1931":1}}],["elif",{"2":{"702":1,"1215":1}}],["eliza",{"2":{"638":1}}],["elimination",{"2":{"305":1}}],["eliminate",{"2":{"305":1}}],["elapsed",{"2":{"385":1}}],["elem",{"2":{"1797":2}}],["elements",{"2":{"590":1,"1086":1,"1714":2,"1736":2,"1737":1,"1738":1,"1797":2,"1799":1,"1800":1,"1801":1,"1802":1,"1806":2}}],["elementwise",{"2":{"503":1}}],["element",{"0":{"1752":1},"2":{"213":1,"642":1,"1083":1,"1087":1,"1714":2,"1732":1,"1737":1,"1738":1,"1752":4,"1754":2,"1755":2,"1797":8,"1914":2}}],["eleutherai",{"2":{"1316":1}}],["electra",{"2":{"1315":2}}],["ele",{"2":{"1086":1}}],["elena",{"2":{"638":1}}],["elele",{"2":{"264":1}}],["elhage∗",{"2":{"233":1}}],["elhage重构了注意力头的表达形式",{"2":{"169":1}}],["elseif",{"2":{"1985":1}}],["else的分支",{"2":{"576":1}}],["else",{"2":{"8":1,"201":1,"374":1,"375":2,"395":2,"422":1,"503":1,"557":1,"590":2,"591":1,"592":1,"700":1,"702":1,"723":2,"1076":1,"1102":1,"1211":1,"1215":1,"1254":3,"1255":1,"1330":1,"1436":1,"1566":1,"1608":2,"1611":1,"1619":8,"1630":2,"1631":1,"1632":1,"1677":1,"1683":1,"1684":1,"1695":2,"1715":2,"1719":1,"1720":1,"1721":1,"1722":1,"1724":1,"1725":1,"1729":2,"1754":1,"1761":1,"1806":1,"1807":1,"1814":2,"1820":3,"1825":4,"1832":2,"1838":3,"1843":4,"1874":1,"1887":1,"1911":1,"1922":1,"1927":4,"2060":1}}],["emmm",{"2":{"2140":1}}],["emitting",{"2":{"1329":1}}],["emissions",{"2":{"1328":4,"1329":7,"1330":6}}],["emission",{"2":{"1323":1,"1328":2,"1329":2,"1330":5}}],["email",{"2":{"2069":1}}],["ema",{"2":{"1168":2}}],["emergence",{"2":{"513":1}}],["emerged",{"2":{"167":1,"259":1}}],["emebdding",{"2":{"460":1}}],["em",{"2":{"456":1,"943":2,"961":2}}],["empowering",{"2":{"2087":1}}],["empowers",{"2":{"431":1,"455":1,"456":1,"460":1,"480":1,"513":1,"698":1}}],["employee",{"2":{"1657":3}}],["employed",{"2":{"399":1,"2081":1}}],["empirical",{"2":{"499":1}}],["empty",{"2":{"395":2,"423":2,"428":1,"472":1,"529":1,"702":1,"1071":2,"1087":3,"1214":1,"1226":1,"1695":2,"1704":1,"1713":2,"1723":1,"1726":3,"1933":1}}],["embt",{"2":{"731":1}}],["emb1",{"2":{"731":1}}],["embd的嵌入矩阵",{"2":{"463":1}}],["embdding",{"2":{"334":1}}],["emb",{"2":{"201":1,"503":2,"723":1,"1345":2}}],["embed采用mean",{"2":{"735":1}}],["embed提出了一个潜在注意力层",{"2":{"735":1}}],["embeder",{"2":{"732":1}}],["embeder将llm的最后一层hidden",{"2":{"731":1}}],["embeds",{"2":{"723":5}}],["embeddng",{"2":{"538":1}}],["embedding在仅解码器的llm嵌入模型中更受欢迎",{"2":{"735":1}}],["embedding能有效将单词的位置关系引入到模型中",{"2":{"722":1}}],["embedding更能表达在这个上下文中的具体含义",{"2":{"717":1}}],["embedding表示",{"2":{"717":1}}],["embedding时",{"2":{"717":1}}],["embedding中已经有了一定的语义信息",{"2":{"717":1}}],["embedding中查找token对应的嵌入向量",{"2":{"699":1}}],["embedding动态调整",{"2":{"717":1}}],["embedding无法解决多义词的问题",{"2":{"716":1}}],["embedding之后",{"2":{"714":1}}],["embedding很可能就是使用同样的模型",{"2":{"710":1}}],["embedding应该尽可能保留文本的语义信息",{"2":{"710":1}}],["embedding则是以文本为基本单位的",{"2":{"710":1}}],["embedding就是将文本转成一组固定维度的向量表示",{"2":{"710":1}}],["embedding过程也是查表过程",{"2":{"709":1}}],["embedding矩阵就是模型训练后的产出",{"2":{"706":1}}],["embedding矩阵中有且仅有一行被激活",{"2":{"700":1}}],["embedding类定义如下",{"2":{"702":1}}],["embedding作为词嵌入层",{"2":{"701":1}}],["embedding查表操作其实就是把",{"2":{"700":1}}],["embedding对应的实现",{"2":{"699":1}}],["embedding嵌入模型经过矩阵算法的优化",{"2":{"696":1}}],["embedding层就是以one",{"2":{"694":1}}],["embedding层对每个token进行embedding编码",{"2":{"445":1}}],["embedding向量是密集的",{"2":{"694":1}}],["embedding向量的大小取决于模型维度",{"2":{"457":1}}],["embedding的数学本质是以one",{"2":{"722":1}}],["embedding的来源",{"0":{"706":1}}],["embedding的输出是一个3x10x512的张量",{"2":{"704":1}}],["embedding的输入是token",{"2":{"704":1}}],["embedding的输入是",{"2":{"702":1}}],["embedding的使用代码示例如下",{"2":{"702":1}}],["embedding的参数本身就是模型参数的一部分",{"2":{"702":1}}],["embedding的特例",{"2":{"694":1}}],["embedding的组合",{"2":{"450":2}}],["embedding可能会受到近因偏差的影响",{"2":{"735":1}}],["embedding可能无法解释",{"2":{"690":1}}],["embedding可以为后续的计算提供更加丰富和表达能力的输入特征",{"2":{"676":1}}],["embedding模型很多是单独训练的",{"2":{"718":1}}],["embedding模型是否可以判断9",{"2":{"560":1}}],["embedding模块将token代表的数字转换为embedding向量",{"2":{"431":1}}],["embedding扮演着举足轻重的角色",{"2":{"545":1}}],["embedding张量来说",{"2":{"520":1,"530":1}}],["embedding序列",{"2":{"516":1}}],["embedding并生成相关隐向量",{"2":{"515":1}}],["embedding输入解码器",{"2":{"515":1}}],["embedding被转换为三个不同的向量",{"2":{"463":1}}],["embedding是以token为基本单位",{"2":{"710":1}}],["embedding是一个",{"2":{"460":1}}],["embedding是每个词元的固定向量表示",{"2":{"457":1}}],["embedding也叫word",{"2":{"458":1}}],["embedding通常是查表操作",{"2":{"458":1}}],["embedding会将文字转换成模型可以理解和处理的数学表示",{"2":{"458":1}}],["embedding负责把token编码",{"2":{"457":1}}],["embedding化",{"0":{"457":1},"1":{"458":1,"459":1,"460":1,"463":1,"464":1,"465":1}}],["embedding生成模块",{"2":{"450":2}}],["embedding维度的大小",{"2":{"523":1}}],["embedding维度",{"2":{"449":1}}],["embedding维度映射到词表长度",{"2":{"397":1}}],["embedding和pe相加其实是一种特征交叉",{"2":{"751":1}}],["embedding和position",{"2":{"450":2}}],["embedding和positional",{"2":{"449":1,"457":1,"460":1,"518":1}}],["embedding和word",{"2":{"710":1}}],["embedding和位置编码来生成word",{"2":{"519":1}}],["embedding和位置编码相加",{"2":{"450":2,"455":1,"698":1}}],["embedding和位置编码之后",{"2":{"71":1}}],["embedding加权平均得到的结果",{"2":{"437":1}}],["embedding或分布之间的相似性或距离",{"2":{"209":1}}],["embeddings作为condition",{"2":{"1363":1}}],["embeddings等技术解析j",{"2":{"740":1}}],["embeddings是两个句子的区分标识",{"2":{"722":1}}],["embeddings是单词的词向量",{"2":{"722":1}}],["embeddings类中最主要逻辑由nn",{"2":{"702":1}}],["embeddings类定义如下",{"2":{"701":1}}],["embeddings=false",{"2":{"700":1}}],["embeddings的特性",{"2":{"475":1}}],["embeddings对应上图的数字标号8",{"2":{"449":1}}],["embeddings对应上图的数字标号6",{"2":{"449":1}}],["embeddings",{"2":{"201":2,"449":4,"460":1,"628":1,"688":2,"692":1,"700":6,"701":2,"702":9,"703":4,"722":1,"723":21,"736":2,"740":5,"751":1,"768":1,"834":1,"2079":1}}],["embedding",{"0":{"458":1,"460":1,"673":1,"688":1,"702":1,"722":1,"728":1,"735":1,"834":1,"1334":1},"1":{"674":1,"675":1,"676":1,"677":1,"678":1,"679":1,"680":1,"681":1,"682":1,"683":1,"684":1,"685":1,"686":1,"687":1,"688":1,"689":2,"690":2,"691":2,"692":2,"693":1,"694":1,"695":1,"696":1,"697":1,"698":1,"699":1,"700":1,"701":1,"702":1,"703":1,"704":1,"705":1,"706":1,"707":1,"708":1,"709":1,"710":1,"711":1,"712":1,"713":1,"714":1,"715":1,"716":1,"717":1,"718":1,"719":1,"720":1,"721":1,"722":1,"723":1,"724":1,"725":1,"726":1,"727":1,"728":1,"729":2,"730":2,"731":2,"732":2,"733":2,"734":1,"735":1,"736":1,"737":1,"738":1,"739":1,"740":1,"1335":1,"1336":1,"1337":1,"1338":1,"1339":1,"1340":1,"1341":1,"1342":1,"1343":1,"1344":1,"1345":1},"2":{"4":2,"10":2,"12":6,"16":1,"27":1,"28":1,"33":1,"83":1,"172":4,"198":1,"289":1,"292":2,"326":6,"394":1,"420":2,"431":1,"448":1,"450":4,"454":2,"455":2,"457":1,"458":1,"460":3,"473":1,"513":1,"515":1,"518":3,"519":1,"520":1,"522":1,"530":1,"538":1,"545":2,"671":1,"674":2,"676":6,"688":1,"694":2,"696":3,"698":8,"700":18,"701":5,"702":18,"704":1,"706":1,"707":1,"709":4,"711":1,"716":1,"717":1,"718":2,"722":1,"723":6,"724":1,"735":3,"737":1,"740":8,"749":1,"751":1,"767":1,"768":1,"808":5,"834":15,"1217":8,"1218":7,"1336":1,"1341":3,"1343":1}}],["embeding",{"2":{"471":1}}],["embed将对再传入的输出进行embedding和位置编码",{"2":{"450":1}}],["embed将对输入进行embedding和位置编码",{"2":{"450":1}}],["embed是nn",{"2":{"449":2}}],["embed和",{"2":{"449":1}}],["embed",{"2":{"82":2,"83":1,"119":1,"402":1,"424":1,"449":1,"450":10,"473":1,"503":13,"529":1,"538":2,"703":8,"735":3,"740":1,"1087":1,"1217":21}}],["emnlp",{"2":{"156":2,"513":1}}],["erase",{"2":{"1713":2,"1719":2,"1720":1,"1721":1,"1722":1,"1724":3,"1725":3}}],["er",{"2":{"567":1}}],["erfinv",{"2":{"1087":2}}],["erfc",{"2":{"1087":2}}],["erf",{"2":{"106":4,"1087":2}}],["errors=",{"2":{"591":1,"592":1}}],["error",{"0":{"844":1},"2":{"103":1,"106":2,"156":1,"572":2,"1083":11,"1214":1,"1566":5,"1728":1,"1761":1,"1762":8,"1763":5,"1821":1,"1825":2,"1839":1,"1843":2,"1902":1,"1922":1,"1930":1,"1933":2,"2062":2}}],["effortlessly",{"2":{"2081":1}}],["efficientnet",{"2":{"1308":8}}],["efficient",{"2":{"139":1,"150":1,"156":1,"185":1,"232":1,"233":2,"348":1,"361":1,"393":1,"429":3,"637":1,"638":1,"713":1,"733":1,"740":1,"974":1}}],["efficiency",{"0":{"955":1},"1":{"956":1,"957":1},"2":{"90":1}}],["effectiveness",{"2":{"393":1,"429":1}}],["effective",{"2":{"47":1,"285":1,"292":1,"1501":1}}],["eps",{"2":{"201":3,"343":5,"346":5,"723":1,"807":2,"808":4,"809":2,"810":2,"1087":2}}],["eps=config",{"2":{"723":1}}],["eps=args",{"2":{"201":2}}],["eps=params",{"2":{"201":1}}],["eps=1e",{"2":{"83":1,"343":1,"346":1,"402":1,"423":1,"424":1,"503":1}}],["epsilonϵ",{"2":{"1130":1,"1174":1}}],["epsilon",{"2":{"289":1,"346":2,"640":1,"1143":1,"1144":1,"1191":1,"1192":1,"1193":1}}],["epsilon在注意力均匀分布在可达token时达到最大值",{"2":{"93":1}}],["epsilon解释为注意力在可达token之间的",{"2":{"93":1}}],["epsilon越小",{"2":{"93":1}}],["epsilon上单调递减",{"2":{"93":1}}],["epsilon^r",{"2":{"93":1}}],["epoch=len",{"2":{"1242":1}}],["epoch=",{"2":{"1233":1,"1234":1,"1235":1,"1236":1,"1237":1,"1238":1,"1239":1,"1240":1}}],["epoch+1",{"2":{"1218":1,"1243":1,"1295":1,"2086":1}}],["epochs=10",{"2":{"1242":1,"1296":1}}],["epochs",{"2":{"364":1,"372":1,"423":1,"1215":2,"1218":3,"1239":1,"1240":1,"1242":2,"1243":4,"1295":3,"1303":1,"1308":3,"1332":1,"2086":6}}],["epoch",{"2":{"83":3,"364":3,"381":2,"385":5,"399":4,"423":13,"424":3,"538":1,"1133":1,"1211":1,"1215":5,"1218":2,"1231":3,"1233":6,"1234":2,"1235":4,"1236":4,"1237":6,"1238":6,"1239":3,"1240":7,"1241":1,"1242":3,"1243":2,"1244":3,"1245":1,"1246":6,"1247":6,"1266":3,"1267":2,"1280":1,"1295":2,"1299":1,"1308":3,"2086":5}}],["exec",{"2":{"2067":1}}],["executable",{"0":{"1973":1},"2":{"1966":1,"1973":1,"1980":1,"1986":1,"1999":1}}],["executeoperation",{"2":{"1706":3}}],["executing",{"2":{"1212":1,"1213":1}}],["exe",{"2":{"1604":1}}],["exchange",{"0":{"1423":1}}],["exception>",{"2":{"1763":1}}],["exception",{"2":{"1713":1,"1761":1,"1762":5,"1763":3,"1902":2,"2062":1,"2063":2}}],["exceptions",{"2":{"1566":1}}],["except",{"2":{"373":2,"935":1,"951":1,"1566":1}}],["ex​l​​",{"2":{"1003":3}}],["exl",{"2":{"1003":3}}],["ex",{"2":{"1002":4,"1003":2,"1083":3}}],["exb−m",{"2":{"943":1,"961":1}}],["exbert",{"2":{"131":1}}],["ex1−m",{"2":{"943":1,"961":1}}],["exact",{"2":{"802":1}}],["examples",{"2":{"557":1,"985":1,"1309":1,"1332":1}}],["example",{"0":{"1102":1,"1373":1},"2":{"83":1,"326":2,"399":1,"424":1,"428":2,"529":1,"590":2,"834":2,"982":1,"983":2,"1078":1,"1215":1,"1254":1,"1284":1,"1404":3,"1528":1,"1820":1,"1821":2,"1838":1,"1839":2,"1902":2,"2086":1}}],["extern",{"2":{"1649":1,"1923":1}}],["externalproject",{"0":{"1989":1},"2":{"1989":2}}],["external",{"2":{"1254":1,"1989":2}}],["extensively",{"2":{"2081":1}}],["extensible",{"2":{"1214":1}}],["extension",{"0":{"1112":1},"1":{"1113":1,"1114":1,"1115":1,"1116":1,"1117":1,"1118":1,"1119":1,"1120":1,"1121":1,"1122":1,"1123":1},"2":{"1605":1}}],["extended",{"2":{"768":1}}],["extending",{"2":{"768":1}}],["extend",{"2":{"572":1}}],["extract",{"2":{"1302":1}}],["extraction",{"2":{"906":1}}],["extra",{"2":{"1214":4,"2079":1}}],["extrapolation",{"2":{"765":1,"768":2}}],["exit",{"2":{"1308":1}}],["existent",{"2":{"1761":1}}],["existing",{"2":{"590":1,"1086":1}}],["exists",{"2":{"372":1,"374":1,"1481":1,"1930":2,"1933":1}}],["exiexie^",{"2":{"178":1}}],["experiments",{"2":{"2073":1}}],["experts",{"2":{"42":1,"737":1,"740":2}}],["expect",{"2":{"1254":1}}],["expected",{"2":{"1095":1,"1096":1}}],["expm1",{"2":{"1087":2}}],["exp2",{"2":{"1087":2}}],["expressions",{"0":{"1907":1},"2":{"1904":1}}],["expression",{"2":{"1621":1,"1629":5}}],["expressivity",{"2":{"320":1}}],["express",{"2":{"1086":1}}],["exponent",{"2":{"1087":8}}],["exponentiallr",{"0":{"1239":1},"2":{"1231":2,"1239":2,"1246":1,"1247":1}}],["exponentially",{"2":{"115":1}}],["exponential",{"0":{"843":1},"2":{"156":1,"402":1,"621":1,"1087":1}}],["exposure",{"2":{"895":1}}],["exported",{"2":{"723":1}}],["export",{"0":{"1298":1},"2":{"558":1,"1215":1,"1272":1,"1283":1,"1284":1,"1298":1,"1309":1,"1332":1}}],["exp",{"2":{"503":2,"838":1,"847":2,"970":1,"971":4,"1087":3,"1100":3,"1114":1,"1241":1,"1329":2}}],["explanation",{"2":{"2086":1}}],["explained",{"2":{"1090":1}}],["explainer让transformer模型透明化",{"2":{"513":1}}],["explainer",{"2":{"233":1,"513":2}}],["explicit",{"0":{"1683":1},"2":{"1902":1}}],["explosion",{"2":{"373":1}}],["exploding",{"2":{"296":1}}],["exploring",{"2":{"625":1,"768":1,"1340":1}}],["exploration",{"2":{"175":1,"698":1}}],["explore",{"2":{"131":1,"2048":1}}],["expansion=forward",{"2":{"1217":1}}],["expansion",{"2":{"113":1,"1217":5}}],["expanded",{"2":{"723":2}}],["expand",{"0":{"824":1,"827":1},"1":{"825":1,"826":1,"827":1,"828":1},"2":{"76":1,"700":1,"723":2,"827":3,"1087":4,"1216":1,"1217":1,"1283":1,"1330":4}}],["e0e0e^0的数值为1",{"2":{"54":1}}],["e^",{"2":{"54":2,"108":1,"109":1,"178":2,"183":3,"191":16,"839":5,"943":4,"961":4,"1393":1}}],["enabling",{"2":{"2081":1}}],["enabled",{"2":{"1086":2,"1095":1,"1116":1}}],["enables",{"2":{"765":1,"768":1,"1086":1}}],["enable",{"2":{"660":1}}],["enjoys",{"2":{"2081":1}}],["ens33",{"2":{"2095":1}}],["ensures",{"2":{"2086":1}}],["ensurefileopen",{"2":{"1902":3}}],["ensemble",{"0":{"13":1},"2":{"0":1}}],["en数据集中美学评分在5分以上的子集",{"2":{"1363":1}}],["en数据集中256以上的样本量共1324m",{"2":{"1363":1}}],["en数据集上以256x256大小训练237",{"2":{"1363":1}}],["energy",{"2":{"1217":5}}],["enum",{"2":{"1083":1,"1727":1,"1728":3}}],["enumerate",{"2":{"381":1,"385":1,"399":1,"410":1,"571":1,"1215":1,"1244":1,"1251":1,"1273":1}}],["enhance",{"2":{"2079":1}}],["enhancement方法",{"2":{"736":1}}],["enhanced",{"2":{"727":2,"763":1,"768":2,"1341":1}}],["enhancing",{"2":{"90":1,"736":1,"740":1}}],["enc",{"2":{"727":3,"1218":7}}],["encoding的输出是位置向量",{"2":{"704":1}}],["encoding的输入是句子长度",{"2":{"704":1}}],["encoding是通过三角函数算出来的",{"2":{"701":1}}],["encodingmultihead",{"2":{"698":1}}],["encodingmasked",{"2":{"698":2}}],["encoding之后",{"2":{"698":1}}],["encoding之间需要相加",{"2":{"698":1}}],["encoding=",{"2":{"591":2,"1481":2}}],["encoding相加之后的结果",{"2":{"518":1}}],["encoding相加",{"2":{"460":1}}],["encoding就负责给每个词增加位置信息",{"2":{"457":1}}],["encoding负责给token加入位置信息",{"2":{"457":1}}],["encoding分别可以作为单独的积木块",{"2":{"449":1}}],["encoding",{"0":{"459":1},"2":{"394":1,"445":1,"454":2,"457":2,"557":1,"567":1,"571":2,"575":1,"638":3,"676":1,"681":1,"698":3,"701":2,"727":2,"740":1,"742":2,"745":1,"751":2,"764":1,"768":3,"1218":4,"1312":1,"1315":1}}],["encoding后到进入encoder前是有一个norm动作的",{"2":{"344":1}}],["encodes",{"2":{"572":2}}],["encoded",{"2":{"165":1,"572":3}}],["encode",{"2":{"82":2,"83":1,"399":1,"428":1,"450":2,"472":1,"528":1,"529":1,"538":3,"572":5,"591":3,"592":4,"703":2}}],["encoder端没什么变化",{"2":{"894":1}}],["encoder把所有的输入序列都编码成一个统一的语义特征context",{"2":{"891":1}}],["encoder阅读笔记",{"2":{"740":1}}],["encoder中的multi",{"2":{"535":1}}],["encoder中使用方式如下",{"2":{"343":1}}],["encoder的nosiy",{"2":{"1363":1}}],["encoder的输入不变",{"2":{"529":1}}],["encoder的核心部分是n个encoderlayer堆叠而成的栈",{"2":{"522":1}}],["encoder类是编码器的实现",{"2":{"522":1}}],["encoder类实例是由n个encoderlayer类实例构建而成",{"2":{"461":1}}],["encoder类实例",{"2":{"450":1}}],["encoder是一次性接受一个完整的句子",{"2":{"453":1}}],["encoder则包括外面的n参数",{"2":{"461":1}}],["encoder则包括外面的n",{"2":{"449":1}}],["encoder和decoder交互部分的multi",{"2":{"535":1}}],["encoder和decoder类中的注意力都是multiheadedattention的实例",{"2":{"449":1}}],["encoder和decoder两个类很像",{"2":{"449":1}}],["encoder由n个encoderlayer构成",{"2":{"449":1}}],["encoder架构如下",{"2":{"330":1}}],["encoderdecoder代码具体如下",{"2":{"450":1}}],["encoderdecoder类会通过decode",{"2":{"538":1}}],["encoderdecoder类会通过encode",{"2":{"538":1}}],["encoderdecoder类的forward",{"2":{"450":1}}],["encoderdecoder类就是基于transformer架构的编码器",{"2":{"450":1}}],["encoderdecoder类对象",{"2":{"385":1}}],["encoderdecoder的输出",{"2":{"385":1}}],["encoderdecoder",{"2":{"82":2,"449":2,"450":2,"529":1,"703":3}}],["encoder只会看src",{"2":{"79":1,"85":1}}],["encoder因为要编码整个句子",{"2":{"57":1}}],["encoder数据流",{"0":{"80":1},"2":{"49":1}}],["encoder",{"0":{"522":1,"887":1,"888":1,"1217":1,"1315":1,"1317":1},"2":{"38":1,"78":2,"82":1,"83":1,"84":2,"95":1,"237":1,"241":2,"282":1,"292":1,"343":3,"344":2,"414":1,"436":2,"449":1,"450":6,"461":1,"520":1,"522":3,"523":1,"535":1,"538":2,"539":1,"540":1,"542":4,"703":6,"719":1,"729":1,"740":2,"768":1,"886":1,"887":1,"888":1,"930":1,"935":1,"937":1,"951":1,"953":1,"1217":2,"1218":5,"1312":3,"1315":6,"1317":8,"1363":2}}],["encoderlayer类是编码器层的实现",{"2":{"523":1}}],["encoderlayer类的forward",{"2":{"344":1}}],["encoderlayer和decoderlayer是基础构建块",{"2":{"461":1}}],["encoderlayer和decoderlayer的数量为2",{"2":{"428":1}}],["encoderlayer只包含一个注意力层",{"2":{"461":1}}],["encoderlayer只包含一个attention层",{"2":{"449":1}}],["encoderlayer的参数是d",{"2":{"449":1}}],["encoderlayer",{"0":{"523":1},"2":{"38":1,"344":4,"449":1,"461":1,"520":2,"523":2,"538":1,"703":1,"1218":3}}],["encoder使用自注意力的目的是",{"2":{"38":1,"200":1}}],["encoder层用到一处",{"2":{"37":1}}],["engine=innodb",{"2":{"1481":1}}],["engine",{"2":{"976":1,"1088":1,"1089":1,"1936":1}}],["engineering",{"2":{"708":1}}],["english",{"2":{"557":1,"1825":9,"1843":9}}],["env",{"2":{"423":1}}],["environment>",{"2":{"1481":1}}],["environments>",{"2":{"1481":1}}],["environments",{"2":{"1481":1}}],["environment",{"2":{"1435":1,"1481":1}}],["environ",{"2":{"422":2}}],["en的封装器",{"2":{"384":1,"558":1}}],["en",{"2":{"364":2,"370":2,"371":2,"372":1,"373":6,"374":2,"375":5,"387":1,"422":5,"423":2,"557":9,"1399":1,"1400":1,"1401":1,"1402":1,"1481":2}}],["entry",{"2":{"1329":2,"1930":2}}],["entropy",{"2":{"397":1,"612":1,"1096":1}}],["entries",{"2":{"201":1}}],["entirely",{"2":{"235":1}}],["entity2tuple",{"2":{"1331":2}}],["entity粒度",{"2":{"1331":1}}],["entity表示实体的可能性",{"2":{"130":1}}],["entity",{"2":{"130":2,"136":1,"156":1,"768":1,"906":1,"1331":6}}],["enough",{"2":{"181":1,"233":1,"513":1}}],["endptr",{"2":{"1633":4}}],["endif",{"2":{"1628":1,"1632":1,"1916":2,"1923":1,"1985":1,"1999":1}}],["endl",{"2":{"1606":1,"1607":13,"1608":3,"1611":4,"1616":3,"1619":3,"1620":1,"1621":3,"1623":2,"1624":5,"1625":2,"1633":5,"1634":3,"1638":1,"1639":1,"1640":1,"1645":9,"1646":6,"1647":3,"1648":5,"1649":5,"1650":10,"1653":2,"1654":2,"1659":3,"1660":3,"1661":2,"1662":1,"1663":5,"1665":14,"1667":3,"1668":2,"1670":1,"1672":1,"1673":4,"1674":6,"1675":3,"1676":2,"1677":2,"1680":4,"1683":4,"1684":3,"1685":4,"1687":4,"1688":6,"1691":8,"1693":3,"1694":6,"1695":7,"1698":2,"1699":2,"1700":2,"1701":1,"1704":6,"1705":6,"1706":3,"1707":6,"1708":4,"1709":2,"1712":2,"1713":19,"1714":7,"1715":8,"1718":1,"1719":7,"1720":5,"1721":5,"1722":5,"1723":1,"1724":4,"1725":5,"1726":1,"1728":9,"1729":6,"1736":1,"1737":1,"1738":1,"1739":1,"1741":1,"1742":1,"1743":1,"1744":1,"1746":1,"1747":1,"1749":1,"1750":1,"1751":1,"1752":2,"1754":2,"1755":1,"1756":2,"1761":2,"1762":4,"1763":3,"1772":1,"1774":2,"1778":1,"1779":2,"1784":4,"1791":1,"1792":1,"1797":4,"1799":1,"1800":1,"1801":1,"1802":1,"1803":1,"1805":1,"1806":3,"1807":4,"1811":2,"1813":3,"1814":3,"1816":2,"1817":10,"1820":6,"1821":5,"1824":3,"1825":4,"1829":2,"1831":3,"1832":3,"1834":2,"1835":10,"1838":6,"1839":5,"1842":3,"1843":4,"1849":1,"1853":1,"1857":1,"1861":1,"1866":3,"1867":1,"1868":1,"1869":5,"1874":4,"1883":1,"1887":6,"1891":4,"1895":1,"1897":2,"1902":3,"1905":1,"1906":3,"1907":2,"1908":2,"1909":2,"1910":2,"1911":4,"1912":2,"1914":6,"1921":3,"1922":5,"1923":1,"1924":2,"1925":2,"1926":5,"1927":7,"1928":2,"1929":4,"1930":9,"1933":4,"1999":4,"2003":2,"2004":2,"2005":1,"2006":7,"2007":1,"2008":1,"2059":6,"2060":3,"2061":1,"2062":11,"2063":2}}],["ends",{"2":{"1328":2,"1330":3}}],["endswith",{"2":{"591":1}}],["end",{"2":{"125":2,"156":4,"241":1,"557":1,"571":1,"572":1,"694":3,"698":1,"899":1,"904":1,"943":1,"944":2,"961":1,"1087":8,"1328":2,"1329":4,"1330":8,"1594":4,"1633":1,"1645":1,"1713":3,"1714":1,"1718":1,"1719":2,"1720":5,"1721":5,"1722":5,"1724":4,"1725":5,"1736":1,"1737":1,"1738":1,"1739":1,"1741":1,"1742":1,"1743":1,"1744":1,"1746":1,"1747":1,"1749":1,"1750":1,"1751":1,"1752":1,"1754":2,"1755":1,"1756":1,"1806":1,"1807":1,"1814":1,"1821":3,"1832":1,"1839":3,"1883":6,"1897":1,"1902":2,"1914":1,"1922":2,"1933":2}}],["譬如有人叫你的名字或是公交车到站播报声",{"2":{"3":1}}],["设当前点为",{"2":{"2018":1}}],["设备驱动开发",{"2":{"1941":1}}],["设备上",{"2":{"1214":1}}],["设",{"2":{"1322":1}}],["设定为warmup",{"2":{"1183":1}}],["设定最大分词词典数量",{"2":{"576":1}}],["设定padding",{"2":{"79":4}}],["设如果",{"2":{"184":1}}],["设计并实现一个简单的计算器程序",{"2":{"1997":1}}],["设计并展开实验",{"2":{"1139":1}}],["设计你的数据",{"2":{"1728":1}}],["设计一个表示交通工具的继承体系",{"2":{"1664":1}}],["设计一个简单的员工管理系统",{"0":{"1657":1},"2":{"1657":1}}],["设计一个",{"2":{"1642":1}}],["设计的集成开发环境",{"2":{"1605":1}}],["设计模式",{"2":{"1500":1}}],["设计下一轮实验",{"0":{"1142":1},"1":{"1143":1,"1144":1,"1145":1},"2":{"1125":1}}],["设计了mla",{"2":{"956":1}}],["设计了一种终身学习的模型编辑方法",{"2":{"143":1}}],["设计训练模型即可",{"2":{"909":1}}],["设计动机",{"0":{"434":1}}],["设计",{"2":{"153":1,"302":1,"1500":1}}],["设计来替代了传统",{"2":{"151":1}}],["设计思路",{"0":{"11":1,"160":1},"1":{"12":1,"13":1,"14":1},"2":{"0":1,"157":1}}],["设置第二位为1",{"2":{"2062":1}}],["设置某一位为1或0",{"2":{"2062":1}}],["设置全局属性",{"0":{"1982":1}}],["设置信用额度为5000元",{"2":{"1873":1}}],["设置浮点数精度为",{"2":{"1817":1,"1835":1}}],["设置字段宽度为",{"2":{"1817":1,"1835":1}}],["设置输出宽度为",{"2":{"1673":1}}],["设置tab为4个空格",{"2":{"1559":1}}],["设置缩进宽度为4",{"2":{"1559":1}}],["设置自动缩进",{"2":{"1559":1}}],["设置密码",{"2":{"1530":1}}],["设置我们input",{"2":{"1298":1}}],["设置我们是否需要求这个tensor的梯度",{"2":{"1106":1}}],["设置每个参数组的学习率",{"2":{"1241":1}}],["设置成train",{"2":{"1215":1}}],["设置成员变量",{"2":{"523":2}}],["设置为0",{"2":{"1330":1}}],["设置为",{"2":{"1214":2,"1242":1}}],["设置为none的参数和缓冲区不包括在内",{"2":{"1214":1}}],["设置为以主语",{"2":{"145":1}}],["设置额外状态",{"2":{"1214":1}}],["设置机制",{"0":{"1210":1}}],["设置管道以跟踪到目前为止在训练期间看到的",{"2":{"1166":1}}],["设置定期评估",{"0":{"1164":1}}],["设置实验跟踪",{"0":{"1167":1},"2":{"1125":1}}],["设置过大的话",{"2":{"1045":1}}],["设置中",{"2":{"986":1}}],["设置",{"0":{"1117":1},"2":{"944":1,"1117":1,"1215":1}}],["设置缺省index为",{"2":{"557":1}}],["设置0",{"2":{"396":1}}],["设置在每个子层的输出位置",{"2":{"334":1}}],["设置一个阈值",{"2":{"143":1}}],["设置一个共享阈值",{"2":{"135":1}}],["设置了padding",{"2":{"83":1}}],["设置填充符号",{"0":{"65":1},"2":{"49":1}}],["设想你在一个拥挤的公交车上看书",{"2":{"3":1}}],["下拉展示",{"2":{"2111":1}}],["下拉展开",{"2":{"1405":1,"1426":1}}],["下载",{"2":{"1605":1}}],["下载一个预训练好的深度学习模型",{"0":{"786":1}}],["下",{"2":{"1547":1,"1557":1}}],["下层是中间层的情况",{"2":{"1449":1}}],["下层是output的情况",{"2":{"1449":1}}],["下层激活",{"0":{"1449":1}}],["下降",{"2":{"1440":1}}],["下采样",{"2":{"1364":2}}],["下step",{"2":{"1330":1}}],["下游任务使用时",{"2":{"1315":1}}],["下产生最佳验证性能的权重衰减值",{"2":{"1150":1}}],["下的deep",{"2":{"2049":1}}],["下的doccano添加普通用户",{"2":{"2043":1}}],["下的",{"2":{"1087":1,"2045":1}}],["下标运算符重载",{"2":{"1712":1}}],["下标从",{"2":{"1634":1}}],["下标",{"2":{"1003":1}}],["下个概念预测",{"2":{"632":1}}],["下一点",{"2":{"2018":1}}],["下一节详细讲解",{"0":{"1424":1}}],["下一个点的更新只会在",{"2":{"2022":1}}],["下一个词是什么",{"2":{"627":1}}],["下一个",{"2":{"575":1}}],["下一篇介绍如何从数字转换到embedding",{"2":{"545":1}}],["下一篇我们介绍transformer的总体架构",{"2":{"291":1}}],["下篇",{"2":{"513":1}}],["下表整理了部分llm使用的tokenizer",{"2":{"569":1}}],["下表给出了详细的操作和张量形状",{"2":{"520":1}}],["下表给出了根据层数",{"2":{"217":1}}],["下表汇总了",{"2":{"141":1}}],["下意识的吸引力",{"2":{"163":1}}],["下意识的关注度",{"2":{"163":1}}],["下方说明通过修改与知识神经元对应的几个值槽",{"2":{"143":1}}],["下方是解码器输入对应的掩码操作",{"2":{"63":1}}],["下面进行优化",{"2":{"2023":1}}],["下面简单介绍一些在自然语言理解",{"2":{"1317":1}}],["下面就简要介绍一些常见的生成模型",{"2":{"1316":1}}],["下面两个常用的预训练任务",{"2":{"1312":1}}],["下面我们将描述如何根据使用恒定学习率",{"2":{"1155":1}}],["下面我们还会介绍评估模式",{"2":{"1116":1}}],["下面我们来详细解析输入嵌入层的关键作用和构建过程",{"2":{"697":1}}],["下面的表格和图例给出了三种分词方法的对比",{"2":{"604":1}}],["下面的decoder",{"2":{"542":1}}],["下面的图是一个例子",{"2":{"87":1}}],["下面部分",{"2":{"519":1}}],["下面给出了具体代码",{"2":{"503":1}}],["下面代码有个正则化的细节",{"2":{"398":1}}],["下面代码处理之后",{"2":{"380":1}}],["下面语句运行之后",{"2":{"380":1}}],["下面是一些里面的经典而富有哲理的话语",{"2":{"2056":1}}],["下面是一个关于如何进行梯度截断的案例",{"2":{"1184":1}}],["下面是参考代码",{"2":{"1825":1,"1843":1}}],["下面是示例图解",{"2":{"1443":1}}],["下面是从",{"2":{"700":1}}],["下面是训练代码示例",{"2":{"472":1}}],["下面是推理代码示例",{"2":{"472":1}}],["下面是哈佛源码中的推理测试代码",{"2":{"428":1}}],["下面是另一个平滑的例子",{"2":{"399":1}}],["下面是论文中的摘录",{"2":{"399":1}}],["下面是论文中推导出的ffn的kv详细结构",{"2":{"126":1}}],["下面是常见数据集",{"2":{"367":1}}],["下面是layernorm的代码",{"2":{"343":1}}],["下面是pytorch官网给出的在nlp和cv领域使用layernorm的例子",{"2":{"326":1}}],["下面展示了三种全连接层中的奇异性",{"2":{"305":1}}],["下面图提供了从2022年到2024年不同前馈网络中间比率的趋势变化",{"2":{"100":1}}],["下面流程图梳理了代码逻辑",{"2":{"85":1}}],["下三角的值全为1",{"2":{"70":1}}],["下图显示了如何为多个传入请求维护",{"2":{"986":1}}],["下图显示了分组查询注意力和多头",{"2":{"937":1,"953":1}}],["下图图1右图所示",{"2":{"940":1,"962":1}}],["下图下方的公式",{"2":{"758":1}}],["下图最左侧展示了llm2vec如何通过改变注意力掩码",{"2":{"734":1}}],["下图在前面文章中已经有涉及",{"2":{"698":1}}],["下图这张表列出了对于不同大小模型理论上最优的词表大小",{"2":{"561":1}}],["下图则给出了部分llm的词表大小",{"2":{"559":1}}],["下图将上述流程的核心部分作了可视化",{"2":{"431":1}}],["下图只是为了展示流程",{"2":{"427":1}}],["下图只是初步演示",{"2":{"323":1}}],["下图给出来不同预训练模型的架构",{"2":{"367":1}}],["下图给出了三角函数编码的改造过程",{"2":{"759":1}}],["下图给出了三种主流架构中注意力模式的比较",{"2":{"541":1}}],["下图给出了从直观角度出发来看",{"2":{"744":1}}],["下图给出了从2022年到2024年间",{"2":{"559":1}}],["下图给出了目前常见模型的总体状态",{"2":{"728":1}}],["下图给出了预训练模型架构的差异",{"2":{"721":1}}],["下图给出了传统transformer和tokenformer之间的区别",{"2":{"620":1}}],["下图给出了几个模块之间的交互关系",{"2":{"615":1}}],["下图给出了现代llm的进化树",{"2":{"540":1}}],["下图给出了连续模型",{"2":{"507":1}}],["下图给出了多头差异注意力机制和代码示例",{"2":{"503":1}}],["下图给出了tem模型的结构",{"2":{"490":1}}],["下图给出了transformer²的总体架构",{"2":{"225":1}}],["下图给出了transformer和稀疏自编码器的对比",{"2":{"137":1}}],["下图给出了在输出token",{"2":{"479":1}}],["下图给出了在注意力头间计算令牌间贡献的三种方法",{"2":{"478":1}}],["下图给出了在最后一个主语",{"2":{"130":1}}],["下图给出了具体推导过程",{"2":{"402":1}}],["下图给出了具体算法",{"2":{"313":1}}],["下图给出了上面代码中的部分数据流程示例",{"2":{"399":1}}],["下图给出了llm的常见数据处理流程",{"2":{"364":1}}],["下图给出了层归一化",{"2":{"346":1}}],["下图给出了一些llm的配置",{"2":{"335":1}}],["下图给出了两者的架构区别",{"2":{"335":1}}],["下图给出了pre",{"2":{"335":1}}],["下图给出了pytorch中bn的示例",{"2":{"315":1}}],["下图给出了自回归模型的示例",{"2":{"239":1}}],["下图给出了svf的概况",{"2":{"224":1}}],["下图给出了",{"2":{"210":1}}],["下图给出了完整流程",{"2":{"145":1}}],["下图给出了基于transformer的语言模型所采用的事实回忆的关键机制",{"2":{"122":1}}],["下图给出了compose的计算方式",{"2":{"46":1}}],["下图给出了注意力头合并方式的一些方案",{"2":{"40":1}}],["下图给出了注意力头展示情况",{"2":{"20":1}}],["下图形象的表示了四种归一化的工作方式",{"2":{"341":1}}],["下图形象化的解释了多头注意力运行机制",{"2":{"1":1}}],["下图上方出了transformer模型的某一层中自注意力机制的计算流程",{"2":{"758":1}}],["下图上方给出了把第二列修改为第三列",{"2":{"143":1}}],["下图上深色区域",{"2":{"320":1}}],["下图即为上述数据组织形式的示意",{"2":{"316":1}}],["下图示意了batchnorm针对三维样本的工作模式",{"2":{"313":1}}],["下图2号标签探究模型的预测分布中",{"2":{"306":1}}],["下图1号标签探究每一层的残差向量所对应的预测分布的最大概率词与整个模型的预测分布的最大概率词的重合率",{"2":{"306":1}}],["下图呈现了全局注意力",{"2":{"285":1}}],["下图就展示了序列建模方式的特点和典型案例",{"2":{"273":1}}],["下图提供了这些函数的概述",{"2":{"268":1}}],["下图从数据库角度展示了自注意力的细节",{"2":{"164":1}}],["下图左侧给出了",{"2":{"153":1}}],["下图左面是vanilla",{"2":{"19":1}}],["下图为三个方案构建词语之间关系所需距离的对比",{"2":{"274":1}}],["下图为论文",{"2":{"150":1}}],["下图为模型回答",{"2":{"130":1}}],["下图展示了记录转移分数的矩阵",{"2":{"1324":1}}],["下图展示了从文本到embedding的流转过程",{"2":{"695":1}}],["下图展示了定理6",{"2":{"507":1}}],["下图展示了定理4",{"2":{"507":1}}],["下图展示了定理3",{"2":{"507":1}}],["下图展示了如何使用",{"2":{"466":1}}],["下图展示了如何使用查询",{"2":{"464":1}}],["下图展示了梯度在",{"2":{"148":1}}],["下图展示了一些与知识编辑相关的几条技术路线",{"2":{"139":1}}],["下图展示了键值对的概念",{"2":{"124":1}}],["下图是哈佛词表的样例",{"2":{"550":1}}],["下图是使用",{"2":{"415":1}}],["下图是加入了掩码之后的teacher",{"2":{"409":1}}],["下图是四个单词各自应该关注的情况",{"2":{"409":1}}],["下图是现有llm预训练中数据源的比率",{"2":{"368":1}}],["下图是对代码的进一步简化",{"2":{"364":1}}],["下图是局部注意力",{"2":{"285":1}}],["下图是连续块注意力",{"2":{"204":1}}],["下图是块间注意力",{"2":{"204":1}}],["下图是块内注意力",{"2":{"204":1}}],["下图是一个实例",{"2":{"147":1}}],["下图是一个简要的汇总",{"2":{"123":1}}],["下图是最简单的人无法理解的语言模型",{"2":{"137":1}}],["下图是常见大模型的信息",{"2":{"103":1}}],["下图右侧就是我们之前介绍的",{"2":{"89":1}}],["下图中用不同颜色标明了两个embedding分别的计算过程",{"2":{"700":1}}],["下图中的output",{"2":{"473":1}}],["下图中的数字代表代码中某模块出现的顺序",{"2":{"449":1}}],["下图中的sandwich",{"2":{"329":1}}],["下图中的粗线是rnn结构中两个词建立关系所需的最长距离",{"2":{"256":1}}],["下图中的粗线为cnn结构中两个词建立关系所需的最长距离",{"2":{"256":1}}],["下图中虚线代表key与query的相关度",{"2":{"277":1}}],["下图中给出了序列中",{"2":{"265":1}}],["下图中",{"2":{"63":1,"321":1}}],["下图去掉",{"2":{"24":1}}],["下划线",{"2":{"3":1}}],["从字符串流中提取数据",{"2":{"1824":1,"1842":1}}],["从字串映射到数字的过程被称为tokenizer的编码过程",{"2":{"563":1}}],["从当前位置向后移动",{"2":{"1821":1,"1839":1}}],["从标准输入读取整数",{"2":{"1811":1,"1829":1}}],["从用户输入读取年份并存储到",{"2":{"1729":1}}],["从右到左",{"2":{"1635":3}}],["从右向左进行",{"2":{"1629":1}}],["从右向左读有助于理解",{"2":{"1614":1}}],["从左到右",{"2":{"1635":10}}],["从左到右依次计算",{"2":{"1630":1}}],["从代码到执行",{"0":{"1604":1}}],["从代码上看",{"2":{"533":1}}],["从哪里开始进行自动化",{"2":{"1598":1}}],["从哪拿",{"2":{"45":1}}],["从进程",{"2":{"1578":1}}],["从进化树中",{"2":{"540":1}}],["从光标位置向上查找",{"2":{"1553":1}}],["从光标位置向下查找",{"2":{"1553":1}}],["从感知机到深度神经网络",{"0":{"1464":1},"1":{"1465":1,"1466":1}}],["从神经元到感知机",{"0":{"1461":1},"1":{"1462":1,"1463":1}}],["从神经网络角度而言",{"2":{"28":1}}],["从图中我们可以看到随着相对距离的变大",{"2":{"1344":1}}],["从图像生成文字",{"2":{"882":1}}],["从step",{"2":{"1330":1}}],["从state",{"2":{"1214":1}}],["从自然语言理解",{"2":{"1318":1}}],["从pytorch",{"2":{"1248":1}}],["从pre",{"2":{"726":1}}],["从此时开始",{"2":{"1173":1}}],["从此自然语言可以计算",{"2":{"685":1}}],["从本质上讲",{"2":{"1157":1}}],["从本质上来说",{"2":{"248":1}}],["从长远来看",{"2":{"1140":1}}],["从长文本中提取关键信息",{"2":{"906":1}}],["从简单的配置开始",{"2":{"1139":1}}],["从简化的角度",{"2":{"222":1}}],["从较小的模型开始",{"2":{"1137":1}}],["从较慢的hbm加载到较快的sram中",{"2":{"942":1,"959":1}}],["从云供应商处计费",{"2":{"1134":1}}],["从更简单的优化器开始会更可取",{"2":{"1130":1}}],["从针对手头问题类型的最常用的优化器开始",{"2":{"1130":1}}],["从研究论文附录中收集的技巧",{"2":{"1127":1}}],["从功能上讲",{"2":{"1122":1}}],["从访问",{"2":{"1114":1}}],["从伯努利分布中绘制二进制随机数",{"2":{"1087":1}}],["从numpy生成",{"0":{"1072":1}}],["从训练开始时积累梯度平方会导致有效学习率过早和过量的减小",{"2":{"1042":1}}],["从基础到凯明",{"2":{"1010":1}}],["从8k到128k",{"2":{"977":1}}],["从context",{"0":{"977":1}}],["从ring",{"0":{"976":1}}],["从rnn",{"0":{"877":1},"1":{"878":1,"879":1,"880":1,"881":1,"882":1,"883":1}}],["从hbm",{"2":{"944":1}}],["从hbm加载输入数据",{"2":{"944":1,"963":1}}],["从mha转换为mqa将h个键和值头减少为单个键和值头",{"2":{"937":1,"953":1}}],["从非结构化文本中提取结构化信息",{"2":{"906":1}}],["从机器到现代",{"2":{"1603":1}}],["从机器学习到深度学习",{"0":{"905":1},"1":{"906":1,"907":1,"908":1,"909":1}}],["从机制上看",{"2":{"169":1}}],["从理论上没有任何的问题",{"2":{"895":1}}],["从类别生成语音或音乐等",{"2":{"882":1}}],["从生物学上看",{"2":{"841":1}}],["从信号方面来看",{"2":{"840":1}}],["从信号处理的角度来看",{"2":{"722":1}}],["从索引",{"2":{"1713":1}}],["从索引1到索引8",{"2":{"832":1}}],["从索引2到索引6",{"2":{"832":1}}],["从绝对编码公式角度看",{"2":{"762":1}}],["从位置编码的角度",{"2":{"761":1}}],["从xlnet论文公式角度看",{"2":{"760":2}}],["从相对位置层面衡量key的重要性",{"2":{"758":1}}],["从相关来源",{"2":{"367":1}}],["从内容层面衡量key的重要性",{"2":{"758":1}}],["从槐树叶底",{"2":{"744":1}}],["从直观角度来看",{"0":{"744":1},"2":{"741":1}}],["从直观上看",{"2":{"335":1}}],["从几何",{"2":{"740":1}}],["从结果上来看",{"2":{"736":2}}],["从结果上可以看得出",{"2":{"175":1}}],["从这个实现也可以看到",{"2":{"1344":1}}],["从这个角度来看的话",{"2":{"751":1}}],["从这个角度来看",{"2":{"145":1,"168":1}}],["从这些试验中",{"2":{"1155":1}}],["从这一点出发",{"2":{"713":1}}],["从这样的一些描述里面",{"2":{"713":1}}],["从嵌入矩阵nn",{"2":{"699":1}}],["从实验得出的结论可能对固定超参数的其他值无效",{"2":{"1143":1}}],["从实验结果中获取经验",{"0":{"1146":1},"1":{"1147":1,"1148":1,"1149":1,"1150":1,"1151":1},"2":{"1139":1}}],["从实验结果中可以看到",{"2":{"620":1}}],["从实际的角度出发",{"2":{"698":1}}],["从建模的角度来看",{"2":{"698":1}}],["从心理学角度看",{"2":{"691":1}}],["从广义上讲",{"2":{"689":1}}],["从稀疏性变成稠密性",{"2":{"682":1,"684":1}}],["从单维变成多维",{"2":{"682":1}}],["从通俗意义来讲",{"2":{"680":1}}],["从离散的符号形式转换为连续的数值向量形式",{"2":{"677":1}}],["从embedding中提取各种丰富的知识和结构",{"2":{"676":1}}],["从2019年到现在",{"2":{"638":1}}],["从所有可能的token对中选择加入语言模型后",{"2":{"599":1}}],["从文本中识别和提取命名实体",{"2":{"906":1}}],["从文本的末尾开始向开头处理",{"2":{"595":1}}],["从文本生成角度看",{"2":{"242":1}}],["从整数列表还原成字节字符串",{"2":{"592":1}}],["从整个训练集来看",{"2":{"122":1}}],["从一个基础小词表开始",{"2":{"575":1}}],["从一维卷积的角度看",{"2":{"116":1}}],["从子词粒度切分",{"2":{"567":1}}],["从词向量",{"2":{"718":1}}],["从词粒度切分",{"2":{"567":1}}],["从词典的角度来看也许可以促进理解",{"2":{"265":1}}],["从数字中选最大",{"0":{"2122":1},"1":{"2123":1,"2124":1,"2125":1,"2126":1}}],["从数字映射回字串称为tokenizer的解码过程",{"2":{"563":1}}],["从数学计算上",{"2":{"779":1}}],["从数学表示来看",{"2":{"689":1}}],["从数学上讲",{"2":{"210":1}}],["从数学到神经网络",{"2":{"156":1,"740":1}}],["从data",{"2":{"557":1}}],["从iterator之中获取句子",{"2":{"557":1}}],["从前向传递中保存伪随机数生成器获取状态并在反向过程中重新生成dropout",{"2":{"964":1}}],["从前面章节我们可以知道",{"2":{"545":1}}],["从前文我们可以了解到",{"2":{"158":1}}],["从逻辑上讲",{"2":{"542":1}}],["从逻辑上来说是做如下操作",{"2":{"28":1}}],["从提示词中进行上下文学习",{"2":{"542":1}}],["从两个来源获得输入",{"2":{"536":1}}],["从论文图上看",{"2":{"523":1}}],["从经验性的积累到理论空白的弥补",{"2":{"513":1}}],["从物理直观出发",{"2":{"506":1}}],["从0到1再到5的转换代表了离散的网络层",{"2":{"496":1}}],["从0到warmup",{"2":{"402":1}}],["从ode的连续视角来研究深度学习模型",{"2":{"495":1}}],["从向量维度投影到词表长度",{"2":{"473":1}}],["从向量角度而言",{"2":{"28":1}}],["从架构图上看",{"2":{"468":1}}],["从tokenformer",{"2":{"623":1}}],["从transformer的构造代码可以看出来",{"2":{"461":1}}],["从t5",{"2":{"99":1}}],["从外部传入参数的目的是更加灵活",{"2":{"450":1}}],["从网络结构来分析",{"2":{"436":1}}],["从模型处理角度来看",{"0":{"745":1},"2":{"741":1,"745":1}}],["从模型来看",{"2":{"417":1}}],["从模型文件中加载模型参数",{"2":{"372":1}}],["从该例子可以看到当模型非常自信的时候就会给予其一个微小的惩罚",{"2":{"399":1}}],["从公式",{"2":{"349":1}}],["从公式中可以看到",{"2":{"175":1}}],["从后续代码中可以看到",{"2":{"344":1}}],["从后续文章中我们会知道这其实是kv",{"2":{"273":1}}],["从形式上来说",{"2":{"343":1}}],["从深度学习上来讲",{"2":{"338":1}}],["从批量维度进行归一化会带来一些问题",{"2":{"338":1}}],["从样本角度的合理性来看",{"2":{"323":1}}],["从下图可以看到",{"2":{"311":1,"698":1}}],["从下面的数据结构之中",{"2":{"558":1}}],["从下面代码中可以看到",{"2":{"201":1}}],["从下面代码可以看出来",{"2":{"83":1}}],["从下面这个例子中可以看到",{"2":{"84":1}}],["从梯度的视角理解一下残差连接",{"2":{"304":1}}],["从另一个角度讲",{"2":{"700":1}}],["从另一个角度来看",{"2":{"276":1}}],["从另一个角度为我们拓展了llm中知识的可解释性",{"2":{"137":1}}],["从输入流中提取数据",{"2":{"1813":1,"1831":1}}],["从输入流读取数据",{"2":{"1673":1}}],["从输入生成的特征向量f会进一步生成键矩阵k和值矩阵v",{"2":{"264":1}}],["从输入的第二个词开始",{"2":{"241":1}}],["从头看到尾",{"2":{"256":1}}],["从第一个h1到最后一个hm",{"2":{"249":1}}],["从表象上来看",{"2":{"300":1}}],["从表象上看",{"2":{"248":1}}],["从表现效果来讲",{"2":{"175":1}}],["从宏观角度看",{"2":{"241":1}}],["从宏观角度来说",{"2":{"161":1}}],["从熵不变性看attention的scale操作",{"2":{"233":1}}],["从泛函分析的角度解释attention机制和卷积神经网络的本质区别",{"2":{"233":1}}],["从泛函分析的角度来看",{"2":{"173":1}}],["从记忆的角度来看",{"2":{"231":1}}],["从人类记忆的角度来看",{"2":{"230":1}}],["从旋转矩阵中提取旋转角度",{"2":{"201":1}}],["从抽象代数的角度来看",{"2":{"174":1}}],["从其他主机获取block的朴素方法会导致两个重要问题",{"2":{"975":1}}],["从其相应的源词中产生",{"2":{"161":1}}],["从其它角度优化",{"0":{"207":1},"2":{"157":1}}],["从源序列中的单个词开始来跟踪它们在",{"2":{"161":1}}],["从微观角度看",{"2":{"161":1}}],["从软硬件层面优化",{"0":{"206":1},"2":{"157":1}}],["从多头模型生成多查询模型分为两个步骤",{"2":{"938":1,"954":1}}],["从多头角度优化",{"0":{"205":1},"2":{"157":1}}],["从多粒子动态系统",{"2":{"498":1}}],["从多个角度进行衡量",{"2":{"3":1}}],["从序列角度优化",{"0":{"204":1},"2":{"157":1}}],["从认知和逻辑思维的角度谈谈自然语言理解",{"2":{"156":1}}],["从计算到构建",{"2":{"156":1}}],["从反向传播矩阵来理解transformer的运作机制",{"2":{"156":1,"483":1,"513":1}}],["从概念上讲",{"2":{"154":1,"480":1}}],["从根本上说",{"2":{"189":1}}],["从根本上减少计算复杂度和资源消耗",{"2":{"152":1}}],["从根本上提升了模型的表达能力",{"2":{"43":1}}],["从某种程度上来看",{"2":{"273":1}}],["从某种意义上说",{"2":{"137":1}}],["从某个角度来看",{"2":{"118":1}}],["从再上面的的网络图可以看到",{"2":{"130":1}}],["从上图可以看到",{"2":{"318":1,"729":1,"1320":1}}],["从上图可以看到f",{"2":{"301":1}}],["从上图可以看出",{"2":{"130":1}}],["从上文的分析可知",{"2":{"265":1}}],["从上面假设进一步拓展",{"2":{"137":1}}],["从上可以看出",{"2":{"17":1}}],["从中能够看到两个空间之间的转换",{"2":{"695":1}}],["从中得到最大的值对应的index",{"2":{"473":1}}],["从中可以看出来残差连接在transformer中的重要性",{"2":{"295":1}}],["从中可以看到对激活函数的使用情况",{"2":{"103":1}}],["从中我们可以看到",{"2":{"284":1}}],["从中提取了无数的结构与关联信息",{"2":{"116":1}}],["从llama的源码中可以看到",{"2":{"114":1}}],["从act2cls可以看出来",{"2":{"110":1}}],["从attention层到transformer网络",{"2":{"47":1}}],["从attention函数中可以看到",{"2":{"36":1}}],["从矩阵运算角度可以理解为变换和平移",{"2":{"101":1}}],["从矩阵内容上来看",{"2":{"70":1}}],["从",{"0":{"1364":1,"1600":1},"1":{"1601":1,"1602":1,"1603":1,"1604":1,"1605":1,"1606":1,"1607":1,"1608":1},"2":{"89":1,"198":1,"595":1,"744":1,"944":1,"1157":1}}],["从掩码角度出发",{"2":{"81":1}}],["从主对角线向上偏移1开始",{"2":{"74":1}}],["从起源到mha",{"2":{"47":1,"292":1}}],["从原理层面上看",{"2":{"33":1}}],["从而用最少的硬币达到目标",{"2":{"2138":1}}],["从而用这些信息来捕获句子的内在结构和表示",{"2":{"498":1}}],["从而访问该类的私有及保护成员",{"2":{"1769":1}}],["从而找到",{"2":{"1589":1}}],["从而找到最优的实验",{"2":{"1175":1}}],["从而从给定的训练数据集生成更真实的新数据",{"2":{"1472":1}}],["从而从这个理解角度出发来帮助分析失败案例",{"2":{"474":1}}],["从而改变其性质和硬度的过程",{"2":{"1242":1}}],["从而节省了许多连续的内核调用",{"2":{"1228":1}}],["从而影响模型对留出集的表现",{"2":{"1165":1}}],["从而影响训练的收敛性",{"2":{"309":1}}],["从而降低了每个示例的计算要求",{"2":{"1164":1}}],["从而降低后续实验的复杂度",{"2":{"1140":1}}],["从而可能增加达到特定验证损失所需的训练步骤",{"2":{"1154":1}}],["从而可以将它们嵌套在树形结构中",{"2":{"1206":1}}],["从而可以加快迭代过程",{"2":{"894":1}}],["从而可以表达相似性",{"2":{"690":1}}],["从而可以学到其中的噪声",{"2":{"603":1}}],["从而可以被作为一个统一的batch输入到模型中进行处理",{"2":{"376":1}}],["从而我们从实验中得出错误结论的风险也越高",{"2":{"1143":1}}],["从而防止过拟合",{"2":{"1017":1}}],["从而避免冗余的内存和计算",{"2":{"985":1}}],["从而避免梯度因参数值过大而爆炸",{"2":{"393":1}}],["从而进一步提高了系统的性能和效率",{"2":{"977":1}}],["从而进行序列的自我生成",{"2":{"529":1}}],["从而扩展了算法隐藏内存和指令发布延迟的能力",{"2":{"973":1}}],["从而支持高效的推理",{"2":{"956":1}}],["从而逐步执行softmax归一化操作",{"2":{"940":1,"959":1}}],["从而多查询注意力在内存带宽和容量上都具有更激进的削减",{"2":{"937":1,"953":1}}],["从而大大减少",{"2":{"935":1,"951":1}}],["从而大大降低了训练大型tokenformer架构的总体成本",{"2":{"618":1}}],["从而将细胞状态更新为神经网络认为相关的新值",{"2":{"867":1}}],["从而将这些非线性特征组合到最终的输出中",{"2":{"117":1}}],["从而无法获得更高的学习率",{"2":{"1147":1}}],["从而无法完成深度网络的训练",{"2":{"840":1}}],["从而无法进行正确的预测",{"2":{"176":1}}],["从而略微缓减了sigmoid",{"2":{"839":1}}],["从而对输入数据进行分类",{"2":{"785":1}}],["从而对当前输出进行一定的纠正",{"2":{"746":1}}],["从而对某类任务达成相变",{"2":{"367":1}}],["从而引出了对位置编码的分类",{"2":{"743":1}}],["从而显著提高了效率",{"2":{"986":1}}],["从而显著提高吞吐量",{"2":{"982":1}}],["从而显著提升模型对语境的理解能力",{"2":{"734":1}}],["从而显著降低了训练负担",{"2":{"624":1}}],["从而显著降低了内存占用",{"2":{"153":1}}],["从而适应embedding任务",{"2":{"732":1}}],["从而适应不同的x尺度",{"2":{"360":1}}],["从而能保留句子原本大部分的信息",{"2":{"727":1}}],["从而能够满足更多小孩",{"2":{"2151":1}}],["从而能够更好地捕捉类别之间的语义关系",{"2":{"702":1}}],["从而能够更准确地重建原始的",{"2":{"213":1}}],["从而能够基于训好的模型上增量的拓展新的更大的模型",{"2":{"624":1}}],["从而同时捕捉到词语在前后文中的信息",{"2":{"717":1}}],["从而完成词的向量化",{"2":{"712":1}}],["从而学习到具体的语义表示",{"2":{"709":1}}],["从而帮助增加训练稳定性",{"2":{"701":1}}],["从而内存消耗高",{"2":{"681":1}}],["从而发生数据不连续",{"2":{"658":1}}],["从而促进更快的收敛并加速整体缩放过程",{"2":{"623":1}}],["从而促进对问题的深入理解和解决",{"2":{"505":1}}],["从而本质上最大化了",{"2":{"616":1}}],["从而动态分配计算资源",{"2":{"612":1}}],["从而简化计算",{"2":{"2018":1}}],["从而简化编译",{"2":{"1963":1}}],["从而简化了",{"2":{"595":1}}],["从而简化为swish",{"2":{"108":1}}],["从而通过一个有限的词表来解决所有单词的分词问题",{"2":{"567":1}}],["从而通过减法消除了噪声",{"2":{"500":1}}],["从而减少冗余的计算和内存占用",{"2":{"985":1}}],["从而减少过拟合",{"2":{"399":1}}],["从而减轻过拟合的风险",{"2":{"684":1}}],["从而减慢处理速度",{"2":{"562":1}}],["从而整个句子将整体右移一位",{"2":{"528":1}}],["从而淹没它们",{"2":{"500":1}}],["从而倾向于将attention权重分配给这些无关的上下文中",{"2":{"500":1}}],["从而把每个位置的上下文编码到更高维度的表示",{"2":{"498":1}}],["从而把这两个英文句子翻译成如下",{"2":{"259":1}}],["从而调整每个与输入对应的",{"2":{"485":1}}],["从而调整每个对应的",{"2":{"148":1}}],["从而训练更深的神经网络",{"2":{"470":1}}],["从而确定每个token在生成预测时应从其它token那里获得多少关注",{"2":{"463":1}}],["从而确保在解码时不会受到未来信息的影响",{"2":{"439":1}}],["从而捕获上下文信息和单词之间的关系",{"2":{"461":1}}],["从而捕捉到不同位置之间的相对关系",{"2":{"750":1}}],["从而捕捉输入和输出之间的复杂依赖关系",{"2":{"538":1}}],["从而捕捉长距离依赖",{"2":{"247":1}}],["从而捕捉数据中更复杂的关系",{"2":{"101":1,"120":1}}],["从而与序列中的其他元素建立动态的关联",{"2":{"442":1}}],["从而享受并行计算或者",{"2":{"415":1}}],["从而切断它从未来获得信息的通路",{"2":{"409":1}}],["从而模型学会作弊",{"2":{"409":1}}],["从而模拟ln的行为",{"2":{"359":1}}],["从而很容易地分析自己的错误",{"2":{"406":1}}],["从而加速参数的更新过程",{"2":{"1032":1}}],["从而加速参数更新",{"2":{"1031":1}}],["从而加速了模型的收敛",{"2":{"321":1}}],["从而加剧梯度的波动",{"2":{"400":1}}],["从而为后面的大胃口小孩保留更大的饼干",{"2":{"2156":1}}],["从而为算法优化提供思路",{"2":{"1602":1}}],["从而为模型提供一个较好的起点",{"2":{"898":1}}],["从而为优化算法提供明确的目标和方向",{"2":{"397":1}}],["从而为对比",{"2":{"386":1}}],["从而削弱模型的表达能力",{"2":{"396":1}}],["从而构建了海量自然语言和代码的概率分布空间",{"2":{"386":1}}],["从而直接可视化它们之间的关系",{"2":{"359":1}}],["从而更容易进行类比和推理",{"2":{"505":1}}],["从而更好的适用不同的输入和任务要求",{"2":{"502":1}}],["从而更好地理解学习",{"2":{"1602":1}}],["从而更好地理解和生成文本",{"2":{"898":1}}],["从而更好地理解和表示文本",{"2":{"734":1}}],["从而更好地理解和控制模型的行为",{"2":{"137":1}}],["从而更好地捕捉序列中的局部结构信息",{"2":{"757":1}}],["从而更好地利用更多数据",{"2":{"732":1}}],["从而更好地管理内存容量",{"2":{"231":1}}],["从而更好地管理有限的记忆容量",{"2":{"230":1}}],["从而更好地管理记忆",{"2":{"228":1}}],["从而更快地学会生成正确的序列",{"2":{"407":1}}],["从而更加准确地设置了权重的初始值",{"2":{"403":1}}],["从而更利于优化",{"2":{"332":1}}],["从而单纯地将均值方差标准化无法实现标准分布",{"2":{"314":1}}],["从而稳定乃至加速了训练",{"2":{"314":1}}],["从而阻碍稳定学习",{"2":{"309":1}}],["从而解决rnn不能并行的问题",{"2":{"290":1}}],["从而解除了mha注意力头的查找选择回路和变换回路的固定绑定",{"2":{"43":1}}],["从而给予更高的权重",{"2":{"284":1}}],["从而重新定义了序列建模的格局",{"2":{"279":1}}],["从而生成针对特定类型的代码",{"2":{"1698":1}}],["从而生成了一个有向无环图",{"2":{"1113":1}}],["从而生成相应的输出",{"2":{"425":1}}],["从而生成更丰富和完整的表示",{"2":{"278":1}}],["从而生成最适合任务的最终响应",{"2":{"225":1}}],["从而消除了距离概念",{"2":{"274":1}}],["从而有效的解决rnn中的信息遗失问题",{"2":{"273":1}}],["从而有助于保持梯度的稳定性",{"2":{"187":1}}],["从而才能了解需要给这个隐状态多少注意力",{"2":{"267":1}}],["从而才能确定彼此间的相互关系",{"2":{"265":1}}],["从而区分输入的不同部分对输出的影响",{"2":{"260":1}}],["从而做到提纲挈领",{"2":{"260":1}}],["从而打破这个顺序结构",{"2":{"256":1}}],["从而克服了马尔可夫模型的主要局限性",{"2":{"250":1}}],["从而理解整个句子",{"2":{"246":1}}],["从而躲避天敌和捕捉猎物",{"2":{"220":1}}],["从而既解决了",{"2":{"217":1}}],["从而在执行过程中获得更高的性能",{"2":{"1287":1}}],["从而在注意力计算中获得了7",{"2":{"940":1,"962":1}}],["从而在搜索空间中找到可能性较高的序列",{"2":{"904":1}}],["从而在pooling前和h进行更深的交互",{"2":{"731":1}}],["从而在数据复杂性较高的地方分配更多的计算资源",{"2":{"611":1}}],["从而在推理和回答问题时能够利用这些知识",{"2":{"437":1}}],["从而在推理过程中保持了恒定的计算复杂度",{"2":{"210":1}}],["从而在综合多层之后就会获得更加强大的表达能力",{"2":{"437":1}}],["从而在长文本分析时可以捕获更多的语义关联关系",{"2":{"434":1}}],["从而在输入序列和输出序列进行对齐",{"2":{"277":1}}],["从而在测试时实现更好的泛化性能",{"2":{"230":1}}],["从而在低维空间中保留高维空间的相似性",{"2":{"204":1}}],["从而达到加速推理或者支持更长context需求",{"2":{"204":1}}],["从而达到模型编辑",{"2":{"145":1}}],["从而得到更好的准确性和泛化",{"2":{"846":1}}],["从而得到该文本的词向量",{"2":{"715":1}}],["从而得到了字",{"2":{"694":1}}],["从而得到一个高阶语义向量序列",{"2":{"516":1}}],["从而得到一个残差向量",{"2":{"294":1}}],["从而得到归一化后的数值",{"2":{"319":1}}],["从而得到最终的结果",{"2":{"185":1}}],["从而得到dcmha如下图所示",{"2":{"46":1}}],["从而挑选出似然最大的那个词",{"2":{"184":1}}],["从而导致i",{"2":{"1161":1}}],["从而导致llm在大规模训练上成本过高",{"2":{"612":1}}],["从而导致了在数字编码方式上的不一致性",{"2":{"595":1}}],["从而导致梯度爆炸",{"2":{"296":1}}],["从而导致注意力分布过于平滑",{"2":{"213":1}}],["从而导致训练和预测的结果不一致",{"2":{"181":1}}],["从而导致模型性能变差",{"2":{"118":1}}],["从而保持参数数量相同并最大化参数共享",{"2":{"154":1}}],["从而保证encoder",{"2":{"116":1}}],["从而快速定位相似数据",{"2":{"153":1}}],["从而绕过了这些缺陷",{"2":{"143":1}}],["从而实现代码复用和扩展",{"2":{"1654":1}}],["从而实现并行计算",{"2":{"1589":1}}],["从而实现并行处理并减少",{"2":{"977":1}}],["从而实现更高的扩展效率",{"2":{"611":1}}],["从而实现了理论上的线性复杂度",{"2":{"216":1}}],["从而实现item间相关性更精细的注意力分布",{"2":{"209":1}}],["从而实现快速计算",{"2":{"175":1}}],["从而实现持续的编辑",{"2":{"143":1}}],["从而实现大模型对待编辑知识的初步识别",{"2":{"141":1}}],["从而实现复杂的行为",{"2":{"137":1}}],["从而形成负责任的",{"2":{"140":1}}],["从而充分发挥它们作为动态",{"2":{"138":1}}],["从而定位出那些普遍的事实信息",{"2":{"135":1}}],["从而预测出客体是french",{"2":{"130":1}}],["从而出现",{"2":{"104":1}}],["从而让第2个",{"2":{"731":1}}],["从而让模型能够适应更多样化的输入形式",{"2":{"553":1}}],["从而让它见多识广",{"2":{"363":1}}],["从而让变长序列的数据在模型中按需分配资源",{"2":{"217":1}}],["从而让",{"2":{"143":1}}],["从而让transformers",{"2":{"143":1}}],["从而让这些不被关注",{"2":{"84":1}}],["从而让最后得到的embedding关注多方面信息",{"2":{"13":1}}],["从而增加完成研究的总体时间",{"2":{"1159":1}}],["从而增加资源成本",{"2":{"1145":1}}],["从而增加过拟合的可能性",{"2":{"1012":1}}],["从而增加模型的容量",{"2":{"21":1}}],["从而增强了派生类的封装性",{"2":{"1861":1}}],["从而增强上下文建模的能力",{"2":{"500":1}}],["从而增强模型的表达能力",{"2":{"5":1}}],["从而提升在特定任务上的性能",{"2":{"560":1}}],["从而提升算法的泛化能力",{"2":{"393":1}}],["从而提升模型的拟合能力",{"2":{"116":1}}],["从而提升模型的表达能力和拟合精度",{"2":{"116":1}}],["从而提高系统的整体性能和吞吐量",{"2":{"977":1}}],["从而提高序列建模的性能",{"2":{"757":1}}],["从而提高句子表示的质量",{"2":{"734":1}}],["从而提高",{"2":{"595":1}}],["从而提高模型性能",{"2":{"561":1}}],["从而提高模型的执行效率",{"2":{"1288":1}}],["从而提高模型的训练效率和性能",{"2":{"294":1}}],["从而提高模型的整体性能",{"2":{"153":1}}],["从而提高模型的性能",{"2":{"106":1,"560":1}}],["从而提高训练效果",{"2":{"407":1}}],["从而提高训练的稳定性和效率",{"2":{"400":1}}],["从而提高了效率",{"2":{"542":1}}],["从而提高了训练的稳定性",{"2":{"310":1}}],["从而提高了计算效率",{"2":{"21":1}}],["从而提高认知效率和精度",{"2":{"257":1}}],["从而提高语言任务的性能",{"2":{"119":1}}],["从而提高推理效率而不牺牲准确率或增加参数数量",{"2":{"42":1}}],["从而提取更丰富的信息",{"2":{"5":1}}],["从而使该函数能够访问该类的私有和受保护成员",{"2":{"1772":1}}],["从而使旧搜索更难重现",{"2":{"1175":1}}],["从而使其更容易从所有位置捕获信息",{"2":{"764":1}}],["从而使其能够捕获数据内的复杂关系和依赖关系",{"2":{"462":1}}],["从而使其能够更好地提取和存储知识",{"2":{"144":1}}],["从而使权重衰减变得不必要",{"2":{"352":1}}],["从而使得上述关系成立",{"2":{"1342":1}}],["从而使得数学家能够在不同领域之间建立联系和发现共性",{"2":{"505":1}}],["从而使得",{"2":{"334":1}}],["从而使得我们可以更平稳地进行训练",{"2":{"314":1}}],["从而使得拥有几十上百层的深度学习模型更加容易训练",{"2":{"296":1}}],["从而使得网络变得不稳定",{"2":{"296":1}}],["从而使得多头的q",{"2":{"9":1}}],["从而使秩崩溃在各层之间更快发生",{"2":{"93":1}}],["从而使模型能够学习和表示复杂的数据模",{"2":{"102":1}}],["从而使模型能够关注更多的信息",{"2":{"16":1}}],["从而使模型只关注当前",{"2":{"50":1}}],["从而使模型在计算注意力分数时忽略这些填充符号",{"2":{"50":1}}],["从而使每个",{"2":{"41":1}}],["从而允许注意力模型在上下文向量计算中引入更多信息",{"2":{"16":1}}],["从而利于下游多样的任务微调时",{"2":{"12":1}}],["从而获得更丰富的特征信息",{"2":{"12":1}}],["从而错过某些重要的依赖关系或特征",{"2":{"3":1}}],["这年代",{"0":{"2141":1}}],["这部经典和最近孔老夫子以及王阳明先生带给我太多太多宝贵的东西",{"2":{"2056":1}}],["这部经典影片我现在每隔两年左右会看一遍",{"2":{"2056":1}}],["这部分它称之为decoder",{"2":{"763":1}}],["这部分称为encoder",{"2":{"763":1}}],["这部分主要通过生成三个embedding层",{"2":{"723":1}}],["这部分原因是因为向量是作用在向量空间之上",{"2":{"689":1}}],["这周周天继续看完了剩下的内容",{"2":{"2056":1}}],["这段路往往是很孤单的",{"2":{"2054":1}}],["这段话",{"2":{"627":1}}],["这真的让现在的我很是敬佩",{"2":{"2054":1}}],["这点女生可能比男生更有经验",{"2":{"2112":1}}],["这点再次让我真正觉得孔子值得被称为",{"2":{"2054":1}}],["这点很重要",{"2":{"2054":1}}],["这点说完我就觉得他应该赚这份钱",{"2":{"2051":1}}],["这完全有可能",{"2":{"2054":1}}],["这或许是",{"2":{"2051":1}}],["这项技术能够捕捉微小的表情变化",{"2":{"2011":1}}],["这项技术可以有效缩减模型推理时所需的计算量",{"2":{"377":1}}],["这行代码会报错",{"2":{"1784":1}}],["这并不是遥不可及的复杂理论",{"2":{"2107":1}}],["这并不是一款由谷歌官方所支持的产品",{"2":{"1196":1}}],["这并不意味着",{"2":{"1781":1}}],["这常用于传递大型对象",{"2":{"1729":1}}],["这类似于宏展开",{"2":{"1709":1}}],["这类模型也被称为隐变量自回归模型",{"2":{"240":1}}],["这被称为封装",{"2":{"1674":1}}],["这被称作是",{"2":{"1009":1}}],["这降低了学习门槛",{"2":{"1611":1}}],["这降低了mfu并与网络资源竞争以在节点之间传输kvcache",{"2":{"977":1}}],["这赋予了开发者更高的灵活性和优化空间",{"2":{"1602":1}}],["这牺牲了一部分执行效率",{"2":{"1602":1}}],["这称之为前向传播",{"2":{"1438":1}}],["这称为完美缩放",{"2":{"1133":1}}],["这构成了相对位置编码的一般做法",{"2":{"1334":1}}],["这构成了绝对位置编码的一般做法",{"2":{"1334":1}}],["这非常有用",{"2":{"1222":2}}],["这非常有益",{"2":{"1131":1}}],["这用于注册一个不被视为模型参数的缓冲区",{"2":{"1211":1}}],["这只是一个编号",{"2":{"1611":1}}],["这只是允许我们使用和重新分配您的贡献作为项目的一部分",{"2":{"1197":1}}],["这只适用于未来不会有这种特定工作需求的情况",{"2":{"1153":1}}],["这至少比",{"2":{"1183":1}}],["这才会是问题",{"2":{"1179":1}}],["这才是有效性的关键所在",{"2":{"305":1}}],["这允许训练作业对计算实例中断具有弹性",{"2":{"1166":1}}],["这允许通过重用较小的预训练对应模型的参数来增量开发较大的模型",{"2":{"623":1}}],["这通常需要对数据进行抽样以进行定期评估",{"2":{"1165":1}}],["这通过cp倍减少了计算和通信",{"2":{"976":1}}],["这有时需要重新参数化搜索空间",{"2":{"1146":1}}],["这有助于生成正确的计算图",{"2":{"1291":1}}],["这有助于生成更准确",{"2":{"898":1}}],["这有助于在梯度方向上形成更大的动量",{"2":{"1031":1}}],["这有助于加快模型的收敛速度",{"2":{"898":1}}],["这有助于保留整体特征信息",{"2":{"815":1}}],["这有助于保持前向传播过程中信息的完整性",{"2":{"304":1}}],["这有助于保持模型对主语本质的理解",{"2":{"145":1}}],["这有助于算法查看每个字符并找到频率最高的字符配对",{"2":{"579":1}}],["这有助于模型更准确地理解和生成领域相关的文本",{"2":{"560":1}}],["这有助于模型捕捉输入序列中的复杂依赖关系",{"2":{"175":1}}],["这有助于理解模型的工作原理",{"2":{"505":1}}],["这有助于我们抛弃",{"2":{"335":1}}],["这包括指定",{"2":{"1137":1}}],["这包括了大约100万对文本",{"2":{"725":1}}],["这份文件是在我们试图实现我们自己的深度学习方法时产生的",{"2":{"1127":1}}],["这份手册是为谁准备的",{"0":{"1126":1},"2":{"1125":1}}],["这正是允许使用任意的",{"2":{"1113":1}}],["这正好是",{"2":{"333":1}}],["这同样导致了gradient太小",{"2":{"995":1}}],["这基于流水线的加速方法已在训练系统中得到探讨",{"2":{"977":1}}],["这将不会导致额外的通信成本",{"2":{"975":1}}],["这将违背减少内存成本的初衷",{"2":{"975":1}}],["这将导致在反向传播过程中引发错误",{"2":{"1115":1}}],["这将导致大约1",{"2":{"970":1}}],["这将导致模型向全局最优收敛的速度减慢",{"2":{"405":1}}],["这避免了反复从hbm读取和写入输入和输出的操作",{"2":{"944":1,"963":1}}],["这比从hbm中读取中间注意力矩阵的标准方法更快",{"2":{"940":1,"959":1}}],["这比绝对位置嵌入有明显的优势",{"2":{"762":1}}],["这确保了相同相对距离的标记始终由相同的偏差表示",{"2":{"762":1}}],["这确保了单词在序列中以相同方式patch化",{"2":{"613":1}}],["这保证了引用的有效性",{"2":{"1612":1}}],["这保证了偏置随着相对位置的单调性",{"2":{"762":1}}],["这保留了",{"2":{"213":1}}],["这句子为例",{"2":{"757":1}}],["这句话换一种说法又可以表述为",{"2":{"713":1}}],["这句话的序号编码为",{"2":{"545":1}}],["这句话中所有token都做一遍点积",{"2":{"170":1}}],["这句话",{"2":{"130":1,"627":1}}],["这足以允许对模型参数进行大幅更改并避免过拟合",{"2":{"733":1}}],["这跟大模型本身的训练任务跟模型架构有关",{"2":{"730":1}}],["这代表了一个有利的权衡",{"2":{"937":1,"953":1}}],["这代表了每个词语的前向和后向表示",{"2":{"718":1}}],["这代表更强的表达能力",{"2":{"542":1}}],["这n个字就是这是个单词的直接邻域",{"2":{"714":1}}],["这实际上优化了冗余超参数",{"2":{"1149":1}}],["这实际上就是构建了这样的二元运算群到线性空间的映射",{"2":{"713":1}}],["这实质是单向transformer",{"2":{"71":1}}],["这属于数学中抽象代数的一个分支",{"2":{"713":1}}],["这取决于具体模型的设计",{"2":{"700":1}}],["这俩参数是咋更新的",{"2":{"665":1}}],["这俩参数是一次forward训练一次还是单独有自己的训练",{"2":{"665":1}}],["这需要在内存中布置fp32累加器块和fp8操作数矩阵的不同布局一致性要求",{"2":{"973":1}}],["这需要",{"2":{"941":1,"960":1}}],["这需要模型具备语义空间中的能力",{"2":{"626":1}}],["这需要经历几个阶段才能完成从512维向量得到10000个单词概率的转换",{"2":{"473":1}}],["这等于是使系统的自由能最小化",{"2":{"612":1}}],["这主要有助于训练小模型",{"2":{"612":1}}],["这主要是运算",{"2":{"314":1}}],["这时就需要用到",{"2":{"1647":1}}],["这时我们想到了变分法",{"2":{"1377":1}}],["这时应该使用贝叶斯优化工具来自动找到最佳超参数配置",{"2":{"1153":1}}],["这时的decoder就是一个语言模型",{"2":{"894":1}}],["这时候它也会出现饱和的现象",{"2":{"838":1}}],["这时使用group",{"2":{"807":1}}],["这时",{"2":{"583":1}}],["这时跟之前的seq2seq就没什么区别了",{"2":{"426":1}}],["这必然会导致更多低频token和无意义的token残片",{"2":{"562":1}}],["这都需要在设置词汇表时考虑",{"2":{"560":1}}],["这3个词编码后的结果",{"2":{"537":1}}],["这其实就是一种分步解决问题的方式",{"2":{"2102":1}}],["这其实跟电脑在成千上万的文件里找某个文件的过程类似",{"2":{"2100":1}}],["这其实也是内外循环置换这个总体思想配套的改进措施",{"2":{"972":1}}],["这其实对应了另外一个逻辑",{"2":{"691":1}}],["这其实对应了另外一种说法",{"2":{"684":1}}],["这其实是一种生活中的排序方式",{"2":{"2099":1}}],["这其实是一种索引表示",{"2":{"679":1}}],["这其实是一个多分类问题",{"2":{"180":1}}],["这其实蕴含了解码器需要对编码器输出的所有信息都有所了解",{"2":{"536":1}}],["这让prompt工程具备了更深层次的科学基础",{"2":{"504":1}}],["这为我们使用llm解决复杂问题提供了一个全新的视角",{"2":{"504":1}}],["这与log",{"2":{"1155":1}}],["这与维数灾难",{"2":{"692":1}}],["这与人类早期认知发展相似",{"2":{"674":1}}],["这与当前基于token的llms技术形成了鲜明对比",{"2":{"628":1}}],["这与当前的惯例",{"2":{"542":1}}],["这与",{"2":{"498":1,"707":1,"1611":1}}],["这证实了论文作者的理论模型",{"2":{"489":1}}],["这证明了记忆层在大型语言模型",{"2":{"154":1}}],["这最后的线性层和softmax函数共同作用",{"2":{"473":1}}],["这鼓励模型学习更强大的特征并减少对特定神经元的依赖",{"2":{"469":1}}],["这和神经科学的研究不太相符",{"2":{"841":1}}],["这和论文相符合",{"2":{"461":1}}],["这和原始论文稍有不同",{"2":{"344":1}}],["这揭示了skip",{"2":{"446":1}}],["这整个句子",{"2":{"409":1}}],["这肯定是正确的",{"2":{"407":1}}],["这么做的原因是因为",{"2":{"399":1}}],["这么写是因为这样推导让我觉得可以给出直观且合理的解释",{"2":{"235":1}}],["这不就是希望吗",{"2":{"2056":1}}],["这不就是向量乘以了一个旋转矩阵吗",{"2":{"1342":1}}],["这不是我夸大",{"2":{"2054":1}}],["这不是官方认证的",{"2":{"1124":1}}],["这不会降低在低学习率下的性能",{"2":{"1178":1}}],["这不一定对应线上模型的实际上线",{"2":{"1139":1}}],["这不符合逻辑",{"2":{"398":1}}],["这不仅节省存储空间",{"2":{"594":1}}],["这不仅提升了整个",{"2":{"232":1}}],["这不仅仅是将已知技术应用到新模型中",{"2":{"139":1}}],["这批batch中有效token的数量",{"2":{"385":1}}],["这往往通过前一代模型来对预训练数据进行智能过滤",{"2":{"369":1}}],["这限制了对整个句子意义的全面理解",{"2":{"734":1}}],["这限制了语言模型在检索任务上的潜力",{"2":{"727":1}}],["这限制了它在理解和区分不同意义时的能力",{"2":{"715":1}}],["这限制了",{"2":{"352":1}}],["这限制了模型的灵活性",{"2":{"172":1}}],["这会抛出异常",{"2":{"2063":1}}],["这会抛出",{"2":{"1713":1}}],["这会对训练有很大帮助",{"2":{"334":1}}],["这会导致程序崩溃",{"2":{"1672":1}}],["这会导致出现缓慢但稳定的训练",{"2":{"1179":1}}],["这会导致oov",{"2":{"565":1}}],["这会导致训练进展缓慢",{"2":{"192":1}}],["这会导致神经元无法更新权重",{"2":{"104":1}}],["这条",{"2":{"332":1}}],["这显然无法满足需求",{"2":{"679":1}}],["这显然这不符合nlp的规律",{"2":{"323":1}}],["这显然也是一种信息泄露",{"2":{"58":1}}],["这四个单词的index进行编码的结果",{"2":{"702":1}}],["这四个字",{"2":{"674":1,"676":1,"681":1}}],["这四个分布均成为标准正态分布",{"2":{"313":1}}],["这四个数据是完全不同的四个分布",{"2":{"313":1}}],["这拖了后腿",{"2":{"291":1}}],["这帮助模型捕捉输入序列中的依赖关系",{"2":{"270":1}}],["这组权重代表了各个信息的重要程度",{"2":{"261":1}}],["这导致几乎最佳的内存利用率",{"2":{"982":1}}],["这导致更好的",{"2":{"977":1}}],["这导致相似字节序列的不一致和非上下文patch化",{"2":{"613":1}}],["这导致对语言模型中的算术性能有显著影响",{"2":{"595":1}}],["这导致同一位置的token并不是一一对应的",{"2":{"323":1}}],["这导致整个语序并不完全满足偏序结构",{"2":{"252":1}}],["这导致的直接后果是",{"2":{"176":1}}],["这对设备内存或显存的占用非常大",{"2":{"495":1}}],["这对大部分的网络和任务都比较有效",{"2":{"396":1}}],["这对许多序列学习任务至关重要",{"2":{"250":1}}],["这对于在头文件中定义静态成员变量或全局变量非常有用",{"2":{"1923":1}}],["这对于处理文本摘要等需要捕获长距离依赖的任务特别有用",{"2":{"1317":1}}],["这对于减少长上下文输入的总体处理时间",{"2":{"977":1}}],["这对于位置编码具有极其重要的指导意义",{"2":{"754":1}}],["这对于理解语义非常重要",{"2":{"568":1}}],["这对于网络训练的稳定性和加速有很大帮助",{"2":{"301":1}}],["这对于基于梯度下降法的优化非常不利",{"2":{"192":1}}],["这对于大型记忆来说很快就变得不可行",{"2":{"154":1}}],["这三种实现方式的性能排序为",{"2":{"1228":1}}],["这三种方式具体如下",{"2":{"736":1}}],["这三种方法共同确保了transformer²能够实现强大且高效的任务适应",{"2":{"225":1}}],["这三种embedding特征分别是",{"2":{"722":1}}],["这三步",{"2":{"313":1}}],["这三个必要条件",{"2":{"1145":1}}],["这三个embedding对应三种不同的频率",{"2":{"722":1}}],["这三个维度都有明确的物理含义",{"2":{"687":1}}],["这三个维度分别为",{"2":{"511":1}}],["这三个单词会被赋予临近的数值",{"2":{"679":1}}],["这三个单词",{"2":{"576":1}}],["这三个子模块的作用如下",{"2":{"525":1}}],["这三个向量的相互作用",{"2":{"265":1}}],["这三个变量综合起来使用就可以满足我们的需求",{"2":{"265":1}}],["这三个词对",{"2":{"245":1}}],["这三个矩阵是固定的",{"2":{"172":1}}],["这三个矩阵列数可以不同",{"2":{"7":1}}],["这三个权重矩阵是在模型训练过程中通过反向传播训练出来的",{"2":{"172":1}}],["这三个概念",{"2":{"162":1}}],["这三个线性层和",{"2":{"36":1}}],["这使模型能够根据当前任务动态修改其行为",{"2":{"222":1}}],["这使得结构体更像一个轻量级的类",{"2":{"1728":1}}],["这使得函数可以返回动态分配的内存地址",{"2":{"1706":1}}],["这使得连续赋值成为可能",{"2":{"1629":1}}],["这使得训练效率提高了",{"2":{"1315":1}}],["这使得算法能够更好地平衡快速收敛和避免震荡之间的权衡",{"2":{"1027":1}}],["这使得mini",{"2":{"1027":1}}],["这使得sgd在大规模数据集上具有优势",{"2":{"1026":1}}],["这使得softmax成为多类分类问题中的首选激活函数",{"2":{"180":1}}],["这使得这些采样方法在llm服务中变得实用",{"2":{"983":1}}],["这使得在部署过程中需要频繁的即时可扩展性的情况下具有挑战性",{"2":{"977":1}}],["这使得在训练和推理过程中上下文大小的扩展零开销",{"2":{"975":1}}],["这使得其对噪声有一些鲁棒性",{"2":{"843":1}}],["这使得",{"2":{"746":1,"1602":1}}],["这使得bert能够更精确地捕捉语言的复杂性和细微差别",{"2":{"721":1}}],["这使得batch",{"2":{"316":1}}],["这使得使用简单的向量运算进行精确的类比推理成为可能",{"2":{"713":1}}],["这使得计算机能够轻松理解这些概念之间的关系",{"2":{"688":1}}],["这使得计算更加有效",{"2":{"29":1}}],["这使得解码器中的每个位置都能关注输入序列中的所有位置",{"2":{"444":1}}],["这使得解码器可以通过使用未来",{"2":{"58":1}}],["这使得pre",{"2":{"335":1}}],["这使得同时克服过拟合和任务干扰变得具有挑战性",{"2":{"222":1}}],["这使得kans在处理高维函数时既能学习组合结构",{"2":{"155":1}}],["这使得模型可以灵活地用于",{"2":{"1317":1}}],["这使得模型能够在处理未知词时仍然具有很好的泛化能力",{"2":{"567":1}}],["这使得模型能够捕捉到词汇之间的语义联关系",{"2":{"696":1}}],["这使得模型能够捕捉长距离依赖关系",{"2":{"261":1}}],["这使得模型能够捕获尖锐的注意力峰值",{"2":{"213":1}}],["这使得模型的训练和推理成本急剧上升",{"2":{"152":1}}],["这使得模型在整合注意力头的时候",{"2":{"19":1}}],["这使得对于算力的挑战变得愈发严峻",{"2":{"138":1}}],["这名外科医生将llm中存储的庞大复杂的知识分解成更小",{"2":{"221":1}}],["这说明ai的",{"2":{"627":1}}],["这说明token列表被有效压缩了",{"2":{"585":1}}],["这说明在各层之间",{"2":{"306":1}}],["这说明了矩阵的大部分的内容可以通过少部分最大的奇异值得到",{"2":{"204":1}}],["这说明使用局部注意力模式不仅使注意力计算更高效",{"2":{"93":1}}],["这趋近于把最大的元素赋值为1",{"2":{"191":1}}],["这在需要在函数内部修改原始数据时非常有用",{"2":{"1729":1}}],["这在项目的初始阶段尤其重要",{"2":{"1130":1}}],["这在训练的时候有效",{"2":{"934":1}}],["这在处理大型词汇表时可以节省内存",{"2":{"702":1}}],["这在处理长序列时尤其成为问题",{"2":{"279":1}}],["这在生物学上似乎是不可能的",{"2":{"488":1}}],["这在近似",{"2":{"213":1}}],["这在分类任务中可能影响不大",{"2":{"184":1}}],["这在模型优化过程中很有帮助",{"2":{"108":1}}],["这要求输出值必须满足两个条件",{"2":{"180":1}}],["这可能表明模型参数在某一时刻发生了非常大的变化",{"2":{"1179":1}}],["这可能不适用",{"2":{"1149":1}}],["这可能导致sgd在搜索空间中陷入局部最小值",{"2":{"1026":1}}],["这可能导致不准确的相似度估计",{"2":{"352":1}}],["这可能对某些任务和网络架构更有利",{"2":{"838":1}}],["这可能需要更强的正则化和",{"2":{"1186":1}}],["这可能需要更多的模型参数和训练数据",{"2":{"749":1}}],["这可能需要额外的实验和调整",{"2":{"739":1}}],["这可能会导致资源泄漏或其他问题",{"2":{"1764":1}}],["这可能会导致训练时间较长",{"2":{"1025":1}}],["这可能会成为问题",{"2":{"1179":1}}],["这可能会增加训练时间并增加计算资源的需求",{"2":{"1157":1}}],["这可能会干扰我们比较目标超参数的不同值的能力",{"2":{"1149":1}}],["这可能会很棘手",{"2":{"1123":1}}],["这可能会对最终嵌入的余弦相似度产生意想不到的影响",{"2":{"692":1}}],["这可能会非常昂贵",{"2":{"542":1}}],["这可能会限制模型的性能",{"2":{"512":1}}],["这可能在特征维度非常大时导致较高的计算开销",{"2":{"348":1}}],["这可能是attention无法长度外推的主要原因",{"2":{"176":1}}],["这可以有效地限制变量的作用域",{"2":{"1922":1}}],["这可以帮助编译器进行优化",{"2":{"1764":1}}],["这可以帮助我们验证不同数据类型的大小",{"2":{"1607":1}}],["这可以避免命名冲突",{"2":{"1649":1}}],["这可以告诉我们该如何选择梯度",{"2":{"1179":1}}],["这可以将吞吐量提高高达2",{"2":{"983":1}}],["这可以很容易地与计算重叠",{"2":{"977":1}}],["这可以看作是一种选择性梯度检查点的形式",{"2":{"946":1,"966":1}}],["这可以造成网络的稀疏性",{"2":{"840":1}}],["这可以让机器人技术能够与自回归transformer训练流程无缝衔接",{"2":{"637":1}}],["这可以保存更多的梯度信息",{"2":{"304":1}}],["这可以减少模型参数数量",{"2":{"250":1}}],["这可以通过将每个head的",{"2":{"41":1}}],["这增加了模型可学习的参数量",{"2":{"172":1}}],["这便把",{"2":{"170":1}}],["这几个字经过tokenizer之后",{"2":{"453":1}}],["这几个邻近单词暗示了此处的",{"2":{"259":1}}],["这几个邻近单词暗示了",{"2":{"167":1}}],["这几个概念",{"2":{"163":1}}],["这相当于将之前概率较低的词汇提升为预测可能性更高的目标",{"2":{"485":1}}],["这相当于将之前概率较低的词汇提升为可能性更高的目标",{"2":{"148":1}}],["这相当于对最有可能的词汇进行强化",{"2":{"148":1}}],["这篇笔记我们讲讲书本中的信号量机制pv操作之",{"2":{"1426":1}}],["这篇笔记我们讲讲进程协作之间的进程同步",{"2":{"1405":1}}],["这篇是bengio",{"2":{"282":1}}],["这篇论文的第一个思路是基于多个",{"2":{"938":1,"954":1}}],["这篇论文的核心创新在于提出了一个能够在测试时学习记忆的神经长期记忆模块",{"2":{"228":1}}],["这篇论文不仅是对现有大语言模型",{"2":{"625":1}}],["这篇论文里提到",{"2":{"8":1}}],["这篇文章分别对",{"2":{"144":1}}],["这也就是说",{"2":{"1175":1}}],["这也成为日后kaiming初始化提出的原因",{"2":{"1000":1}}],["这也可以看成一种",{"2":{"706":1}}],["这也可以说明",{"2":{"20":1}}],["这也行",{"2":{"429":1}}],["这也要添加掩码",{"2":{"382":1}}],["这也正是layernorm的归一化方式",{"2":{"318":1}}],["这也被称为近因效应",{"2":{"242":1}}],["这也是为什么大家都说深度学习的可解释性很差",{"2":{"1470":1}}],["这也是该领域的一个活跃研究领域",{"2":{"1180":1}}],["这也是我根本不想去深入了解这个技术的原因",{"2":{"908":1}}],["这也是我们将整个操作命名为",{"2":{"360":1}}],["这也是一个10",{"2":{"704":1}}],["这也是",{"2":{"581":1}}],["这也是因果关系的一种体现",{"2":{"536":1}}],["这也是ode的另一种视角",{"2":{"498":1}}],["这也是论文中提到的点积注意力",{"2":{"463":1}}],["这也是整个transformer的核心操作所在",{"2":{"197":1}}],["这也是当前许多模型编辑工作的前提",{"2":{"136":1}}],["这表明应该更改搜索空间边界",{"2":{"1175":1}}],["这表明我们处于",{"2":{"1149":1}}],["这表明我们在",{"2":{"1149":1}}],["这表明rw和hs嵌入在结构化和主题上捕获了不同的信息",{"2":{"739":1}}],["这表明它们具有互补性",{"2":{"739":1}}],["这表明它们彼此相似",{"2":{"485":1}}],["这表明组件a对数学任务至关重要",{"2":{"224":1}}],["这表明在分析和修改llms中的知识时",{"2":{"131":1}}],["这表明因果掩码在缓解秩崩溃速率方面相对于全掩码具有优势",{"2":{"94":1}}],["这佐证了高层和底层关注的模式抽象层次不同的结论",{"2":{"127":1}}],["这大大简化了编译过程",{"2":{"1589":1}}],["这大大减少了网络消耗并提高了mfu",{"2":{"977":1}}],["这大大增强了",{"2":{"624":1}}],["这大大加快了训练速度",{"2":{"406":1}}],["这大大提高了计算效率",{"2":{"172":1}}],["这大大限制了网络处理复杂问题的能力",{"2":{"102":1}}],["这大约相当于编码器总头数的",{"2":{"20":1}}],["这意味着它们可以在编译时执行",{"2":{"1924":1}}],["这意味着函数内部对形参的修改不会影响到外部的实参",{"2":{"1729":1}}],["这意味着你可以使用像",{"2":{"1712":1}}],["这意味着你不能修改数组名指向的地址",{"2":{"1704":1}}],["这意味着一个成员函数可以处理多种类型的参数",{"2":{"1701":1}}],["这意味着如果初始条件为假",{"2":{"1620":1}}],["这意味着维特比算法中的局部最优性原理仅考虑前一个时刻的最优路径",{"2":{"1326":1}}],["这意味着cp被禁用",{"2":{"976":1}}],["这意味着无论该词出现在何种上下文中",{"2":{"715":1}}],["这意味着我们可以将",{"2":{"713":1}}],["这意味着相似的单词",{"2":{"709":1}}],["这意味着所计算的距离可能会根据特征的单位发生倾斜",{"2":{"692":1}}],["这意味着词典长度|v||v||v|越大越好",{"2":{"681":1}}],["这意味着lcm的输入和输出都是sonar嵌入向量",{"2":{"629":1}}],["这意味着lcm处理的不再是单个的词语",{"2":{"628":1}}],["这意味着只有",{"2":{"595":1}}],["这意味着计算注意力分数时",{"2":{"525":1}}],["这意味着在函数内部对数组元素的修改会直接影响到原始数组",{"2":{"1667":1}}],["这意味着在下一步扩展时",{"2":{"904":1}}],["这意味着在同一语料库上使用相同的算法进行训练时",{"2":{"576":1}}],["这意味着在设计提示时",{"2":{"504":1}}],["这意味着在这些操作中使用的权重不是预先确定的",{"2":{"161":1}}],["这意味着每个进程都有自己的独立内存空间",{"2":{"1572":1}}],["这意味着每个单词在新的表示中不只是自身的信息",{"2":{"270":1}}],["这意味着每个值都通过细化的高阶注意力权重进行缩放",{"2":{"209":1}}],["这意味着很难以并行化的方式开展训练",{"2":{"239":1}}],["这意味着细化这些权重",{"2":{"209":1}}],["这意味着中间层很多的token其实作用不大",{"2":{"204":1}}],["这意味着中间层通常比隐藏层小四倍",{"2":{"100":1}}],["这意味着模型将难以收敛",{"2":{"187":1}}],["这意味着q和k可以在不同的语义空间中进行表达",{"2":{"172":1}}],["这意味着",{"2":{"137":1,"231":1,"504":1,"628":1,"719":1,"1222":1,"1620":1,"1728":1}}],["这意味着直径较小的掩码在函数逼近能力方面更高效",{"2":{"93":1}}],["这意味着ϵϵ",{"2":{"93":1}}],["这意味着对于每一行",{"2":{"970":1}}],["这意味着对于半径较大的图",{"2":{"93":1}}],["这意味着对于实践中使用的各种注意力掩码",{"2":{"93":1}}],["这意味着可能一个",{"2":{"87":1}}],["这一算法优化了碰撞检测和矩阵计算公式",{"2":{"2011":1}}],["这一术语拥有一种更加精确的含义",{"2":{"1185":1}}],["这一最流行的llm库以及huggingface文本生成推理",{"2":{"980":1}}],["这一属性源自这样一个事实",{"2":{"975":1}}],["这一突破被认为是深度学习在计算机视觉领域的重要里程碑",{"2":{"840":1}}],["这一模型也可以使用反向传播算法进行训练",{"2":{"769":1}}],["这一说法背后的想法",{"2":{"756":1}}],["这一步对应的就是发射分数",{"2":{"1323":1}}],["这一步帮助模型习惯于同时关注前后文",{"2":{"734":1}}],["这一步是将实数域的分数映射到概率分布上",{"2":{"173":1}}],["这一步是计算在高维空间中度量向量之间的相似性",{"2":{"173":1}}],["这一系列工作主要使用句尾的作为特殊token",{"2":{"731":1}}],["这一系列物品对人下意识的吸引力",{"2":{"163":1}}],["这一阶段的难点在于",{"2":{"726":1}}],["这一阶段的核心技术是对比学习",{"2":{"726":1}}],["这一篇就够了",{"2":{"638":1}}],["这一概念来建模所有的计算",{"2":{"624":1}}],["这一维度独立于输入和输出维度",{"2":{"621":1}}],["这一维度各个特征的量纲应该相同",{"2":{"326":1}}],["这一改变在保持训练稳定性和提升模型收敛速度的同时",{"2":{"346":1}}],["这一发现不仅深化了我们对transformer模型的理解",{"2":{"507":1}}],["这一发现打破了人们将各种",{"2":{"320":1}}],["这一发现为设计更高效的注意力机制提供了理论基础",{"2":{"92":1}}],["这一性质进而决定了经过归一化操作后",{"2":{"314":1,"322":1}}],["这一想法源自英语翻译练习",{"2":{"284":1}}],["这一理论有助于阐明为什么先前的特征映射会提高信息熵",{"2":{"213":1}}],["这一套思想到底怎么去理解",{"2":{"162":1}}],["这一机制不仅使模型能够学习新的信息",{"2":{"148":1,"484":1}}],["这一事实",{"2":{"130":1}}],["这一结论被后来改进的实验方案证明是基本正确的",{"2":{"129":1}}],["这一部分就很好的解释了",{"2":{"83":1}}],["这一行代码",{"2":{"66":1,"74":1,"382":1}}],["这两种模型在实践中体现出极好的性能",{"2":{"1371":1}}],["这两种方案有如下分别",{"2":{"745":1}}],["这两种方法都有一定的局限性",{"2":{"735":1}}],["这两种方法都能达到学习如何写作的目的",{"2":{"542":1}}],["这两种方法有一点区别",{"2":{"659":1}}],["这两点我们在第一部分中已给出详细说明",{"2":{"972":1}}],["这两项改进措施",{"2":{"501":1}}],["这两个",{"2":{"2054":1}}],["这两个函数用于获取当前文件指针位置",{"2":{"1821":1,"1839":1}}],["这两个参数是用来学习的参数",{"2":{"807":1}}],["这两个参数其实对应batchnorm的参数",{"2":{"343":1}}],["这两个方案就分别对应了绝对位置编码和相对位置编码",{"2":{"744":1}}],["这两个字",{"2":{"714":1}}],["这两个和嵌入层密切相关的概念",{"2":{"676":1}}],["这两个概念",{"2":{"629":1}}],["这两个较小的局部模型将字节序列编码为",{"2":{"614":1}}],["这两个词都有一个共同的",{"2":{"579":1}}],["这两个词然后把这两个词关联起来",{"2":{"246":1}}],["这两个来源分别来自不同的范畴",{"2":{"536":1}}],["这两个向量的维度是一样的",{"2":{"519":1}}],["这两个构成了",{"2":{"515":1}}],["这两个输出分别代表t时刻前的所有memory和hidden",{"2":{"287":1}}],["这两个维度合并",{"2":{"36":1}}],["这两个维度",{"2":{"36":2}}],["这两个维度改变",{"2":{"31":1}}],["这两篇论文也做了深入研究",{"2":{"126":1}}],["这两者共同工作以提高模型的性能",{"2":{"120":1}}],["这两步分别是",{"2":{"79":1}}],["这是由这四个数字能构成的最大四位数",{"2":{"2126":1}}],["这是由于在整个模型的运行过程中",{"2":{"173":1}}],["这是贪心算法的基本策略",{"2":{"2124":1}}],["这是本专题的基本大纲",{"2":{"2111":1}}],["这是无价的",{"2":{"2056":1}}],["这是很难做到的",{"2":{"2054":1}}],["这是很多模型的基础",{"2":{"450":1}}],["这是所谓的",{"2":{"2054":1}}],["这是做人的基本",{"2":{"2054":1}}],["这是判断闰年的核心逻辑",{"2":{"1729":1}}],["这是最常用的传递方式",{"2":{"1729":1}}],["这是最终手段",{"2":{"1180":1}}],["这是递归函数不再调用自身的条件",{"2":{"1646":1}}],["这是初学者常常混淆的一个概念",{"2":{"1614":1}}],["这是构建任何程序的基础",{"2":{"1607":1}}],["这是大多数优化器支持的简化版本",{"2":{"1223":1}}],["这是大规模预训练任务的核心目标",{"2":{"542":1}}],["这是在推理阶段的第一个应用",{"2":{"977":1}}],["这是否意味着decoder模型学到了一些通用的知识呢",{"2":{"898":1}}],["这是电脑硬件中所对应的一个软件",{"2":{"794":1}}],["这是绝对位置编码所不具备的",{"2":{"757":1}}],["这是相对位置编码的重要理论支撑之一",{"2":{"754":1}}],["这是通用embedding模型能训练出来的前提",{"2":{"725":1}}],["这是通过查找",{"2":{"700":1}}],["这是通过data",{"2":{"603":1}}],["这是通过在mlp的activation后接上一个过完备的autoencoder来完成的",{"2":{"137":1}}],["这是输入序列对自身的注意力计算",{"2":{"525":1}}],["这是",{"2":{"522":1,"1623":1,"1704":1}}],["这是自注意力机制的唯一弱点",{"2":{"511":1}}],["这是从一个语义空间迁移到了另一个语义空间的过程",{"2":{"510":1}}],["这是从原始的源序列文本得到的token列表",{"2":{"453":1}}],["这是n个粒子共同作用的一个n元函数",{"2":{"498":1}}],["这是新一类的深度神经网络",{"2":{"493":1}}],["这是识别和编辑存储知识的重要领域",{"2":{"485":1}}],["这是transformer模型高效并行处理的关键",{"2":{"519":1}}],["这是transformer应用的经典案例",{"2":{"510":1}}],["这是transformer所缺乏的",{"2":{"480":1}}],["这是transformer主体类",{"2":{"449":1}}],["这是anthropic的工作",{"2":{"475":1}}],["这是解码器的实现",{"2":{"450":1}}],["这是编码器的实现",{"2":{"450":1}}],["这是语境化操作",{"2":{"431":1}}],["这是post",{"2":{"330":1}}],["这是按照样本数计算归一化统计量的",{"2":{"316":1}}],["这是为了平滑",{"2":{"398":1}}],["这是为了保证所有源元素贡献的特征总量保持一定",{"2":{"168":1}}],["这是为何",{"2":{"301":1}}],["这是不合理的",{"2":{"252":1}}],["这是块间注意力的特殊case",{"2":{"204":1}}],["这是一份在线文档",{"2":{"1196":1}}],["这是一项核心技术",{"2":{"980":1}}],["这是一件更难的事情",{"2":{"140":1}}],["这是一种非常危险的行为",{"2":{"1670":1}}],["这是一种不用于禁用梯度计算的方法",{"2":{"1116":1}}],["这是一种受操作系统中虚拟内存和分页",{"2":{"981":1}}],["这是一种常见且众所周知的表示分类数据的方法",{"2":{"681":1}}],["这是一种新的完全基于注意力的更灵活的架构",{"2":{"618":1}}],["这是一种旨在识别与特定特征相关的",{"2":{"477":1}}],["这是一种自适应操作",{"2":{"276":1}}],["这是一种数据依赖的加权平均",{"2":{"261":1,"267":1}}],["这是一种使用强化学习",{"2":{"224":1}}],["这是一种",{"2":{"125":1}}],["这是一个很好的",{"2":{"1175":1}}],["这是一个开放性问题",{"2":{"1171":1}}],["这是一个可能变化的实现细节",{"2":{"1114":1}}],["这是一个用于快速llm推理和服务的开源库",{"2":{"980":1}}],["这是一个保存",{"2":{"700":1}}],["这是一个基本的语言学假设",{"2":{"685":1}}],["这是一个基于transformer解码器的基础模型",{"2":{"632":1}}],["这是一个将字节分组为静态token集的启发式预处理步骤",{"2":{"612":1}}],["这是一个低阶语义向量序列",{"2":{"516":1}}],["这是一个固定长度的高维特征向量c=",{"2":{"241":1}}],["这是一个显而易见的结论",{"2":{"190":1}}],["这是一个非线性激活函数",{"2":{"99":1}}],["这是一个对角线以及之下都是true的矩阵",{"2":{"74":1,"382":1}}],["这是一个下三角矩阵",{"2":{"70":1}}],["这是relu的平滑版本",{"2":{"106":1}}],["这是第一个线性层的逆操作",{"2":{"99":1}}],["这是神经网络中常用的结构",{"2":{"98":1}}],["这是因为训练模型的时间越长",{"2":{"1157":1}}],["这是因为较小的数据集可能无法很好地捕捉到真实数据的复杂性",{"2":{"1012":1}}],["这是因为如果把parameters初始化",{"2":{"992":1}}],["这是因为如果注意力模块是没有mask的self",{"2":{"409":1}}],["这是因为现代gpu具有专门的计算单元",{"2":{"968":1}}],["这是因为现有的llm都缺少人类智能的一个重要的特点",{"2":{"627":1}}],["这是因为无论query位置如何",{"2":{"760":1}}],["这是因为在反向传播中需要在sram中保持更多的值来执行5次矩阵乘法",{"2":{"964":1}}],["这是因为在计算注意力分数时会涉及到一个点积操作",{"2":{"701":1}}],["这是因为在实际情况中",{"2":{"301":1}}],["这是因为成功的语言交流依赖于对世界的共同体验",{"2":{"689":1}}],["这是因为计算机无法直接处理非数值性计算",{"2":{"677":1}}],["这是因为ground",{"2":{"412":1}}],["这是因为对输入层神经元进行丢弃时",{"2":{"396":1}}],["这是因为我们最后一次的输入tgt是",{"2":{"381":1}}],["这是因为我们认为",{"2":{"137":1}}],["这是因为大多数llm总是选择最后一个",{"2":{"378":1}}],["这是因为cv使用卷积核来输出特征图",{"2":{"325":1}}],["这是因为微调的过程虽然简单",{"2":{"222":1}}],["这是因为缺乏对注意力权重的明确强调",{"2":{"209":1}}],["这是因为长文本往往在长度分布上可以跨越多个数量级",{"2":{"87":1}}],["这是因为",{"2":{"79":1,"402":1}}],["这是属于未来的信息",{"2":{"74":1}}],["这是个10维度的下三角矩阵",{"2":{"70":1}}],["这是柳浩老师的transformer系列文章",{"2":{"48":1}}],["这是把",{"2":{"33":1}}],["这是独立的分组",{"2":{"33":1}}],["这提高了训练和推理的效率",{"2":{"21":1}}],["这种贪心策略能够保证最多的小孩得到满足",{"2":{"2156":1}}],["这种简单的选择方式虽然看起来随意",{"2":{"2101":1}}],["这种行为也可以看作是一种",{"2":{"2100":1}}],["这种行为称之为",{"2":{"992":1}}],["这种决策方式本质上就是我们在生活中对任务的优先级进行排序",{"2":{"2099":1}}],["这种远不是距离远",{"2":{"2054":1}}],["这种强烈的感觉或许会让你顿时感到无比欣慰",{"2":{"2054":1}}],["这种见习是自然而然的",{"2":{"2054":1}}],["这种老板会不成功吗",{"2":{"2051":1}}],["这种过程充满挑战",{"2":{"2010":1}}],["这种指向一维数组的指针被称为行指针或数组指针",{"2":{"1705":1}}],["这种指针被称为野指针",{"2":{"1611":1}}],["这种继承关系的逻辑结构图类似于菱形",{"2":{"1661":1}}],["这种继承方式称为多重继承",{"2":{"1659":1}}],["这种技术适用于所有网络层",{"2":{"1443":1}}],["这种技术可以通过在训练时向解码器输入整个目标序列来一次性并行解码全部输出",{"2":{"406":1}}],["这种迁移学习的好处是",{"2":{"1313":1}}],["这种迁移学习的思想使得使用预训练语言模型来初始化decoder参数成为一种有效的策略",{"2":{"898":1}}],["这种编译过程可以包括静态类型推断",{"2":{"1288":1}}],["这种减小学习率的过程类似于金属冶炼中的加热和冷却过程",{"2":{"1242":1}}],["这种误差会造成优化效果变差",{"2":{"1175":1}}],["这种考虑很少是一个实际的问题",{"2":{"1165":1}}],["这种更好的运行时性能伴随着一个缺点",{"2":{"1121":1}}],["这种更精确的依赖关系建模带来了二次方的计算成本",{"2":{"227":1}}],["这种随机性也有助于sgd跳出局部最小值并继续搜索更好的解",{"2":{"1026":1}}],["这种策略自20",{"2":{"1016":1}}],["这种数据结构使得前缀搜索",{"2":{"985":1}}],["这种数学运算",{"2":{"769":1}}],["这种内存效率的提升非常有益",{"2":{"982":1}}],["这种进一步的分解会导致动态调整每个组中节点数量的问题",{"2":{"977":1}}],["这种重叠机制适用于我们方法的正向和反向传递",{"2":{"975":1}}],["这种掩码结合将输出嵌入偏移一个位置",{"2":{"915":1}}],["这种attention机制都与循环网络一起使用",{"2":{"911":1}}],["这种对应关系可以是一对一",{"2":{"908":1}}],["这种对模型中的ffn层的值矩阵进行编辑的方案可能会引起遗忘和其他的副作用",{"2":{"143":1}}],["这种greedy的方式",{"2":{"901":1}}],["这种预训练",{"2":{"898":1}}],["这种操作的目的就是为了使得训练过程更容易",{"2":{"896":1}}],["这种操作增强了模型的表达能力",{"2":{"117":1}}],["这种1",{"2":{"882":1}}],["这种结构通常用来处理序列分类问题",{"2":{"881":1}}],["这种情况有可能会被mish改变",{"2":{"846":1}}],["这种情况下",{"2":{"87":1,"89":1,"1326":1}}],["这种池化方法通常在网络最后",{"2":{"816":1}}],["这种做法过程简单",{"2":{"907":1}}],["这种做法使得全连接层参数很多",{"2":{"816":1}}],["这种做法生成的词汇表在很大程度上取决于输入到",{"2":{"595":1}}],["这种关系能否用公式来表达",{"2":{"773":1}}],["这种修改独立于值向量",{"2":{"745":1}}],["这种双向设计使得elmo不仅能看到本单词前面的单词",{"2":{"717":1}}],["这种嵌入被认为是静态的缺失语境信息的",{"2":{"715":1}}],["这种灵活性使得transformer模型可以广泛应用于多种自然语言处理任务",{"2":{"696":1}}],["这种作用包括向量之间的几何关系",{"2":{"689":1}}],["这种基于连续向量空间的建模方式",{"2":{"629":1}}],["这种缩放方案允许在不改变输入或输出维度的情况下集成任意数量的参数",{"2":{"623":1}}],["这种解耦使输入数据能够与可变数量的参数动态交互",{"2":{"621":1}}],["这种解释主要是基于概率分布的",{"2":{"314":1}}],["这种线性投影设计限制了模型的灵活性和可扩展性",{"2":{"618":1}}],["这种patch函数存在显著的缺点",{"2":{"613":1}}],["这种权衡限制了基于token化的方法在token大小和推理成本上实现显著提升",{"2":{"612":1}}],["这种偏重于如何压缩字符串的token化方式会导致一些缺点",{"2":{"612":1}}],["这种偏移会导致每个后续层接收到的输入分布各不相同",{"2":{"309":1}}],["这种无需",{"2":{"610":1}}],["这种语言模型可以给多种分词结果赋予概率",{"2":{"603":1}}],["这种算法的字面理解是把word拆成一片一片",{"2":{"597":1}}],["这种算法总是在给定语料库中最适合选择的子词",{"2":{"576":1}}],["这种歧义可能在解码阶段无法解决",{"2":{"595":1}}],["这种词表长尾效应严重",{"2":{"565":1}}],["这种目标与网络架构直接对齐",{"2":{"542":1}}],["这种缓存机制避免了为已经处理过的标记token重新计算",{"2":{"542":1}}],["这种上下文信息可以被视为具有与梯度下降类似的效果",{"2":{"542":1}}],["这种设置是因为在编码阶段",{"2":{"525":1}}],["这种设计使得系统能够高效地利用内存",{"2":{"986":1}}],["这种设计使其不仅在自然语言处理领域",{"2":{"512":1}}],["这种设计是transformer模型能够有效处理类似文本翻译任务的关键所在",{"2":{"535":1}}],["这种设计是transformer模型能够并行训练的关键所在",{"2":{"535":1}}],["这种设计确保了模型在多种交互模式下的泛化能力",{"2":{"369":1}}],["这种设计的优势在于",{"2":{"153":1}}],["这种复杂度使得该架构在涉及长输入序列或资源受限情况下计算成本高昂且占用内存巨大",{"2":{"512":1}}],["这种类比和推理的过程是通过模型内部的神经网络层次结构和权重参数实现的",{"2":{"505":1}}],["这种概念的应用可以使得类比更加灵活和高效",{"2":{"505":1}}],["这种系统能够模拟任意其他可计算的计算机",{"2":{"504":1}}],["这种新的注意力机制设计具有以下优势",{"2":{"621":1}}],["这种新的",{"2":{"498":1}}],["这种常微分方程的数值解法对应一种神经网络的话",{"2":{"498":1}}],["这种动态系统的常微分方程叫做对流扩散方程",{"2":{"498":1}}],["这种序列学习器可以捕捉海马体和内嗅皮层",{"2":{"490":1}}],["这种自注意机制可能发生在大脑中",{"2":{"488":1}}],["这种限制可以通过考虑值加权或输出值加权向量",{"2":{"478":1}}],["这种组合表示捕获了标记的语义及其在输入序列中的位置",{"2":{"460":1}}],["这种串行操作会极大影响训练模型的时间",{"2":{"443":1}}],["这种抑制具体又分为两种",{"2":{"437":1}}],["这种将同一结构重复多次的分层机制就是栈",{"2":{"436":1}}],["这种将输入输出进行连接的操作被称为shortcut",{"2":{"298":1}}],["这种约束在训练过程中减少模型发散",{"2":{"411":1}}],["这种因为训练和推理之间数据分布存在差异",{"2":{"411":1}}],["这种模块化的设计方法可以降低程序的复杂性",{"2":{"1729":1}}],["这种模块化还支持持续学习",{"2":{"222":1}}],["这种模型架构避免循环并完全依赖于attention机制来绘制输入和输出之间的全局依赖关系",{"2":{"911":1}}],["这种模式适合有集中管理需求的任务分解",{"2":{"1578":1}}],["这种模式简单高效",{"2":{"1568":1}}],["这种模式我们称之为free",{"2":{"894":1}}],["这种模式具体如下图所示",{"2":{"406":1}}],["这种现象被称作进程互斥",{"2":{"1409":1}}],["这种现象和人说话的逻辑是相似的",{"2":{"405":1}}],["这种现象在较小数据集或窄任务领域训练时尤为普遍",{"2":{"222":1}}],["这种计算方式使得rmsnorm",{"2":{"346":1}}],["这种归一化会导致语义发生改变",{"2":{"323":1}}],["这种诠释也无法进一步解释其他归一化手段",{"2":{"314":1}}],["这种表征更加丰富",{"2":{"261":1}}],["这种架构通常称为多层感知机",{"2":{"1464":1}}],["这种架构更类似于传统的序列到序列模型",{"2":{"635":1}}],["这种架构简单直接",{"2":{"632":1}}],["这种架构创新可以有助于缓解梯度消失问题",{"2":{"470":1}}],["这种架构因而被称为编码器",{"2":{"241":1}}],["这种架构设计在文献中更为常见",{"2":{"231":1}}],["这种神经记忆具有快速并行化训练的优势",{"2":{"228":1}}],["这种",{"2":{"222":1,"536":1,"626":1,"986":1}}],["这种有自适应能力的模型明显学习效率更高",{"2":{"220":1}}],["这种优化大大减少了计算和内存需求",{"2":{"216":1}}],["这种优化可以在不更新mhsa权重的情况下实现",{"2":{"144":1}}],["这种分解允许机制分别考虑正相似度和负相似度对注意力权重的影响",{"2":{"213":1}}],["这种均匀性削弱了模型区分强弱",{"2":{"212":1}}],["这种选择性表示限制了模型捕获全面关系范围的能力",{"2":{"212":1}}],["这种机制使得模型能够在训练过程中更有效地调整权重",{"2":{"180":1}}],["这种机制使得模型能够从上下文中提取",{"2":{"122":1}}],["这种变换不是固定的",{"2":{"170":1,"173":1}}],["这种映射关系在模型训练的过程中一直在更新",{"2":{"706":1,"709":1}}],["这种映射允许模型将离散的类别信息转化为连续的向量表示",{"2":{"702":1}}],["这种映射方式确保了哈希表中存储的特征能够不断适应输入数据",{"2":{"153":1}}],["这种映射可以看作是完成了部分",{"2":{"122":1}}],["这种投影有助于理解",{"2":{"147":1,"482":1}}],["这种观察可能意味着mhsa在捕捉和编码输入数据中的某些模式或关系时",{"2":{"144":1}}],["这种参数共享在处理简单任务的时候",{"2":{"119":1}}],["这种相互作用随着参数数量的增加而逐渐增强",{"2":{"119":1}}],["这种方案将偏置渗透进特征向量",{"2":{"766":1}}],["这种方案面临两个问题",{"2":{"287":1}}],["这种方案可以让注意力头的表示方式更加灵活",{"2":{"19":1}}],["这种方法比起正式的逻辑学推理演算更具有优势",{"2":{"1456":1}}],["这种方法称为",{"2":{"897":1}}],["这种方法要简单得多",{"2":{"756":1}}],["这种方法允许根据不同任务的需求调整rw和hs的权重",{"2":{"739":1}}],["这种方法很简单",{"2":{"739":1}}],["这种方法能够有效的将大模型改造成通用化的文本编码器",{"2":{"734":1}}],["这种方法能够更好地捕捉文本的整体语义结构",{"2":{"628":1}}],["这种方法除了考虑向量表示外",{"2":{"692":1}}],["这种方法可以在不损失太多性能的情况下显著降低计算成本",{"2":{"636":1}}],["这种方法可以生成更具多样性和创造性的文本",{"2":{"634":1}}],["这种方法可以提高压缩率",{"2":{"553":1}}],["这种方法显著提高了更大规模模型的训练效率",{"2":{"623":1}}],["这种方法使用熵估计来推导patch边界",{"2":{"613":1}}],["这种方法尤其适用于处理大量的未知词汇或拼写错误的情况",{"2":{"594":1}}],["这种方法在理论上可以支持较深层resnets的训练",{"2":{"497":1}}],["这种方法不仅扩大了训练数据的规模",{"2":{"369":1}}],["这种方法不仅提高了数据质量",{"2":{"369":1}}],["这种方法太复杂且实现困难",{"2":{"284":1}}],["这种方法的潜力常常受到约束",{"2":{"151":1}}],["这种方法的优势是",{"2":{"148":1}}],["这种方法类似于人类认知过程中的掌握阶段",{"2":{"141":1}}],["这种方法类似于人类认知过程中的识别阶段",{"2":{"141":1}}],["这种方法与人类认知过程中的关联阶段非常相似",{"2":{"141":1}}],["这种方法旨在在保持模型处理通用输入的总体性能的同时",{"2":{"121":1}}],["这种方式适用于需要确保消息到达的情景",{"2":{"1574":1}}],["这种方式带来的开销是我们无法忍受的",{"2":{"908":1}}],["这种方式很可能",{"2":{"901":1}}],["这种方式摒弃了网络中大量的冗余信息",{"2":{"814":1}}],["这种方式直接将偏置加在自注意力矩阵",{"2":{"766":1}}],["这种方式允许每个单词同时考虑上下文中的所有其他单词",{"2":{"734":1}}],["这种方式往往受限于数据增强的质量",{"2":{"727":1}}],["这种方式并没有考虑注意力权重内的潜在相关性",{"2":{"209":1}}],["这种方式叫作log",{"2":{"183":1}}],["这种方式类似于通过大规模的记忆存储",{"2":{"118":1}}],["这种方式计算快",{"2":{"32":1}}],["这种逻辑分割",{"2":{"29":1}}],["这种拆分完全可以看做将前一步input",{"2":{"28":1}}],["这种特性使得transformer可以更好地理解数据中的复杂模式和语义信息",{"2":{"1":1}}],["这个点就是全局最优解",{"2":{"2115":1}}],["这个小山峰是最高的",{"2":{"2115":1}}],["这个在孔子看来是很重要的一件事",{"2":{"2054":1}}],["这个维修的话会怎么怎么样",{"2":{"2051":1}}],["这个表达式实现了闰年的判断规则",{"2":{"1729":1}}],["这个表征是本单词被相同序列中的其它单词所影响的结果",{"2":{"261":1}}],["这个房间可以用来存放不同类型的",{"2":{"1728":1}}],["这个新的数据类型就像一个",{"2":{"1728":1}}],["这个新的向量就是",{"2":{"170":1}}],["这个指针的类型是",{"2":{"1705":1}}],["这个指针被称为",{"2":{"1638":1}}],["这个例子定义了一个",{"2":{"1700":1,"1701":1}}],["这个例子定义了一个通用的",{"2":{"1698":1,"1699":1}}],["这个例子里",{"2":{"122":1}}],["这个大小取决于",{"2":{"1678":1}}],["这个大小取决于你的计算机系统架构",{"2":{"1611":1}}],["这个空字符标志着字符串的结尾",{"2":{"1624":1}}],["这个水管网络有许多层",{"2":{"1466":1}}],["这个差异",{"2":{"1443":1}}],["这个目标值可以在训练数据集中找到",{"2":{"1443":1}}],["这个目标token是",{"2":{"170":1}}],["这个变分表示了函数y",{"2":{"1377":1}}],["这个improved",{"2":{"1363":1}}],["这个设计的思路其实也很直观",{"2":{"1340":1}}],["这个设计因其简单易实现的优点已在实践中大获成功",{"2":{"44":1}}],["这个转移分数矩阵是crf中的一个可学习的参数矩阵",{"2":{"1324":1}}],["这个微调的过程只需要很少的数据",{"2":{"1313":1}}],["这个跟踪模型可以在后续的推理过程中重复使用",{"2":{"1292":1}}],["这个脚本是模型的静态表示形式",{"2":{"1291":1}}],["这个方法在torch",{"2":{"1214":1}}],["这个方案有一个问题",{"2":{"679":1}}],["这个说法",{"2":{"1185":1}}],["这个术语来表示学习率",{"2":{"1185":1}}],["这个阈值的大小与工作量有关",{"2":{"1184":1}}],["这个计划会将学习率从0提升到某个稳定的",{"2":{"1183":1}}],["这个上下文管理器使得在不必临时将张量设置为",{"2":{"1120":1}}],["这个grad",{"2":{"1099":1}}],["这个名字派生自短语",{"2":{"1059":1}}],["这个名称来源于纽约的曼哈顿",{"2":{"692":1}}],["这个score代表了当前到第t步的输出序列的一个综合得分",{"2":{"903":1}}],["这个特殊标记",{"2":{"899":1}}],["这个projection层",{"2":{"899":1}}],["这个context",{"2":{"888":1}}],["这个模型目前没有开源",{"2":{"1316":1}}],["这个模型最初设计用于改进机器翻译技术",{"2":{"885":1}}],["这个模块经常被用来存储单词嵌入",{"2":{"834":1}}],["这个模块想要做的事情就是想把输入x转换为另外一个向量r",{"2":{"519":1}}],["这个模块由两个线性变换组成",{"2":{"517":1}}],["这个模块的工作方式如下",{"2":{"228":1}}],["这个函数可以接受任意类型的参数",{"2":{"1701":1}}],["这个函数已被弃用",{"2":{"1214":1}}],["这个函数决定了哪些值将被更新",{"2":{"866":1}}],["这个函数的有一个输入参数",{"2":{"344":1}}],["这个函数的输入是",{"2":{"344":1}}],["这个非线性函数被称为激活函数",{"2":{"838":1}}],["这个图就是我们常说的深度学习网络图",{"2":{"785":1}}],["这个偏置根据",{"2":{"765":1}}],["这个时候其实模型已经具备生成高质量的token级别表征的能力",{"2":{"734":1}}],["这个时候可能需要借助上下文的信息和一些动态规划的算法来进行解码",{"2":{"608":1}}],["这个主要借鉴于cv",{"2":{"718":1}}],["这个操作等于将单词融合进了上下文的语义",{"2":{"718":1}}],["这个操作的目的是为了数值稳定性和训练稳定性",{"2":{"701":1}}],["这个字",{"2":{"714":1,"715":1}}],["这个观点就是语言学家",{"2":{"713":1}}],["这个观点饱受质疑",{"2":{"314":1}}],["这个统一结构和语言",{"2":{"689":1}}],["这个torch",{"2":{"1105":1}}],["这个token",{"2":{"620":1}}],["这个token为例",{"2":{"418":1}}],["这个tensor",{"2":{"661":1,"1104":1}}],["这个阶段涉及模型参数和输入token之间的交互",{"2":{"620":1}}],["这个子词也被我们称为token",{"2":{"584":1}}],["这个子层的目的是让解码器结合编码器的输出",{"2":{"525":1}}],["这个基础词表就是我们词表的初始状态",{"2":{"580":1}}],["这个标识符我们称之为token",{"2":{"547":1}}],["这个序号就是每个token对应的one",{"2":{"545":1}}],["这个序列进行编码",{"2":{"525":1}}],["这个张量中",{"2":{"530":1}}],["这个合并其实就是简单的加法",{"2":{"517":1}}],["这个合并操作本质上是与分割操作相反",{"2":{"35":1}}],["这个nx就表示编码器是由几个encoderlayer堆叠而成",{"2":{"517":1}}],["这个高阶语义向量序列将被后续的解码器使用并生成最终输出序列",{"2":{"516":1}}],["这个解码过程其实就是标准的seq2seq流程",{"2":{"515":1}}],["这个固定大小的模型在计算复杂度上几乎可以达到所有不限大小的transformer模型的理论上限",{"2":{"504":1}}],["这个想法类似于电气工程中提出的差分放大器",{"2":{"502":1}}],["这个网络叫做macaron结构",{"2":{"498":1}}],["这个网络相对简单",{"2":{"301":1}}],["这个值可以通过一个常微分方程求解器进行计算",{"2":{"494":1}}],["这个回答时",{"2":{"485":1}}],["这个句子时",{"2":{"629":1}}],["这个句子并不能被模型理解",{"2":{"455":1}}],["这个句子编码成隐状态",{"2":{"249":1}}],["这个深度拷贝函数重新命名为c",{"2":{"449":1}}],["这个等式会被打破",{"2":{"402":1}}],["这个类会构造",{"2":{"344":1}}],["这个切片是",{"2":{"341":1}}],["这个面里所有数据叫一个batch",{"2":{"341":1}}],["这个层面应该具有一个较为清晰和严格的逻辑边界",{"2":{"1478":1}}],["这个层的输出结合了编码器输出的上下文信息以及解码器当前步骤生成的目标词的表示",{"2":{"525":1}}],["这个层对经过注意力处理后的向量进一步进行处理和优化",{"2":{"517":1}}],["这个层次结构以表层特征作为基础",{"2":{"437":1}}],["这个层就越",{"2":{"334":1}}],["这个层增强了网络控制和调节信息流的能力",{"2":{"98":1}}],["这个关系可以无限制的表示整个序列",{"2":{"287":1}}],["这个对象本身的大小",{"2":{"1678":1}}],["这个对齐系数就是元素之间的相似度或者相关性",{"2":{"265":1}}],["这个对称矩阵的对角线上的值大概率是本行最大的",{"2":{"172":1}}],["这个完整的句子",{"2":{"263":1}}],["这个单词",{"2":{"259":1,"713":1}}],["这个问题与变分推断遇到的几乎一样",{"2":{"1376":1}}],["这个问题没有办法笼统地回答",{"2":{"1177":1}}],["这个问题在应用于注意力矩阵的其他逐元素操作时会变得更加严重",{"2":{"941":1,"960":1}}],["这个问题",{"2":{"595":1}}],["这个问题也可以得到一定缓解",{"2":{"413":1}}],["这个问题点主要存在于rnn",{"2":{"256":1}}],["这个问题时",{"2":{"130":1}}],["这个信息",{"2":{"247":2}}],["这个矩阵的每一行都表示decoder的一个输入token对隐向量中所有token的注意力",{"2":{"536":1}}],["这个矩阵会非常庞大",{"2":{"210":1}}],["这个矩阵代表若干个时刻",{"2":{"79":1,"382":1}}],["这个向量的方差就很大",{"2":{"189":1}}],["这个向量是词向量",{"2":{"176":1}}],["这个向量可以通过跟模型一起学习得到",{"2":{"19":1}}],["这个词",{"2":{"2096":1}}],["这个词都有一个与之对应的固定向量",{"2":{"715":1}}],["这个词的token",{"2":{"579":1}}],["这个词对于的上下文向量",{"2":{"516":1}}],["这个词也可以理解为",{"2":{"170":1}}],["这个词在",{"2":{"130":2}}],["这个机制让模型可以基于相同的注意力机制学习到不同的行为",{"2":{"168":1}}],["这个输出是一个概率分布",{"2":{"267":1}}],["这个输出表征携带了其它单词的信息",{"2":{"168":1}}],["这个输入序列是一个向量列表",{"2":{"158":1}}],["这个多义词进行语义区分",{"2":{"167":1,"259":1}}],["这个相似度得分决定了相应value在最终输出中的权重",{"2":{"164":1}}],["这个三个独立的矩阵q",{"2":{"161":1}}],["这个过程跟计算机里的时间复杂度分析很相似",{"2":{"2106":1}}],["这个过程其实就是一种简单的逻辑推理",{"2":{"2105":1}}],["这个过程称为栈展开",{"2":{"1762":1}}],["这个过程称为模板实例化",{"2":{"1698":1}}],["这个过程称为自注意力",{"2":{"488":1}}],["这个过程不仅需要海量的训练数据",{"2":{"1313":1}}],["这个过程不是一次性的事情",{"2":{"709":1}}],["这个过程也被称为",{"2":{"938":1,"954":1}}],["这个过程我们可以看到",{"2":{"860":1}}],["这个过程重复足够多次后",{"2":{"709":1}}],["这个过程中",{"2":{"601":1}}],["这个过程中会抑制后面的层的学习速度",{"2":{"333":1}}],["这个过程反复迭代直到达到预设的词汇表大小或合并次数为止",{"2":{"576":1}}],["这个过程将给定输入的",{"2":{"485":1}}],["这个过程类似于逐层",{"2":{"437":1}}],["这个过程类似于分组讨论",{"2":{"216":1}}],["这个过程会有一个潜在风险",{"2":{"326":1}}],["这个过程可以通过",{"2":{"222":1}}],["这个过程需要两次从内存读取和一次写回内存操作",{"2":{"179":1}}],["这个过程赋予了",{"2":{"148":1}}],["这个过程就是我们之前提到的teacher",{"2":{"70":1}}],["这个假设认为神经网络将有意义的概念",{"2":{"137":1}}],["这个发现表明",{"2":{"135":1}}],["这个发现类似于cnn里",{"2":{"127":1}}],["这个头部的输出是与关系相关的token",{"2":{"130":1}}],["这个",{"2":{"128":1,"1728":1}}],["这个分量与残差流相加的操作可以被认为是",{"2":{"122":1}}],["这个分量在与残差流相加的时候会完成对残差流方向的操控",{"2":{"122":1}}],["这个线性变换在不同的位置都表现相同",{"2":{"99":1}}],["这个结论可以推广到更一般的注意力模式类别",{"2":{"93":1}}],["这个mask矩阵的特点如下",{"2":{"70":1}}],["这个注意力分数矩阵以后",{"2":{"67":1}}],["这个head通常会把大的权值分配给稀有词",{"2":{"20":1}}],["这个head计算的权值通常会将词语之间的关系联系起来",{"2":{"20":1}}],["这个head计算的权值通常指向临近的词",{"2":{"20":1}}],["这个参数量往往不多",{"2":{"9":1}}],["这里吧前面说的i和j换成了child和cookie",{"2":{"2153":1}}],["这里这样说不太严谨",{"2":{"2117":1}}],["这里讲的",{"2":{"2112":1}}],["这里汇集了我的日常思考",{"2":{"2109":1}}],["这里展示create",{"2":{"2070":1}}],["这里只会记录一些使用过程中的问题",{"2":{"2088":1}}],["这里只考虑了0",{"2":{"2031":1}}],["这里只取最后一个词的输出进行预测",{"2":{"428":1}}],["这里故意使用分号来表示初始化语句",{"2":{"1922":1}}],["这里指的是非法的内存写入",{"2":{"1648":1}}],["这里指大脑关于世界的内部模型的不确定性",{"2":{"612":1}}],["这里我不再过多表达",{"2":{"2097":1}}],["这里我把相关的知识和解决方案大概写一写",{"2":{"2088":1}}],["这里我用我的远程win系统给大家演示",{"2":{"2069":1}}],["这里我借用刘崇军老师给说说的话",{"2":{"1598":1}}],["这里我们提前看看flashattention所要解决的困境",{"2":{"180":1}}],["这里就不展示了",{"2":{"2070":1}}],["这里就不涉及窥探未来信息的问题了",{"2":{"84":1}}],["这里就引出了一个关键问题",{"2":{"2022":1}}],["这里就拿单核单处理器简单演示",{"2":{"1566":1}}],["这里主要讲解mpi实现分布式消息传递",{"2":{"1563":1}}],["这里简单提一下",{"2":{"1408":1}}],["这里其实不是椭圆",{"2":{"1373":1}}],["这里面",{"2":{"1342":1}}],["这里面有很多可以重用的代码",{"2":{"344":1}}],["这里可能还需要进行其他错误处理",{"2":{"1761":1}}],["这里可能对应着很多条不同的路径",{"2":{"1320":1}}],["这里可能存在无穷大",{"2":{"1115":1}}],["这里至少有两种类型的不稳定的训练任务值得需要进行区分",{"2":{"1179":1}}],["这里假设",{"2":{"1004":1}}],["这里假设编码器也用到了掩码",{"2":{"462":1}}],["这里保留训练时的均值和方差",{"2":{"807":1}}],["这里说主要是因为还有一些难以划分的位置编码",{"2":{"742":1}}],["这里重建的目标不仅仅是被mask掉的token",{"2":{"727":1}}],["这里通常会采取比encoder部分更加激进的mask比例",{"2":{"727":1}}],["这里通常会采用中等的mask比例",{"2":{"727":1}}],["这里要注意的是",{"2":{"704":1}}],["这里要对mask做一下说明",{"2":{"198":1}}],["这里d",{"2":{"699":1}}],["这里以增量式",{"2":{"623":1}}],["这里以梯度消失为例",{"2":{"304":1}}],["这里看看如何构建词表",{"2":{"557":1}}],["这里采用xavier初始化",{"2":{"449":1}}],["这里会有一些贪心策略的选择",{"2":{"2117":1}}],["这里会调用到",{"2":{"703":2}}],["这里会预测出第一个单词",{"2":{"445":1}}],["这里会融入目前一些较新的或者有特色的解释",{"2":{"235":1}}],["这里用到dropout",{"2":{"394":3}}],["这里单独阐释",{"2":{"323":1}}],["这里有人会去解读",{"2":{"2054":1}}],["这里有三个白色框",{"2":{"185":1}}],["这里有两步",{"2":{"79":1}}],["这里提到的",{"2":{"172":1}}],["这里的高分辨率数据集是图像尺寸在1024x1024以上",{"2":{"1363":1}}],["这里的向量都是指行向量",{"2":{"1339":1}}],["这里的计算不会被跟踪梯度",{"2":{"1116":2}}],["这里的计算量就越不可忽略",{"2":{"17":1}}],["这里的重点就在于这个",{"2":{"903":1}}],["这里的相对位置其实就是一个分段函数",{"2":{"759":1}}],["这里的其它格式包括我们常见的一切格式的数据",{"2":{"676":1}}],["这里的每个路口如何打方向盘就是一个过于细粒度的单元",{"2":{"626":1}}],["这里的q",{"2":{"533":1}}],["这里的交叉熵就是信息熵",{"2":{"397":1,"612":1}}],["这里的out是decoder的输出",{"2":{"385":1}}],["这里的wμwμw",{"2":{"209":1}}],["这里的bit是语义意义上的",{"2":{"147":1}}],["这里的",{"2":{"83":1,"301":1,"423":1,"567":1,"1611":1,"2054":2}}],["这里调用词表",{"2":{"558":2}}],["这里调用到",{"2":{"83":1}}],["这里调用multiheadedattention",{"2":{"38":1}}],["这里是最新的一个单词的",{"2":{"83":1}}],["这里是",{"2":{"79":1,"198":1}}],["这里不做记录",{"2":{"2089":1}}],["这里不会调用复制构造函数",{"2":{"1931":1}}],["这里不多赘述",{"2":{"1408":1}}],["这里不再对细节进行赘述",{"2":{"750":1}}],["这里不再赘述",{"2":{"74":1,"82":1,"286":1}}],["这里不同请求的矩阵乘向量是不规则的",{"2":{"17":1}}],["这里从第一个编码层来演示",{"2":{"24":1}}],["这里算术强度始终不到",{"2":{"17":1}}],["这里也是",{"2":{"17":1}}],["这里",{"2":{"17":1,"82":1,"125":1,"335":1,"344":1,"912":1,"941":1,"960":1,"1891":1}}],["这里变成矩阵乘矩阵",{"2":{"17":1}}],["这里涉及很多步骤和矩阵运算",{"2":{"16":1}}],["这即是降维操作",{"2":{"16":1}}],["这些技术的进步为游戏",{"2":{"2011":1}}],["这些技术增加了代码复杂性的下届或额外的计算",{"2":{"1149":1}}],["这些都是目前最前沿的技术之一",{"2":{"2010":1}}],["这些都是从数理角度进行优化",{"2":{"185":1}}],["这些成员在",{"2":{"1874":1}}],["这些成员只能在类内部和派生类中访问",{"2":{"1655":1}}],["这些类都包含在",{"2":{"1819":1,"1823":1,"1837":1,"1841":1}}],["这些异常可能是由程序逻辑错误引起的",{"2":{"1761":1}}],["这些算法涵盖了各种数据处理操作",{"2":{"1734":1}}],["这些算子通过张量",{"2":{"785":1}}],["这些函数定义在",{"2":{"1715":1}}],["这些函数的名字是",{"2":{"1712":1}}],["这些是",{"2":{"1703":1}}],["这些知识是构建复杂",{"2":{"1709":1}}],["这些知识是掌握",{"2":{"1678":1}}],["这些知识和结构会体现为概率分布的距离与关系",{"2":{"386":1}}],["这些数据会被转换为字符串形式",{"2":{"1824":1,"1842":1}}],["这些数据是分散的",{"2":{"1728":1}}],["这些数据结构的",{"2":{"1647":1}}],["这些数字在下面代码的注释中也有标明",{"2":{"449":1}}],["这些字符不能直接输入或具有特殊的含义",{"2":{"1616":1}}],["这些字者不具备相同属性或相同物理意义",{"2":{"316":1}}],["这些功能可以帮助我们更快更准确地编写代码",{"2":{"1605":1}}],["这些功能在类似于",{"2":{"137":1}}],["这些工具可以",{"2":{"1589":1}}],["这些工作为后续模块",{"2":{"674":1}}],["这些输入信号通过带权重的连接",{"2":{"1459":1}}],["这些输入构成了矩阵",{"2":{"319":1}}],["这些发射分数",{"2":{"1320":1}}],["这些标签向量将被作为发射分数传入crf中",{"2":{"1320":1}}],["这些强大的模型在各种",{"2":{"1318":1}}],["这些变化使得可以用更少的参数训练更大的模型",{"2":{"1315":1}}],["这些改变显著地提高了模型的性能",{"2":{"1315":1}}],["这些具有开创性的工作促成了两个著名",{"2":{"1312":1}}],["这些优化包括常量折叠",{"2":{"1288":1}}],["这些优势对于实现非常重要",{"2":{"183":1}}],["这些调度器在优化过程中按顺序调用",{"2":{"1247":1}}],["这些钩子函数将在调用self的state",{"2":{"1214":1}}],["这些钩子函数将被传入以下参数",{"2":{"1214":1}}],["这些钩子函数可以原地修改state",{"2":{"1214":1}}],["这些推荐有帮于证明我们创建更多这样的内容是合理的",{"2":{"1196":1}}],["这些推理能力",{"2":{"386":1}}],["这些东西对于每个batch",{"2":{"1186":1}}],["这些填充的数据可以被赋予零的权重",{"2":{"1164":1}}],["这些问题可以通过在固定步长间隔进行评估来更容易地检测到",{"2":{"1164":1}}],["这些问题对于进一步理解语言模型中的事实回忆机制至关重要",{"2":{"122":1}}],["这些试验会产生分歧或得到非常糟糕的结果",{"2":{"1147":1}}],["这些搜索空间往往非常重要",{"2":{"1139":1}}],["这些总是可以在以后添加",{"2":{"1137":1}}],["这些指标应该尽可能地代表在部署环境中测量的内容",{"2":{"1128":1}}],["这些语言模型虽然可以对训练过的语言产生统计意义上的理解",{"2":{"1312":1}}],["这些语句可以在每次迭代时改变图的整体形状和大小",{"2":{"1113":1}}],["这些语料涵盖了各种主题",{"2":{"505":1}}],["这些策略被统称为正则化",{"2":{"1011":1}}],["这些策略被认为有利于外推",{"2":{"756":1}}],["这些查询共享相同的少样本示例",{"2":{"986":1}}],["这些请求包括两个聊天会话",{"2":{"986":1}}],["这些kv",{"2":{"986":1}}],["这些程序在当前系统中处理得很差",{"2":{"985":1}}],["这些cached",{"2":{"981":1}}],["这些序列并行性方法利用了注意力运算的结合特性",{"2":{"977":1}}],["这些序列形的数据就不太好用原始的神经网络处理了",{"2":{"878":1}}],["这些操作不需要与其他主机进行通信",{"2":{"975":1}}],["这些位置",{"2":{"933":1}}],["这些位置的概率就会接近0",{"2":{"651":1,"933":1}}],["这些位置的权重将接近于0",{"2":{"63":1}}],["这些位置的注意力分数也会变成非常大的负数",{"2":{"63":1}}],["这些任务代表了nlp领域中的一些核心问题和应用",{"2":{"906":1}}],["这些门可以在训练过程中学习哪些信息是需要保留或遗忘的",{"2":{"863":1}}],["这些门是不同的神经网络",{"2":{"863":1}}],["这些层之间存在着多层的非线性转换关系",{"2":{"785":1}}],["这些差异如下图所示",{"2":{"745":1}}],["这些小规模数据可以激发llm本身所具备的语义泛化能力",{"2":{"733":1}}],["这些结构对于句子级别的",{"2":{"720":1}}],["这些plm的嵌入空间被证明是各向异性的",{"2":{"711":1}}],["这些patch将由全局",{"2":{"614":1}}],["这些预训练的词向量已经通过大量语料训练",{"2":{"709":1}}],["这些词之间的相关性",{"2":{"709":1}}],["这些词向量构成嵌入矩阵",{"2":{"699":1}}],["这些新的向量是一个很大的痛苦",{"2":{"708":1}}],["这些表达有以下的特征",{"2":{"714":1}}],["这些表示使得不同事物可根据相似性或相关性",{"2":{"696":1}}],["这些表征能力被称为抽象能力或抽象思维",{"2":{"689":1}}],["这些用法在语言语境中的变化",{"2":{"689":1}}],["这些embedding也是可以训练的",{"2":{"676":1}}],["这些信息会被自动移除",{"2":{"1648":1}}],["这些信息被统一称为语言特征",{"2":{"676":1}}],["这些信息不需要",{"2":{"72":1}}],["这些考虑因素促使作者探索对sonar表示进行量化",{"2":{"636":1}}],["这些可学习的token可以和输入token进行注意力计算",{"2":{"620":1}}],["这些transformer",{"2":{"614":1}}],["这些嵌入向量会作为输入送入模型的后续层进行处理",{"2":{"706":1}}],["这些嵌入向量构成的矩阵就是x",{"2":{"161":1}}],["这些嵌入可以选择以散列嵌入的形式来添加附加信息",{"2":{"614":1}}],["这些方案还不能对cpu的速度和数目做出任何的假设",{"2":{"1412":1}}],["这些方案通过以下方式将字节分组为patch",{"2":{"613":1}}],["这些方法的专家能够获得良好的结果",{"2":{"1175":1}}],["这些方法需要领域专家手动选择和设计特征",{"2":{"711":1}}],["这些方法根据网络的层数和激活函数的特点来设置权重的初始值",{"2":{"403":1}}],["这些方法可以分为两类",{"2":{"142":1}}],["这些方法将新知识与原始模型相结合",{"2":{"141":1}}],["这些空白字节是许多语言中语言单元的自然边界",{"2":{"613":1}}],["这些启发式方法并不总是与预测的复杂性相关",{"2":{"612":1}}],["这些",{"2":{"562":1,"567":1}}],["这些提取的特征被一个接一个地馈送到解码器中",{"2":{"540":1}}],["这些隐向量将被每个解码层用于自身的encoder",{"2":{"532":1}}],["这些隐向量可以帮助解码器知道它应该更加关注输入序列哪些位置",{"2":{"532":1}}],["这些隐状态不断将前面信息积累",{"2":{"249":1}}],["这些参数在训练过程中可能并没有学习到有效的信息",{"2":{"512":1}}],["这些细胞可以保存并整合从神经元传递给它们的信息",{"2":{"488":1}}],["这些探索试图从本质上理解神经网络和transformer内部运作的机理",{"2":{"474":1}}],["这些概率决定了每个token成为序列中下一个单词的可能性",{"2":{"473":1}}],["这些关系在本质上就是范畴论概念下事物的米田嵌入",{"2":{"472":1}}],["这些向量之间的远近关系则由他们之间的语义关系决定",{"2":{"714":1}}],["这些向量会被合并成最终的向量进行输出",{"2":{"517":1}}],["这些向量是通过将输入嵌入矩阵与学习到的wqwqw^q",{"2":{"463":1}}],["这些向量能够近似矩阵乘法的结果",{"2":{"153":1}}],["这些块一个接一个地顺序堆叠",{"2":{"461":1}}],["这些整数就是词表的索引",{"2":{"456":1}}],["这些片段被成为token",{"2":{"456":1}}],["这些网络层会逐渐学习到各种范畴之间的关系和相似性",{"2":{"437":1}}],["这些行进行相同的线性变换后",{"2":{"419":1}}],["这些路径两两之间是相互依赖的",{"2":{"419":1}}],["这些子网络都共享原始网络的部分参数",{"2":{"393":1}}],["这些曲线的形状非常类似于完整的或部分的s形曲线",{"2":{"359":1}}],["这些核学习到的特征并不完全是独立的",{"2":{"338":1}}],["这些逐个叠加的同构子层像极了",{"2":{"334":1}}],["这些非线性运算在总体运算中所占的比例相对较小",{"2":{"327":1}}],["这些样本的均值和方差不能反映全局的统计分布息",{"2":{"316":1}}],["这些奇点会导致模型退化",{"2":{"305":1}}],["这些更新较好层的输入是前面没有更新好的层的输出",{"2":{"296":1}}],["这些权重能够被提取出来",{"2":{"1009":1}}],["这些权重在训练时候固定下来",{"2":{"276":1}}],["这些权重值加起来的和为1",{"2":{"199":1}}],["这些典型案例都包括三个组件",{"2":{"273":1}}],["这些角度彼此联系又各有特色",{"2":{"258":1}}],["这些内存负担对训练也造成了很大挑战",{"2":{"255":1}}],["这些专家也很乐意承认他们所做的一些事情可能没有充分的理由",{"2":{"1127":1}}],["这些专家向量可以缩放权重矩阵的奇异值",{"2":{"224":1}}],["这些专家模块的灵活组合也带来了目前尚未解决的挑战",{"2":{"222":1}}],["这些专家模块往往容易过拟合",{"2":{"222":1}}],["这些模式可以影响",{"2":{"1118":1}}],["这些模式可以帮助模型更好地从数据中提取和理解有价值的信息或知识",{"2":{"144":1}}],["这些模式在",{"2":{"985":1}}],["这些模型主要用于编码和表示输入序列",{"2":{"540":1}}],["这些模块的累积大小也会快速增加",{"2":{"222":1}}],["这些矩阵是模型参数的一部分",{"2":{"463":1}}],["这些矩阵是",{"2":{"221":1}}],["这些矩阵的每一",{"2":{"161":1}}],["这些无不体现着那句经典的名言",{"2":{"220":1}}],["这些无效部分参与运算会产生很大隐患",{"2":{"54":1}}],["这些交互作用在两个流中处理",{"2":{"213":1}}],["这些机制通常会丢弃来自负成分的有价值信息",{"2":{"213":1}}],["这些特性使",{"2":{"1932":1}}],["这些特殊标记在构建词汇表时会被添加到词汇表中",{"2":{"557":1}}],["这些特征映射仅保留了正",{"2":{"212":1}}],["这些特点使得知识编辑可能成为更新和优化",{"2":{"139":1}}],["这些增强功能共同提供了一个更稳健的解决方案",{"2":{"211":1}}],["这些各自的处理帮助模型有效捕捉序列中的长距离和短距离依赖",{"2":{"204":1}}],["这些相关的训练参数就在三个权重矩阵中",{"2":{"172":1}}],["这些研究并没有充分利用现代技术",{"2":{"155":1}}],["这些效果应该是相同的",{"2":{"136":1}}],["这些注意力头具有对与特定主语",{"2":{"122":1}}],["这些观察提供了很有价值的研究基础",{"2":{"122":1}}],["这些激活函数是用单变量样条函数来表示的",{"2":{"155":1}}],["这些激活函数的使用发生了变化",{"2":{"103":1}}],["这些激活函数在论文",{"2":{"103":1}}],["这些组件可能缓解秩崩溃问题",{"2":{"91":1}}],["这些组q",{"2":{"16":1}}],["这些被掩盖位置在经过softmax激活函数后",{"2":{"62":1}}],["这些自注意力",{"2":{"9":1}}],["这就像",{"2":{"2101":1}}],["这就像排查电路故障",{"2":{"1729":1}}],["这就像你可以通过不同的",{"2":{"1645":1}}],["这就像我们日常生活中的",{"2":{"1619":1}}],["这就像是测量两个向量之间的夹角",{"2":{"692":1}}],["这就限制了大模型在处理长文本或多轮对话等任务时的效果",{"2":{"1341":1}}],["这就可能存在问题",{"2":{"1149":1}}],["这就可能造成偏移量会很大",{"2":{"326":1}}],["这就导致loss",{"2":{"991":1}}],["这就导致在训练",{"2":{"316":1}}],["这就解释了为什么在进行向量比较时",{"2":{"692":1}}],["这就解决了中间语义编码上下文的长度是固定的问题",{"2":{"444":1}}],["这就需要我们深入分析",{"2":{"677":1}}],["这就需要重新训练整个模型",{"2":{"618":1}}],["这就需要依据后面的",{"2":{"516":1}}],["这就需要模型采用某种机制进行学习",{"2":{"267":1}}],["这就涉及到链式法则",{"2":{"495":1}}],["这就涉及到自回归模型",{"2":{"238":1}}],["这就很方便的进行矩阵计算",{"2":{"415":1}}],["这就避免了错误的持续累加",{"2":{"406":1}}],["这就意味着bge需要支持所有的embedding使用场景",{"2":{"724":1}}],["这就意味着很难以并行化的方式开展训练以提升效率",{"2":{"405":1}}],["这就意味着批次的构成和规模会直接影响batchnorm的效果",{"2":{"316":1}}],["这就意味着",{"2":{"314":1}}],["这就构成了源序列的字典",{"2":{"265":1}}],["这就要解决两个问题",{"2":{"262":1}}],["这就得到了n",{"2":{"242":1}}],["这就好比解码器要去编码器的编码序列那里查字典一样",{"2":{"165":1}}],["这就好像是八个有不同阅读习惯的翻译家一同翻译同一个句子",{"2":{"13":1}}],["这就是二级指针",{"2":{"1611":1}}],["这就是我们今天课程要探讨的内容",{"2":{"1601":1}}],["这就是一个生活中的",{"2":{"2097":1}}],["这就是一个语言模型",{"2":{"908":1}}],["这就是一直沿用至今的",{"2":{"1459":1}}],["这就是数据压缩过程我们所要考虑的问题",{"2":{"1372":1}}],["这就是高斯误差函数名称的由来",{"2":{"844":1}}],["这就是为什么使用",{"2":{"1713":1}}],["这就是为什么叫做旋转位置编码的原因",{"2":{"1342":1}}],["这就是为什么",{"2":{"765":1}}],["这就是注意力模块",{"2":{"757":1}}],["这就是注意力机制的作用",{"2":{"259":1}}],["这就是对应单词的最初词表示",{"2":{"718":1}}],["这就是收敛",{"2":{"709":1}}],["这就是word",{"2":{"704":1}}],["这就是token",{"2":{"676":1}}],["这就是transformer训练时可以并行计算的原因",{"2":{"408":1}}],["这就是说话的逻辑",{"2":{"405":1}}],["这就是归一化方案",{"2":{"309":1}}],["这就是权重退化",{"2":{"296":1}}],["这就是在发展初期",{"2":{"279":1}}],["这就是要查询的内容",{"2":{"265":1}}],["这就是人们经常提到的自注意力机制",{"2":{"261":1}}],["这就是梯度消失问题或者梯度爆炸问题",{"2":{"255":1}}],["这就是编码器最终输出的从多角度集自身与其他各个字关系的矩阵",{"2":{"537":1}}],["这就是编码器",{"2":{"240":1}}],["这就是",{"2":{"117":1,"861":1,"2104":1}}],["这就是sequence",{"2":{"59":1}}],["这就是padding",{"2":{"55":1}}],["这就有效地将每个头的注意得分向量连接成一个合并的注意得分",{"2":{"35":1}}],["这就类似于增加了对应头的权重",{"2":{"10":1}}],["这就带来了几个问题",{"2":{"10":1}}],["这样你可以保证时间用在最重要最紧急的事情上",{"2":{"2134":1}}],["这样你就能去",{"2":{"2131":1}}],["这样你就可以找到最优的超参数组合",{"2":{"1175":1}}],["这样看来",{"2":{"2054":1}}],["这样指针就指向了数组的起始位置",{"2":{"1704":1}}],["这样接收方函数就可以在合适的时机",{"2":{"1645":1}}],["这样便于实现多个子任务并行执行",{"2":{"1577":1}}],["这样便于计算机进行处理和理解",{"2":{"679":1}}],["这样只需要调整控制符就可以生成多样化的文本",{"2":{"1316":1}}],["这样只需要有限个位置编码",{"2":{"759":1}}],["这样实验才能真正朝着目标取得进展",{"2":{"1141":2}}],["这样形成一整块整块",{"2":{"1019":1}}],["这样神经网络就可以对权重参数w不停地迭代更新",{"2":{"988":1}}],["这样虽然可以提升推理的速度",{"2":{"938":1,"954":1}}],["这样训练非常困难",{"2":{"895":1}}],["这样训练就没有效果",{"2":{"58":1}}],["这样与batchsize无关",{"2":{"810":1}}],["这样通过将每个元素的位置嵌入向量与其他位置的偏移向量进行组合",{"2":{"757":1}}],["这样通过矩阵运算实现并行操作",{"2":{"391":1}}],["这样经过调整后的word",{"2":{"717":1}}],["这样经过大量训练之后",{"2":{"172":1}}],["这样我们更有信心相信",{"2":{"1145":1}}],["这样我们从实验中得出的结论就不会受到固定超参数的限定",{"2":{"1143":1}}],["这样我们可以把我们见过的事物和从未见过的事物联系起来",{"2":{"685":1}}],["这样我们就可以放心采样进行数据生成",{"2":{"1374":1}}],["这样我们就可以根据这个规律去抽取样本进行生成",{"2":{"1372":1}}],["这样我们就可以更好的了解模型的性能和限制",{"2":{"1157":1}}],["这样我们就可以通过起始符预测",{"2":{"528":1}}],["这样我们就得到了新的细胞状态",{"2":{"867":1}}],["这样我们就得到了每个头的输入",{"2":{"36":1}}],["这样基本上将所有单词视为相互独立的实体",{"2":{"681":1}}],["这样基于错误输入继续解码就是在错误道路上越走越远",{"2":{"405":1}}],["这样向量维度容易过大",{"2":{"681":1}}],["这样计算效率更高",{"2":{"680":1}}],["这样计算机才可以识别",{"2":{"679":1}}],["这样不利于计算机理解共性",{"2":{"679":1}}],["这样避免了静态词汇表的限制",{"2":{"612":1}}],["这样底层字节层面的共享就可能带来知识迁移",{"2":{"608":1}}],["这样每次解码都会利用前面已经解码输出的所有单词嵌入信息",{"2":{"529":1}}],["这样每个position",{"2":{"101":1}}],["这样每个注意力头可以关注文本中不同的方面和特征",{"2":{"5":1}}],["这样使得整个模型更具有鲁棒性",{"2":{"1019":1}}],["这样使得blt可以依据上下文动态分配资源",{"2":{"613":1}}],["这样使得推理时的行为与训练时完全匹配",{"2":{"525":1}}],["这样使用范畴论来构建和分析深度学习模型",{"2":{"505":1}}],["这样使用该",{"2":{"163":1}}],["这样参数就不是原本的离散序列",{"2":{"495":1}}],["这样有助于在注意力层进行元素间的信息交换之后",{"2":{"466":1}}],["这样有效的减缓了梯度的问题",{"2":{"298":1}}],["这样后续代码会比较简洁",{"2":{"449":1}}],["这样在计算注意力时",{"2":{"443":1}}],["这样在预测token",{"2":{"58":1}}],["这样llm才能处理",{"2":{"431":1}}],["这样推理时遇到的错误输出对于下次推理来说就是在训练数据分布之外",{"2":{"411":1}}],["这样的神经网络结构通常称为多层前馈神经网络",{"2":{"1464":1}}],["这样的数量级进行尝试",{"2":{"1183":1}}],["这样的好处在于",{"2":{"1175":1}}],["这样的好处就是再也没有",{"2":{"88":1}}],["这样的构件可以轻松地在评估完成后进行即时模型检查",{"2":{"1164":1}}],["这样的目标没有什么意义",{"2":{"1143":1}}],["这样的前向后向lstm网络可能是多层的",{"2":{"717":1}}],["这样的表示通常是通过feature",{"2":{"708":1}}],["这样的词之间的相似性几乎为零",{"2":{"691":1}}],["这样的话",{"2":{"651":1,"933":1}}],["这样的token需要被不同地处理",{"2":{"579":1}}],["这样的设计也确保了模型在多个编码器层之间能够有效地传递和处理信息",{"2":{"518":1}}],["这样的偷窥行为会让模型学会偷懒而不是学习规律",{"2":{"409":1}}],["这样写代码的好处是更加通用",{"2":{"344":1}}],["这样最后的xtxtx",{"2":{"334":1}}],["这样可能会导致学习过程出现异常",{"2":{"309":1}}],["这样可以确保大饼干用于更大胃口的小孩",{"2":{"2151":1}}],["这样可以确保模型能够从各种类型的数据中学习",{"2":{"368":1}}],["这样可以尽快减少剩下的钱",{"2":{"2138":1}}],["这样可以尽早让模型进入工作状态",{"2":{"1129":1}}],["这样可以更灵活地处理变长序列数据",{"2":{"1287":1}}],["这样可以更加突出重要的权重",{"2":{"173":1}}],["这样可以在有限的时间和资源内获得最大的理解",{"2":{"1157":1}}],["这样可以在训练后期通过减小学习率来让模型稳定训练",{"2":{"402":1}}],["这样可以很好的模拟人脑神经元工作的原理",{"2":{"840":1}}],["这样可以让我们在短时间内完成更多的事情",{"2":{"2106":1}}],["这样可以让你在有限的时间内完成更多的事情",{"2":{"2099":1}}],["这样可以让生成的各个向量间尽可能的独立",{"2":{"709":1}}],["这样可以让解码器同时对同一序列中的多个token进行解码和预测",{"2":{"525":1}}],["这样可以减少计算和存储的双重负担",{"2":{"684":1}}],["这样可以支持模型参数的动态扩展",{"2":{"622":1}}],["这样可以最大化潜在transformer与字节级模块之间的信息流动",{"2":{"614":1}}],["这样可以恢复原始单词",{"2":{"588":1}}],["这样可以标识单词边界",{"2":{"579":1}}],["这样可以把词的本身的意思和时态分开",{"2":{"576":1}}],["这样可以逐渐构建出更长的词汇或短语表示",{"2":{"576":1}}],["这样可以自适应地动态构建词汇表",{"2":{"575":1}}],["这样可以较好的平衡了词汇量",{"2":{"567":1}}],["这样可以获取输入句子中不同单词之间的相关性",{"2":{"535":1}}],["这样可以获取更多维度的信息和相互关系",{"2":{"462":1}}],["这样可以避免深度神经网络中的梯度消失问题",{"2":{"517":1}}],["这样可以能放大对答案范围的注意力并消除噪音",{"2":{"500":1}}],["这样可以刻画输入序列和输出序列之间的全局依赖关系",{"2":{"444":1}}],["这样可以使模型更好地捕捉序列中的重要信息",{"2":{"442":1}}],["这样可以使得每个神经网络层的输出都在相似的尺度上",{"2":{"331":1}}],["这样可以使得表达式中至少存在一个非线性激活函数relu",{"2":{"301":1}}],["这样可以使得所有权重总和为1",{"2":{"269":1}}],["这样可以即使模型预测对了",{"2":{"399":1}}],["这样可以有效的缓解过拟合的发生",{"2":{"393":1}}],["这样可以降低计算复杂度",{"2":{"382":1}}],["这样可以将不同样本相同维度的特征处理为相同的分布",{"2":{"313":1}}],["这样可以将序列中的任意两个位置之间的距离缩小为一个常量",{"2":{"274":1}}],["这样可以提高代码的可读性和可维护性",{"2":{"1728":1}}],["这样可以提高学习的精度",{"2":{"840":1}}],["这样可以提高softmax函数的运算效率",{"2":{"185":1}}],["这样可以提取关键信息",{"2":{"262":1}}],["这样可以区别不同元素所携带的信息量",{"2":{"256":1}}],["这样可以大幅减少计算复杂度",{"2":{"204":1}}],["这样可以用更少的资源快速获取最有用的信息",{"2":{"169":1}}],["这样可以单独调节每一个源元素与每一个目标元素之间的注意力强度",{"2":{"70":1}}],["这样可以忽略填充部分和未来词汇",{"2":{"380":1}}],["这样可以忽略填充部分",{"2":{"66":1,"380":1}}],["这样可以保证模型在生成序列时只依赖于已经生成的部分",{"2":{"50":1}}],["这样设计的好处是",{"2":{"301":1}}],["这样它可以并行操作",{"2":{"291":1}}],["这样会导致训练时的累积损失太大",{"2":{"895":1}}],["这样会导致不正确的结构",{"2":{"378":1}}],["这样会分身乏术",{"2":{"288":1}}],["这样会削弱模型关注其它高价值位置上的能力",{"2":{"3":1}}],["这样注意力机制就和rnn一起配合",{"2":{"284":1}}],["这样注意力机制才能找到词义与句子位置的关系",{"2":{"36":1}}],["这样能够让模型更好地区分相似和不同的句子",{"2":{"734":1}}],["这样能够让转置后的张量应用view方法",{"2":{"36":1}}],["这样能够保留原始的一些信息",{"2":{"470":1}}],["这样能够有效得考虑到输入序列中的其他位置",{"2":{"274":1}}],["这样新的token就可以和过去所有上下文进行交互",{"2":{"273":1}}],["这样编码器可以将更多数据传递给解码器",{"2":{"273":1}}],["这样序列中每一个元素都有机会根据自身特征",{"2":{"261":1}}],["这样模型将更正训练过程中的统计属性",{"2":{"407":1}}],["这样模型会更加关注靠近尾部的输入",{"2":{"253":1}}],["这样模型就可以从不同角度来分析和理解输入信息",{"2":{"5":1}}],["这样c1这个窗口才可以同时包含",{"2":{"247":1}}],["这样才能在训练时候模拟实际推理的效果",{"2":{"409":1,"528":1}}],["这样才能准确确定其含义",{"2":{"318":1}}],["这样才能确保在解码过程中模型一定收到了所需的全部信息",{"2":{"245":1}}],["这样才能和后续的注意力分数进行处理",{"2":{"36":1}}],["这样逐次生成后面的预测token",{"2":{"239":1}}],["这样获得最终的注意力张量",{"2":{"199":1}}],["这样softmax",{"2":{"187":1}}],["这样self",{"2":{"172":1}}],["这样简单的分类就使得softmax的计算量大大减少",{"2":{"185":1}}],["这样既保证了每个输出值在0到1之间",{"2":{"180":1}}],["这样违背了transformer用自注意力机制捕捉上下文信息的初衷",{"2":{"172":1}}],["这样",{"2":{"172":1,"180":1,"260":1,"274":1,"316":1,"323":1,"352":1,"376":1,"449":1,"541":1,"621":1,"709":1,"714":1,"904":1,"1157":1,"2124":1}}],["这样背离了自注意力操作的初衷",{"2":{"172":1}}],["这样当我们全部看完以上几本书后",{"2":{"169":1}}],["这样一来",{"2":{"138":1,"334":1,"679":1,"1339":1}}],["这样一来计算量其实跟直接",{"2":{"14":1}}],["这样做可以提供更精确的错误处理",{"2":{"1764":1}}],["这样做的好处是可以使用标准的异常处理框架",{"2":{"1763":1}}],["这样做的好处是可以并行操作",{"2":{"457":1}}],["这样做的好处是丰富了图片的背景",{"2":{"1015":1}}],["这样做的好处是",{"2":{"278":1}}],["这样做是为了使模型能够学习更复杂的函数",{"2":{"99":1}}],["这样做有两个好处",{"2":{"57":1}}],["这样将",{"2":{"74":1}}],["这样处理后",{"2":{"71":1}}],["这样对应位置的token表征就不参与上文提到的按照权重加和的过程",{"2":{"62":1}}],["这样解码器就可以像编码器一样进行并行计算",{"2":{"57":1}}],["这样即可以提纲挈领",{"2":{"275":1}}],["这样即可以有效定位网页中强调的内容",{"2":{"5":1}}],["这样即使模型预测错了",{"2":{"122":1}}],["这样即插即用",{"2":{"43":1}}],["这样就导致负的梯度在这个relu被置零",{"2":{"840":1}}],["这样就排除了基于transformer编码器的预训练语言模型",{"2":{"711":1}}],["这样就完成了从离散的词",{"2":{"700":1}}],["这样就很容易使用神经网络把一串字符向量处理成任意级别的向量",{"2":{"694":1}}],["这样就将词语或短语从词汇表映射到向量的实数空间中",{"2":{"690":1}}],["这样就把每个token的标签设置为它之后的token",{"2":{"528":1}}],["这样就没有距离的概念",{"2":{"415":1}}],["这样就保证了第i个单词的预测只使用了前i个时间步的信息",{"2":{"412":1}}],["这样就不会参与计算",{"2":{"380":1}}],["这样就避开了batch",{"2":{"338":1}}],["这样就减少了ics",{"2":{"314":1}}],["这样就算",{"2":{"304":1}}],["这样就允许网络中的信息直接跨过一个或多个层进行传播",{"2":{"304":1}}],["这样就改优化",{"2":{"301":1}}],["这样就和上面的",{"2":{"267":1}}],["这样就摆脱了rnn中隐向量是固定长度的弊端",{"2":{"267":1}}],["这样就可以利用",{"2":{"1342":1}}],["这样就可以利用gpu的并行能力",{"2":{"408":1}}],["这样就可以为同一个词在不同句子中生成不同的嵌入表示",{"2":{"721":1}}],["这样就可以把将一个对象从复杂的空间投射到相对简单的空间",{"2":{"688":1}}],["这样就可以把对其他单词的",{"2":{"158":1}}],["这样就可以句子转换为词表索引的序列",{"2":{"558":1}}],["这样就可以进行并行处理",{"2":{"415":1}}],["这样就可以决定在输出某个单词时",{"2":{"277":1}}],["这样就可以通过在多维空间中找到表示不同概念的",{"2":{"137":1}}],["这样就可以屏蔽或限制模型在计算注意力分数时对某些位置的关注",{"2":{"70":1}}],["这样就升级了transformer模型的核心",{"2":{"42":1}}],["这样就意味着每个头可以获得一部分词特征组成的句子",{"2":{"36":1}}],["这样得到的单头对应的",{"2":{"30":1}}],["这样代码上更清晰一点",{"2":{"29":1}}],["这样子空间能表达的内容就减少了",{"2":{"20":1}}],["这样效果比较好也是比较符合直觉的",{"2":{"13":1}}],["kron",{"2":{"1087":1}}],["krizhevsky等人提出了一种名为alexnet的深度卷积神经网络架构",{"2":{"840":1}}],["krizhevsky",{"2":{"840":1}}],["k×k×ck",{"2":{"1003":1}}],["k​​x​m​​",{"2":{"1343":1}}],["k​​",{"2":{"1343":1}}],["k​n​​",{"2":{"1342":1}}],["k​i​​",{"2":{"974":1}}],["k​j​​",{"2":{"944":1,"974":1}}],["k​t​c​​​​",{"2":{"944":1}}],["k​1​​",{"2":{"944":1}}],["kthvalue",{"2":{"1087":4}}],["ktck",{"2":{"944":1}}],["ktktk^t",{"2":{"189":1}}],["kw​i​k​​",{"2":{"927":1}}],["kwik",{"2":{"927":1}}],["kwargs=none",{"2":{"1083":1}}],["kwargs",{"2":{"702":2,"1208":3,"1214":8,"1215":9,"1227":4,"1228":1}}],["kxmf",{"2":{"1343":1}}],["kx",{"2":{"766":1,"2017":1}}],["k被拆分为基于内容的和基于相对位置的两个w",{"2":{"760":1}}],["k表示第k个位置的绝对位置编码",{"2":{"748":1}}],["k表示第k个位置的词嵌入",{"2":{"748":1}}],["k+p",{"2":{"748":1}}],["k之后输入模型",{"2":{"748":1}}],["k上加位置编码pkpkp",{"2":{"748":1}}],["ksksk",{"2":{"746":1,"765":2,"766":1}}],["ksq",{"2":{"745":1}}],["ksqt",{"2":{"745":1}}],["kudo",{"2":{"638":1}}],["k采样",{"2":{"636":1}}],["koldpkpoldk",{"2":{"623":1}}],["kolmogorov理论指出",{"2":{"1465":1}}],["kolmogorov",{"2":{"155":2,"156":1}}],["korean",{"2":{"597":1,"638":1}}],["k2",{"2":{"503":2}}],["k2k2k",{"2":{"502":1}}],["k100002i",{"2":{"1336":2}}],["k1",{"2":{"503":2,"647":1,"944":1}}],["k1k1k",{"2":{"502":1}}],["k1|k2|",{"2":{"145":1}}],["kη",{"2":{"402":1}}],["k可以认为是编码器内部的隐向量",{"2":{"268":1}}],["k计算一个新的注意力分布",{"2":{"209":1}}],["k很大时",{"2":{"189":1}}],["k的新向量",{"2":{"189":1}}],["k^",{"2":{"188":1,"621":1,"918":1}}],["k^t",{"2":{"175":1,"197":1,"646":1}}],["k^t大概率会得到一个类似单位矩阵的attention矩阵",{"2":{"172":1}}],["k取什么值",{"2":{"187":1}}],["k解耦了",{"2":{"187":1}}],["k较大时",{"2":{"187":1}}],["kq",{"2":{"180":1}}],["kjkjk",{"2":{"176":1,"766":2}}],["kj",{"2":{"176":8,"210":2,"944":1,"974":1}}],["k矩阵的转置之间做矩阵乘法",{"2":{"173":1}}],["k那就是模型的维度dmodeldmodeld",{"2":{"173":1}}],["k是向量的维度",{"2":{"173":1}}],["k是单个头的注意力维度",{"2":{"23":1}}],["k代表了我们用来与q匹配的键或者说序列中各位置能提供的信息",{"2":{"172":1}}],["k提取的特征是如何获得的呢",{"2":{"168":1}}],["knk",{"2":{"1342":1}}],["knewpkpnewk",{"2":{"623":1}}],["kn",{"2":{"402":1}}],["knight",{"2":{"156":1}}],["known",{"2":{"1027":1}}],["know",{"2":{"141":1}}],["knowledge",{"0":{"134":1,"135":1},"2":{"96":2,"123":1,"126":1,"130":2,"133":2,"136":1,"139":1,"141":1,"143":2,"156":4,"1315":1}}],["kl散度",{"2":{"1377":1}}],["kldivloss",{"2":{"398":1,"399":1,"429":2}}],["klettert",{"2":{"370":1}}],["kleines",{"2":{"370":1}}],["klklk",{"2":{"268":1}}],["kl",{"2":{"145":1}}],["klikilk",{"2":{"128":2}}],["k∗",{"2":{"145":7}}],["k=​x​1​​−x​0​​​​y​1​​−y​0​​​​",{"2":{"2018":1}}],["k=",{"2":{"145":1}}],["k=d",{"2":{"7":1,"34":1,"173":2}}],["kinds",{"2":{"2079":1}}],["kingma",{"2":{"1059":1}}],["kill",{"2":{"1523":1,"1524":1}}],["kit",{"2":{"1435":1}}],["kivi",{"2":{"361":1}}],["ki",{"2":{"173":2,"189":2,"270":4,"271":2,"974":1}}],["kimi",{"2":{"768":1}}],["kim",{"2":{"131":1}}],["ki𝑘𝑖𝑘",{"2":{"125":2}}],["ki∈rdx",{"2":{"125":2}}],["kafka",{"2":{"1498":1}}],["kashgari",{"2":{"1309":1}}],["kaggle",{"2":{"1153":1}}],["kaiming初始化的推导过程只包含卷积和relu激活函数",{"2":{"1001":1}}],["kaiming",{"0":{"1001":1},"1":{"1002":1,"1003":1,"1004":1,"1005":1,"1006":1,"1007":1}}],["kaur",{"2":{"692":1}}],["karl",{"2":{"129":1,"363":1}}],["karpathy分享了dzmitry",{"2":{"284":1}}],["karpathy",{"2":{"47":1,"279":1,"292":1,"638":1}}],["kans的主要特点是去掉了线性权重和固定的激活函数",{"2":{"155":1}}],["kans的设计灵感来源于kolmogorov",{"2":{"155":1}}],["kan",{"0":{"155":1},"2":{"96":1,"155":1,"156":1}}],["kkk",{"2":{"2018":1}}],["kk",{"2":{"71":1}}],["k个头",{"2":{"42":1}}],["k个节点的全连接网络",{"2":{"28":1}}],["k和xq位于同一设备",{"2":{"201":1}}],["k和",{"2":{"41":2}}],["k和v矩阵",{"2":{"620":2}}],["k和v矩阵沿行维度划分为多个块",{"2":{"216":1}}],["k和v是原文",{"2":{"536":1}}],["k和v是编码器输出m",{"2":{"533":1}}],["k和v是序列中的所有位置的词向量",{"2":{"442":1,"443":1}}],["k和v来自编码器输出的注意力向量",{"2":{"444":1}}],["k和v来进行计算",{"2":{"5":1}}],["k和v扮演着不同的角色",{"2":{"172":1}}],["k和v被划分为不同head时",{"2":{"41":1}}],["k和v的前两个维度",{"2":{"34":1}}],["k和v可以理解为把输入的高维向量线性投影到比较低的维度上",{"2":{"16":1}}],["k和v经过这些权重矩阵进行多个线性变换后得到",{"2":{"16":1}}],["k和v切分成多个小段",{"2":{"14":1}}],["k和v通过独立的线性变换来生成",{"2":{"5":1}}],["k和v",{"2":{"5":1,"9":1,"161":1,"172":1}}],["k长度的行向量",{"2":{"28":1}}],["k向量",{"2":{"23":1}}],["keeping",{"2":{"1254":1}}],["keep",{"2":{"1214":4}}],["keepdim",{"2":{"1087":42}}],["keepdim=false",{"2":{"1083":1}}],["keepdim=true",{"2":{"343":2,"346":2,"833":1,"1215":1}}],["keepdims=true0",{"2":{"807":1,"808":1,"809":1,"810":1}}],["keepdims=true",{"2":{"807":1,"808":1,"809":1,"810":1}}],["kevin",{"2":{"1175":1}}],["ke",{"2":{"768":1}}],["kernel的shape",{"2":{"775":1}}],["kernel的通道数",{"2":{"773":1}}],["kernel",{"2":{"101":1,"210":2,"462":1,"770":2,"773":4,"775":1,"944":1,"963":1,"1003":1,"1283":1}}],["kernels",{"2":{"8":1,"801":3,"802":2}}],["keygen",{"2":{"1594":1}}],["key应与优化器接受的关键字参数匹配",{"2":{"1222":1}}],["key的注意力得分",{"2":{"765":1}}],["key的变换矩阵wkwkw",{"2":{"760":1}}],["key的变换矩阵也区分为基于内容的和基于相对位置的两个w",{"2":{"760":1}}],["key之间的距离成比例的一个",{"2":{"765":1}}],["key之间的向量内积展开式",{"2":{"758":1}}],["key之间的相对位置信息",{"2":{"745":1}}],["key=stats",{"2":{"592":1}}],["key=lambda",{"2":{"557":1,"592":1}}],["key和",{"2":{"463":1,"525":1}}],["key和value头的投影矩阵被平均汇总为单个投影矩阵",{"2":{"938":1,"954":1}}],["key和value执行单个attention函数",{"2":{"926":1}}],["key和value分别用不同的",{"2":{"926":1}}],["key和value则用新的参数组件中获取",{"2":{"620":1}}],["key和value是数据库的组件",{"2":{"164":1}}],["key和value",{"2":{"1":1,"36":1,"41":1,"172":1,"288":1,"926":1}}],["key通过相似度计算来得到注意力权重",{"2":{"276":1}}],["key归一化助力长度外推",{"2":{"233":1,"361":1,"768":1}}],["keys是tensor",{"2":{"666":1}}],["keys和queries没有什么明显的几何结构",{"2":{"320":1}}],["keys和values的取值从数组的最左边开始",{"2":{"201":1}}],["keys和values重叠来实现",{"2":{"41":1}}],["keys和values与相邻head的",{"2":{"41":1}}],["keys",{"2":{"201":6,"230":1,"935":1,"951":1,"1086":1,"1214":2,"1217":10}}],["keyi",{"2":{"170":2,"271":2}}],["key与query越相似或者说越相关",{"2":{"168":1}}],["key是在页面上返回的商品描述",{"2":{"463":1}}],["key是字典的索引",{"2":{"265":1}}],["key是编码器中每个时间步的隐状态",{"2":{"165":1}}],["key是query",{"2":{"164":1}}],["key是地址",{"2":{"164":1}}],["key这个地址里面就存放了value",{"2":{"164":1}}],["key向量代表序列中每个token的唯一标识",{"2":{"162":1}}],["key模式",{"0":{"127":1},"2":{"96":1}}],["key",{"0":{"126":1,"2074":1},"1":{"2075":1,"2076":1,"2077":1,"2078":1,"2079":1},"2":{"16":1,"23":1,"29":4,"33":2,"36":10,"38":3,"39":2,"67":2,"76":6,"84":5,"96":1,"122":1,"125":2,"126":3,"127":2,"151":1,"156":3,"158":1,"162":2,"163":2,"164":5,"169":1,"170":1,"172":3,"198":4,"199":6,"201":3,"213":1,"265":3,"271":1,"288":3,"306":1,"394":2,"437":1,"442":1,"463":1,"517":2,"533":3,"535":1,"592":1,"621":1,"623":4,"723":3,"765":1,"916":2,"933":2,"935":2,"951":2,"957":1,"975":2,"981":1,"1216":2,"1217":7,"1218":2,"1222":1,"1227":1,"1284":1,"1342":1,"1344":3,"1481":1,"1485":1,"1725":6,"1807":1,"2087":1}}],["kvcache",{"2":{"977":1}}],["kv通信发生在一个gpu和其他tp组中的对等gpu之间",{"2":{"976":1}}],["kv对",{"2":{"512":1}}],["kv",{"0":{"948":1,"978":1},"2":{"8":8,"83":1,"201":19,"503":10,"746":2,"768":1,"937":1,"948":2,"953":1,"956":1,"957":3,"976":1,"977":1,"978":2,"981":2,"985":2,"986":1}}],["k",{"2":{"3":1,"5":1,"9":3,"10":1,"12":1,"16":1,"17":2,"23":4,"26":1,"28":6,"29":3,"30":2,"31":2,"34":2,"35":3,"36":12,"57":4,"67":3,"71":6,"125":1,"158":1,"161":7,"162":1,"164":3,"169":1,"172":4,"173":8,"175":22,"176":4,"180":1,"185":1,"186":3,"187":1,"188":1,"189":10,"194":6,"197":2,"198":1,"199":6,"200":1,"201":6,"210":4,"211":1,"212":3,"213":6,"231":1,"233":1,"265":4,"270":2,"271":1,"313":2,"355":1,"394":6,"399":3,"402":1,"417":1,"442":3,"443":3,"444":1,"477":1,"503":10,"507":1,"510":5,"511":1,"525":1,"533":1,"535":2,"542":3,"646":2,"647":3,"656":1,"745":2,"748":2,"759":2,"760":2,"763":1,"764":2,"765":1,"804":3,"918":4,"923":1,"924":4,"926":1,"927":8,"933":2,"941":2,"942":2,"943":1,"944":4,"946":2,"948":1,"959":2,"960":2,"961":1,"963":2,"964":1,"966":2,"974":1,"978":1,"986":1,"1003":2,"1016":2,"1087":4,"1205":1,"1216":4,"1218":4,"1335":6,"1336":7,"1339":9,"1340":8,"1342":2,"1343":6,"1398":2,"1547":1,"1557":1,"2014":1,"2018":2,"2019":1,"2027":1,"2031":3}}],["而你有",{"2":{"2135":1}}],["而现在的教育体系和教育方式",{"2":{"2097":1}}],["而现在市面上的语言模型",{"2":{"627":1}}],["而正是这里所言",{"2":{"2054":1}}],["而正数输入则保持不变",{"2":{"107":1}}],["而志于学",{"2":{"2054":1}}],["而指针就像一个房子的地址",{"2":{"1612":1}}],["而指针就像记录着某个房子地址的纸条",{"2":{"1611":1}}],["而持久化的实现过程则大多通过各种关系数据库来完成",{"2":{"1478":1}}],["而处理数据的深度学习网络是一个由管道和阀门组成的巨大水管网络",{"2":{"1466":1}}],["而给出的数据可能是一个或多个随机数",{"2":{"1371":1}}],["而给这两个矩阵分别左乘一个矩阵wkwkw^k和wqwqw^q就可以解决上述两个问题",{"2":{"172":1}}],["而数据经过降维以后",{"2":{"1370":1}}],["而之前的",{"2":{"1364":1}}],["而目前很火的",{"2":{"1341":1}}],["而目前注意力只注重单独某个向量空间",{"2":{"4":1}}],["而参数本身不会被保存",{"2":{"1227":1}}],["而参数量的增加",{"2":{"119":1}}],["而残差大大增加了训练参的敏感性",{"2":{"1180":1}}],["而彻底性则指调整结果的详尽程度",{"2":{"1157":1}}],["而我们也可以从中学到更多解决问题的思维方式",{"2":{"2107":1}}],["而我们日常生活中做的很多事情",{"2":{"2096":1}}],["而我们的思维方式中也经常蕴含着算法",{"2":{"2096":1}}],["而我们的耐心和计算资源就成为了限制因素",{"2":{"1157":1}}],["而我们一般期望两个token在一句话中先后顺序也能反映一定的不同信息",{"2":{"172":1}}],["而将不同的任务分配到不同的函数中",{"2":{"1729":1}}],["而将",{"2":{"1143":1}}],["而将词嵌入矩阵乘以嵌入维度",{"2":{"701":1}}],["而偏重于基本原理",{"2":{"1127":1}}],["而原地操作需要更改表示该操作的函数的所有输入的创建者",{"2":{"1123":1}}],["而根节点是输出张量",{"2":{"1113":1}}],["而虚部取负值",{"2":{"1082":1}}],["而adadelta只累加固定大小的项",{"2":{"1052":1}}],["而a和an这两个同质的词却隔得非常远",{"2":{"679":1}}],["而无法达到全局最优解",{"2":{"1026":1}}],["而无需复制字符串内容",{"2":{"1929":1}}],["而无需复制存储空间",{"2":{"1214":1}}],["而无需了解容器底层的具体实现",{"2":{"1718":1}}],["而无需为每种类型都写一遍代码",{"2":{"1698":1}}],["而无需修改其内部代码",{"2":{"1645":1}}],["而无需显式指定",{"2":{"1615":1,"1906":1}}],["而无需重新开始整个训练过程",{"2":{"1265":1}}],["而无需进行任何模型架构更改",{"2":{"980":1}}],["而无需增加额外开销",{"2":{"975":1}}],["而无需不断重新调整",{"2":{"222":1}}],["而无需通过后续的mlp进一步处理",{"2":{"122":1}}],["而过小的初始值则相反",{"2":{"991":1}}],["而收敛结果实际上又很大程度取决于网络参数的最开始的初始化",{"2":{"990":1}}],["而序列长度具有极大的变化和不可预测性",{"2":{"981":1}}],["而key",{"2":{"975":1}}],["而模型的flops和参数随着模型维度的平方增加",{"2":{"937":1,"953":1}}],["而工程上通常使用luong",{"2":{"892":1}}],["而输出的y序列就是一段句子",{"2":{"882":1}}],["而输出结果就是一个嵌入表示向量",{"2":{"700":1}}],["而gqa",{"2":{"937":1,"953":1}}],["而gap对整个特征图求平均值",{"2":{"816":1}}],["而gpt使用从左往右的解码器",{"2":{"720":1}}],["而cuda正是英伟达开发的gpu的编程接口",{"2":{"795":1}}],["而相对位置编码则直接地体现了相对位置信号",{"2":{"767":1}}],["而从参数角度",{"2":{"765":1}}],["而从文本到向量的转换分为两个阶段",{"2":{"545":1}}],["而下图展示了可以添加到注意力矩阵中的绝对和相对位置偏差的示例",{"2":{"757":1}}],["而llm2vec正是为了解决这个问题",{"2":{"734":1}}],["而layernorm对深度网络的某一层的所有神经元进行标准化操作",{"2":{"808":1}}],["而layernorm消除了",{"2":{"320":1}}],["而layer",{"2":{"312":1}}],["而仅解码器模型通常使用因果注意力仅与之前的词标记交互",{"2":{"729":1}}],["而后向语言模型则从右到左进行编码",{"2":{"717":1}}],["而后者",{"2":{"908":1}}],["而后者在于带宽",{"2":{"420":1}}],["而后者是做数据融合",{"2":{"276":1}}],["而知新",{"2":{"715":1}}],["而具有小偏导的参数在学习率上有相对较小的下降",{"2":{"1042":1}}],["而具有相似模式的词语就会把这些相似更新累积到可观的程度",{"2":{"714":1}}],["而具体如何还原则可以在训练过程中决定什么样的分布是适合的",{"2":{"313":1}}],["而对数组的理解是内存管理的基础",{"2":{"1715":1}}],["而对不同目标超参数值进行了不公平的比较",{"2":{"1145":1}}],["而对词义进行这么精细的切分是非常困难的",{"2":{"712":1}}],["而对于一些有明显的上下文特征的序列化输入",{"2":{"850":1}}],["而对于语言模型来说",{"2":{"715":1}}],["而对于有效的自然语言模型来说",{"2":{"707":1}}],["而对于其他模型来说",{"2":{"692":1}}],["而对于其理论和理解非常缺乏甚至是空白",{"2":{"474":1}}],["而对于",{"2":{"341":1}}],["而对于今天的大语言模型来讲",{"2":{"123":1}}],["而特征的质量和数量限制了它们的有效性",{"2":{"711":1}}],["而如果使用",{"2":{"701":1}}],["而如果是ont",{"2":{"684":1}}],["而基于关键词的检索方法很难做到这一点",{"2":{"696":1}}],["而基于关键词的检索方法对这些情况的处理相对较弱",{"2":{"696":1}}],["而one",{"2":{"694":1}}],["而ode网络则定义了一个向量场",{"2":{"496":1}}],["而嵌入向量通过训练可以学习到类别之间的语义相似性",{"2":{"694":1}}],["而嵌入向量是低维的稠密向量",{"2":{"694":1}}],["而独热向量相当于是对",{"2":{"694":1}}],["而word",{"2":{"694":1}}],["而wordpiece基于语言模型似然概率的最大值来相邻子词合并",{"2":{"598":1}}],["而值为",{"2":{"692":1}}],["而认知心理学及相关领域最近的理论和计算进展表明",{"2":{"689":1}}],["而语言模型则侧重于学习一种语言内部的语法结构",{"2":{"908":1}}],["而语言模型在预测词的时候往往需要预测每个词的概率",{"2":{"185":1}}],["而语义分析实现的基础却是稠密多维向量",{"2":{"687":1}}],["而稠密向量恰好可以弥补这个缺点",{"2":{"685":1}}],["而降维是一种压缩方法",{"2":{"684":1}}],["而每块饼干只能给一个小孩",{"2":{"2147":1}}],["而每种词性可能又有多种含义",{"2":{"683":1}}],["而每个注意力头的输出形状为",{"2":{"34":1}}],["而elmo是一种基于特征的方法",{"2":{"721":1}}],["而elmo之后则爆发了直接将embedding层和上面的语言模型层共同训练的浪潮",{"2":{"718":1}}],["而embedding就是一种经过专门训练的用来向量化数据的神经网络模型",{"2":{"711":1}}],["而embedding则是更高层次的智能化的向量化",{"2":{"676":1}}],["而encoder",{"2":{"542":2}}],["而有的文章做了精细区分",{"2":{"676":1}}],["而有些则会非常小",{"2":{"190":1}}],["而所谓的",{"2":{"628":1,"779":1}}],["而所有这些的转换都是可训练的操作",{"2":{"161":1}}],["而必须改变这些线性投影层的维度",{"2":{"618":1}}],["而导致没有为信息密集的字节",{"2":{"613":1}}],["而选择新句子的第一个单词则更为困难",{"2":{"612":1}}],["而使用token可以避免这个问题",{"2":{"612":1}}],["而utf",{"2":{"608":1}}],["而较少见到的数字则被拆分成多个",{"2":{"595":1}}],["而训练tokenizer是一个统计过程",{"2":{"576":1}}],["而罕见的词被分解为两个或多个subword",{"2":{"575":1}}],["而划分出来的",{"2":{"567":1}}],["而一些分类任务则可能只需较小的词汇表即可达到较高性能",{"2":{"560":1}}],["而一个理想的知识库",{"2":{"121":1}}],["而分词能保持语义",{"2":{"547":1}}],["而用户的输入是文本句子",{"2":{"547":1}}],["而其它方法要么很慢",{"2":{"1057":1}}],["而其它两种架构难以做到",{"2":{"542":1}}],["而其他的候选项会被淘汰",{"2":{"904":1}}],["而其他的候选项会被丢弃",{"2":{"904":1}}],["而其他磁头的输出被擦除",{"2":{"122":1}}],["而其他位置的权重将保持不变",{"2":{"63":1}}],["而双向attention的模型如果不带位置编码",{"2":{"542":1}}],["而双向注意力反而会变得不足",{"2":{"542":1}}],["而闭源模型用空心方块表示",{"2":{"540":1}}],["而decoder",{"2":{"542":2}}],["而decoderlayer有两个多头自注意力模块",{"2":{"533":1}}],["而decoder的输入会是之前decoder输出的组合",{"2":{"529":1}}],["而自注意力层和卷积一样可以完全并行",{"2":{"511":1}}],["而自注意力是完全并行的",{"2":{"160":1}}],["而大模型时代对长文本的诉求",{"2":{"511":1}}],["而真正学到的知识或者信息大多存储在",{"2":{"510":1}}],["而鉴于他自身的习惯",{"2":{"506":1}}],["而可以将提示看作一种编程语言",{"2":{"504":1}}],["而能量函数在球面上点的最优配置已经有成熟的研究方法",{"2":{"499":1}}],["而星形胶质细胞",{"2":{"488":1}}],["而ff2ff2ff",{"2":{"485":1}}],["而ffn则没有这些依赖性",{"2":{"419":1}}],["而ffn几乎与记忆网络的key",{"2":{"125":1}}],["而ffn就是提取信息的关键模块",{"2":{"116":1}}],["而ffn在处理序列数据时只考虑单个位置的信息",{"2":{"101":1}}],["而ffn完成对实体从特征空间a到特征空间b的映射",{"2":{"99":1}}],["而足够长的思维链",{"2":{"480":1}}],["而物理的尽头则为数学",{"2":{"474":1}}],["而单词",{"2":{"456":1}}],["而源语言和目标语言",{"2":{"453":1}}],["而源语句需要padding",{"2":{"382":1}}],["而neural",{"2":{"495":1}}],["而noam",{"2":{"434":1}}],["而nn",{"2":{"343":1}}],["而人们最近也在序列维度上进行了并行尝试",{"2":{"420":1}}],["而结合teacher",{"2":{"412":1}}],["而损失函数是把所有的out放在一起",{"2":{"410":1}}],["而预测时候不应该预测到这个词",{"2":{"399":1}}],["而词汇表较大的模型",{"2":{"562":1}}],["而词汇表的设计就会直接影响数字的表示",{"2":{"560":1}}],["而词表中其它词的概率都应该是0",{"2":{"398":1}}],["而词向量之间的高度相关性说明在一定程度上",{"2":{"176":1}}],["而为了提高训练效率",{"2":{"390":1}}],["而没有目标序列",{"2":{"389":1}}],["而the",{"2":{"735":1}}],["而text",{"2":{"710":1}}],["而teacher",{"2":{"407":1}}],["而transformer所学习到的表示应该是依赖内容的",{"2":{"698":1}}],["而transformer可以被认为是在p",{"2":{"499":1}}],["而trg",{"2":{"382":2}}],["而tgt",{"2":{"79":1}}],["而和目标句子相关的成员变量有两个",{"2":{"381":1}}],["而解码器的每一个输入token本质是一条查询语句",{"2":{"536":1}}],["而解码器的预测目标要除掉第一个",{"2":{"381":1}}],["而解码器的输入不应该有",{"2":{"380":1}}],["而解决这个不一致的主要思路是将注意力局部化",{"2":{"204":1}}],["而海量的文本或者多模态语料组成了大模型需要认知的外部世界的基本信息",{"2":{"363":1}}],["而小的batch",{"2":{"338":1}}],["而到了大模型时代",{"2":{"335":1}}],["而post",{"2":{"335":2}}],["而梯度消失意味着越靠近输入层",{"2":{"333":1}}],["而sgd",{"2":{"333":1}}],["而softmax函数恰好能够同时满足这两个条件",{"2":{"180":1}}],["而softmax函数的输出提供了一个概率分布",{"2":{"180":1}}],["而β为全0",{"2":{"319":1}}],["而句中词与词之间的比较关系不会遭到破坏",{"2":{"318":1}}],["而bert则是以",{"2":{"721":1}}],["而blt论文作者认为",{"2":{"612":1}}],["而bbpe是先通过utf",{"2":{"607":1}}],["而bpe的贪心算法无法对随机分布进行学习",{"2":{"595":1}}],["而bn需要依靠滑动平均来获得预测用的统计量",{"2":{"316":1}}],["而bahdanau使用相加方式",{"2":{"285":1}}],["而bahdanau",{"2":{"285":1}}],["而时间序列不定长",{"2":{"316":1}}],["而保留通道",{"2":{"315":1,"341":1}}],["而relu函数简单",{"2":{"840":1}}],["而resnet的identity",{"2":{"301":1}}],["而rnn方案需要对序列",{"2":{"256":1}}],["而论文",{"2":{"298":1}}],["而论文long",{"2":{"287":1}}],["而上图注意力模型中",{"2":{"265":1}}],["而注视区域外的视觉环境的无关特征",{"2":{"260":1}}],["而注意力机制可以无视序列的先后顺序来捕捉序列间的关系",{"2":{"434":1}}],["而注意力机制可以将序列中的任意两个位置之间的距离是缩小为一个常量",{"2":{"434":1}}],["而注意力机制是向量化+模糊匹配+信息合并",{"2":{"265":1}}],["而注意力机制是向量化+模糊匹配+合并的组合使用",{"2":{"164":1}}],["而注意力机制则允许模型关注整个上下文窗口",{"2":{"227":1}}],["而注意力权重在分配注意力和理解item间相关性方面起着关键作用",{"2":{"209":1}}],["而注意力分数是三个维度",{"2":{"66":2,"380":1,"382":1}}],["而忽略靠前的输入",{"2":{"256":1}}],["而忽略了其它位置的信息",{"2":{"3":1}}],["而权重共享也导致会对输入中的每个单词都赋予同样权重",{"2":{"252":1}}],["而神经记忆模块充当模型的衰减记忆",{"2":{"231":1}}],["而神经记忆由于其记忆数据的能力",{"2":{"226":1}}],["而神经网络的输入需要一个规整的张量",{"2":{"53":1}}],["而组件c几乎不影响其表现",{"2":{"224":1}}],["而微调实际上是将该能力涌现出来",{"2":{"221":1}}],["而奇异值分解",{"2":{"221":1}}],["而剩下",{"2":{"217":1}}],["而关键的正",{"2":{"212":1}}],["而以存储为中心",{"2":{"206":1}}],["而非",{"2":{"1868":1}}],["而非数组大小",{"2":{"1667":1}}],["而非整个数组的拷贝",{"2":{"1667":1}}],["而非线性可分问题的数据集包含两种以上的类别",{"2":{"1462":1}}],["而非线性层和标准化层也不再是互不相交的神经网络模块",{"2":{"320":1}}],["而非用于实际的模型执行",{"2":{"1290":1}}],["而非e所代表的",{"2":{"698":1}}],["而非它们的长度",{"2":{"692":1}}],["而非一个数",{"2":{"679":1}}],["而非其它形式",{"2":{"677":1}}],["而非作为整体的",{"2":{"560":1}}],["而非目标序列",{"2":{"427":1}}],["而非依赖zi−1zi−1z",{"2":{"418":1}}],["而非单纯输入某个标签",{"2":{"406":1}}],["而非单独计算某一列",{"2":{"268":1}}],["而非平移不变性",{"2":{"346":1}}],["而非对单个样本内所有",{"2":{"323":1}}],["而非对单个句子内所有",{"2":{"318":1}}],["而非由推理数据均值和方差得到的参数",{"2":{"316":1}}],["而非空想理论家",{"2":{"284":1}}],["而非最后一个时间步的输出",{"2":{"284":1}}],["而非取决于距离",{"2":{"274":1}}],["而非sft",{"2":{"218":1}}],["而非lora",{"2":{"218":1}}],["而非全部",{"2":{"204":1}}],["而非注意多个方面",{"2":{"3":1}}],["而并非最终结果",{"2":{"199":1}}],["而概率较小的数值几乎不会得到更新",{"2":{"192":1}}],["而当预测某个位置的输出时",{"2":{"525":1,"528":1}}],["而当喂入的数组内部数量级相差较大时",{"2":{"191":1}}],["而当有足够多的头",{"2":{"20":1}}],["而在较短的训练时间内得出的结论可能不能完全适用于长时间训练后的模型",{"2":{"1157":1}}],["而在bp的时候",{"2":{"992":1}}],["而在正向传播中只需要执行2次矩阵乘法",{"2":{"964":1}}],["而在具体任务上进行微调",{"2":{"898":1}}],["而在曼哈顿两点之间的最短路线将是l1距离",{"2":{"692":1}}],["而在embedding中",{"2":{"676":1}}],["而在右侧给出了对应的时间演化",{"2":{"507":1}}],["而在图的下方",{"2":{"485":1}}],["而在于这些单元之间形成的复杂关系和涌现的行为",{"2":{"446":1}}],["而在大模型时代是否需要使用dropout",{"2":{"396":1}}],["而在大模型领域",{"2":{"222":1}}],["而在文本生成的每个阶段",{"2":{"257":1}}],["而在现实世界中",{"2":{"189":1}}],["而在推理过程中",{"2":{"143":1}}],["而hs嵌入则更好地捕捉到句子的整体结构和意义",{"2":{"739":1}}],["而h",{"2":{"184":1}}],["而矩阵乘法是满足结合率的",{"2":{"180":1}}],["而进行长度外推时",{"2":{"176":1}}],["而点积操作要求query和key具有相同的长度",{"2":{"175":1}}],["而直接从q得到v会忽略了通过k来确定相关性的重要性",{"2":{"172":1}}],["而v代表了一旦找到匹配",{"2":{"172":1}}],["而应该是可学习的",{"2":{"172":1}}],["而应该是和对应任务相关",{"2":{"172":1}}],["而应该是分开存储的",{"2":{"136":1}}],["而要对其进行线性变换",{"2":{"172":1}}],["而与模型结构",{"2":{"147":1}}],["而✗表示不存在",{"2":{"139":1}}],["而这跟计算机里的",{"2":{"2099":1}}],["而这一个f关于概率密度函数的",{"2":{"1377":1}}],["而这一行就是这个单词学习到的在嵌入空间的语义",{"2":{"700":1}}],["而这个输出结果实际上是可逆的",{"2":{"700":1}}],["而这个全连接层的参数",{"2":{"694":1}}],["而这个空间中的距离会与人们在这两个概念之间进行泛化的意愿相一致",{"2":{"691":1}}],["而这个更新过程是端对端地优化一个目标函数",{"2":{"506":1}}],["而这是传统分词方法做不到的",{"2":{"587":1}}],["而这可能比较困难",{"2":{"542":1}}],["而这种操作大多采用全局注意力以建模更广范围的依赖关系",{"2":{"276":1}}],["而这些子类又共同派生出一个新的子类",{"2":{"1661":1}}],["而这些共用资源又无法同时被多个线程访问的特性",{"2":{"1413":1}}],["而这些并发进程必须有好的解决方案",{"2":{"1412":1}}],["而这些模块在训练模式下可能表现不同",{"2":{"1122":1}}],["而这些词汇在通用词表中可能不存在或不够丰富",{"2":{"560":1}}],["而这些相关性可能会进一步揭示注意力权重本身内的高阶转换",{"2":{"209":1}}],["而这些特征在神经元中实际上是不可见的",{"2":{"137":1}}],["而这三个概念就分别可以对应到key张量",{"2":{"163":1}}],["而这里直接使用",{"2":{"128":1}}],["而",{"2":{"135":1,"332":1,"420":1,"461":1,"485":2,"510":1,"511":1,"702":2,"756":1,"765":1,"840":1,"935":1,"951":1,"957":1,"985":1,"986":1,"1154":1,"1317":1,"1344":1,"1728":2,"1821":1,"1839":1}}],["而高层趋向于捕捉抽象的特征",{"2":{"127":1}}],["而高层的",{"2":{"127":1}}],["而第二层前馈网络权重",{"2":{"126":1}}],["而记忆可以通过两个基本能力实现普遍计算",{"2":{"118":1}}],["而q来自带有mask的multiheadattention模块的输出",{"2":{"72":1}}],["而只对进一步的派生类可见",{"2":{"1867":1}}],["而只要相对距离足够大",{"2":{"765":1}}],["而只考虑它们的方向",{"2":{"692":1}}],["而只改变稀疏程度",{"2":{"176":1}}],["而只能计算当前词和前面所有词的注意力",{"2":{"58":1}}],["而只是简单地做了矩阵的前后链接",{"2":{"10":1}}],["而编码器的下一层希望得到是一个矩阵",{"2":{"35":1}}],["而多头机制进一步增强了这一点",{"2":{"34":1}}],["而不考虑未来的其他情况",{"2":{"2121":1}}],["而不用反复思考",{"2":{"2101":1}}],["而不用逐个token计算",{"2":{"70":1}}],["而不一定考虑未来可能的影响",{"2":{"2101":1}}],["而不影响外部变量本身的值",{"2":{"1907":1}}],["而不希望暴露或继承基类的接口时",{"2":{"1860":1}}],["而不希望新token的引入过多地",{"2":{"194":1}}],["而不改变数据内容",{"2":{"1758":1}}],["而不必浪费大量的计算资源和时间在不优秀的超参数上",{"2":{"1157":1}}],["而不必从头执行整个推理过程",{"2":{"629":1}}],["而不原地操作只是分配新的对象并保留对旧计算图的引用",{"2":{"1123":1}}],["而不敏感于其他",{"2":{"1041":1}}],["而不向relu那样直接置0",{"2":{"844":1}}],["而不加到v上了",{"2":{"760":1}}],["而不同位置编码方案添加偏置的方式则各不相同",{"2":{"758":1,"766":1}}],["而不同语义的文本在向量空间应该保持对应的距离",{"2":{"710":1}}],["而不相似的单词则被关联到相距较远的数字",{"2":{"691":1}}],["而不仅仅是碰运气找到的配置",{"2":{"1139":1}}],["而不仅仅是比较它们的数值大小或直线距离",{"2":{"692":1}}],["而不仅仅是进行词语的排列组合",{"2":{"629":1}}],["而不仅仅是在token序列空间视角中获取到的能力",{"2":{"626":1}}],["而不仅仅是标记混合注意力操作",{"2":{"446":1}}],["而不仅仅是最后一个单词",{"2":{"427":1}}],["而不需要重新运行任何实验",{"2":{"1175":1}}],["而不需要其他适配任务或者数据",{"2":{"734":1}}],["而不需要通过训练得到",{"2":{"680":1}}],["而不需要为每个新词创建一个新的词条",{"2":{"594":1}}],["而不需要循环",{"2":{"406":1}}],["而不需要h个单独操作",{"2":{"29":1}}],["而不计算统计量或其他类型的聚合",{"2":{"360":1}}],["而不会被自动求导记录",{"2":{"1120":1}}],["而不会丢失所学的知识",{"2":{"623":1}}],["而不会降低性能",{"2":{"623":1}}],["而不会假设未来的信息存在",{"2":{"525":1}}],["而不会增加",{"2":{"154":1}}],["而不会过分关注自身位置",{"2":{"20":1}}],["而不是开始就借用别的人话",{"2":{"2097":1}}],["而不是充要条件",{"2":{"1961":1}}],["而不是仅仅使用",{"2":{"1764":1}}],["而不是仅仅一个算子",{"2":{"1107":1}}],["而不是进行实际的函数调用过程",{"2":{"1709":1}}],["而不是在函数定义中",{"2":{"1708":1}}],["而不是属于某个对象的成员",{"2":{"1639":1}}],["而不是它们的定义",{"2":{"1628":1}}],["而不是具体的类型",{"2":{"1615":1}}],["而不是改变引用的指向",{"2":{"1612":1}}],["而不是基于",{"2":{"1364":1}}],["而不是使用验证误差",{"2":{"1175":1}}],["而不是使用单个语料库",{"2":{"367":1}}],["而不是按时间间隔进行",{"2":{"1164":1}}],["而不是10分钟评估一次",{"2":{"1162":1}}],["而不是固定的时间间隔",{"2":{"1162":1}}],["而不是受我们拥有多少训练数据或其他因素的限制",{"2":{"1154":1}}],["而不是立即选择较小数量的隐藏层",{"2":{"1149":1}}],["而不是降低验证集错误率",{"2":{"1140":1}}],["而不是任何客观事实",{"2":{"1127":1}}],["而不是二次关系",{"2":{"964":1}}],["而不是用",{"2":{"926":1}}],["而不是平移不变性",{"2":{"812":1}}],["而不是资源限制",{"2":{"733":1}}],["而不是要求程序员自己设计它们",{"2":{"708":1}}],["而不是像ape那样只在第一层之前出现",{"2":{"745":1}}],["而不是像余弦相似度那样仅在学习后进行归一化",{"2":{"692":1}}],["而不是像rnn那样逐个位置处理",{"2":{"519":1}}],["而不是局限于任何单个维度的局部",{"2":{"683":1}}],["而不是你必须给出明确的指令",{"2":{"678":1}}],["而不是毫无关系的离散向量",{"2":{"676":1}}],["而不是真正的连续分布",{"2":{"636":1}}],["而不是离散的词语",{"2":{"629":1}}],["而不是对推理在特定语言中的实例建模",{"2":{"628":1}}],["而不是对输入的复杂理解",{"2":{"542":1}}],["而不是",{"2":{"542":1,"579":1,"1228":1,"1478":1,"1607":1}}],["而不是让训练根据之前的输出来生成下一步",{"2":{"404":1}}],["而不是一开始就从一个指定大小开始训练",{"2":{"401":1}}],["而不是一个",{"2":{"315":1,"1083":1}}],["而不是单纯除以",{"2":{"315":1}}],["而不是单义表示较少的",{"2":{"118":1}}],["而不是整个序列",{"2":{"204":1}}],["而不是mlps所依据的通用逼近定理",{"2":{"155":1}}],["而不是激活参数",{"2":{"154":1}}],["而不是重新训练或微调整个模型",{"2":{"139":1}}],["而不是h个矩阵",{"2":{"35":1}}],["而不能看到未来的词",{"2":{"81":1,"525":1}}],["而不能attend",{"2":{"74":1}}],["而不能依赖",{"2":{"59":1,"934":1}}],["而不能使用",{"2":{"58":1}}],["而言",{"2":{"29":1,"89":1,"326":1}}],["而哈佛代码中此处依然是用三个大的权重矩阵wq",{"2":{"26":1}}],["而是优先满足容易被小饼干满足的小孩",{"2":{"2156":1}}],["而是看到一个需要的东西就顺手放进购物车",{"2":{"2101":1}}],["而是是一种发自内心的",{"2":{"2054":1}}],["而是作为字符串结束的标记",{"2":{"1704":1}}],["而是重新申请内存空间",{"2":{"1694":1}}],["而是现有变量的另一个名字",{"2":{"1612":1}}],["而是包装器",{"2":{"1589":1}}],["而是机器自动提取的",{"2":{"1470":1}}],["而是采用v",{"2":{"1363":1}}],["而是直接将位置编码当作可训练参数",{"2":{"1337":1}}],["而是直接在对比学习过程中去除了decoder",{"2":{"735":1}}],["而是传递一个",{"2":{"1222":1}}],["而是传入所有的隐藏状态",{"2":{"273":1}}],["而是传入所有的隐藏状态到解码器",{"2":{"267":1}}],["而是将别人预训练好的模型权重通过迁移学习应用到自己的模型中",{"2":{"1313":1}}],["而是将节点中的每个与其相连的输入权值以1",{"2":{"1018":1}}],["而是将其保留在基数树",{"2":{"985":1}}],["而是隐式地操作头部的索引来执行相同的计算",{"2":{"971":1}}],["而是保持在k个",{"2":{"904":1}}],["而是用一个和query",{"2":{"765":1}}],["而是用一种概率的方式标记",{"2":{"399":1}}],["而是为我们所知道的一切建模",{"2":{"754":1}}],["而是对相对位置i",{"2":{"745":1}}],["而是对样本维度进行处理",{"2":{"322":1}}],["而是全部token",{"2":{"727":1}}],["而是统计一个单词有哪些单词以多高的频率出现在它的上下文",{"2":{"713":1}}],["而是融入了更加丰富的上下文含义",{"2":{"709":1}}],["而是来自于这些向量在更大计算过程中所起的作用",{"2":{"689":1}}],["而是考虑整句话的含义",{"2":{"628":1}}],["而是在算attention的时候考虑当前位置与被attention的位置的相对距离",{"2":{"1338":1}}],["而是在为每个单词分配嵌入之前查看整个句子",{"2":{"717":1}}],["而是在更高的语义层级",{"2":{"628":1}}],["而是在推理时分两次推理",{"2":{"207":1,"218":1}}],["而是思考",{"2":{"627":1}}],["而是像婴儿和小动物那样通过观察和互动来理解世界",{"2":{"627":1}}],["而是从原始字节数据中直接学习",{"2":{"612":1}}],["而是从随机开始",{"2":{"9":1}}],["而是主要由运行在每个字节上的大型ffn来主导",{"2":{"612":1}}],["而是使用训练误差",{"2":{"1175":1}}],["而是使用训练标签的真值",{"2":{"406":1}}],["而是使用因式分解",{"2":{"698":1}}],["而是使用字节级别来编码来解决oov的问题",{"2":{"608":1}}],["而是最后一个token对应的向量out",{"2":{"472":1}}],["而是一种人的本性",{"2":{"2112":1}}],["而是一种可以帮助我们提高生活效率的工具",{"2":{"2107":1}}],["而是一种神经网络层",{"2":{"105":1}}],["而是一个连续的向量场",{"2":{"495":1}}],["而是一个单词一个单词进行输出",{"2":{"453":1}}],["而是把所有时刻",{"2":{"444":1}}],["而是具备深刻的语境信息",{"2":{"437":1}}],["而是与某种更复杂的词元关系的理解相关联",{"2":{"437":1}}],["而是给模型一点惩罚",{"2":{"399":1}}],["而是",{"2":{"334":1,"1678":1}}],["而是平等看待序列中每个单词的顺序",{"2":{"274":1}}],["而是扩散算法本身的稳定性",{"2":{"222":1}}],["而是依赖于输入数据的内容来决定的",{"2":{"170":1,"173":1}}],["而是利用反向传播机制进行学习得到的",{"2":{"161":1}}],["而是训练一个超网络来学习模型权重的变化",{"2":{"142":1}}],["而是更关乎理解和操纵",{"2":{"139":1}}],["而是多个value的加权和",{"2":{"129":1}}],["而是分布在神经元中",{"2":{"129":1}}],["而是由不同的组件共同构成的",{"2":{"123":1}}],["而是通过自己不停的尝试来学会某些技能",{"2":{"1472":1}}],["而是通过一个训练好的模型即时计算出来",{"2":{"715":1}}],["而是通过一系列线性变换来拟合一个高维的映射空间",{"2":{"116":1}}],["而是通过",{"2":{"90":1,"170":1}}],["而是原始输入x",{"2":{"36":1}}],["而是所有的注意力头共用线性层",{"2":{"29":1}}],["而是要进行降维变化",{"2":{"27":1}}],["而是合用",{"2":{"24":1}}],["而融合是做数据融合",{"2":{"11":1}}],["而且不想了解编程",{"2":{"2146":1}}],["而且需要管理员权限",{"2":{"2070":1}}],["而且需要把多头注意力的输出恢复为原",{"2":{"10":1}}],["而且只是二十出头的我的理解",{"2":{"2054":1}}],["而且只从存储内容里面找出一条内容",{"2":{"164":1}}],["而且在",{"2":{"2054":1}}],["而且有点搞笑",{"2":{"2054":1}}],["而且有望成为与现实世界动态本质始终保持一致的终生模型",{"2":{"220":1}}],["而且维持成本也高",{"2":{"1477":1}}],["而且时间和经济成本都非常高",{"2":{"1313":1}}],["而且可以使用恒定的学习率更新策略来实现",{"2":{"1156":1}}],["而且越错越离谱",{"2":{"895":1}}],["而且这个神经元有可能再也不会被任何数据激活",{"2":{"840":1}}],["而且这种巨大的计算量也限制了模型扩展到训练数据以外的更长的序列",{"2":{"203":1}}],["而且由于位置信息是通过位置编码隐式提供的",{"2":{"749":1}}],["而且要看见第一个token",{"2":{"727":1}}],["而且每个mask掉的token都是基于同一个上下文重建的",{"2":{"727":1}}],["而且重点优化了retrieval任务",{"2":{"726":1}}],["而且embedding在实际的训练中更新地也比较稀疏",{"2":{"698":1}}],["而且还要让这些商品的价值尽可能的大",{"2":{"2112":1}}],["而且还",{"2":{"2054":1}}],["而且还应该能够处理心理学中的关键现象",{"2":{"689":1}}],["而且还能够存储一些通用的知识提取模式",{"2":{"144":1}}],["而且支持特定形式的逻辑推断和复杂的计算",{"2":{"689":1}}],["而且解释性很强",{"2":{"687":1}}],["而且该维度上只是1",{"2":{"681":1}}],["而且该状态还可以有效的捕捉它们的底层结构和关系",{"2":{"256":1}}],["而且将",{"2":{"624":1}}],["而且主要发生在中间层",{"2":{"477":1}}],["而且博客作者用代码实现了论文的模型",{"2":{"432":1}}],["而且是一次性把输入序列全部传给解码器",{"2":{"426":1}}],["而且我们可以一次性的输入全部目标序列",{"2":{"411":1}}],["而且容易在错误道路上越走越远",{"2":{"405":1}}],["而且后面的词一定会被前面的词所影响",{"2":{"405":1}}],["而且大型网络往往使用超大的批量大小",{"2":{"402":1}}],["而且近期也有把dropout进一步应用的工作",{"2":{"396":1}}],["而且它会不断在这个基础上进行叠加训练",{"2":{"393":1}}],["而且会利用其它模型对这些合成数据进行进一步筛选",{"2":{"369":1}}],["而且bn在训练的时候可以根据mini",{"2":{"316":1}}],["而且之前解码器每个时刻用到的语义向量都是编码器生成的同一个上下文信息",{"2":{"284":1}}],["而且当面对长序列时",{"2":{"255":1}}],["而且当新输入进入时",{"2":{"253":1}}],["而且信息传递过程不够透明",{"2":{"247":1}}],["而且n",{"2":{"242":1}}],["而且log",{"2":{"183":1}}],["而且耗时较长",{"2":{"142":1}}],["而且ffn中间的激活难以看出低秩",{"2":{"118":1}}],["而且隐式地缓解了秩崩溃问题",{"2":{"93":1}}],["而且因为有残差连接的存在",{"2":{"10":1}}],["而且",{"2":{"4":1,"13":1,"144":1,"172":1,"175":1,"279":1,"490":1,"516":1,"525":1,"560":1,"565":1,"567":1,"681":1,"707":1,"721":1}}],["而且自注意力机制让我们在处理序列数据时",{"2":{"1":1}}],["而迄今为止",{"2":{"3":1}}],["迄今为止",{"2":{"3":1,"474":1}}],["并指定其源文件",{"2":{"1973":1,"1974":1}}],["并显示每次取款后的余额",{"2":{"1873":1}}],["并只向后续派生类公开基类的接口时",{"2":{"1856":1}}],["并只保存第一个设备的ema",{"2":{"1168":1}}],["并换算成",{"2":{"1825":1,"1843":1}}],["并换行",{"2":{"1606":1}}],["并处理可能出现的错误",{"2":{"1814":1,"1832":1}}],["并学会根据需要创建自定义异常",{"2":{"1765":1}}],["并按频率从高到低输出结果",{"2":{"1933":1}}],["并按升序排序输出",{"2":{"1759":1}}],["并按顺序排列",{"2":{"1751":1}}],["并查找特定元素",{"2":{"1719":1,"1720":1,"1721":1,"1722":1}}],["并输出",{"2":{"1716":1}}],["并输出到控制台",{"2":{"1607":1}}],["并掌握动态数组的内存管理",{"2":{"1711":1}}],["并调用每个动物的",{"2":{"1690":1}}],["并调用其",{"2":{"1664":1}}],["并实现各自的",{"2":{"1866":1}}],["并实现",{"2":{"1690":1}}],["并列举至少三种避免内存泄漏的方法",{"2":{"1678":1}}],["并列举了替换为relu",{"2":{"103":1}}],["并释放分配的内存",{"2":{"1678":1}}],["并释放了额外的性能潜力",{"2":{"42":1}}],["并认识到内存越界和内存泄漏的危害以及如何避免",{"2":{"1678":1}}],["并认为其完整结构",{"2":{"446":1}}],["并了解格式化输出",{"2":{"1666":1}}],["并了解其局限性",{"2":{"1627":1}}],["并根据目标变量的类型进行转换",{"2":{"1813":1,"1831":1}}],["并根据需要自动调整大小",{"2":{"1797":1}}],["并根据返回结果输出相应的提示信息",{"2":{"1729":1}}],["并根据模块或功能进行组织",{"2":{"1628":1}}],["并根据实际应用场景做出最合适的决策",{"2":{"568":1}}],["并找出数组中的最大值和最小值",{"2":{"1623":1}}],["并找到最佳的学习率",{"2":{"1179":1}}],["并计算输入的正数的个数",{"2":{"1620":1}}],["并计算两个单独的softmax注意力",{"2":{"502":1}}],["并演示单行",{"2":{"1619":1}}],["并打印出",{"2":{"1611":1}}],["并优化程序性能",{"2":{"1611":1}}],["并注明单位",{"2":{"1608":1}}],["并避免潜在的错误",{"2":{"1607":1}}],["并避免频繁动态调整节点分区",{"2":{"977":1}}],["并赋值",{"2":{"1611":1}}],["并赋值为字符",{"2":{"1607":1}}],["并赋值为",{"2":{"1607":3}}],["并赋值给self",{"2":{"522":1}}],["并配置其编译器",{"2":{"1605":1}}],["并更容易发现和修复错误",{"2":{"1605":1}}],["并持续引入泛型编程",{"2":{"1603":1}}],["并编写出更高效的代码",{"2":{"1602":1}}],["并确保在类销毁时文件被正确关闭",{"2":{"1902":1}}],["并确保库文件路径和头文件路径正确设置",{"2":{"1589":1}}],["并确保它能够有效地适应新任务",{"2":{"221":1}}],["并管理各个进程之间的通信",{"2":{"1589":1}}],["并自动添加",{"2":{"1589":1}}],["并自主掌握这些知识",{"2":{"142":1}}],["并尝试使用远程连接工具连接访问",{"2":{"1502":1,"1561":1}}],["并尝试通过反复查看各种超参数轴图来校准我们的直觉",{"2":{"1148":1}}],["并产生输出",{"2":{"1438":1}}],["并保证程序按预期执行",{"2":{"1407":1}}],["并保持快速推理",{"2":{"226":1}}],["并构建一个计算图的表示形式",{"2":{"1291":1}}],["并构建表示计算梯度的函数的图",{"2":{"1113":1}}],["并生成高效的机器代码",{"2":{"1288":1}}],["并理解其含义",{"2":{"1248":1}}],["并接收里程碑点列表",{"2":{"1247":1}}],["并一次性执行大块的计算",{"2":{"1228":1}}],["并一次又一次地执行相同的迭代",{"2":{"581":1}}],["并回顾性地从中选择最佳检查点",{"2":{"1166":1}}],["并仍然产生准确的估计",{"2":{"1165":1}}],["并定期在训练和评估之间交替",{"2":{"1164":1}}],["并像",{"2":{"1159":1}}],["并尽可能地减少对最终长时间运行的影响",{"2":{"1157":1}}],["并尽可能轻松地找到每项研究中最好的几次试验并检查它们的训练曲线",{"2":{"1151":1}}],["并决定了哪些超参数甚至应该被调整",{"2":{"1153":1}}],["并最终选择不同的超参数值",{"2":{"1152":1}}],["并最终得到新的一组可能的命题结构",{"2":{"510":1}}],["并偏向那些不会出现过拟合问题的配置",{"2":{"1149":1}}],["并试图了解有多少点位于搜索空间的",{"2":{"1148":1}}],["并允许每个冗余超参数有尽可能大的值域",{"2":{"1145":1}}],["并允许任意大的上下文大小",{"2":{"975":1}}],["并设置",{"2":{"1144":2}}],["并随着我们理解的深度不断更新我们的搜索空间",{"2":{"1139":1}}],["并随着时间的推移将不同注视点的信息结合起来",{"2":{"260":1}}],["并可选地指向新的对象",{"2":{"1695":1}}],["并可能导致程序难以维护和调试",{"2":{"1649":1}}],["并可帮助您进行调试",{"2":{"1112":1}}],["并可以在",{"2":{"504":1}}],["并应用利率调整余额",{"2":{"1873":1}}],["并应用上述点",{"2":{"1115":1}}],["并应用激活函数",{"2":{"1086":1}}],["并应用旋转位置嵌入",{"2":{"355":1}}],["并返回一个布尔类型的值",{"2":{"1729":1}}],["并返回首元素的地址",{"2":{"1668":1}}],["并返回该内存的地址",{"2":{"1668":1}}],["并返回指向数组首元素的指针",{"2":{"1647":1}}],["并返回指向该内存空间的指针",{"2":{"1647":1}}],["并返回变换后的张量",{"2":{"1086":1}}],["并返回",{"2":{"1083":1}}],["并有助于跳过局部极小点",{"2":{"1032":1}}],["并有助于理解高级人类可解释的特征如何在此类模型的神经元激活中表示",{"2":{"477":1}}],["并没阻止学习到有效特征",{"2":{"1019":1}}],["并没有具体模板",{"2":{"2114":1}}],["并没有显著更新",{"2":{"1344":1}}],["并没有后来常见的relu函数等",{"2":{"1000":1}}],["并没有尝试去理解句子内的语义",{"2":{"715":1}}],["并没有将模型结构进行搬迁",{"2":{"662":1}}],["并没有位置上的限制",{"2":{"525":1}}],["并没有实际意义",{"2":{"382":1}}],["并没有建立权重和输入实体本身的关系",{"2":{"276":1}}],["并没有任何的非线性变换",{"2":{"117":1}}],["并没有物理切分成对应于每个注意力头的独立矩阵",{"2":{"29":1}}],["并重新训练通过贝叶斯优化找到的最佳配置",{"2":{"1153":1}}],["并重新计算每个token出现的频率",{"2":{"581":1}}],["并重用其",{"2":{"986":1}}],["并响应",{"2":{"986":1}}],["并结合了一个缓存感知调度策略",{"2":{"985":1}}],["并结合双向",{"2":{"284":1}}],["并具有易于使用的接口",{"2":{"983":1}}],["并具有理论上的无限存储",{"2":{"504":1}}],["并减少了与",{"2":{"977":1}}],["并为后续学习动态内存管理",{"2":{"1610":1}}],["并为长上下文prefill实现了分块管道并行性",{"2":{"977":1}}],["并为通常跟随空白的困难预测分配flop",{"2":{"613":1}}],["并写出到hbm",{"2":{"944":1}}],["并再次映射",{"2":{"926":1}}],["并与用户进行自然语言交互",{"2":{"906":1}}],["并与召回权重进行加权求和",{"2":{"118":1}}],["并以过滤取代惩罚",{"2":{"766":1}}],["并以",{"2":{"764":1}}],["并以此关系为权重来确定如何吸取其它对象的信息",{"2":{"276":1}}],["并解耦了内容和位置的注意力",{"2":{"763":1}}],["并常常结合裁剪或分箱策略来避免出现分布外的位置嵌入",{"2":{"756":1}}],["并声称它可以外推到训练之外的更长的序列",{"2":{"756":1}}],["并捕捉到位置信息对任务的影响",{"2":{"749":1}}],["并捕捉序列中的依赖关系",{"2":{"250":1}}],["并研究这些代数结构上的模",{"2":{"713":1}}],["并乘以词嵌入维度的平方根进行缩放",{"2":{"701":1}}],["并概述了替代方案",{"2":{"692":1}}],["并预测下一个句子的嵌入",{"2":{"632":1}}],["并利用句子嵌入",{"2":{"628":1}}],["并利用这些模型中固有的知识机制",{"2":{"139":1}}],["并能正确地进行动态分配内存的释放",{"2":{"1666":1}}],["并能够处理可变数量的参数",{"2":{"616":1}}],["并能更好地处理多样化和带噪声的输入",{"2":{"612":1}}],["并引入了一种新的混合了字节和patch信息的模型架构",{"2":{"612":1}}],["并基于这个概率来进行分词",{"2":{"601":1}}],["并对其进行初始化",{"2":{"1907":1}}],["并对其进行因果编码",{"2":{"635":1}}],["并对外界公开基类的public接口",{"2":{"1853":1}}],["并对输入块进行多次处理",{"2":{"940":1,"959":1}}],["并对w剩余的字符串继续匹配",{"2":{"587":1}}],["并对每个token",{"2":{"249":1}}],["并看到基础词的词频",{"2":{"582":1}}],["并开始接触",{"2":{"1644":1}}],["并开始学习合并规则",{"2":{"580":1}}],["并开发更有效的注意力机制变体",{"2":{"507":1}}],["并逐一自回归预测输出token",{"2":{"541":1}}],["并突出了一些最著名的模型",{"2":{"540":1}}],["并完成整个前向传播过程",{"2":{"532":1}}],["并参数化",{"2":{"505":1}}],["并表明如果残差函数与输入符合李普希茨",{"2":{"497":1}}],["并表明带有layernorm的自注意力构成了一个比最初认为的更具表达力和多功能的非线性动力系统",{"2":{"91":1}}],["并制作了一个数学模型",{"2":{"487":1}}],["并进行取款操作",{"2":{"1873":1}}],["并进行独立的测试和调试",{"2":{"1729":1}}],["并进行初始化",{"2":{"1629":1}}],["并进行大块的计算",{"2":{"1228":1}}],["并进行了1000次warmup",{"2":{"1183":1}}],["并进行相应微调",{"2":{"731":1}}],["并进行德译英实战",{"2":{"361":1,"429":3}}],["并进一步限制模型的表达能力",{"2":{"764":1}}],["并进一步整合和转化这些信息",{"2":{"120":1}}],["并进一步转化这些信息",{"2":{"101":1}}],["并移动到超球面上最能预测下一个标记嵌入向量的点",{"2":{"352":1}}],["并归一化",{"2":{"323":1}}],["并详细讨论了",{"2":{"320":1}}],["并不影响结果",{"2":{"1472":1}}],["并不意味我们只能通过更长",{"2":{"1154":1}}],["并不严谨",{"2":{"779":1}}],["并不具备可实现性",{"2":{"712":1}}],["并不擅长运动",{"2":{"628":2}}],["并不需要将概率分布再转成对应的token",{"2":{"426":1}}],["并不能实现teacher",{"2":{"409":1}}],["并不是",{"2":{"1781":1}}],["并不是generator的输出",{"2":{"385":1}}],["并不是对单个样本内所有",{"2":{"326":1}}],["并不在这里构造",{"2":{"344":1}}],["并不会破坏不同样本同一特征之间的关系",{"2":{"314":1}}],["并不要求归一化",{"2":{"125":1}}],["并递归的将每个词与之前的记忆结合起来",{"2":{"287":1}}],["并将每个计算得到的坐标点向下取整后绘制出来",{"2":{"2017":1}}],["并将每个时间步的隐状态拼接起来作为输出",{"2":{"278":1}}],["并将所有单词转换为小写",{"2":{"1933":1}}],["并将所有注意力操作融合到一个gpu内核中",{"2":{"940":1,"962":1}}],["并将原有数据复制到新的空间",{"2":{"1713":1}}],["并将原指针指向的内容拷贝到新申请的内存中",{"2":{"1694":1}}],["并将用作该组的优化选项",{"2":{"1222":1}}],["并将与参数一起保存",{"2":{"1211":1}}],["并将复现该论文中的模型作为起点",{"2":{"1129":1}}],["并将最后两个维度进行转置",{"2":{"1087":1}}],["并将在不久的将来进行更改",{"2":{"1086":1}}],["并将结果作为",{"2":{"1729":1}}],["并将结果reduce",{"2":{"944":1}}],["并将结果返回给主进程",{"2":{"1578":1}}],["并将结果返回",{"2":{"522":1}}],["并将注意力计算的输出写回hbm",{"2":{"940":1,"962":1}}],["并将相对距离的误差控制在一定范围内",{"2":{"684":1}}],["并将它们加载到快速的片上sram上",{"2":{"940":1,"962":1}}],["并将它们映射到模型的隐藏维度",{"2":{"632":1}}],["并将它们添加到token列表中",{"2":{"581":1}}],["并将",{"2":{"614":1}}],["并将输出结果赋值给x",{"2":{"522":1}}],["并将这些层放在self",{"2":{"522":1}}],["并将其转换为适当的类型",{"2":{"1824":1,"1842":1}}],["并将其转换为一个可重放的跟踪模型",{"2":{"1292":1}}],["并将其存储到数组中",{"2":{"1716":1}}],["并将其与容器的值一起打印出来",{"2":{"1701":1}}],["并将其插入树中",{"2":{"986":1}}],["并将其写入",{"2":{"942":1,"959":1}}],["并将其通过多个transformer编码器块",{"2":{"540":1}}],["并将其设置为可训练参数",{"2":{"343":2}}],["并将其传递给",{"2":{"122":1}}],["并帮助注意力机制在利用过去已久信息的同时处理当前上下文",{"2":{"228":1}}],["并提供不同的存储方式",{"2":{"1867":1}}],["并提供了更安全的",{"2":{"1713":1}}],["并提供更准确和更全面的响应",{"2":{"559":1}}],["并提出了一种新的激活函数",{"2":{"845":1}}],["并提出了关于",{"2":{"314":1}}],["并提出了三种变体来有效地将记忆融入架构中",{"2":{"226":1}}],["并提高系统的性能和效率",{"2":{"986":1}}],["并提高生成文本的语义质量",{"2":{"898":1}}],["并提高模型的训练效率",{"2":{"684":1}}],["并提高模型的泛化能力和解释性",{"2":{"512":1}}],["并提高对领域文本的理解和生成能力",{"2":{"560":1}}],["并提高了模型的性能和稳定性",{"2":{"294":1}}],["并提高transformer模型的表达能力",{"2":{"117":1}}],["并选择合适的",{"2":{"225":2}}],["并动态重组其功能网络以响应不断变化的任务需求",{"2":{"222":1}}],["并集成了",{"2":{"215":1}}],["并独立计算它们之间的相互作用",{"2":{"213":1}}],["并用传入的值初始化数组元素",{"2":{"1678":1}}],["并用于分类",{"2":{"1455":1}}],["并用膨胀率参数",{"2":{"778":1}}],["并用一种叫做sonar的句子嵌入技术来表示这些概念",{"2":{"628":1}}],["并用inter",{"2":{"287":1}}],["并用",{"2":{"201":1}}],["并探索更鲁棒的模型架构",{"2":{"181":1}}],["并弱化不相关的单词",{"2":{"180":1}}],["并依据匹配程度对这些value做加权求和",{"2":{"164":1}}],["并使用上面讨论中选择的最佳检查点来将这个实验与baseline进行比较",{"2":{"1183":1}}],["并使用验证误差作为评估指标",{"2":{"1175":1}}],["并使用随机搜索来调整超参数",{"2":{"1175":1}}],["并使用搜索算法在单个研究中对两种超参数的值进行采样",{"2":{"1144":1}}],["并使用前一步的输出作为下一步的输入",{"2":{"885":1}}],["并使用索引来检索它们",{"2":{"834":1}}],["并使用单个数据集进行微调",{"2":{"733":1}}],["并使用字节对编码",{"2":{"637":1}}],["并使用",{"2":{"354":1,"1114":1}}],["并使用这一指标来更新记忆",{"2":{"228":1}}],["并使用这些信息来捕捉句子的内部结构和表示",{"2":{"120":1,"158":1}}],["并使用权重对所有与之关联的值进行加权平均",{"2":{"158":1}}],["并宣称kan是mlps的有力替代者",{"2":{"155":1}}],["并检查模型对",{"2":{"145":1}}],["并通过gpu并行化显著提高了执行效率",{"2":{"2011":1}}],["并通过gsgsg^s和gogog^o进行缩放",{"2":{"213":1}}],["并通过公有的成员函数提供受控的访问方式",{"2":{"1677":1}}],["并通过底层网络协议",{"2":{"1589":1}}],["并通过一次调用执行它们各自的连续step",{"2":{"1246":1}}],["并通过一个线性变换",{"2":{"718":1}}],["并通过额外的预训练阶段来适应新的结构",{"2":{"938":1,"954":1}}],["并通过交叉注意力来管理",{"2":{"621":1}}],["并通过网络进行前向传播",{"2":{"359":1}}],["并通过有界的tanh函数压缩极端值",{"2":{"359":1}}],["并通过以下公式从记忆",{"2":{"230":1}}],["并通过在这些方向上进行叠加和组合",{"2":{"137":1}}],["并通过另一个线性变换进行整合",{"2":{"5":1}}],["并从两个角度观察效果",{"2":{"136":1}}],["并加权积累生成新的embedding",{"2":{"676":1}}],["并加上偏置项",{"2":{"129":1}}],["并加工的方法包含两个环节",{"2":{"97":1}}],["并非总是可以回滚到旧的实现",{"2":{"1175":1}}],["并非",{"2":{"1174":1}}],["并非所有的任务和网络都能从零中心激活函数中受益",{"2":{"838":1}}],["并非实际nlp的ln版本",{"2":{"318":1,"323":1}}],["并非像rnn那样更容易注意到靠后的内容",{"2":{"274":1}}],["并非输入上下文的所有片段都同样重要",{"2":{"257":1}}],["并非定论",{"2":{"243":1}}],["并非每个片段中的所有",{"2":{"231":1}}],["并非越大越好",{"2":{"116":1}}],["并非论文公式中经过w^q",{"2":{"36":1}}],["并非论文所列出的3×h3×h3",{"2":{"26":1}}],["并关注注意力掩码的影响",{"2":{"93":1}}],["并且还能够包容他们",{"2":{"2054":1}}],["并且不允许重复的值",{"2":{"1806":1}}],["并且不应该被直接用于调整验证集性能",{"2":{"1131":1}}],["并且没有其他更好的解决方案时",{"2":{"1793":1}}],["并且都支持继承",{"2":{"1728":1}}],["并且被多重继承到同一个子类时",{"2":{"1660":1}}],["并且一旦绑定就不能重新绑定到其他变量",{"2":{"1650":1}}],["并且一旦初始化",{"2":{"1612":2}}],["并且每次递归调用都逐步接近该终止条件",{"2":{"1646":1}}],["并且每个头",{"2":{"765":1}}],["并且每个维度只能取",{"2":{"698":1}}],["并且每个词都可以依赖于上下文中的任何其他词来获得其表示",{"2":{"525":1}}],["并且每个重要子模块的输出都会经过层归一化",{"2":{"525":1}}],["并且每个重要子模块的输出都会经过layer",{"2":{"517":1}}],["并且它指向的内存地址中的值也是常量",{"2":{"1614":1}}],["并且改名为mybatis",{"2":{"1476":1}}],["并且尽可能占满整个空间的位置",{"2":{"1374":1}}],["并且中间的",{"2":{"1364":1}}],["并且对小众的语言也能生成高质量的翻译",{"2":{"1317":1}}],["并且放弃了",{"2":{"1315":1}}],["并且放大了较大的",{"2":{"178":1}}],["并且与完整的预训练相比",{"2":{"1313":1}}],["并且几乎在所有",{"2":{"1312":1}}],["并且用户没有指定任何与实现相关的",{"2":{"1228":1}}],["并且所有参数都将使用动量",{"2":{"1222":1}}],["并且所有的参数会原样传递给torch",{"2":{"1214":1}}],["并且应包含一个",{"2":{"1222":1}}],["并且接口足够通用",{"2":{"1219":1}}],["并且搜索点更为分散",{"2":{"1175":1}}],["并且计算成本很高",{"2":{"1163":1}}],["并且计算效率是关键因素",{"2":{"1159":1}}],["并且检查点足够频繁",{"2":{"1155":1}}],["并且已经过充分采样",{"2":{"1153":1}}],["并且我们是使用quasi",{"2":{"1150":1}}],["并且我们无法进行大量相对独立的研究",{"2":{"1144":1}}],["并且如果表现最好的试验在该区域的边缘具有学习率",{"2":{"1147":1}}],["并且如果被修改的输入的存储被其他张量引用",{"2":{"1123":1}}],["并且范围要足够小",{"2":{"1141":2}}],["并且训练步数足够",{"2":{"1131":1}}],["并且您不打算在后续的自动求导记录的计算中使用在推断模式下创建的张量时",{"2":{"1121":1}}],["并且输出会保持与自动求导图的关联",{"2":{"1087":1}}],["并且也不直接存储这些项",{"2":{"1052":1}}],["并且继续沿该方向移动",{"2":{"1028":1}}],["并且能在新输入上泛化好的算法",{"2":{"1011":1}}],["并且能够捕获序列内各种范围的依赖关系",{"2":{"168":1}}],["并且梯度不会出现弥散现象",{"2":{"998":1}}],["并且键",{"2":{"975":1}}],["并且使用的内存量与序列长度呈线性关系",{"2":{"940":1,"962":1}}],["并且需要加载的数据量减少了h倍",{"2":{"937":1,"953":1}}],["并且需要采用非常庞大数量的的负样本",{"2":{"727":1}}],["并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出",{"2":{"851":1}}],["并且保持每个图像实例之间的独立",{"2":{"809":1}}],["并且保证尽可能接近",{"2":{"89":1}}],["并且通常来说会进行截断",{"2":{"759":1,"1339":1}}],["并且通过适当的调整",{"2":{"1154":1}}],["并且通过学习这些范畴",{"2":{"505":1}}],["并且通过softmax进行归一化",{"2":{"431":1}}],["并且自然地解开了值",{"2":{"756":1}}],["并且人们普遍认为上下文中单词的相对顺序更重要",{"2":{"756":1}}],["并且针对自身想额外抑制的语义关系",{"2":{"746":1}}],["并且把这个n维向量作为模型参数学习",{"2":{"714":1}}],["并且完美地符合以上给出的两点性质",{"2":{"712":1}}],["并且subword序列由subword出现概率的乘积产生",{"2":{"599":1}}],["并且标记一个索引",{"2":{"592":1}}],["并且token的子字符串将被替换为我们子词列表中已经存在的子词",{"2":{"587":1}}],["并且在某些情况下可以提高代码的可靠性",{"2":{"1764":1}}],["并且在两者上都实现了最优性能",{"2":{"1317":1}}],["并且在执行过程中不会改变",{"2":{"1287":1}}],["并且在设计中没有明确使用异步性和低精度",{"2":{"973":1}}],["并且在多头之间共享",{"2":{"759":1}}],["并且在推理效率上具有显著优势",{"2":{"611":1}}],["并且在语料中将这个字符对也同步全部替换为这个新的整体",{"2":{"576":1}}],["并且在反向传播的时候再利用这些激活值进行计算",{"2":{"495":1}}],["并且将特殊字符也加入词典",{"2":{"576":1}}],["并且列出了当前主流的大型语言模型",{"2":{"561":1}}],["并且强调了在设计和训练",{"2":{"561":1}}],["并且给每个单元分配一个唯一的标识符",{"2":{"547":1}}],["并且给前面的层更多的优化时间",{"2":{"333":1}}],["并且最终映射到一个新的语义空间",{"2":{"516":1}}],["并且最后的gamma和beta是",{"2":{"341":1}}],["并且最后一个",{"2":{"90":1}}],["并且其能达到的深度以及所需的优化时间与梯度下降算法一致",{"2":{"497":1}}],["并且编码了词的位置",{"2":{"490":1}}],["并且结合实现的模型对原始论文做了详细解读",{"2":{"432":1}}],["并且这些不确定性可能存在着正则化效果",{"2":{"1186":1}}],["并且这些tensor保留在gpu内存中以生成下一个token",{"2":{"981":1}}],["并且这些操作是独立的",{"2":{"417":1}}],["并且这些计算是相互独立的",{"2":{"417":1}}],["并且这个",{"2":{"344":1}}],["并且依据数据来生成对应的掩码",{"2":{"379":1}}],["并且网络中不止一种类型的attention模块时也适用",{"2":{"349":1}}],["并且提出了仅含线性层和",{"2":{"320":1}}],["并且随着网络层数的增加",{"2":{"305":1}}],["并且随着规模的增加",{"2":{"154":1}}],["并且可以重写",{"2":{"1763":1}}],["并且可以被程序中的其他部分多次调用",{"2":{"1729":1}}],["并且可以根据不同的输入数据进行灵活的控制流操作",{"2":{"1287":1}}],["并且可以使用训练误差作为评估指标",{"2":{"1175":1}}],["并且可以很容易地为各种感兴趣的模型执行训练和预测工作",{"2":{"1128":1}}],["并且可以很好地处理未知词汇",{"2":{"575":1}}],["并且可以有效地控制模型的过拟合",{"2":{"1012":1}}],["并且可以更有效地利用多个节点的处理能力",{"2":{"977":1}}],["并且可以在八个p100",{"2":{"911":1}}],["并且可以在多任务上分配注意力从而达到同时处理多个信息的目的",{"2":{"257":1}}],["并且可能负载不均衡",{"2":{"90":1}}],["并且该通路上的计算包含多次非线性激活操作",{"2":{"255":1}}],["并且更新预测xtxtx",{"2":{"240":1}}],["并且会在参考中列出",{"2":{"235":1}}],["并且会返回索引",{"2":{"83":1}}],["并且具有很强的可解释性",{"2":{"137":1}}],["并且再一次验证了知识神经元是由知识探测",{"2":{"135":1}}],["并且包含浅表模式",{"2":{"127":1}}],["并且ffn第一层前馈网络权重",{"2":{"126":1}}],["并且基于内部知识和逻辑推理得到的答案",{"2":{"121":1}}],["并且函数在所有点上都是可微的",{"2":{"108":1}}],["并且去除了glu中偏置项bias",{"2":{"103":1}}],["并且带有layernorm的自注意力动态可以同时拥有从1到满秩的任意秩的平衡点",{"2":{"91":1}}],["并且负载很均衡",{"2":{"90":1}}],["并且占比很高",{"2":{"90":1}}],["并且作为多头注意力的输出来返回",{"2":{"36":1}}],["并且因为序列长度可能不同",{"2":{"17":1}}],["并且",{"2":{"17":1,"1185":1,"1186":1}}],["并在虚拟角色中实现高精度的面部表情再现",{"2":{"2011":1}}],["并在实际开发中灵活运用",{"2":{"1913":1,"1932":1}}],["并在实际应用中发挥了重要作用",{"2":{"711":1}}],["并在lambda内部使用",{"2":{"1907":1}}],["并在控制台打印一句问候语",{"2":{"1729":1}}],["并在定义时也要带上模板参数列表",{"2":{"1700":1}}],["并在前面加上波浪号",{"2":{"1676":1}}],["并在每次替换前确认",{"2":{"1554":1}}],["并在每个进程中运行",{"2":{"1589":1}}],["并在每个单词的末尾添加一个特殊的结束标记",{"2":{"579":1}}],["并在每个head中独立计算自注意力",{"2":{"1":1}}],["并在",{"2":{"1315":1,"1702":1}}],["并在后续的转换过程中进行类型检查",{"2":{"1291":1}}],["并在模型的其他部分中进行访问",{"2":{"1214":1}}],["并在开始时延长恒定的lr期",{"2":{"1159":1}}],["并在最终长时间训练后再进行验证和确认",{"2":{"1157":1}}],["并在分桶定义的每个垂直切片中取最佳试验来近似绘制隔离图",{"2":{"1150":1}}],["并在此过程中学到了很多东西",{"2":{"1127":1}}],["并在读取时将其解包为不同的张量",{"2":{"1114":1}}],["并在现代神经网络继续保持显著特色",{"2":{"1016":1}}],["并在过去两个月部署在chatbot",{"2":{"980":1}}],["并在调整期间考虑缓存重用利用率和slo要求违规等指标时使conductor的设计复杂化",{"2":{"977":1}}],["并在ring",{"2":{"977":1}}],["并在计算和通信之间取得最佳折衷来实现最佳性能",{"2":{"976":1}}],["并在gpu之间实现最佳负载平衡",{"2":{"976":1}}],["并在反向过程中再次收集kv",{"2":{"976":1}}],["并在中间层中编码了语言要素的复杂层次结构",{"2":{"437":1}}],["并在需要精细细节的任务中降低了性能",{"2":{"212":1}}],["并在推理阶段根据输入特征的相似性高效检索出近似的输出结果",{"2":{"153":1}}],["并在h个低维空间里求解各自的注意力",{"2":{"5":1}}],["并行的类型",{"0":{"1567":1},"1":{"1568":1,"1569":1}}],["并行的实际案例",{"0":{"1566":1}}],["并行的引入",{"0":{"1562":1},"1":{"1563":1,"1564":1,"1565":1,"1566":1}}],["并行有啥用",{"0":{"1565":1}}],["并行运行与顺序运行不同数量的试验不会产生统计上不同的结果",{"2":{"1175":1}}],["并行执行所有token的推理",{"2":{"540":1}}],["并行度最高",{"2":{"511":1}}],["并行预测",{"2":{"427":1}}],["并行处理可以增加填充信息",{"2":{"480":1}}],["并行处理",{"2":{"172":1}}],["并行化计算量可以用所需的序列操作的最小数目来衡量",{"2":{"160":1}}],["并行化能力",{"2":{"21":1}}],["并行计算可以在不同的硬件架构上实现",{"2":{"1566":1}}],["并行计算变得愈发重要",{"2":{"1564":1}}],["并行计算",{"2":{"160":1,"1564":2}}],["并行记忆",{"2":{"154":1}}],["并行组合为",{"2":{"7":1}}],["并行地学习多组自注意力权重",{"2":{"5":1}}],["并行",{"0":{"34":1,"413":1},"1":{"414":1,"415":1,"416":1,"417":1,"418":1,"419":1,"420":1},"2":{"0":1,"1570":1}}],["在你所处的位置",{"2":{"2115":1}}],["在你下意识中",{"2":{"163":1}}],["在物理主机开启nat和dhcp服务",{"0":{"2093":1}}],["在坐标系上绘制点",{"2":{"2018":1}}],["在虚拟机里重新start网络或者重新启动虚拟机",{"2":{"2093":1}}],["在虚拟角色的面部动画方面",{"2":{"2011":1}}],["在虚继承中",{"2":{"1693":1}}],["在3d环境中检测物体的相互碰撞",{"2":{"2009":1}}],["在安全性上",{"2":{"1853":1}}],["在安装",{"2":{"1589":1}}],["在需要返回多个数据时使用",{"2":{"1805":1}}],["在能够合理处理异常的地方捕获异常",{"2":{"1764":1}}],["在栈展开的过程中",{"2":{"1762":1,"1764":1}}],["在栈区分配内存",{"2":{"1648":1}}],["在有限的预算下",{"2":{"2130":1}}],["在有序范围内返回第一个和最后一个等于某个值的元素的范围",{"2":{"1756":1}}],["在有序范围内快速检查是否存在某个值",{"2":{"1755":1}}],["在有疑问的时候",{"2":{"1155":1}}],["在引用传递中",{"2":{"1729":1}}],["在引入更多样化的数据时",{"2":{"222":1}}],["在值传递中",{"2":{"1729":1}}],["在函数调用时传递给函数的具体值或表达式",{"2":{"1729":1}}],["在函数定义中声明的参数",{"2":{"1729":1}}],["在函数内部用",{"2":{"1649":1}}],["在函数内部或复合语句",{"2":{"1649":1}}],["在功能上几乎完全相同",{"2":{"1728":1}}],["在之前的课程中",{"2":{"1728":1}}],["在之前图中",{"2":{"899":1}}],["在队列末尾添加元素",{"2":{"1723":1}}],["在指定范围内寻找第一个等于指定值的元素",{"2":{"1754":1}}],["在指定迭代器位置之后插入元素",{"2":{"1721":1}}],["在指定迭代器位置插入元素",{"2":{"1719":1,"1720":1,"1722":1}}],["在指定位置插入字符串或字符",{"2":{"1713":1}}],["在声明时指定数组的大小",{"2":{"1714":1}}],["在声明变量时",{"2":{"1611":1}}],["在字符串末尾追加字符串或其子串",{"2":{"1713":1}}],["在字节词汇表v上的lm分布pepep",{"2":{"613":1}}],["在接下来的学习中",{"2":{"1709":1,"1729":1}}],["在接口方法中",{"2":{"1485":1}}],["在接口方法的参数前加",{"2":{"1485":1}}],["在表达式中使用时",{"2":{"1705":1}}],["在显示继承来的基本信息的同时显示该类动物的特定信息",{"2":{"1690":1}}],["在赋值之前进行范围检查",{"2":{"1684":1}}],["在构造函数和析构函数中添加输出语句",{"2":{"1680":1}}],["在构建采样数据集时",{"2":{"1165":1}}],["在构建大模型的过程中",{"2":{"545":1}}],["在类模板内部声明成员函数模板时",{"2":{"1701":1}}],["在类中",{"2":{"1677":1}}],["在类定义外部初始化静态成员变量",{"2":{"1639":1}}],["在堆上动态分配内存",{"2":{"1714":1}}],["在堆上分配一块足以存储",{"2":{"1668":1}}],["在堆上分配一块足以存储一个",{"2":{"1668":1}}],["在堆区分配内存",{"2":{"1648":1}}],["在所在的代码块执行完毕后被销毁",{"2":{"1649":1}}],["在所有函数外部用",{"2":{"1649":1}}],["在所有函数外部定义的变量",{"2":{"1649":1}}],["在所有输入信息上计算注意力分布",{"2":{"267":1}}],["在释放内存后",{"2":{"1647":1}}],["在程序结束时被销毁",{"2":{"1649":1}}],["在程序结束时释放内存",{"2":{"1639":1}}],["在程序运行时根据需要动态地申请和释放内存",{"2":{"1647":1}}],["在成员函数声明后加上",{"2":{"1640":1}}],["在成员函数声明前加上",{"2":{"1639":1}}],["在成员变量声明前加上",{"2":{"1639":1,"1640":1}}],["在成型的陶瓷坯体表面施以釉浆",{"2":{"437":1}}],["在编译时计算数组大小",{"2":{"1924":1}}],["在编译时无法确定大小",{"2":{"1647":1}}],["在编译之前进行处理",{"2":{"1632":1}}],["在编程中",{"2":{"1645":1,"1728":1}}],["在编码器内部",{"2":{"520":1}}],["在编码器层内部的流转过程中",{"2":{"520":1}}],["在编码器",{"2":{"284":1}}],["在循环开始前执行一次",{"2":{"1621":1}}],["在菜单驱动的程序中",{"2":{"1620":1}}],["在讨论指针和引用时",{"2":{"1613":1}}],["在现在这个繁杂而又急躁的社会",{"2":{"2056":1}}],["在现代",{"2":{"1607":1}}],["在现实中",{"2":{"628":1}}],["在首次启动",{"2":{"1605":1}}],["在win10网络中将vm8的网络手动设置成规范格式",{"2":{"2095":1}}],["在windows环境中进行高性能计算",{"2":{"1569":1}}],["在windows平台上实现的mpi",{"2":{"1569":1}}],["在warmup",{"2":{"1183":1}}],["在同步通信中",{"2":{"1574":1}}],["在同时进行的前提下",{"2":{"1564":1}}],["在同一层内的头之间共享",{"2":{"503":1}}],["在同一张图像上学习到的特征应该是具有相同的分布",{"2":{"338":1}}],["在同一训练设置下",{"2":{"334":1}}],["在可视模式中选中文本",{"2":{"1559":1}}],["在普通模式中输入",{"2":{"1553":1}}],["在普通模式中按",{"2":{"1551":1}}],["在光标前粘贴",{"2":{"1549":1}}],["在光标前插入",{"2":{"1545":1}}],["在光标后粘贴",{"2":{"1549":1}}],["在光标后插入",{"2":{"1545":1}}],["在文件末尾添加内容",{"2":{"1820":1,"1838":1}}],["在文件之间切换",{"2":{"1556":1}}],["在文件中查找",{"2":{"1516":1}}],["在文本中注入噪声",{"2":{"1015":1}}],["在文本时代",{"2":{"678":1}}],["在java代码中添加sql通配符",{"2":{"1489":1}}],["在usermapper接口中添加对应的方法",{"2":{"1486":1}}],["在usermapper",{"2":{"1485":1,"1486":1}}],["在usermapper中添加对应方法",{"2":{"1485":1}}],["在生活中",{"2":{"1477":1,"2106":1}}],["在生成新单词时不改变",{"2":{"746":1}}],["在生成时",{"2":{"612":1}}],["在生成下一个token时",{"2":{"529":1}}],["在生成模型中",{"2":{"337":1}}],["在生成语义向量时存在区别",{"2":{"285":1}}],["在生成过程中逐渐构建输出的模式",{"2":{"147":1,"482":1}}],["在操作系统中",{"2":{"1407":1}}],["在y0",{"2":{"1377":1}}],["在变分推断中",{"2":{"1377":1}}],["在解引用指针之前",{"2":{"1672":1}}],["在解决上面问题后",{"2":{"1374":1}}],["在解码器",{"2":{"886":1}}],["在解码器交叉注意力中",{"2":{"614":1}}],["在解码器内部",{"2":{"530":1}}],["在解码器层内部的流转过程中",{"2":{"530":1}}],["在解码器中",{"2":{"525":1}}],["在解码器的自注意力层之后",{"2":{"307":1}}],["在解码器的self",{"2":{"78":1}}],["在解码过程中",{"2":{"290":1,"532":1}}],["在解码decoder",{"2":{"71":1}}],["在解码第",{"2":{"58":1}}],["在经过一系列数学研究后",{"2":{"1372":1}}],["在经过掩码矩阵处理之后",{"2":{"62":1}}],["在vmware虚拟化工具中把对应的nat网络手动设置成一样的信息",{"2":{"2095":1}}],["在vae中",{"2":{"1372":1}}],["在vllm中",{"2":{"981":1}}],["在512x512模型基础上用图像分辨率大于768x768的子集继续训练",{"2":{"1363":1}}],["在improved",{"2":{"1363":4}}],["在init时候也放入",{"2":{"668":1}}],["在性能上可以媲美较小版本的",{"2":{"1316":1}}],["在流行的",{"2":{"1315":1}}],["在流体力学中通常以如下图标号2的方程进行模拟",{"2":{"498":1}}],["在流体力学中通常以动态系统的常微分方程来模拟",{"2":{"498":1}}],["在流体力学中",{"2":{"498":1}}],["在绝大部分情况下",{"2":{"1313":1}}],["在绝对位置编码中",{"2":{"764":1}}],["在翻译任务上超过了之前最优秀的循环神经网络模型",{"2":{"1312":1}}],["在翻译时",{"2":{"284":1,"908":1}}],["在动态图中",{"2":{"1287":1}}],["在动态向量表示中",{"2":{"715":1}}],["在静态图中",{"2":{"1287":1}}],["在适当的层次捕获异常",{"2":{"1764":1}}],["在适当选择值矩阵的情况下",{"2":{"92":1}}],["在适用的情况下",{"2":{"1228":1}}],["在参数组未指定时使用",{"2":{"1225":1}}],["在未覆盖它们的组中生效",{"2":{"1222":1}}],["在极少数情况下",{"2":{"1214":1}}],["在极端训练预算限制下",{"2":{"1158":1}}],["在设置完self的state",{"2":{"1214":1}}],["在设计一项研究或一系列研究时",{"2":{"1145":1}}],["在设计新一轮实验时",{"2":{"1143":1}}],["在访问不存在的属性时被调用",{"2":{"1214":1}}],["在forward",{"2":{"1214":1}}],["在ffn中",{"2":{"101":1}}],["在模块上注册一个前向传播钩子",{"2":{"1214":1}}],["在模块上注册一个前向传播预钩子",{"2":{"1214":1}}],["在模块上注册一个反向传播钩子",{"2":{"1214":2}}],["在模块上注册一个反向传播",{"2":{"1214":1}}],["在模型训练期间收集的指标可能是离线评估的代理",{"2":{"1163":1}}],["在模型微调期间需要冻结预训练模型的某些部分",{"2":{"1117":1}}],["在模型预测中的贡献来定位模型行为",{"2":{"478":1}}],["在模型的推理阶段",{"2":{"721":1}}],["在模型的最后若干层",{"2":{"437":1}}],["在模型的执行阶段",{"2":{"172":1}}],["在模型对文本建模中的影响",{"2":{"382":1}}],["在模型生成",{"2":{"277":1}}],["在模型无法预测正确的类别时会惩罚模型",{"2":{"183":1}}],["在梯度计算之后执行的钩子函数",{"2":{"1208":1}}],["在梯度计算之前执行的钩子函数",{"2":{"1208":1}}],["在子类上进行赋值之前",{"2":{"1206":1}}],["在不再使用时必须使用",{"2":{"1669":1}}],["在不更改训练工作流其他细节的情况下",{"2":{"1186":1}}],["在不引入额外可学习参数的情况下处理这两种类型的交互作用",{"2":{"213":1}}],["在发布一篇研究论文的时候",{"2":{"1185":1}}],["在发布使用此类衰减方案的结果之前",{"2":{"1173":1}}],["在贝叶斯机器学习中",{"2":{"1185":1}}],["在样本学习中可以以",{"2":{"1183":1}}],["在哪里可以找到quasi",{"0":{"1176":1}}],["在哪里做注意力计算",{"2":{"262":1}}],["在高并行机制中击败",{"2":{"1175":1}}],["在高维空间中",{"2":{"709":1}}],["在高维空间中显著地改变夹角大小相对来说没有那么容易",{"2":{"176":1}}],["在许多调优试验并行运行时特别高效",{"2":{"1175":1}}],["在优化过程中",{"2":{"1171":1}}],["在优先保证残差连接有效性的同时",{"2":{"329":1}}],["在日志记录",{"2":{"1169":1}}],["在测试模式中使用的指数移动平均",{"2":{"1168":1}}],["在电子表格中跟踪实验结果有助于我们解决各种建模问题",{"2":{"1167":1}}],["在跟踪不同的实验时",{"2":{"1167":1}}],["在稀有类别样本上的表现往往会有噪音",{"2":{"1165":1}}],["在线评估是最佳标准",{"2":{"1163":1}}],["在线评估",{"2":{"1163":1}}],["在固定步长间隔进行评估",{"2":{"1162":1}}],["在短时间和不完整训练中找到的超参数在增加训练长度后仍然是好选择的保证是没有的",{"2":{"1158":1}}],["在调用成员函数模板时",{"2":{"1701":1}}],["在调用函数时",{"2":{"1650":3}}],["在调用",{"2":{"1227":2}}],["在调用torch",{"2":{"1227":2}}],["在调用self上的state",{"2":{"1227":1}}],["在调用模块的load",{"2":{"1214":1}}],["在调整学习率衰减计划时的一个常见问题是使用了太小的学习率",{"2":{"1157":1}}],["在调优过程中的许多抉择",{"2":{"1128":1}}],["在较少的训练步骤上调整开根号衰减",{"2":{"1158":1}}],["在较佳的超参数上进行少量长时间的训练来得到最终模型",{"2":{"1157":1}}],["在较短的序列后面用特殊符号来填充",{"2":{"53":1}}],["在搜索中最快达到完美训练的实验所需的步数就是我们对",{"2":{"1156":1}}],["在没有数据增强和正则化的情况下运行恒定的学习率搜索",{"2":{"1156":1}}],["在采用一个候选变化之前",{"2":{"1152":1}}],["在试验中",{"2":{"1149":1}}],["在检查训练曲线时",{"2":{"1149":1}}],["在当前行上方插入新行",{"2":{"1545":1}}],["在当前行下方插入新行",{"2":{"1545":1}}],["在当前行行尾插入",{"2":{"1545":1}}],["在当前行行首插入",{"2":{"1545":1}}],["在当前搜索空间中",{"2":{"1145":1}}],["在当前轮次实验中取固定值的参数",{"2":{"1143":1}}],["在足够大的搜索空间上调整冗余超参数",{"2":{"1145":1}}],["在足够深的情况下",{"2":{"320":1}}],["在更深的调用栈中",{"2":{"1761":1}}],["在更深的ln层中",{"2":{"359":1}}],["在更多的训练数据上",{"2":{"1315":1}}],["在更改批大小或主机数量时会有一些棘手的细节",{"2":{"1168":1}}],["在更复杂的情况下",{"2":{"1144":1}}],["在探索结束后",{"2":{"1144":1}}],["在探索阶段",{"2":{"1144":1}}],["在探讨如何设计和训练一个可以在测试时学习记忆的长期记忆模块之后",{"2":{"230":1}}],["在比较目标超参数的不同值时",{"2":{"1143":1}}],["在添加花哨的衰减方案之前以恒定的学习率开始",{"2":{"1137":1}}],["在项目开始时选择batch",{"2":{"1135":1}}],["在项目的初始阶段",{"2":{"1130":1}}],["在开始超参数调整之前",{"2":{"1137":1}}],["在开始一个新项目时",{"2":{"1129":1}}],["在开始调优之前",{"2":{"1128":1}}],["在开发",{"2":{"985":1}}],["在准备创建此文档时",{"2":{"1127":1}}],["在评估模型",{"2":{"1122":1}}],["在退出推断模式后",{"2":{"1121":1}}],["在初始化参数时避免了在原地更新初始化的参数时的自动求导跟踪",{"2":{"1120":1}}],["在执行效率上具有天然的优势",{"2":{"1602":1}}],["在执行训练更新时",{"2":{"1120":1}}],["在执行的过程中",{"2":{"408":1}}],["在无梯度模式下",{"2":{"1120":2}}],["在启动训练之前",{"2":{"1113":1}}],["在内存中以连续的形式存储数据",{"2":{"1797":1}}],["在内存中会依次存储",{"2":{"1705":1}}],["在内存中",{"2":{"1705":1}}],["在内存中的地址",{"2":{"1611":1}}],["在内存占用减少",{"2":{"1315":1}}],["在内部处理计算的方式",{"2":{"1118":1}}],["在内部",{"2":{"1113":1,"1114":1}}],["在内层有样条来优化这些学习的特征以达到高准确性",{"2":{"155":1}}],["在概念",{"2":{"1469":1}}],["在概念上",{"2":{"1113":1}}],["在概念空间里",{"2":{"689":1}}],["在正向的时候自动生成",{"2":{"1107":1}}],["在正确的架构中",{"2":{"689":1}}],["在dag",{"2":{"1089":1}}],["在decoder端",{"2":{"894":1}}],["在decoderlayer类的forward",{"2":{"82":1}}],["在adam",{"2":{"1059":1}}],["在add操作后进行norm操作",{"2":{"329":1}}],["在凸优化背景中",{"2":{"1042":1}}],["在批量梯度下降中",{"2":{"1025":1}}],["在0点处导数值为1",{"2":{"1000":1}}],["在后几层中",{"2":{"994":1}}],["在后续的课程中",{"2":{"1607":1}}],["在后续的文章中",{"2":{"473":1}}],["在后续步骤的预测中",{"2":{"427":1}}],["在步骤",{"2":{"986":9}}],["在连续批处理",{"2":{"986":1}}],["在涉及多个",{"2":{"985":1}}],["在涉及需要从其他主机获取块的key",{"2":{"975":1}}],["在并行采样中",{"2":{"983":1}}],["在pytorch中",{"2":{"1290":1}}],["在pagedattention中",{"2":{"982":1}}],["在prefill中已经有将sequence",{"2":{"420":1}}],["在prefill时",{"2":{"83":1}}],["在gpt中已经添加了cp支持",{"2":{"976":1}}],["在反向计算中重新计算激活可以避免oom",{"2":{"976":1}}],["在反向传播期间检索它们",{"2":{"1114":1}}],["在反向传播中应用reduce",{"2":{"976":1}}],["在反向传播中",{"2":{"971":1}}],["在反向传播过程",{"2":{"1117":1}}],["在反向传播过程时",{"2":{"839":1}}],["在反向传播过程中",{"2":{"192":1}}],["在我们的系统中",{"2":{"986":1}}],["在我们的实验中",{"2":{"980":1}}],["在我们的github存储库中",{"2":{"980":1}}],["在我们的学习历程中",{"2":{"3":1}}],["在我们算法的两阶段版本中",{"2":{"973":1}}],["在异步块级gemm下隐藏softmax",{"2":{"973":1}}],["在常见的序列长度",{"2":{"945":1,"965":1}}],["在常规的pooling前加上注意力机制",{"2":{"731":1}}],["在芯片上更新",{"2":{"944":1}}],["在芯片上计算",{"2":{"944":2}}],["在外循环",{"2":{"940":1,"962":1}}],["在外层有mlps来学习特征",{"2":{"155":1}}],["在将输入序列分布到不同主机时",{"2":{"975":1}}],["在将多头检查点转换为gqa检查点时",{"2":{"937":1,"953":1}}],["在将表示分割成不同个头进行运算时",{"2":{"175":1}}],["在各种优化器超参数",{"2":{"1143":1}}],["在各种任务中",{"2":{"911":1}}],["在各种seq2seq任务中实现卓越的性能",{"2":{"538":1}}],["在于",{"2":{"909":1}}],["在负数部分",{"2":{"840":1}}],["在非线性激活函数的作用下数据分布进行再映射",{"2":{"838":1}}],["在几何中",{"2":{"838":1}}],["在新轴上拼接tensor",{"2":{"826":1}}],["在新的向量中",{"2":{"166":1,"170":1}}],["在卷积神经网络训练初期",{"2":{"816":1}}],["在卷积核里面的元素之间插入空格来",{"2":{"778":1}}],["在理论上也缺少前后token间相对方向的感知",{"2":{"761":1}}],["在两侧做了截断",{"2":{"759":1}}],["在两个线性变换之间除了",{"2":{"99":1}}],["在长度外推上的表现难以令人满意",{"2":{"756":1}}],["在很多任务中",{"2":{"755":1}}],["在很多场景中",{"2":{"696":1}}],["在刻画长距离语义关系时",{"2":{"746":1}}],["在人类的语言中",{"2":{"744":1}}],["在人工智能正在重塑人类社会方方面面的同时",{"2":{"506":1}}],["在人工智能领域",{"2":{"220":1}}],["在人工智能快速发展的今天",{"2":{"121":1}}],["在直接生成向量时",{"2":{"736":1}}],["在末尾添加了一个embedding序列",{"2":{"731":1}}],["在backbone选择上",{"2":{"729":1}}],["在bert",{"2":{"698":1}}],["在bert带来的最初爆炸性增长之后",{"2":{"540":1,"541":1}}],["在bert时代由于层数较浅",{"2":{"335":1}}],["在任务间存在差异的情况下",{"2":{"726":1}}],["在保证通用性的同时",{"2":{"726":1}}],["在中文世界",{"2":{"724":1}}],["在中间比值上表现出更大的灵活性",{"2":{"100":1}}],["在encoder",{"2":{"891":1}}],["在encoder的多头自注意力中",{"2":{"525":1}}],["在elmo之前的模型中",{"2":{"718":1}}],["在语境中",{"2":{"715":1}}],["在特定位置添加批标准化是否有帮助",{"2":{"1143":1}}],["在特定的语言任务中必然具有相似的词向量",{"2":{"714":1}}],["在特定领域的表现",{"2":{"121":1}}],["在用语言模型进行无监督训练时会使用窗口机制",{"2":{"714":1}}],["在用gpu计算时",{"2":{"36":1}}],["在智利",{"2":{"713":1}}],["在拉普拉塔",{"2":{"713":1}}],["在1954",{"2":{"713":1}}],["在过去的几年里",{"2":{"711":1}}],["在过去的几年中",{"2":{"311":1}}],["在早期",{"2":{"711":1}}],["在获取输入词向量之后需要对矩阵乘以embedding",{"2":{"701":1}}],["在嵌入空间中会有相似的向量表示",{"2":{"709":1}}],["在嵌入矩阵中查找相应的词向量",{"2":{"700":1}}],["在嵌入的空间中",{"2":{"125":1}}],["在选择替代方案时",{"2":{"692":1}}],["在通过点积优化来学习嵌入时",{"2":{"692":1}}],["在通道方向上",{"2":{"341":1}}],["在机器学习和认知科学领域",{"2":{"1456":1}}],["在机器学习和深度学习中",{"2":{"1023":1}}],["在机器学习和数据科学领域",{"2":{"692":1}}],["在机器学习中",{"2":{"1011":1,"1455":1}}],["在机器学习之前",{"2":{"708":1}}],["在机器翻译业务中",{"2":{"453":1}}],["在机器翻译中",{"2":{"277":1}}],["在最简单的情况下",{"2":{"1144":1}}],["在最基本的层面上",{"2":{"689":1}}],["在最后一层之后不需要额外的归一化",{"2":{"354":1}}],["在最后一个维度计算均方值",{"2":{"346":1}}],["在空间中相邻",{"2":{"689":1}}],["在空白patch化中",{"2":{"613":1}}],["在独热编码中",{"2":{"681":2}}],["在讲transformer的padding",{"2":{"665":1}}],["在句子中随机交换任意两个单词",{"2":{"1015":1}}],["在句子表征空间中进行建模",{"2":{"628":1}}],["在句尾也加入一个特殊",{"2":{"379":1}}],["在架构图的右下方",{"2":{"620":1}}],["在标准llm中",{"2":{"612":1}}],["在标准方法之外进一步采用了门限层",{"2":{"98":1}}],["在低熵区域节省资源",{"2":{"612":1}}],["在数学中",{"2":{"680":1,"943":1,"961":1}}],["在数学任务上的表现",{"2":{"595":1}}],["在数据生成过程中要输入一些数进去",{"2":{"1372":1}}],["在数据加载时",{"2":{"65":1}}],["在数据集的生成过程中",{"2":{"53":1,"376":1}}],["在拼接时",{"2":{"588":1}}],["在得到subword词表后",{"2":{"587":1}}],["在对象生命周期结束时自动调用",{"2":{"1676":1}}],["在对象创建时自动调用",{"2":{"1675":1}}],["在对词进行向量表示时",{"2":{"715":1}}],["在对预语料进行预分词",{"2":{"577":1}}],["在对两个序列的alignment",{"2":{"287":1}}],["在英语中不同后缀的词非常的多",{"2":{"576":1}}],["在资源有限的环境下",{"2":{"562":1}}],["在专业领域",{"2":{"560":1}}],["在总体趋势上",{"2":{"559":1}}],["在c语言基础上发展而来",{"2":{"1603":1}}],["在c++多继承特性下",{"2":{"1869":1}}],["在c++",{"2":{"1097":1}}],["在c",{"2":{"726":1}}],["在collate",{"2":{"558":1}}],["在cnn中ln规范化效果不如bn",{"2":{"808":1}}],["在cnn中",{"2":{"314":1}}],["在cnn方案中",{"2":{"247":1}}],["在大语言模型中",{"2":{"700":1}}],["在大模型技术体系中",{"2":{"676":1}}],["在大模型上训练时",{"2":{"612":1}}],["在大规模语料库上进行自我监督学习十分契合",{"2":{"542":1}}],["在大多数情况下",{"2":{"108":1,"1123":1,"1185":1,"1704":1}}],["在纯解码器模型",{"2":{"542":1}}],["在纯解码器decoder",{"2":{"542":1}}],["在纯粹的自监督预训练后",{"2":{"542":1}}],["在纯自注意力下",{"2":{"93":1}}],["在知乎上有一个知名的帖子",{"2":{"542":1}}],["在随后的工作中",{"2":{"540":1}}],["在key上引入相对位置信息",{"2":{"760":1}}],["在k",{"2":{"537":1}}],["在单核单处理器的环境下",{"2":{"1566":1}}],["在单个训练步骤中计算得出",{"2":{"1443":1}}],["在单卡内部做的分块优化扩展到多卡上",{"2":{"974":1}}],["在单次推理过程中",{"2":{"530":1}}],["在单头注意力机制中",{"2":{"26":1}}],["在假设全部预测正确的情况下",{"2":{"528":1}}],["在假设模型只有一层",{"2":{"445":1}}],["在组装过程中的玩具就是解码器在之前步骤中的预测输出结果",{"2":{"524":1}}],["在和位置编码相加时",{"2":{"520":1,"530":1}}],["在做embedding操作时",{"2":{"520":1,"530":1}}],["在符合一定假设条件下",{"2":{"507":1}}],["在提示πφπφπ",{"2":{"504":1}}],["在提升训练稳定性的同时",{"2":{"335":1}}],["在读取阶段",{"2":{"489":1}}],["在读取数据集的句子时",{"2":{"379":1}}],["在图形学中",{"2":{"2010":1}}],["在图形用户界面",{"2":{"1645":1}}],["在图形界面",{"2":{"1645":1}}],["在图1中",{"2":{"976":1}}],["在图中",{"2":{"975":1}}],["在图像分类任务上获得了远远超过其他参赛者的结果",{"2":{"840":1}}],["在图像或语音生成领域",{"2":{"636":1}}],["在图的左侧",{"2":{"507":1}}],["在图的左侧展示了10个token在球体上的运动",{"2":{"507":1}}],["在图的上方",{"2":{"485":1}}],["在图半径r上是单调的",{"2":{"93":1}}],["在分屏间切换",{"2":{"1557":1}}],["在分析一组给定的实验以朝着最初的目标取得进展之前",{"2":{"1146":1}}],["在分析中",{"2":{"485":1}}],["在分词领域有时也被称为凝固度",{"2":{"598":1}}],["在分布式计算时改进了",{"2":{"217":1}}],["在宽度上有机会影响采样的概率分布",{"2":{"480":1}}],["在整数类型",{"2":{"1607":1}}],["在整个工作量的范围中并不存在真正意义上的标准值",{"2":{"1183":1}}],["在整个学习过程中自动适应这些学习率是有道理的",{"2":{"1041":1}}],["在整体词表上的概率分布",{"2":{"471":1}}],["在整理过程中",{"2":{"235":1}}],["在词表",{"2":{"455":1}}],["在词表中的序号是0",{"2":{"379":1}}],["在全局自注意力机制中",{"2":{"442":1}}],["在全句中",{"2":{"170":1}}],["在全句中位置逻辑",{"2":{"4":1}}],["在torch",{"2":{"1214":1}}],["在tokenformer中",{"2":{"622":1}}],["在teacher",{"2":{"407":1,"416":1,"528":1}}],["在transformer类模型中",{"2":{"674":1}}],["在transformer架构图中",{"2":{"457":1}}],["在transformer架构中",{"2":{"294":1,"397":1}}],["在transformer实现中叫做memory",{"2":{"453":1}}],["在transformer中的位置",{"2":{"745":2}}],["在transformer中引入递归",{"2":{"512":1}}],["在transformer中有三种注意力结构",{"2":{"439":1}}],["在transformer中",{"2":{"437":1,"674":1}}],["在transformer中使用bn也是可以的",{"2":{"326":1}}],["在transformer论文中",{"2":{"436":1}}],["在transformer原文中即为",{"2":{"316":1}}],["在transformer出现之前",{"2":{"249":1}}],["在transformer的多头注意力模块",{"2":{"44":1}}],["在真值",{"2":{"399":1}}],["在某个维度上",{"2":{"1147":1}}],["在某种程度上也增加了泛化性",{"2":{"396":1}}],["在某些底层编程或需要直接操作内存的情况下",{"2":{"1728":1}}],["在某些实际问题中",{"2":{"1326":1}}],["在某些深度学习模型上效果不错",{"2":{"1042":1}}],["在某些情况下",{"2":{"709":1,"838":3,"1139":1,"1143":1,"1146":1,"1157":1,"1615":1,"1670":1}}],["在某些应用中",{"2":{"692":1}}],["在某些应用场景中",{"2":{"568":1}}],["在某些领域",{"2":{"245":2}}],["在某些任务中",{"2":{"50":1}}],["在小模型中可能dropout的效果比较显著",{"2":{"396":1}}],["在一些情况下",{"2":{"1638":1}}],["在一些实验中",{"2":{"346":1}}],["在一个表示消息的结构体中",{"2":{"1728":1}}],["在一个函数中动态分配内存并需要修改调用者传递进来的指针变量",{"2":{"1650":1}}],["在一个进程或节点中创建待发送的信息",{"2":{"1563":1}}],["在一个好的词嵌入中",{"2":{"691":1}}],["在一群猫和狗中",{"2":{"1462":1}}],["在一般的条件随机场",{"2":{"1326":1}}],["在一定模型下",{"2":{"841":1}}],["在一定程度上达到正则化的效果",{"2":{"393":1}}],["在哈佛代码中",{"2":{"703":1,"704":1}}],["在哈佛代码中使用的是多头自注意力",{"2":{"199":1}}],["在哈佛源码中构建了几个数据相关的全局变量",{"2":{"371":1}}],["在实现一个通用库时",{"2":{"1866":1}}],["在实验中",{"2":{"542":1}}],["在实际的类设计中选择恰当的继承方式",{"2":{"1871":1}}],["在实际的深度学习超参数调优条件下",{"2":{"1175":1}}],["在实际开发中",{"2":{"1715":1}}],["在实际模型中",{"2":{"717":1}}],["在实际操作中",{"2":{"560":1}}],["在实际中去解常微分方程的时候",{"2":{"498":1}}],["在实际应用中",{"2":{"400":1,"402":1,"562":1}}],["在实际训练的时候一个",{"2":{"89":1}}],["在实践中dot",{"2":{"921":1}}],["在实践中人们发现",{"2":{"895":1}}],["在实践中需要仔细设置迭代次数",{"2":{"582":1}}],["在实践中",{"2":{"367":1,"982":1,"1015":1}}],["在每一轮中",{"2":{"1159":1}}],["在每一步中",{"2":{"912":1}}],["在每一步选择k个候选项时",{"2":{"904":1}}],["在每一步",{"2":{"902":1}}],["在每一步的预测时",{"2":{"896":1}}],["在每次循环体执行完毕后执行",{"2":{"1621":1}}],["在每次循环迭代之前进行判断",{"2":{"1621":1}}],["在每次参数更新时",{"2":{"1031":1}}],["在每次迭代训练中",{"2":{"1017":1}}],["在每次训练步骤后",{"2":{"357":1}}],["在每句话中依据上下文有不同的意思",{"2":{"715":1}}],["在每个序列的末尾",{"2":{"1330":1}}],["在每个标签",{"2":{"1324":1}}],["在每个训练步骤中只有被遮盖掉词语的表示会得到更新",{"2":{"1315":1}}],["在每个训练周期之前更新学习率",{"2":{"1239":1}}],["在每个阶段",{"2":{"1315":1,"1316":1,"1317":1}}],["在每个周期内进行训练",{"2":{"1243":1}}],["在每个块中",{"2":{"940":1,"962":1}}],["在每个注意力层和",{"2":{"394":1}}],["在每个epoch中",{"2":{"385":1}}],["在每个时刻",{"2":{"241":1}}],["在每个时间步",{"2":{"239":1,"251":1,"427":1}}],["在每个步骤中",{"2":{"154":1}}],["在每个位置上把序列中的信息做了一次全局的汇聚",{"2":{"101":1}}],["在每个组内进行注意力操作",{"2":{"33":1}}],["在每个子空间里面学习到的相似性的重要度是一样的",{"2":{"10":1}}],["在基于",{"2":{"1320":1}}],["在基于bpe基础上提出了一种新的subword算法",{"2":{"605":1}}],["在基于moe的transformers中",{"2":{"150":1}}],["在基线",{"2":{"355":1}}],["在原始的仅解码器",{"2":{"352":1}}],["在原生的ffn中采用两层全连接",{"2":{"109":1}}],["在超球面上进行表示学习的归一化",{"2":{"350":1,"361":1}}],["在xavier初始化过程中",{"2":{"347":1}}],["在进行调整时",{"2":{"1157":2}}],["在进行调整时要在两个方面进行平衡",{"2":{"1157":1}}],["在进行一项研究后",{"2":{"1145":1}}],["在进行训练之前",{"2":{"807":1}}],["在进行",{"2":{"536":1}}],["在进行类比时",{"2":{"505":1}}],["在进行ln之前",{"2":{"347":1}}],["在进行softmax计算之前",{"2":{"70":1}}],["在具体的计算过程中会进行相乘",{"2":{"346":1}}],["在向量经过position",{"2":{"344":1}}],["在规范化公式的分母中出现的微小值",{"2":{"343":1}}],["在sql语句中拼接通配符",{"2":{"1489":1}}],["在sql的配置文件中",{"2":{"1488":1}}],["在smt中具体怎么解码",{"2":{"908":1}}],["在self上生成state",{"2":{"1227":1}}],["在search结束之后",{"2":{"904":1}}],["在seq2seq的训练过程中",{"2":{"897":1}}],["在seq2seq结构中",{"2":{"886":1}}],["在sequence",{"2":{"665":1}}],["在sublayer之后新增了一个层归一化",{"2":{"329":1}}],["在softmax的情况下",{"2":{"327":1}}],["在典型硬件上计算这些非线性运算更具挑战性",{"2":{"327":1}}],["在使用linux系统的虚拟机过程中",{"2":{"2088":1}}],["在使用指针之前",{"2":{"1611":1}}],["在使用方法的时候",{"2":{"1485":1}}],["在使用批次进行训练后",{"2":{"1241":1,"1242":1}}],["在使用quasi",{"2":{"1175":1}}],["在使用余弦相似度之前",{"2":{"692":1}}],["在使用",{"2":{"542":1}}],["在使用bn的transformer训练过程中",{"2":{"316":1}}],["在使用时",{"2":{"154":1}}],["在序列的位置n上",{"2":{"633":1}}],["在序列的适当位置添加特殊token",{"2":{"555":1}}],["在序列模型的背景下",{"2":{"316":1}}],["在序列中随机引入额外的注意力连接",{"2":{"204":1}}],["在normalization层的时候不是有两个学习的参数吗",{"2":{"665":1}}],["在norm",{"2":{"394":1}}],["在nlp任务中是最常见的基础单元",{"2":{"565":1}}],["在nlp里面",{"2":{"341":1}}],["在nlp中就会出现问题",{"2":{"316":1}}],["在nlp中常用的注意力操作在每个token的embedding上也进行大量的叠加操作",{"2":{"314":1}}],["在nlp领域中这是极不合理的",{"2":{"316":1}}],["在nlp领域中",{"2":{"316":1}}],["在num",{"2":{"315":1}}],["在上图中用红色",{"2":{"1147":1}}],["在上面的代码中",{"2":{"1763":1}}],["在上面的",{"2":{"1729":1}}],["在上面的例子中",{"2":{"1729":1}}],["在上面的内容中",{"2":{"1183":1}}],["在上面的示例中",{"2":{"1143":1}}],["在上面例子中",{"2":{"314":1}}],["在上述代码中",{"2":{"1114":1}}],["在上述算法执行后",{"2":{"584":1}}],["在上述的演进过程中",{"2":{"291":1}}],["在完成标准化之后",{"2":{"313":1}}],["在该比赛中",{"2":{"840":1}}],["在该结构中",{"2":{"446":1}}],["在该示例中",{"2":{"313":1,"316":1}}],["在该状态下不会发生秩崩溃",{"2":{"94":1}}],["在输入的第",{"2":{"1335":1}}],["在输入层上应用深度卷积",{"2":{"776":1}}],["在输入张量中",{"2":{"773":1}}],["在输入阶段就将位置信息融入到token的输入表征中",{"2":{"745":1}}],["在输入状态下",{"2":{"519":1}}],["在输出端生成新的概念序列",{"2":{"628":1}}],["在输出第i个元素的时候",{"2":{"409":1}}],["在输出爱的时候",{"2":{"58":1}}],["在输送给神经元之前先对其进行平移和伸缩变换",{"2":{"310":1}}],["在深度学习领域",{"2":{"1127":1}}],["在深度学习时代之前",{"2":{"908":1}}],["在深度学习时代",{"2":{"907":1}}],["在深度学习发展初期",{"2":{"839":1}}],["在深度学习中",{"2":{"505":1,"841":1,"1021":1,"1229":1}}],["在深度神经网络中",{"2":{"309":1}}],["在深层注意力模块应用短路",{"2":{"437":1}}],["在深层神经网络中",{"2":{"296":1}}],["在神经网络中",{"2":{"992":1}}],["在神经网络中唯一能够处理的数据格式就只有一种",{"2":{"680":1}}],["在神经网络系统中",{"2":{"298":1}}],["在神经网络的训练过程中",{"2":{"53":1,"180":1}}],["在增加模型深度的同时还能保持甚至提高准确度",{"2":{"296":1}}],["在学习过程中",{"2":{"1017":1}}],["在学习过程中或之前应用归一化或减少流行度偏差",{"2":{"692":1}}],["在学习了lm和tm这两个模型之后",{"2":{"908":1}}],["在学习完本篇之后",{"2":{"294":1}}],["在学习新知识时的内在机制",{"2":{"148":1,"484":1}}],["在马尔科夫条件下",{"2":{"287":1}}],["在时刻t",{"2":{"287":1}}],["在时间序列的",{"2":{"245":1}}],["在递归过程中记忆存在压缩的问题",{"2":{"287":1}}],["在传统计算理论中",{"2":{"504":1}}],["在传统的自注意力机制中",{"2":{"5":1}}],["在传递上下文向量进行预测时也有区别",{"2":{"285":1}}],["在本阶段",{"2":{"270":1}}],["在查找中",{"2":{"265":1}}],["在rnn方案中",{"2":{"251":1}}],["在例句中的距离太长了",{"2":{"246":1}}],["在此代码块结束时",{"2":{"1676":1}}],["在此阶段",{"2":{"1143":1}}],["在此论文之前",{"2":{"736":1}}],["在此步骤中",{"2":{"734":1}}],["在此概念空概念内",{"2":{"689":1}}],["在此架构中",{"2":{"540":1}}],["在此之后",{"2":{"484":1}}],["在此基础上提出了scaled",{"2":{"434":1}}],["在此基础上",{"2":{"351":1}}],["在此基础之上",{"2":{"7":1}}],["在此处发挥了作用",{"2":{"344":1}}],["在此视角下",{"2":{"238":1}}],["在此表示感谢",{"2":{"235":1}}],["在其他应用中",{"2":{"1152":1}}],["在其他情况下",{"2":{"1139":1}}],["在其他两种模式中",{"2":{"1119":1}}],["在其论文和著作",{"2":{"754":1}}],["在其推理过程中",{"2":{"510":1}}],["在其归一化形状内的每一个元素上分别进行缩放以及平移",{"2":{"341":1}}],["在其中一个分支中",{"2":{"231":1}}],["在其权重矩阵中存储知识",{"2":{"221":1}}],["在形式上",{"2":{"230":1}}],["在给定的维度上拼接给定的序列张量",{"2":{"825":1}}],["在给定算力的情况下",{"2":{"561":1}}],["在给定",{"2":{"230":1}}],["在给定一组数组成的向量",{"2":{"180":1}}],["在第2",{"2":{"964":1}}],["在第3",{"2":{"941":1,"960":1}}],["在第三项和第四项中引入引入了两个新的可学习的参数",{"2":{"760":1}}],["在第二行的首地址上偏移",{"2":{"1705":1}}],["在第二",{"2":{"763":1}}],["在第二个分支中",{"2":{"231":1}}],["在第二阶段",{"2":{"225":1}}],["在第一次实验中",{"2":{"1175":1}}],["在第一次阶段",{"2":{"225":1}}],["在第一轮调参中",{"2":{"1158":1}}],["在第一个时间步使用仅包含句子开头标记的空序列",{"2":{"427":1}}],["在第一层",{"2":{"58":1}}],["在推理阶段",{"2":{"225":1,"413":1,"426":1}}],["在推理时使用相同权重进行推理",{"2":{"276":1}}],["在推理时",{"2":{"224":2,"316":1,"427":1,"472":1}}],["在推理过程中识别任务",{"2":{"225":1}}],["在推理过程中",{"2":{"150":1,"162":1,"957":1}}],["在研究中发现",{"2":{"221":1}}],["在预算内购买到更多的商品",{"2":{"2112":1}}],["在预热的时候",{"2":{"1184":1}}],["在预训练过程中",{"2":{"718":1}}],["在预训练数据上使用数据打包",{"2":{"217":1}}],["在预测时",{"2":{"532":1,"533":1}}],["在预测一个特定的词时",{"2":{"59":1}}],["在预测某个位置的词时",{"2":{"58":1}}],["在业内首次实现了新的线性注意力机制",{"2":{"214":1}}],["在处理输入时",{"2":{"1814":1,"1832":1}}],["在处理动态分配的内存时",{"2":{"1670":1}}],["在处理一词多义的情况时更具优势",{"2":{"696":1}}],["在处理大规模数据集时",{"2":{"568":1}}],["在处理任何一个单词时",{"2":{"274":1}}],["在处理完整个输入序列后",{"2":{"245":1}}],["在处理非常大的序列",{"2":{"230":1}}],["在处理负成分时",{"2":{"213":1}}],["在处理变长序列时",{"2":{"50":1}}],["在应用余弦相似度之前",{"2":{"692":1}}],["在应用",{"2":{"193":1}}],["在应用softmax函数之前",{"2":{"62":1}}],["在关注当前词a的基础上",{"2":{"176":1}}],["在注意力计算过程中",{"2":{"431":1,"981":1}}],["在注意力中引入记忆信息",{"2":{"512":1}}],["在注意力中",{"2":{"394":1}}],["在注意力机制中加入softmax是因为其有如下优点或者功能",{"2":{"180":1}}],["在注意力机制中",{"2":{"172":1,"194":1,"709":1}}],["在注意力之后会通过线性投影进行处理",{"2":{"41":1}}],["在训练期间记录下未截断梯度范数",{"2":{"1184":1}}],["在训练结束时",{"2":{"1166":1}}],["在训练后期",{"2":{"1149":1}}],["在训练长度内的表现比大多数基线差",{"2":{"765":1}}],["在训练中",{"2":{"714":1}}],["在训练中基本是如下流程",{"2":{"709":1}}],["在训练中逐步用重新参数化的batchnorm替换layernorm",{"2":{"348":1}}],["在训练或推理过程中",{"2":{"700":1}}],["在训练和推理时有何不同",{"2":{"640":1,"807":1}}],["在训练和预测的时候",{"2":{"72":1}}],["在训练数据中常见的数字",{"2":{"595":1}}],["在训练任务中",{"2":{"595":1}}],["在训练过程中记录全损失梯度的",{"2":{"1179":1}}],["在训练过程中常使用预训练的语言模型来初始化decoder的参数",{"2":{"898":1}}],["在训练过程中",{"2":{"676":1,"835":1,"898":1,"990":1,"1438":1}}],["在训练过程中以一定概率随机地将这些组合",{"2":{"553":1}}],["在训练过程的每一步会以一定的概率随机选择是用模型输出还是用真值",{"2":{"411":1}}],["在训练大语言模型时",{"2":{"505":1}}],["在训练大型网络和将特征转移到计算机视觉任务中",{"2":{"338":1}}],["在训练的时候",{"2":{"895":1}}],["在训练的每一步都为其提供正确的输作为指导",{"2":{"404":1}}],["在训练的过程中",{"2":{"309":1}}],["在训练模型之前是需要构建好词表的",{"2":{"363":1,"550":1}}],["在训练刚开始的阶段",{"2":{"335":1}}],["在训练完成后通常表现更优",{"2":{"335":1}}],["在训练初期",{"2":{"239":1}}],["在训练时有一半的神经元被丢弃",{"2":{"396":1}}],["在训练时",{"2":{"224":2,"405":1,"407":1,"409":1,"426":1}}],["在训练时把词语分成高频词和低频次两类",{"2":{"185":1}}],["在训练时指导模型学习",{"2":{"180":1}}],["在训练阶段可以并行地输出所有的词",{"2":{"406":1}}],["在训练阶段",{"2":{"172":1,"224":1}}],["在掌握阶段",{"2":{"142":1}}],["在必要时进行自我转换",{"2":{"140":1}}],["在回路中",{"2":{"130":1}}],["在2010年提出的",{"2":{"999":1}}],["在2017年",{"2":{"235":1}}],["在2021年之后",{"2":{"540":1}}],["在2024年六月份发表了一篇文章想要通过范畴论来统一描述和研究深度学习架构",{"2":{"505":1}}],["在2022年",{"2":{"103":1}}],["在20世纪早期就给出了一个非定位结论",{"2":{"129":1}}],["在下游任务中的特殊作用",{"2":{"764":1}}],["在下图中",{"2":{"685":1}}],["在下图左侧",{"2":{"507":1}}],["在下图上可以看到由模式到value的流程",{"2":{"126":1}}],["在下一时间步",{"2":{"406":1}}],["在下一层attention",{"2":{"58":1}}],["在与残差流相加的时候",{"2":{"122":1}}],["在浅层形成的任务语义会激活一些中至深层的特定任务的注意力头",{"2":{"122":1}}],["在残差流中解码出来的答案会首先形成被查询的国家",{"2":{"122":1}}],["在linux虚拟机里定制对应网络号",{"2":{"2095":1}}],["在laion2b",{"2":{"1363":1}}],["在layernorm的情况下",{"2":{"91":1}}],["在lm的情况下为token",{"2":{"478":1}}],["在llama",{"2":{"981":1}}],["在llama中采用常数ββ",{"2":{"110":1}}],["在llm时代",{"2":{"563":1}}],["在llm开发的早期阶段",{"2":{"540":1}}],["在llm",{"2":{"138":1}}],["在前两节课中",{"2":{"1610":1}}],["在前面讲解过",{"2":{"1373":1}}],["在前向传播期间保存张量",{"2":{"1114":1}}],["在前向传播过程中",{"2":{"1114":1,"1117":1}}],["在前向传播中存储了softmax归一化因子",{"2":{"940":1,"959":1}}],["在前向传播中",{"2":{"1":1}}],["在前馈神经网络最后",{"2":{"1443":1}}],["在前馈神经网络",{"2":{"103":1}}],["在论文的基础上",{"2":{"692":1}}],["在论文on",{"2":{"334":1}}],["在论文",{"2":{"90":1,"210":1,"542":1}}],["在交叉注意力中所使用的",{"2":{"82":1}}],["在交叉注意力中会用到padding",{"2":{"78":1}}],["在多核环境或集群中",{"2":{"1589":1}}],["在多道程序环境下",{"2":{"1412":1}}],["在多个进程上运行同一个",{"2":{"1589":1}}],["在多个基准上取得了最优性能",{"2":{"1317":1}}],["在多个多语言",{"2":{"1315":1}}],["在多个自注意力头捕获输入token之间的不同关系后",{"2":{"466":1}}],["在多级别抽象上显式的推理和规划",{"2":{"627":1}}],["在多种条件下",{"2":{"507":1}}],["在多分类问题中",{"2":{"180":1}}],["在多层",{"2":{"82":1}}],["在多头注意力中",{"2":{"9":1}}],["在多头注意力机制中",{"2":{"5":1}}],["在这一点上",{"2":{"1153":1}}],["在这一阶段",{"2":{"148":1}}],["在这项工作中论文提出transformer",{"2":{"911":1}}],["在这本书中我们遵循把两种运算都叫做卷积的这个传统",{"2":{"770":1}}],["在这三者中",{"2":{"721":1}}],["在这点上",{"2":{"715":1}}],["在这些领域扮演着重要的角色",{"2":{"1602":1}}],["在这些子空间上再构建多个注意力来替代单个注意力",{"2":{"462":1}}],["在这些表面性能的背后",{"2":{"121":1}}],["在这句话中的本质通过",{"2":{"170":1}}],["在这个模型中",{"2":{"1459":1}}],["在这个时候",{"2":{"1153":1}}],["在这个点上的调整会变得越来越小",{"2":{"709":1}}],["在这个过程中",{"2":{"510":1}}],["在这个例子中",{"2":{"169":1,"770":1}}],["在这个领域",{"2":{"150":1}}],["在这个表达式中",{"2":{"148":1,"485":1}}],["在这个基础上人们使用知识编辑",{"2":{"123":1}}],["在这种背景下",{"2":{"816":1}}],["在这种方案中",{"2":{"247":1}}],["在这种设置下",{"2":{"230":1}}],["在这种情况下",{"2":{"122":1,"136":1,"147":1,"507":1,"983":1,"1012":1,"1041":1,"1144":1,"1146":1,"1154":2,"1155":1,"1164":1,"1168":1,"1181":1,"1214":1,"1242":1}}],["在这种基础上",{"2":{"4":1}}],["在这里定义友元函数",{"2":{"1791":1}}],["在这里添加友元函数声明",{"2":{"1791":1}}],["在这里补充三个函数的实现",{"2":{"1710":1}}],["在这里除了输入token外",{"2":{"620":1}}],["在这里",{"2":{"541":1,"938":1,"954":1,"1114":1}}],["在这里解码器的输入序列会进行内部信息交换",{"2":{"445":1}}],["在这里每个token会依据一定权重把自己的信息和其它token的信息进行交换融合",{"2":{"445":1}}],["在这里和各位朋友说下万分抱歉",{"2":{"235":1}}],["在这里进行配置",{"2":{"83":1}}],["在这里可以直接使用",{"2":{"82":1}}],["在这两步设定之后tgt",{"2":{"79":1}}],["在代码中",{"2":{"79":1}}],["在代码之中",{"2":{"38":1}}],["在module属性设置过程中插入自己的逻辑",{"2":{"1214":1}}],["在make",{"2":{"449":1}}],["在masked",{"2":{"71":1}}],["在mlp17层以后",{"2":{"130":1}}],["在mha中",{"2":{"45":1}}],["在作softmax操作时",{"2":{"70":1}}],["在query和key的转置相乘得出",{"2":{"67":1}}],["在加了掩码的注意力分数上应用softmax函数",{"2":{"63":1}}],["在掩码矩阵中",{"2":{"62":1}}],["在自定义异常类中",{"2":{"1763":1,"1764":1}}],["在自己任务上获得优秀性能所需的时间和计算成本都可以很小",{"2":{"1313":1}}],["在自回归解码过程中",{"2":{"981":1}}],["在自回归情况下",{"2":{"507":1}}],["在自回归模式下",{"2":{"416":1}}],["在自然界",{"2":{"220":1}}],["在自然语言处理模型中",{"2":{"559":1}}],["在自然语言处理",{"2":{"1":1}}],["在自注意力self",{"2":{"419":1}}],["在自注意力层中",{"2":{"417":1}}],["在自注意力机制中",{"2":{"172":1}}],["在自注意力中会用到padding",{"2":{"78":1}}],["在自注意力中会用到",{"2":{"78":1}}],["在自注意力模型中",{"2":{"50":1}}],["在共享头中巩固共同知识",{"2":{"42":1}}],["在共享映射矩阵的基础上",{"2":{"19":1}}],["在计算机科学中",{"2":{"1411":1}}],["在计算机视觉和语音识别等多个领域也能发挥出色的适应性",{"2":{"512":1}}],["在计算前向传播时",{"2":{"1113":1}}],["在计算其query",{"2":{"975":1}}],["在计算块并行注意力和前馈时",{"2":{"975":1}}],["在计算损失的时候",{"2":{"899":1}}],["在计算完multi",{"2":{"419":1}}],["在计算最后会将其恢复为hidden",{"2":{"346":1}}],["在计算过程的每一步t",{"2":{"249":1}}],["在计算注意力时",{"2":{"975":1}}],["在计算注意力分数eijeije",{"2":{"759":1}}],["在计算注意力分数时",{"2":{"54":1}}],["在计算注意力的时候",{"2":{"70":1}}],["在计算当前head的注意力时",{"2":{"41":1}}],["在计算qktqktqk^t时",{"2":{"3":1}}],["在投影的向量上批量应用注意力机制",{"2":{"36":1}}],["在若干低维空间内分别求解各自的scaled",{"2":{"16":1}}],["在",{"2":{"12":1,"37":1,"78":1,"153":1,"323":1,"333":1,"334":1,"344":1,"352":1,"394":3,"460":1,"473":1,"627":1,"714":1,"759":1,"844":1,"934":1,"938":1,"954":1,"995":1,"1115":1,"1161":1,"1176":1,"1177":1,"1178":1,"1312":2,"1316":1,"1320":1,"1329":1,"1330":1,"1344":1,"1566":1,"1594":3,"1605":2,"1624":1,"1630":1,"1647":1,"1664":1,"1667":3,"1668":1,"1678":1,"1690":1,"1698":1,"1700":1,"1701":1,"1704":2,"1705":1,"1719":1,"1720":3,"1721":2,"1722":3,"1724":1,"1728":1,"1761":1,"1766":3,"1810":1,"1825":1,"1828":1,"1843":1,"1851":1,"1855":1,"1859":1,"1874":1,"1922":2,"2054":1}}],["在橄榄球领域内有一种说法",{"2":{"5":1}}],["能先满足他们可以为后面胃口更大的小孩留出更大的饼干",{"2":{"2151":1}}],["能静下心来真的很不容易",{"2":{"2056":1}}],["能不带有一点情绪去面对吗",{"2":{"2054":1}}],["能不生气吗",{"2":{"2054":1}}],["能被",{"2":{"1619":1,"1729":3}}],["能帮助你更好地理解其他语言的内存管理机制",{"2":{"1611":1}}],["能有效利用分布式集群的计算能力",{"2":{"1579":1}}],["能提升程序并发性",{"2":{"1574":1}}],["能更多地测试新想法",{"2":{"1131":1}}],["能使固定时间间隔内超参数调整更彻底",{"2":{"1131":1}}],["能随着我们理解的改变从而成长和演变",{"2":{"1127":1}}],["能随卡数线性扩展",{"2":{"974":1}}],["能一定程度上抑制过拟合",{"2":{"1017":1}}],["能直接存在缓存中",{"2":{"948":1,"978":1}}],["能的话规则是什么样的",{"2":{"829":1}}],["能很好区分特征维度",{"2":{"766":1}}],["能从12",{"2":{"713":1}}],["能表达的信息太少",{"2":{"681":1}}],["能把动作像语言一样",{"2":{"637":1}}],["能最大程度地增加训练数据概率的token对作为新的子词",{"2":{"599":1}}],["能高效利用海量的非结构化文本数据",{"2":{"542":1}}],["能将transformer的表达能力扩展到tc0tc0tc^0之外",{"2":{"480":1}}],["能在哪些维度做归一化",{"2":{"321":1}}],["能在常数时间内有效地捕捉到它们之间的依赖关系",{"2":{"274":1}}],["能力",{"2":{"217":1,"679":1}}],["能同时捕捉输入序列在不同子空间的信息",{"2":{"1":1}}],["能够使用位运算解决常见问题",{"2":{"2057":1}}],["能够极大提升编码水平和思维能力",{"2":{"2010":1}}],["能够更真实地模拟光线的多次反射和漫射",{"2":{"2009":1}}],["能够更好地处理序列的局部结构",{"2":{"746":1}}],["能够熟练运用",{"2":{"1826":1,"1844":1}}],["能够删除",{"2":{"1808":1}}],["能够修改",{"2":{"1808":1}}],["能够新增加",{"2":{"1808":1}}],["能够访问被友元声明的类中的所有私有",{"2":{"1769":1}}],["能够清晰地表达函数的功能",{"2":{"1729":1}}],["能够将自己心中的虚拟世界创造出来",{"2":{"2010":1}}],["能够将数据和行为封装在一起",{"2":{"1728":1}}],["能够将大数据量的图片有效的降维成小数据量",{"2":{"1472":1}}],["能够结合自定义数据类型和函数",{"2":{"1727":1}}],["能够灵活运用其进行字符串操作",{"2":{"1711":1}}],["能够实现直接的内存操作",{"2":{"1611":1}}],["能够保留图片的特征",{"2":{"1472":1}}],["能够较好地进行非线性分类",{"2":{"1465":1}}],["能够重构出原始的文本",{"2":{"1317":1}}],["能够处理动态控制流和变长输入数据等情况",{"2":{"1287":1}}],["能够满足上述信息的记录要求并且对使用者友好易用是非常重要的",{"2":{"1167":1}}],["能够约束梯度",{"2":{"1044":1}}],["能够放大梯度",{"2":{"1044":1}}],["能够减轻这种限制",{"2":{"781":1}}],["能够捕捉每个词与句子中其他所有词之间的关系",{"2":{"719":1}}],["能够表示文本",{"2":{"680":1}}],["能够与基于token的模型",{"2":{"611":1}}],["能够自动适应不同语言的特性",{"2":{"603":1}}],["能够容忍更多的噪声",{"2":{"595":1}}],["能够让算法知道每个单词的结束位置",{"2":{"579":1}}],["能够让第i个时间步的上下文向量只是由前i个时间步的向量计算而来",{"2":{"412":1}}],["能够把上面的3个单词拆分成",{"2":{"576":1}}],["能够完成任何可以编程的任务",{"2":{"504":1}}],["能够进行分布式训练",{"2":{"434":1}}],["能够进行持续学习而需要对其频繁更新参数外",{"2":{"138":1}}],["能够并行",{"2":{"418":1}}],["能够并行计算序列中的每个元素",{"2":{"1":1}}],["能够帮助",{"2":{"320":1}}],["能够涵盖众多现实世界的算法",{"2":{"294":1}}],["能够涵盖许多现实世界的算法",{"2":{"1":1}}],["能够根据上下文为词语赋予不同的向量表示",{"2":{"696":1}}],["能够根据当前数据决定是否需要长期记忆信息",{"2":{"231":1}}],["能够根据任务需求灵活调整存储容量和检索精度",{"2":{"153":1}}],["能够在保持高效计算的同时",{"2":{"155":1}}],["能够对齐人类的期望并且维持以前的知识和能力",{"2":{"143":1}}],["能够调节函数的插值程度",{"2":{"107":1}}],["因变量是q与真实后验密度函数的",{"2":{"1377":1}}],["因果注意力机制的特点是每个单词只能关注之前的单词",{"2":{"734":1}}],["因果解码器架构结合了单向注意力掩码",{"2":{"541":1}}],["因果解码器",{"2":{"541":1,"542":1}}],["因果解码器和前缀解码器",{"2":{"541":1}}],["因简洁和计算高效",{"2":{"637":1}}],["因而对hw做归一化",{"2":{"809":1}}],["因而模型需要更多的参数才能描述同等的信息量",{"2":{"679":1}}],["因而能够输出带概率的多个分词结果",{"2":{"602":1}}],["因而能够输出带概率的多个子词分段",{"2":{"601":1}}],["因而按子词粒度分词方法能够降低词典的大小",{"2":{"567":1}}],["因而可以期待正确的结果",{"2":{"406":1}}],["因而可以发挥长期记忆的作用",{"2":{"231":1}}],["因而需要通过升维来完整表示这一空间",{"2":{"116":1}}],["因此要确保它所指向的字符串在",{"2":{"1929":1}}],["因此要优化器要最小化交叉熵",{"2":{"391":1}}],["因此无法访问非静态成员变量或调用非静态成员函数",{"2":{"1639":1}}],["因此无法保证其注意力矩阵的全秩状态",{"2":{"542":1}}],["因此广泛用于大规模并行计算场景",{"2":{"1572":1}}],["因此避免了共享内存架构中的竞争条件和数据一致性问题",{"2":{"1572":1}}],["因此干脆就直接固定",{"2":{"1344":1}}],["因此通常来说它不会改变原模型的稳定性",{"2":{"1343":1}}],["因此通过学习",{"2":{"320":1}}],["因此任意偶数维的",{"2":{"1343":1}}],["因此一些旨在重新创建和发布",{"2":{"1316":1}}],["因此一般我们会设定一些规则",{"2":{"904":1}}],["因此一般情况下无法训练很深层数的网络",{"2":{"495":1}}],["因此移除了",{"2":{"1315":1}}],["因此迭代尝试不同的微调方案也更快",{"2":{"1313":1}}],["因此被称为迁移学习",{"2":{"1313":1}}],["因此如果用户没有指定特定的实现方式",{"2":{"1228":1}}],["因此如何在这三个需求之间分配资源需要一定程度的领域专业知识",{"2":{"1145":1}}],["因此您需要传递一个闭包",{"2":{"1223":1}}],["因此当启动新试验时",{"2":{"1175":1}}],["因此你可以在不重新运行实验的情况下",{"2":{"1175":1}}],["因此请确保合适地标记它们",{"2":{"1169":1}}],["因此第一轮非常有用",{"2":{"1158":1}}],["因此第一次推理输出",{"2":{"529":1}}],["因此固定权重衰减的强度来比较不同的模型大小",{"2":{"1143":1}}],["因此xavier方法主要是围绕tanh激活函数可能存在的梯度爆炸或梯度消失进行的优化",{"2":{"1000":1}}],["因此x的维度变成",{"2":{"36":1}}],["因此优化ttft至关重要",{"2":{"977":1}}],["因此优化目标是最小化交叉熵",{"2":{"397":1}}],["因此其键值缓存仅有",{"2":{"957":1}}],["因此其效率是相对可观的",{"2":{"349":1}}],["因此内存带宽通常不是主要瓶颈",{"2":{"937":1,"953":1}}],["因此有时也称为",{"2":{"1460":1}}],["因此有必要对transformer模型有一个全面认识",{"2":{"911":1}}],["因此有了多头注意力",{"2":{"14":1}}],["因此大家把这种方法称为teacher",{"2":{"896":1}}],["因此出现了rnn最重要的一种任务",{"2":{"883":1}}],["因此相对于lstm来说",{"2":{"874":1}}],["因此相对位置编码不适合推理",{"2":{"746":1}}],["因此单调性并不是硬性条件",{"2":{"838":1}}],["因此本身也没有被截断",{"2":{"760":1}}],["因此将偏差项直接添加到注意力公式中成为将位置信息集成到",{"2":{"756":1}}],["因此该任务被称为遮盖语言建模",{"2":{"1312":1}}],["因此该任务被称为因果语言建模",{"2":{"1312":1}}],["因此该方法不光建模了绝对位置",{"2":{"751":1}}],["因此该论文提出优化词汇表结构和tokenizer算法是解决token训练不足问题的关键",{"2":{"562":1}}],["因此该论文对每个时间步产生的输出向量进行分割成三个向量key",{"2":{"288":1}}],["因此也被称为自回归",{"2":{"1316":1}}],["因此也有人将其归为混合位置编码",{"2":{"750":1}}],["因此也不会遇到因为特征的量纲不同而导致的缩放问题",{"2":{"326":1}}],["因此使用位置编码将序列中元素顺序信息融入transformer成为一种常见做法",{"2":{"742":1}}],["因此使用了注意力机制来完成这个分析工作",{"2":{"287":1}}],["因此路由权重包含了隐藏状态嵌入可能丢失的输入的语义信息",{"2":{"739":1}}],["因此三种embedding先用one",{"2":{"722":1}}],["因此才需要将embedding模型和大模型区分开来",{"2":{"711":1}}],["因此符号通常被初始化为随机的高维向量",{"2":{"709":1}}],["因此理论上",{"2":{"698":1}}],["因此很容易想到的做法是让文字转换为数字",{"2":{"679":1}}],["因此patch的数量直接决定了计算开销",{"2":{"613":1}}],["因此post",{"2":{"335":1}}],["因此易于控制flop成本",{"2":{"613":1}}],["因此模型不会过度依赖某些神经元",{"2":{"1017":1}}],["因此模型步骤更少",{"2":{"612":1}}],["因此模型只能从多个地方提取信息而没有明确的重点",{"2":{"194":1}}],["因此绘制下图进行辅助分析",{"2":{"556":1}}],["因此tokenizer",{"2":{"549":1}}],["因此transformer作者将乘法函数按照因子1",{"2":{"187":1}}],["因此transformer具备非常强大的表现力",{"2":{"1":1}}],["因此这种类型的语言模型",{"2":{"894":1}}],["因此这些",{"2":{"1168":1}}],["因此这些向量不仅仅编码一个单独的词",{"2":{"709":1}}],["因此这些标记token的",{"2":{"542":1}}],["因此这个系列也是自己一个整理",{"2":{"235":1}}],["因此目前只剩encoder",{"2":{"541":1}}],["因此目前也有相关优化工作",{"2":{"377":1}}],["因此交叉注意力可以理解为是自注意力的双塔实践",{"2":{"536":1}}],["因此源文本每个位置的token对于当下要预测的token应该有不一样的影响",{"2":{"536":1}}],["因此哈佛代码把他们封装在sublayerconnection类中",{"2":{"523":1}}],["因此哈佛代码把这些可以重复的代码封装成",{"2":{"344":1}}],["因此编码器栈和解码器栈之间通过交叉注意力在编码器和解码器之间传递信息",{"2":{"515":1}}],["因此只会进行一次前向传播",{"2":{"515":1}}],["因此头之间的统计信息更加多样化",{"2":{"503":1}}],["因此显著减少了原本反向传播时的大量内存使用空间",{"2":{"495":1}}],["因此引入了一个伴随状态方法",{"2":{"495":1}}],["因此具有单个键头和值头",{"2":{"937":1,"953":1}}],["因此具有更好的并行性",{"2":{"434":1}}],["因此具备了并行推理的可能",{"2":{"408":1}}],["因此预测场景的解码器必定是串行的循环输出",{"2":{"427":1}}],["因此从计算量角度来看一共有五个维度",{"2":{"420":1}}],["因此从理论上来说",{"2":{"249":1}}],["因此研究人员使用以下几种方法来修改模型架构",{"2":{"732":1}}],["因此研究人员也针对exposure",{"2":{"411":1}}],["因此研究者将每个梯度矩阵的外积形式转换为一组较小的向量",{"2":{"485":1}}],["因此研发人员提出来把融合直接做成可学习",{"2":{"10":1}}],["因此人们引入了掩码机制来隐藏未来信息",{"2":{"409":1}}],["因此下图进行了简化",{"2":{"399":1}}],["因此总体效率上肯定是更低的",{"2":{"396":1}}],["因此3",{"2":{"380":1}}],["因此难免会将长度不一的句子放到一个batch里",{"2":{"376":1}}],["因此语料的全面数据治理十分必要",{"2":{"369":1}}],["因此就不需要前向传播去一一计算",{"2":{"495":1}}],["因此就不如把q",{"2":{"14":1}}],["因此就是",{"2":{"344":1}}],["因此整体来说更不容易发生梯度爆炸或梯度消失的问题",{"2":{"334":1}}],["因此整体复杂度其实和乘法方案接近",{"2":{"175":1}}],["因此原本一个",{"2":{"334":1}}],["因此对于参数矩阵",{"2":{"765":1}}],["因此对于其内部的多头自注意力",{"2":{"38":1}}],["因此对参数归一化的效果更好",{"2":{"331":1}}],["因此它在模型的调试和开发过程中更具交互性和可读性",{"2":{"1287":1}}],["因此它只代表作者在撰写本文时的观点",{"2":{"1127":1}}],["因此它只需要对输入进行一次传递",{"2":{"327":1}}],["因此它会继续学习越来越大的权重",{"2":{"1016":1}}],["因此它们的更新几乎都是相同的",{"2":{"714":1}}],["因此它构成了语言理解的核心",{"2":{"462":1}}],["因此它需要将目标序列作为q",{"2":{"444":1}}],["因此它不改变注意力的相对大小",{"2":{"176":1}}],["因此实际常用sigmoid函数作为激活函数",{"2":{"1460":1}}],["因此实际可以完全忽略批次的存在",{"2":{"326":1}}],["因此实际中有使用滑动平均来计算均值和方差",{"2":{"316":1}}],["因此layernorm的归一化操作只在样本内部",{"2":{"326":1}}],["因此两个参数也是",{"2":{"325":1}}],["因此不能直接使用",{"2":{"1611":1}}],["因此不再有oom问题",{"2":{"976":1}}],["因此不满足上述属性",{"2":{"612":1}}],["因此不同解码器层的输入输出不尽相同",{"2":{"526":1}}],["因此不同encoderlayer的输入输出不尽相同",{"2":{"518":1}}],["因此不会进行元素间的信息交换",{"2":{"466":1}}],["因此不会破坏不同样本同一特征之间的关系",{"2":{"322":1}}],["因此不用去关注",{"2":{"409":1}}],["因此不做掩码处理",{"2":{"66":1,"382":1}}],["因此bn无法成立",{"2":{"316":1}}],["因此针对不同样本的同一特征以跨样本的方式开展归一化",{"2":{"314":1}}],["因此加入还原线性变换进行适度还原",{"2":{"313":1}}],["因此输出权重是可识别的",{"2":{"305":1}}],["因此降低了模型的有效维数",{"2":{"305":1}}],["因此哪怕后面的层更新好了",{"2":{"296":1}}],["因此训练速度快",{"2":{"290":1}}],["因此作者做了进一步改进",{"2":{"288":1}}],["因此作者使用了一个网络来计算两种语言间不同单词的关联分数",{"2":{"284":1}}],["因此随着输入序列长度的增加",{"2":{"279":1}}],["因此会在解码器输出每一个预测值之前",{"2":{"267":1}}],["因此会初始化一个线性层",{"2":{"36":1}}],["因此每个单词都要去询问其它单词",{"2":{"265":1}}],["因此rnn在处理长距离关联的复杂语法结构时力不从心",{"2":{"252":1}}],["因此rnn可以捕捉序列中的长期依赖关系",{"2":{"250":1}}],["因此所有token推理的消耗基本相同",{"2":{"250":1}}],["因此hthth",{"2":{"240":1}}],["因此后续我们主要使用机器翻译来进行讲解",{"2":{"237":1}}],["因此导致测试时性能不佳",{"2":{"230":1}}],["因此仅需要一个一维向量z",{"2":{"224":1}}],["因此奇异值微调的数学意义是具备了充分的可解释性",{"2":{"222":1}}],["因此可以高效地进行随机访问",{"2":{"1797":1}}],["因此可以解决任何复杂的分类问题",{"2":{"1465":1}}],["因此可以在未来的实验中固定住",{"2":{"1140":1}}],["因此可以用于batchsize为1和rnn中对边长的输入sequence的normalize操作",{"2":{"808":1}}],["因此可以使用一个字母x去代替",{"2":{"575":1}}],["因此可以使用少量数据训练而不会导致大规模遗忘或崩溃",{"2":{"221":1}}],["因此可以对顶层应用较高学习率而对底层应用较低学习率",{"2":{"402":1}}],["因此可以把",{"2":{"340":1}}],["因此可以先把图像在",{"2":{"337":1}}],["因此可以避免",{"2":{"322":1}}],["因此可以忽视cot带来的影响",{"2":{"224":1}}],["因此可以组合多个tuner共同使用",{"2":{"222":1}}],["因此可以通过构建子矩阵并仅调整奇异值矩阵部分主成分的方法来达到轻量微调的效果",{"2":{"221":1}}],["因此可以发现",{"2":{"125":1}}],["因此还不是最终的解决方案",{"2":{"204":1}}],["因此除了δmax=0δmax=0",{"2":{"191":1}}],["因此得到的向量接近一个one",{"2":{"180":1}}],["因此计算attention权重需要加入scaled",{"2":{"175":1}}],["因此提出了一种新的神经网络架构kan",{"2":{"155":1}}],["因此稀疏查询和更新是必需的",{"2":{"154":1}}],["因此叫做偏移",{"2":{"148":1}}],["因此必须通过降维来控制网络的规模和计算复杂度",{"2":{"116":1}}],["因此swish函数可以看作是线性函数和relu函数之间的光滑非线性插值结果",{"2":{"845":1}}],["因此swish也被成为self",{"2":{"108":1}}],["因此softmax中被padding的部分就参与了运算",{"2":{"54":1}}],["因此论文提出了一个解决方案",{"2":{"301":1}}],["因此论文提供了两个初等函数作为近似计算",{"2":{"106":1}}],["因此论文作者设计了一个混合向量来提取注意力头之间的通用信息",{"2":{"19":1}}],["因此需要一起重新调整",{"2":{"1140":1}}],["因此需要对架构进行改进",{"2":{"730":1}}],["因此需要对自注意力机制进行优化",{"2":{"203":1}}],["因此需要做两步",{"2":{"699":1}}],["因此需要采用某些方法来降维",{"2":{"682":1}}],["因此需要多个数字一起来表达概念",{"2":{"679":1}}],["因此需要多头",{"2":{"13":1}}],["因此需要寻找一个更好的与token不同的更接近语义粒度的建模方式",{"2":{"626":1}}],["因此需要将局部编码器为字节序列生成的隐藏表征输入给局部解码器",{"2":{"614":1}}],["因此需要首先对句子进行分词或者说符号化",{"2":{"547":1}}],["因此需要借助掩码把后面单词的信息隐藏掉",{"2":{"528":1}}],["因此需要把tgt复制为tgt",{"2":{"380":1}}],["因此需要在翻译时把",{"2":{"277":1}}],["因此需要结合源序列和目标序列进行综合考虑",{"2":{"277":1}}],["因此需要知道哪个英文单词对应到哪个中文",{"2":{"246":1}}],["因此需要新的模型",{"2":{"242":1}}],["因此需要进行更有针对的修改",{"2":{"140":1}}],["因此需要频繁地更新",{"2":{"138":1}}],["因此需要综合考虑",{"2":{"100":1}}],["因此在给一个成员赋值后",{"2":{"1728":1}}],["因此在函数内部无法直接获取数组的长度",{"2":{"1667":1}}],["因此在下一轮实验中添加一个或多个这些通常问题不大",{"2":{"1149":1}}],["因此在决定是否包含它时",{"2":{"1143":1}}],["因此在当前点存在梯度",{"2":{"1115":1}}],["因此在和注意力机制结合时",{"2":{"745":1}}],["因此在这种情况下",{"2":{"706":1}}],["因此在测试",{"2":{"392":1}}],["因此在考虑单词上下文时候",{"2":{"260":1}}],["因此在实际训练中需要添加",{"2":{"89":1}}],["因此在",{"2":{"83":1}}],["因此我们建议避免使用它们",{"2":{"1173":1}}],["因此我们建议在每轮调整中逐渐增加训练步数限制",{"2":{"1157":1}}],["因此我们的",{"2":{"1139":1}}],["因此我们的输入tgt一定不会出现目标的最后一个token",{"2":{"381":1}}],["因此我们拆分来自",{"2":{"986":1}}],["因此我们使用缩放的方法对大型",{"2":{"943":1,"961":1}}],["因此我们通常提取神经网络的最后一层隐藏状态作为嵌入",{"2":{"730":1}}],["因此我们通常包含拉伸参数",{"2":{"343":1}}],["因此我们可以更确定哪些参数是最有可能在生产环境中表现良好的参数",{"2":{"1157":1}}],["因此我们可以使用长度来对score函数进行细微的调整",{"2":{"904":1}}],["因此我们可以通过上下文来理解某个单词的意义",{"2":{"713":1}}],["因此我们可以用这种多粒子的对流扩散方程来解释",{"2":{"498":1}}],["因此我们可以用矩阵运算一下子把所有的ziziz",{"2":{"418":1}}],["因此我们可以用",{"2":{"344":1}}],["因此我们有必要在实际上下文中继续训练",{"2":{"707":1}}],["因此我们看看nn",{"2":{"702":1}}],["因此我们看看nlp中",{"2":{"318":1}}],["因此我们来到了第三种方案",{"2":{"679":1}}],["因此我们先考虑一般的带绝对位置编码的注意力机制",{"2":{"758":1}}],["因此我们先将这些单词分割成单个字符",{"2":{"579":1}}],["因此我们先来看看cnn和rnn方案的问题所在",{"2":{"242":1}}],["因此我们最终需要得到词表中所有单词作为下一个单词的概率",{"2":{"473":1}}],["因此我们还希望能够尽可能的降低",{"2":{"382":1}}],["因此我们要看看人类如何预判",{"2":{"689":1}}],["因此我们要特殊分析下",{"2":{"381":1}}],["因此我们要面对的问题是",{"2":{"256":1}}],["因此我们需要把自然语言进行编码",{"2":{"455":1,"698":1}}],["因此我们需要一种更简化的解决思路",{"2":{"309":1}}],["因此我们需要想一个办法",{"2":{"69":1,"934":1}}],["因此我们提出了一个问题",{"2":{"186":1}}],["因此我们从搜索领域的业务来看q",{"2":{"164":1}}],["因此我们分别得到8个头的64维的key和64维的value",{"2":{"36":1}}],["因此自然也无法计算得到当前词和下文还没出现词的注意力",{"2":{"57":1}}],["因此线性投影会将其减小回原始大小",{"2":{"41":1}}],["因此最终输出还是",{"2":{"36":1}}],["因此前面怎么拆分",{"2":{"35":1}}],["因此图上省略",{"2":{"24":1}}],["因此dk=dv=dmodel",{"2":{"7":1}}],["因此",{"2":{"4":1,"16":1,"39":1,"58":1,"71":1,"115":1,"117":1,"119":1,"121":1,"130":1,"136":1,"138":1,"139":1,"142":1,"143":1,"145":1,"164":1,"166":1,"168":1,"170":1,"172":1,"192":1,"193":1,"194":2,"204":2,"230":3,"231":1,"238":1,"241":1,"245":1,"247":1,"249":1,"261":2,"271":1,"273":1,"276":1,"277":1,"290":1,"291":1,"296":2,"309":1,"316":2,"320":1,"321":1,"325":1,"333":1,"334":1,"337":1,"352":1,"363":1,"367":1,"397":1,"400":1,"417":2,"434":1,"459":1,"488":1,"498":2,"500":1,"504":1,"505":1,"515":1,"529":1,"536":1,"541":1,"560":1,"579":1,"583":1,"595":1,"601":1,"606":1,"620":1,"621":1,"676":1,"680":1,"691":1,"692":2,"696":1,"698":1,"700":1,"708":1,"714":1,"715":2,"720":1,"729":1,"739":1,"756":2,"757":2,"759":1,"761":1,"764":3,"765":1,"767":1,"779":1,"812":1,"838":1,"863":1,"891":1,"904":1,"908":1,"937":1,"941":1,"944":1,"953":1,"960":1,"963":1,"976":2,"981":1,"986":2,"990":1,"1059":1,"1117":1,"1129":1,"1133":2,"1135":1,"1149":1,"1151":1,"1152":2,"1157":1,"1183":1,"1185":1,"1186":1,"1197":1,"1210":1,"1228":1,"1312":1,"1313":1,"1344":1,"1374":1,"1611":1,"1623":1,"1647":1,"1667":1,"1678":1,"1704":1,"1729":1,"2155":1}}],["因为往上爬可能还有更高的",{"2":{"2115":1}}],["因为的的确确不知道",{"2":{"2054":1}}],["因为他可能指向常量字符串字面量",{"2":{"1929":1}}],["因为他们的gradient相同",{"2":{"992":1}}],["因为他们会发生data",{"2":{"658":1}}],["因为虚函数需要在编译时确定类型",{"2":{"1905":1}}],["因为createstring",{"2":{"1887":1}}],["因为cnn和rnn方案也是在不停的发展",{"2":{"243":1}}],["因为代码在编译后被展开",{"2":{"1709":1}}],["因为局部变量在函数执行结束后会被销毁",{"2":{"1706":1}}],["因为局部解码器根据解码的字节来预测原始字节序列",{"2":{"614":1}}],["因为只有一个小孩能得到满足",{"2":{"2154":1}}],["因为只有一句话",{"2":{"460":1}}],["因为只计算两个",{"2":{"1653":1}}],["因为纯粹的attention模块是无法捕捉输入顺序的",{"2":{"1334":1}}],["因为输出依赖于过去和当前的输入",{"2":{"1312":1}}],["因为输入语句是一个连贯的整体",{"2":{"260":1}}],["因为最佳学习率恰好位于可行的边缘",{"2":{"1182":1}}],["因为最佳的特定衰减方案将对许多其他超参数选择敏感",{"2":{"1173":1}}],["因为贝叶斯优化没有机会观察先前试验的结果",{"2":{"1175":1}}],["因为正是这种不均匀性才是一个好的优化算法所需要的",{"2":{"1175":1}}],["因为采样空间已经变化",{"2":{"1175":1}}],["因为网络权重可以无限制地增长",{"2":{"1155":1}}],["因为生产模型可能不会像研究中那样在",{"2":{"1149":1}}],["因为某些值恰好有更好的冗余超参数配置",{"2":{"1145":1}}],["因为很难指定对应的搜索空间",{"2":{"1144":1}}],["因为像",{"2":{"1143":1}}],["因为实验固定了某些超参数",{"2":{"1143":1}}],["因为您使用的某个模块可能会在训练和评估模式下表现不同",{"2":{"1122":1}}],["因为使用这些参数作为输入的计算在前向传播中不会被记录",{"2":{"1117":1}}],["因为使用teaching",{"2":{"382":1}}],["因为内存",{"2":{"1106":1}}],["因为反向求梯度的",{"2":{"1105":1}}],["因为新查询与现有节点不共享任何前缀",{"2":{"986":1}}],["因为可能的重用模式非常多样",{"2":{"985":1}}],["因为可以使用相同的操作和技术",{"2":{"975":1}}],["因为可以有效地处理变长的序列数据",{"2":{"248":1}}],["因为静态并行设置可能导致集群中利用率较低",{"2":{"977":1}}],["因为静态向量表示无法解决多义词问题",{"2":{"715":1}}],["因为激活的内存占用呈线性增长",{"2":{"976":1}}],["因为激活稀疏性可以在不影响性能的情况下显著加速大型语言模型的推理过程",{"2":{"111":1}}],["因为系统等待接收必要的key",{"2":{"975":1}}],["因为kv",{"2":{"937":1,"953":1}}],["因为神经网络本来就是非凸的",{"2":{"838":1}}],["因为判别模型中结果取决于数据整体分布",{"2":{"809":1}}],["因为并不会完全还原到跟输入图像一样",{"2":{"779":1}}],["因为已经使用了相对位置编码",{"2":{"763":1}}],["因为进行了截断",{"2":{"759":1,"1339":1}}],["因为相对位置编码大多是在正弦位置编码的基础上修改得到",{"2":{"758":1}}],["因为相同的前缀可以根据延续序列以不同方式来token化",{"2":{"612":1}}],["因为两种嵌入的类型和结构不同",{"2":{"739":1}}],["因为基于编码器的模型可以通过双向注意力捕获语义",{"2":{"729":1}}],["因为深度学习的核心思想是",{"2":{"708":1}}],["因为深度学习其实是带有实践性质的科学",{"2":{"162":1}}],["因为position",{"2":{"701":1}}],["因为padding位置的信息不需要带有权重去干扰有实词位置的embedding表征",{"2":{"80":1}}],["因为预先指定了试验预算并保留了迄今为止看到的",{"2":{"1166":1}}],["因为预训练不光做lm还要做以两个句子为输入的分类任务",{"2":{"722":1}}],["因为预训练的embedding是固化的",{"2":{"707":1}}],["因为预训练模型主要的捕获目标是h所代表的",{"2":{"698":1}}],["因为预测时候没有目标语言句子",{"2":{"381":1}}],["因为你需要遵循街道网格",{"2":{"692":1}}],["因为向量空间的存在",{"2":{"689":1}}],["因为向量把自然语言转化为一串数字",{"2":{"685":1}}],["因为one",{"2":{"685":1}}],["因为人的本性就是贪心的",{"2":{"2120":1}}],["因为人类的概念就是复杂多样",{"2":{"683":1}}],["因为人脑并不在单词层面运作",{"2":{"626":1}}],["因为随着互联网技术的发展",{"2":{"682":1}}],["因为随着模型计算量的增加",{"2":{"561":1}}],["因为单一的数字缺少表达能力",{"2":{"679":1}}],["因为要更新梯度的时候要用叶子结点",{"2":{"660":1}}],["因为推理只需操作概念",{"2":{"629":1}}],["因为推理时候不需要传入目标语言句子",{"2":{"380":1}}],["因为第一个字符显著减少了可能的选择",{"2":{"613":1}}],["因为接下来要涉及到熵的概念",{"2":{"612":1}}],["因为原有词表中的单词逐步被合并",{"2":{"582":1}}],["因为频率计数将在每个合并步骤后发生变化",{"2":{"581":1}}],["因为存在大量if",{"2":{"576":1}}],["因为存在跳跃连接",{"2":{"344":1}}],["因为没有任何的引导",{"2":{"895":1}}],["因为没有出现多次的字节对",{"2":{"575":1}}],["因为没办法通过几个样本的数据量",{"2":{"338":1}}],["因为哈佛代码中tokenizer部分比较简单",{"2":{"570":1}}],["因为罗列出单词的所有组合明显比穷举出所有字符更加困难",{"2":{"565":1}}],["因为大词汇表意味着更稀疏的token分布",{"2":{"562":1}}],["因为大模型都是深层结构",{"2":{"396":1}}],["因为上面的这些描述里面",{"2":{"713":1}}],["因为上面一部分的输出中",{"2":{"82":1}}],["因为上述两个矩阵分别都做了自注意力转换",{"2":{"537":1}}],["因为显然源文本中每个位置的字符应该和目标翻译文本各位置字符存在一定的对照关系",{"2":{"536":1}}],["因为解码器的输入是整个目标句子",{"2":{"528":1}}],["因为解码器现在的输入是整个目标句子",{"2":{"525":1}}],["因为解码器依然是rnn",{"2":{"284":1}}],["因为组装需要一面查询组装说明书",{"2":{"524":1}}],["因为多个encoderlayer是串联在一起",{"2":{"518":1}}],["因为每次使用的样本数量较少",{"2":{"1027":1}}],["因为每个高斯的中心部分因为被采样次数多必须特色鲜明",{"2":{"1375":1}}],["因为每个试验都随机地在",{"2":{"1149":1}}],["因为每个批次输入序列长度是不一样的也就是说",{"2":{"933":1}}],["因为每个单词的位置编码是相对于其他单词的位置差异而得到的",{"2":{"757":1}}],["因为每个标记的嵌入会随着每个新时间步的变化而变化",{"2":{"746":1}}],["因为每个字",{"2":{"172":1}}],["因为每一个计算都依赖前面的结果",{"2":{"511":1}}],["因为mha",{"2":{"498":1}}],["因为星形胶质细胞的运作时间比神经元长得多",{"2":{"488":1}}],["因为数学的具象化为物理",{"2":{"474":1}}],["因为一旦依据搜索词",{"2":{"463":1}}],["因为一个单词的上下文包括很多其它单词",{"2":{"260":1}}],["因为理解语言不仅关乎单词",{"2":{"431":1}}],["因为传入tgt的词数是变化的",{"2":{"428":1}}],["因为ground",{"2":{"413":1}}],["因为模型结构变化会影响服务和训练成本",{"2":{"1143":1}}],["因为模型需要学习如何在不",{"2":{"464":1}}],["因为模型生成的结果都必须和参考句一一对应",{"2":{"411":1}}],["因为模型已经知道了全部句子内容",{"2":{"58":1}}],["因为发现了错误",{"2":{"407":1}}],["因为doccano后端是采用django框架搭建的",{"2":{"2068":1}}],["因为decoder每一次只会预测一个单词",{"2":{"405":1}}],["因为dropout引入了随机性",{"2":{"392":1}}],["因为梯度消失的原因",{"2":{"401":1}}],["因为前向传播的时候激活变换为",{"2":{"1004":1}}],["因为前面提到了labelsmoothing类分装了损失函数",{"2":{"399":1}}],["因为前沿的大模型研究工作很大一部分就是着qkv矩阵去做的",{"2":{"162":1}}],["因为词典包括填充符",{"2":{"399":1}}],["因为小模型针对的是特定领域且少量数据的情况",{"2":{"396":1}}],["因为通过给参数增加约束项",{"2":{"393":1}}],["因为通常一个词的语义更依赖与原句子中这个词附近的词的意思",{"2":{"284":1}}],["因为是整数除法",{"2":{"1607":1}}],["因为是使用self",{"2":{"538":2}}],["因为是并行训练",{"2":{"528":1}}],["因为是并行操作",{"2":{"391":1}}],["因为是最后做layernorm",{"2":{"523":1}}],["因为是要预测",{"2":{"473":1}}],["因为是多头注意力的结果",{"2":{"465":1}}],["因为是权重乘以激活",{"2":{"17":1}}],["因为score函数的每一项都是负的",{"2":{"904":1}}],["因为src",{"2":{"382":1}}],["因为softmax是一个归一化函数",{"2":{"63":1}}],["因为语言是相当复杂的",{"2":{"678":1}}],["因为语言文本的复杂性是很高的",{"2":{"323":1}}],["因为语料中的偏差与错误会让大模型学到扭曲的外部信息",{"2":{"369":1}}],["因为feature",{"2":{"337":1}}],["因为ff包含了两个参数矩阵",{"2":{"128":1}}],["因为ffn是割裂开的",{"2":{"519":1}}],["因为ffn是对序列中每个位置的token向量分别进行相同的操作",{"2":{"101":1}}],["因为ffn本身还有第二层全连接",{"2":{"109":1}}],["因为我们的耐心和计算资源有限",{"2":{"1157":1}}],["因为我们已经把相对位置编码融入了key端",{"2":{"760":1}}],["因为我们接下来会优先匹配最长的",{"2":{"587":1}}],["因为我们统计相邻字符对时不能把分别位于两个单词中的字符对算进去",{"2":{"579":1}}],["因为我们没有正确的目标语句",{"2":{"426":1}}],["因为我们不需要预测",{"2":{"381":1}}],["因为我们希望不同图像",{"2":{"325":1}}],["因为我们本系列介绍的主角是transformer",{"2":{"242":1}}],["因为llm的最后一个token一般和下一词的token表征对齐",{"2":{"731":1}}],["因为ln不涉及其他样本",{"2":{"326":1}}],["因为ln",{"2":{"322":1}}],["因为layer",{"2":{"317":1}}],["因为这将在inductor",{"2":{"1227":1}}],["因为这将有助于您编写更高效",{"2":{"1112":1}}],["因为这个句子并不能被模型理解",{"2":{"698":1}}],["因为这个岛有一个街道网格布局",{"2":{"692":1}}],["因为这是在方向上的度量",{"2":{"692":1}}],["因为这决定了是否通过潜在transformer来调用更多计算",{"2":{"612":1}}],["因为这些小孩容易被小饼干满足",{"2":{"2151":1}}],["因为这些神经元的输出值是未知的",{"2":{"1443":1}}],["因为这些方法在探索阶段有多种优势",{"2":{"1144":1}}],["因为这些填充的位置",{"2":{"933":1}}],["因为这些填充符号其实并不携带实际的语义信息",{"2":{"377":1}}],["因为这些投影层的参数大小是固定的",{"2":{"618":1}}],["因为这些是相对简单",{"2":{"612":1}}],["因为这些子词如果被删除",{"2":{"602":1}}],["因为这种类型的架构仍在积极探索中",{"2":{"540":1}}],["因为这两个阶段的流程不尽相同",{"2":{"527":1}}],["因为这两个优点",{"2":{"320":1}}],["因为这不属于句子成分",{"2":{"77":1}}],["因为batchnorm在批次中执行跨样本的归一化操作",{"2":{"316":1}}],["因为batch",{"2":{"315":3}}],["因为不管哪一层的输入都不可能严格满足正态分布",{"2":{"314":1}}],["因为不同位置的单词可能需要不同程度和不同方面的信息",{"2":{"252":1}}],["因为掩码已经确保了模型不能",{"2":{"307":1}}],["因为掩码矩阵中填充词的位置是非常大的负数",{"2":{"63":1}}],["因为残差连接本身并不涉及屏蔽或注意力计算",{"2":{"307":1}}],["因为经过了一个近乎随机的变换",{"2":{"296":1}}],["因为目前训练主要用的是基于梯度的优化器",{"2":{"296":1}}],["因为同一个词",{"2":{"288":1}}],["因为此特点",{"2":{"274":1}}],["因为此处提到了q",{"2":{"265":1}}],["因为感兴趣的对象可以放置在注视的中心",{"2":{"260":1}}],["因为需要处理的",{"2":{"260":1}}],["因为需要在模型参数中修改知识",{"2":{"136":1}}],["因为时间步增加带来的多层激活函数的嵌套会导致梯度反传时指数级地衰减",{"2":{"255":1}}],["因为rnn的表达能力缺失",{"2":{"253":1}}],["因为信息被",{"2":{"247":1}}],["因为信息当到达ffn时",{"2":{"101":1}}],["因为处理这种依赖关系的关键因素之一是信号在网络中穿越路径的长度",{"2":{"246":1}}],["因为北平处处全长着树",{"2":{"246":1}}],["因为其思路之一是希望通过绝对编码方式来实现相对编码",{"2":{"750":1}}],["因为其输入文本通常是变长的",{"2":{"376":1}}],["因为其隐向量大小固定",{"2":{"256":1}}],["因为其贡献最大",{"2":{"245":1}}],["因为其属于系统的短板处",{"2":{"17":1}}],["因为后置形式需要保存原始值",{"2":{"1630":1}}],["因为后面的推理对之前推理的输出会有依赖",{"2":{"239":1}}],["因为后续还要进行softmax操作",{"2":{"199":1}}],["因为后续要和注意力分数进行掩码计算",{"2":{"66":1,"380":1}}],["因为后续要给每个头分配等量的词特征",{"2":{"23":1}}],["因为后续src",{"2":{"66":1,"382":1}}],["因为机器翻译是大家较熟悉且容易理解的领域",{"2":{"237":1}}],["因为笔者的时间和精力有限",{"2":{"235":1}}],["因为本系列参考文章太多",{"2":{"235":1}}],["因为各种事情",{"2":{"235":1}}],["因为毕竟还是有一个小小的数值",{"2":{"199":1}}],["因为除以维度的平方根意味着",{"2":{"193":1}}],["因为embedding和position",{"2":{"698":1}}],["因为embedding的长度实质上代表特征的维度数目",{"2":{"684":1}}],["因为exmaxexmaxe^",{"2":{"191":1}}],["因为encoder可以看到整条输入序列",{"2":{"72":1}}],["因为方差变大就是代表了数据之间的差异性变大",{"2":{"190":1}}],["因为去除softmax之后",{"2":{"180":1}}],["因为看起来softmax是有害无益",{"2":{"180":1}}],["因为在评估期间不需要保持模型运行",{"2":{"1164":1}}],["因为在每次参数更新步骤中需要计算整个数据集的梯度",{"2":{"1025":1}}],["因为在每个channel上",{"2":{"325":1}}],["因为在测试时是单个图片测试",{"2":{"807":1}}],["因为在使用模型时",{"2":{"706":1}}],["因为在预测场景下不存在答案文本",{"2":{"427":1}}],["因为在训练中",{"2":{"417":1}}],["因为在encoderdecoder的forward中并没有使用generator",{"2":{"385":1}}],["因为在注意力机制中",{"2":{"180":1}}],["因为在机器翻译中",{"2":{"176":1}}],["因为在实际情境中",{"2":{"59":1}}],["因为计算机只处理数字",{"2":{"167":1}}],["因为状态会根据文本中",{"2":{"145":1}}],["因为它避免了将小饼干浪费在大胃口的小孩身上",{"2":{"2156":1}}],["因为它避免了因滥用",{"2":{"1185":1}}],["因为它不会有空指针解引用的风险",{"2":{"1927":1}}],["因为它无法存储任何元素",{"2":{"1704":1}}],["因为它包含了维护输出流状态所需的各种数据成员",{"2":{"1678":1}}],["因为它包含了两种以上的类别",{"2":{"1462":1}}],["因为它统一但随机地探索给定的搜索空间",{"2":{"1175":1}}],["因为它可以隐藏表面下发生的事情的重要细节",{"2":{"1149":1}}],["因为它可以使用高度优化的矩阵乘法代码来实现",{"2":{"921":1}}],["因为它确保我们能相对均匀的采样目标超参数值",{"2":{"1144":1}}],["因为它往往会对训练速度和内存使用产生巨大影响",{"2":{"1143":1}}],["因为它",{"2":{"1131":1}}],["因为它这得基于最近的结果和正在进行的研究",{"2":{"1127":1}}],["因为它对模型的性能有显著的影响",{"2":{"1041":1}}],["因为它关注的是元素之间的相对位置",{"2":{"746":1}}],["因为它考虑了在这些属性的值中实际可以采用的路径",{"2":{"692":1}}],["因为它考虑了relu激活函数在零点的不连续性",{"2":{"403":1}}],["因为它们能提供更好的安全性和便捷性",{"2":{"1715":1}}],["因为它们对于调试来说非常有价值",{"2":{"1164":1}}],["因为它们与训练预算有特别强的相互作用",{"2":{"1154":1}}],["因为它们往往与其他变化相互影响",{"2":{"1143":1}}],["因为它们一开始就不是反向图的一部分",{"2":{"1117":1}}],["因为它们是最近最少使用的",{"2":{"986":1}}],["因为它们仅对kv使用一个或少量注意力头",{"2":{"976":1}}],["因为它们没有跨token的操作",{"2":{"976":1}}],["因为它们之间也是一个一一映射的关系",{"2":{"709":1}}],["因为它们之间有着各种各样的句法",{"2":{"135":1}}],["因为它们无关",{"2":{"691":1}}],["因为它允许模型通过组合已知的部分来推测新的词汇",{"2":{"594":1}}],["因为它在用于生成任务之前不需要先转化为中间语境的特征表示",{"2":{"542":1}}],["因为它在前向传播过程中独立地对张量中的每个输入元素进行操作",{"2":{"360":1}}],["因为它捕捉到了单词的语义意义",{"2":{"457":1}}],["因为它没有考虑分值间的相对差异",{"2":{"180":1}}],["因为它涉及到选择性地改变或增强模型知识库的特定方面",{"2":{"139":1}}],["因为编辑的知识涉及同一条信息",{"2":{"136":1}}],["因为编码器是层叠的栈结构",{"2":{"518":1}}],["因为编码器使用了注意力机制",{"2":{"516":1}}],["因为编码器的输出作为k和v",{"2":{"78":1}}],["因为编码时不需要对当前时刻之后的信息进行掩盖",{"2":{"78":1}}],["因为因为",{"2":{"131":1}}],["因为",{"2":{"118":1,"247":1,"382":1,"713":1,"1665":1,"1684":1,"1891":1}}],["因为如果这些正则化参数的低强度设置导致有问题的过拟合是不足为奇的",{"2":{"1149":1}}],["因为如果重复则会增加计算机理解语言的难度",{"2":{"689":1}}],["因为如果大于1",{"2":{"101":1}}],["因为如果在注意力的计算过程中考虑到填充位置上的信息",{"2":{"54":1}}],["因为ϵϵ",{"2":{"93":1}}],["因为所有head的mask都一样",{"2":{"79":1,"198":1}}],["因为对于",{"2":{"74":1}}],["因为t5作者认为输入信息与位置信息应该是独立",{"2":{"762":1}}],["因为token",{"2":{"751":1}}],["因为tgt",{"2":{"380":1}}],["因为tgt存储的是解码器的输入",{"2":{"380":1}}],["因为tgt的第一维是batch",{"2":{"74":1}}],["因为transformer不是按顺序分析",{"2":{"291":1}}],["因为transformer的计算图是浅而宽的",{"2":{"1":1}}],["因为attention",{"2":{"58":1}}],["因为训练的时候每次我们是将target数据完整输入进decoder中地",{"2":{"934":1}}],["因为训练的时候并不是一个生成的过程",{"2":{"897":1}}],["因为训练和推理的基本流程类似",{"2":{"425":1}}],["因为训练可以",{"2":{"411":1}}],["因为训练是并行执行",{"2":{"399":1}}],["因为训练使用teacher",{"2":{"81":1}}],["因为训练时上一时刻的预测值可能是错误的",{"2":{"57":1}}],["因为训练阶段采用自回归模式",{"2":{"57":1}}],["因为填充词是人为添加的",{"2":{"54":1}}],["因为有时候深度学习虽然能有好的表现",{"2":{"1470":1}}],["因为有时候它会被误解为这样的机制",{"2":{"1122":1}}],["因为有些逻辑可以复用",{"2":{"523":1}}],["因为有些信息可能在某一层之中无法捕捉到",{"2":{"20":1}}],["因为有一部分参数直接加在了后面",{"2":{"334":1}}],["因为有残差连接的存在使得输入和输出的维度至少是一样的",{"2":{"36":1}}],["因为head数量已经放到了第二维度",{"2":{"36":1}}],["因为概念需要有足够的内部复杂度才能应对外部世界的复杂度",{"2":{"4":1}}],["因为q是来自解码器的掩码自注意力",{"2":{"536":1}}],["因为qwen2",{"2":{"204":1}}],["因为q",{"2":{"3":1,"34":1}}],["即f",{"2":{"2021":1}}],["即ff层第二个参数矩阵的第",{"2":{"128":1}}],["即程序无法正常继续执行的错误",{"2":{"1764":1}}],["即地址",{"2":{"1694":1}}],["即指针本身",{"2":{"1614":1}}],["即指针指向的内容",{"2":{"1614":1}}],["即y",{"2":{"1377":1}}],["即相对位置是",{"2":{"1340":1}}],["即简化为",{"2":{"1340":1}}],["即无法区分不同位置的token",{"2":{"1334":1}}],["即满足一阶马尔可夫性质",{"2":{"1326":1}}],["即满足马尔可夫性",{"2":{"1322":1}}],["即标签",{"2":{"1324":1}}],["即已经解码生成的词语",{"2":{"1317":1}}],["即具有",{"2":{"1315":1}}],["即优化器超参数",{"2":{"1186":1}}],["即网格搜索学习率",{"2":{"1156":1}}],["即网络的计算就是一些",{"2":{"620":1}}],["即出现分歧的试验",{"2":{"1146":1}}],["即出现在相似语境中的单词在语义上是相互关联的",{"2":{"685":1}}],["即充分搜索了足够大的空间",{"2":{"1145":1}}],["即它不是数学函数",{"2":{"1115":1}}],["即它们代表的是附属信息或者上下文信息",{"2":{"135":1}}],["即参数",{"2":{"988":1}}],["即参数置为0",{"2":{"130":1}}],["即先前的技术水平进行比较",{"2":{"980":1}}],["即解码",{"2":{"908":1}}],["即解码器层的尺寸",{"2":{"533":1}}],["即解码器的每个预测都受到先前输出词和隐状态的微妙影响",{"2":{"241":1}}],["即解码器将编码器学习到的特征信息再转化为相应的句子",{"2":{"241":1}}],["即源和目标两种语言的互相对照的语料",{"2":{"908":1}}],["即源词典的大小",{"2":{"448":1}}],["即找到概率最大的那个y",{"2":{"908":1}}],["即找出包含最相似含义的文本",{"2":{"678":1}}],["即x翻译成y的概率有多大",{"2":{"908":1}}],["即xi=xmax−δixi=xmax−δix",{"2":{"191":1}}],["即得到总体的损失函数",{"2":{"899":1}}],["即得到了下一个词在词典中的编号",{"2":{"473":1}}],["即神经元同时只对输入信号的少部分选择性响应",{"2":{"840":1}}],["即导数符号不变",{"2":{"838":1}}],["即导数不是常数",{"2":{"838":1}}],["即梯度消失",{"2":{"838":1}}],["即梯度更新迭代步数增多",{"2":{"309":1}}],["即非线性映射层",{"2":{"838":1}}],["即非自主性提示",{"2":{"163":1}}],["即均方根",{"2":{"812":1}}],["即是将同一个batch中的所有样本的同一层特征图抽出来一起求mean和variance",{"2":{"807":1}}],["即邻近位置的语义信息",{"2":{"765":2}}],["即移除了单词",{"2":{"764":1}}],["即moe嵌入",{"2":{"737":1}}],["即mhsa可以视为一个",{"2":{"144":1}}],["即c",{"2":{"725":2}}],["即cnn对相对位置敏感",{"2":{"247":1}}],["即预测两个句子的前后关系",{"2":{"721":1}}],["即从词袋模型",{"2":{"711":1}}],["即从头开始构建的单词关系图",{"2":{"709":1}}],["即从一个基础小词表出发",{"2":{"598":1}}],["即嵌入向量",{"2":{"702":1}}],["即便我们无法从新的实验中进一步了解问题的结构了",{"2":{"1140":1}}],["即便再添加更多的隐藏层",{"2":{"838":1}}],["即便词向量只有",{"2":{"698":1}}],["即便这些token的注意力系数在分布内是尖锐的",{"2":{"181":1}}],["即用图上的概念来表示",{"2":{"698":1}}],["即用自回归模式进行",{"2":{"405":1}}],["即连接两个点的线段的长度",{"2":{"692":1}}],["即假设有n个向量",{"2":{"684":1}}],["即兴的",{"2":{"683":1}}],["即刻的",{"2":{"683":1}}],["即向量化",{"2":{"680":1}}],["即数字",{"2":{"680":1}}],["即数据的压缩",{"2":{"581":1}}],["即数据集中的语义相同但措辞可能不同的数据条目的模板都算做相同的信息",{"2":{"147":1}}],["即应用到图像搜索",{"2":{"678":1}}],["即去噪器",{"2":{"635":1}}],["即上下文标注模型",{"2":{"635":1}}],["即上一状态的输出",{"2":{"416":1}}],["即上一个时刻的输出",{"2":{"241":1}}],["即句子嵌入",{"2":{"628":1}}],["即句子中的每个词",{"2":{"71":1}}],["即字节表示是查询",{"2":{"615":1}}],["即字节表示现在是query",{"2":{"614":1}}],["即采用相对位置编码",{"2":{"763":1}}],["即采用数据驱动的方法来识别高不确定性的下一个字节预测",{"2":{"613":1}}],["即采用类似编码器中的矩阵并行算法",{"2":{"57":1}}],["即更多的flop",{"2":{"613":1}}],["即byte",{"2":{"605":1}}],["即batch",{"2":{"343":1}}],["即unigram",{"2":{"601":1}}],["即decoder",{"2":{"542":1}}],["即encoderdecoder类的forward",{"2":{"538":1}}],["即encoderlayer的数量",{"2":{"522":1}}],["即encoderlayer的输入处理部分",{"2":{"519":1}}],["即包括输入序列",{"2":{"536":1}}],["即当前时刻的状态与前面多个时刻的状态相关",{"2":{"1326":1}}],["即当前的输入",{"2":{"533":1}}],["即当前单词的概率取决于前n个单词",{"2":{"242":1}}],["即实施解码过程",{"2":{"533":1}}],["即开始字符的编码",{"2":{"530":1}}],["即后续每一个解码器层中交叉注意力的k",{"2":{"526":1}}],["即前一时刻解码器的输入",{"2":{"526":1}}],["即训练和推理在每个时间步的输入和操作的区别",{"2":{"525":1}}],["即编码器",{"2":{"541":1}}],["即编码器层数和解码器层数",{"2":{"448":1}}],["即编码过程",{"2":{"523":1}}],["即经过embedding处理和添加位置编码后的输入",{"2":{"522":1}}],["即多个解码器层交互过程中",{"2":{"530":1}}],["即多个编码器层之间交互的过程中",{"2":{"520":1}}],["即多头注意力机制调用到attention",{"2":{"198":1}}],["即两层线性映射并且中间使用激活函数来激活",{"2":{"519":1}}],["即两种归一化方法都是将数据在相应的维度上转换为标准正态分布",{"2":{"312":1}}],["即图上的input",{"2":{"518":1}}],["即输入经过嵌入层和位置编码后",{"2":{"517":1}}],["即输入输出相等",{"2":{"301":1}}],["即resnets可以被视为一组特定neural",{"2":{"496":1}}],["即接近极限时",{"2":{"494":1}}],["即该模型可以像",{"2":{"489":1}}],["即所谓的三方突触",{"2":{"488":1}}],["即所有的词都是有效的",{"2":{"428":1}}],["即下标之差",{"2":{"745":1}}],["即下图的黄色部分",{"2":{"517":1}}],["即下一个token的分类",{"2":{"473":1}}],["即下面效果反而不好",{"2":{"301":1}}],["即概率化的内部世界模型",{"2":{"472":1}}],["即根据token",{"2":{"458":1}}],["即批量大小为1",{"2":{"450":1}}],["即依据decoder的隐状态输出来预测当前时刻的词",{"2":{"450":1}}],["即依据匹配的键来返回其对应的值",{"2":{"164":1,"265":1}}],["即如果预训练最大长度为512的话",{"2":{"1337":1}}],["即如果有",{"2":{"765":1}}],["即如果",{"2":{"598":1}}],["即如果参数的维度大于1",{"2":{"449":1}}],["即如何把一个词编码成embedding的过程",{"2":{"676":1}}],["即如何把单词在词表里面的索引转换为一个transformer可以使用的向量",{"2":{"457":1}}],["即如何更好的利用知识",{"2":{"121":1}}],["即重新生成一个新实例",{"2":{"449":1}}],["即word",{"2":{"449":1,"523":1}}],["即weight矩阵的第i行中",{"2":{"71":1}}],["即隐藏层之间的节点不再无连接而是有连接的",{"2":{"851":1}}],["即隐层神经元的数量",{"2":{"448":1}}],["即隐向量",{"2":{"289":1}}],["即词向量",{"2":{"448":1}}],["即词向量的维度",{"2":{"344":1}}],["即目标词典的大小",{"2":{"448":1}}],["即token",{"2":{"431":1}}],["即tgt是从开始",{"2":{"426":2}}],["即transformer的中间层大部分都是",{"2":{"204":1}}],["即序列并行",{"2":{"420":1}}],["即序列中任意两个单词之间距离都是一个固定的常数",{"2":{"415":1}}],["即各个单词同时流入编码器中",{"2":{"419":1}}],["即提示一下上一个词的正确答案",{"2":{"896":1}}],["即提高了计算效率",{"2":{"412":1}}],["即提出了注意力概念",{"2":{"284":1}}],["即一个embedding",{"2":{"808":1}}],["即一个向量",{"2":{"674":1}}],["即一个句子",{"2":{"428":1}}],["即一开始学生是小白",{"2":{"411":1}}],["即一次接收解码时所有时刻的输入",{"2":{"57":1}}],["即可",{"2":{"409":1,"2070":1}}],["即可以表达某个具体dense矩阵的改变",{"2":{"224":1}}],["即真值",{"2":{"406":1}}],["即2个数据的标签分别为2",{"2":{"399":1}}],["即随机丢弃的神经元比例",{"2":{"392":1}}],["即随着模型层层递进",{"2":{"306":1}}],["即spacy",{"2":{"384":2,"558":2}}],["即sequence",{"2":{"382":1}}],["即softmax将one",{"2":{"191":1}}],["即我们设置一个概率p",{"2":{"897":1}}],["即我们将一对数字相乘",{"2":{"692":1}}],["即我们的label不包含",{"2":{"381":1}}],["即我们希望模型预测出这几个token",{"2":{"380":1}}],["即计算框架中的",{"2":{"1344":1}}],["即计算第n层的注意力矩阵时",{"2":{"349":1}}],["即计算多头注意力的q",{"2":{"36":1}}],["即本层的attention",{"2":{"349":1}}],["即本例中的",{"2":{"122":1}}],["即直接计算输入特征的平方均值",{"2":{"346":1}}],["即直接输出",{"2":{"301":1}}],["即layernorm",{"2":{"344":1}}],["即归一化形状为",{"2":{"341":1}}],["即求每页书的",{"2":{"340":1}}],["即求整本书的",{"2":{"340":1}}],["即padding",{"2":{"382":1}}],["即pre",{"2":{"334":1}}],["即post",{"2":{"334":1}}],["即position",{"2":{"101":1}}],["即样本内的特征关系更为重要",{"2":{"318":1}}],["即选择损失信息的维度不同",{"2":{"311":1,"321":1}}],["即模型所能识别的所有词或子词的数量",{"2":{"700":1}}],["即模型根据每一层的残差来输出token的概率",{"2":{"306":1}}],["即模型只应该依据部分输入来进行预测",{"2":{"58":1}}],["即有一路的梯度不会经过梯度f",{"2":{"304":1}}],["即有局部感受野",{"2":{"204":1}}],["即难以对token之间的关系进行建模",{"2":{"287":1}}],["即交叉注意力",{"2":{"287":1}}],["即交换头部和序列维度来重塑注意力分数矩阵",{"2":{"35":1}}],["即针对输入序列中的每个单词",{"2":{"267":1}}],["即不能创建该类的对象",{"2":{"1693":1}}],["即不被大量标签噪声影响",{"2":{"1165":1}}],["即不同类别的总数",{"2":{"702":1}}],["即不同时间步使用相同的权重",{"2":{"250":1}}],["即不要把真值完全标记成非0即1",{"2":{"399":1}}],["即不存在残差",{"2":{"301":1}}],["即不仅仅要考虑到词本身",{"2":{"167":1,"259":1}}],["即没有相关性",{"2":{"709":1}}],["即没有单一窗口可以同时看到这两个词",{"2":{"247":1}}],["即没有注意力分配到这个上面",{"2":{"62":1}}],["即人们知道",{"2":{"246":1}}],["即人类不断的渐进的获取新的知识",{"2":{"143":1}}],["即系统的未来状态只与当前状态有关",{"2":{"242":1}}],["即总结ht=g",{"2":{"240":1}}],["即第一个解码器层中交叉注意力的k",{"2":{"526":1}}],["即第二种方法",{"2":{"478":1}}],["即第二次模型的输入是",{"2":{"239":1}}],["即第三次模型的输入是",{"2":{"239":1}}],["即原有句子加上新预测单词后",{"2":{"238":1}}],["即推理",{"2":{"230":1}}],["即令人惊讶的输入",{"2":{"228":1}}],["即z向量",{"2":{"224":1}}],["即权重矩阵",{"2":{"224":1}}],["即权重代表了信息的重要性",{"2":{"169":1}}],["即大脑根据当前任务激活特定区域",{"2":{"222":1}}],["即存在长度外推问题",{"2":{"203":1}}],["即自注意力调用到attention",{"2":{"198":1}}],["即自主性提示",{"2":{"163":1}}],["即几乎所有梯度都接近于0",{"2":{"192":1}}],["即最原始的字符匹配",{"2":{"678":1}}],["即最终结果",{"2":{"199":1}}],["即最终输出的结果和输入的词嵌入形状一样",{"2":{"10":1}}],["即最大值为",{"2":{"188":1}}],["即长尾理论或28原则",{"2":{"185":1}}],["即缩小qiqiq",{"2":{"176":1}}],["即缩放每一个元素",{"2":{"54":1,"179":1}}],["即将预训练的模型",{"2":{"1009":1}}],["即将两token的相对位置信息添加到对应的at",{"2":{"745":1}}],["即将两个向量的乘积分解为了两个向量各自模长与夹角余弦的乘积",{"2":{"176":1}}],["即将token的绝对位置信息添加到对应的qt",{"2":{"745":1}}],["即将单词编码成多少维的向量",{"2":{"702":1}}],["即将待分词的中文文本根据一定规则切分和调整",{"2":{"550":1}}],["即将每个token",{"2":{"458":1}}],["即将词映射到一个向量空间",{"2":{"431":1}}],["即将数据减去均值",{"2":{"298":1}}],["即将本层的输入张量与输出张量相加",{"2":{"294":1}}],["即将query矩阵的每一行与key的转置矩阵的每一列进行点积",{"2":{"199":1}}],["即将attention从",{"2":{"194":1}}],["即将one",{"2":{"180":1}}],["即将其除以向量维度的平方根√dkdk",{"2":{"173":1}}],["即将多个头再合并起来",{"2":{"36":1}}],["即点积",{"2":{"173":1}}],["即等于8",{"2":{"173":1}}],["即查询向量",{"2":{"172":1}}],["即让模型在注意邻近信息的同时",{"2":{"746":1}}],["即让",{"2":{"170":1}}],["即对y添加0",{"2":{"2019":1}}],["即对话的第一轮",{"2":{"986":1}}],["即对多义词进行建模",{"2":{"689":1}}],["即对高熵区域",{"2":{"612":1}}],["即对两个填充的pad进行掩码",{"2":{"450":1}}],["即对应位置进行乘法操作",{"2":{"343":1}}],["即对所有句子处于相同位置的词向量执行",{"2":{"316":1}}],["即对上一个词的预测输出",{"2":{"165":1}}],["即对于一个序列",{"2":{"59":1}}],["即你有多大概率是我要查的东西",{"2":{"164":1}}],["即注意力计算",{"2":{"529":1}}],["即注意力在预测某个词时可以提前关注到其后面的单词",{"2":{"409":1}}],["即注意力分数",{"2":{"268":1,"284":1}}],["即注意力分数分布",{"2":{"71":1}}],["即注意力赋予了模型分辨的能力",{"2":{"260":1}}],["即注意力是把你要查询的q与淘宝数据库中的k进行比较",{"2":{"164":1}}],["即注意力权重",{"2":{"164":1}}],["即key和query会得出对齐系数",{"2":{"168":1}}],["即key",{"2":{"163":1}}],["即kans",{"2":{"155":1}}],["即专家",{"2":{"150":1}}],["即autoencoder是用来解释模型内在激活",{"2":{"137":1}}],["即改变特征的值",{"2":{"137":1}}],["即通过运算符",{"2":{"1083":2}}],["即通过对比学习的方式去拉进相似文本的距离",{"2":{"734":1}}],["即通过对batchsize这个维度归一化来让分布稳定下来",{"2":{"314":1}}],["即通过一个字附近n个字来预测这个字",{"2":{"714":1}}],["即通过丘奇编码目标系统来动态模拟任何其他计算系统",{"2":{"691":1}}],["即通过动态规划",{"2":{"284":1}}],["即通过引入外部模型",{"2":{"141":1}}],["即通过value得到的词表分布遵循了key捕获的模式",{"2":{"128":1}}],["即通过权重矩阵将它们从原始维度映射到较低的维度",{"2":{"27":1}}],["即这些向量无法通过获取最大注意力得分进行选择",{"2":{"320":1}}],["即这些冗余层数会带来网络退化",{"2":{"296":1}}],["即这些头的权重是一样的",{"2":{"10":1}}],["即这个查询向量在少数几个输入数据",{"2":{"194":1}}],["即这个查询向量在多个输入数据",{"2":{"194":1}}],["即这里的截距项",{"2":{"122":1}}],["即涌现现象",{"2":{"119":1}}],["即面对看似不相关的输入",{"2":{"118":1}}],["即glu",{"2":{"105":1}}],["即每一步",{"2":{"901":1}}],["即每一层的encoderlayer都对输入序列的所有位置同时进行操作",{"2":{"519":1}}],["即每行",{"2":{"99":1}}],["即每个索引出现的次数",{"2":{"702":1}}],["即每个",{"2":{"204":1}}],["即每个输出节点的值代表了相应类别的概率",{"2":{"180":1}}],["即每个position",{"2":{"101":1}}],["即每个token只关注非常有限个其他token",{"2":{"14":1}}],["即每个字都转换为若干512",{"2":{"9":1}}],["即秩崩溃",{"2":{"94":1}}],["即秩r",{"2":{"46":1}}],["即为每个输入下标训练一个嵌入向量来刻画绝对位置特征",{"2":{"749":1}}],["即为每个单词分配一个基于模型已知的所有单词列表的唯一数字",{"2":{"679":1}}],["即为每个token计算一组召回权重",{"2":{"118":1}}],["即为",{"2":{"84":1}}],["即为了并行一次喂入所有解码部分的输入",{"2":{"39":1}}],["即其实用一个向量就够了",{"2":{"79":1}}],["即q",{"2":{"620":1}}],["即qkv均来自一个序列",{"2":{"517":1,"525":1}}],["即qktqktqk^t对角线上的激活值会明显比较大",{"2":{"3":1}}],["即q对应的第i个时间步",{"2":{"71":1}}],["即保持score矩阵的下三角部分不变",{"2":{"71":1}}],["即遮蔽的意思",{"2":{"71":1}}],["即互不相关",{"2":{"70":1}}],["即在原有的翻译模型上",{"2":{"908":1}}],["即在一定数据范围内",{"2":{"839":1}}],["即在内核元素之间插入",{"2":{"778":1}}],["即在计算自注意力分布时考虑两个token间的相对位置信息",{"2":{"745":1}}],["即在任何空白字节后创建新patch",{"2":{"613":1}}],["即在rdrd",{"2":{"499":1}}],["即在归一化操作之前和之后的张量",{"2":{"359":1}}],["即在残差之后进行归一化",{"2":{"331":1}}],["即在模型层数加深后新加入的层就学习不到什么东西了",{"2":{"296":1}}],["即在序列足够长的时候",{"2":{"287":1}}],["即在解码过程中",{"2":{"245":1}}],["即在自注意力层之后",{"2":{"97":1}}],["即在",{"2":{"70":1,"217":1,"326":1,"541":1,"1083":1}}],["即被padding的位置是true",{"2":{"66":1}}],["即只能迭代地基于已经生成的词语来逐个预测后面的词语",{"2":{"1316":1}}],["即只能获取到训练长度内的信息",{"2":{"765":1}}],["即只对有效的临近词进行建模",{"2":{"759":1}}],["即只需改变位置表示方法就可以实现长度外推",{"2":{"756":1}}],["即只使用输出序列中最后一个单词的猜测结果",{"2":{"472":1}}],["即只考虑语义上不同的信息",{"2":{"147":1}}],["即只",{"2":{"50":1}}],["即映射矩阵由输入动态生成",{"2":{"46":1}}],["即单头计算是使用最后两个维度",{"2":{"34":1}}],["即单头注意力机制",{"2":{"9":1}}],["即把数据",{"2":{"1477":1}}],["即把token从原本的词向量维度",{"2":{"699":1}}],["即把one",{"2":{"694":1}}],["即把每一个单词的向量维度从词典大小降维到较小的维度",{"2":{"684":1}}],["即把多个数字分散嵌入到一个数学空间中",{"2":{"679":1}}],["即把ids中出现的pair替换成idx",{"2":{"592":1}}],["即把解码器的输出转化为与词典大小相同的向量",{"2":{"431":1}}],["即把若干数据聚集成一个batch",{"2":{"375":2}}],["即把原来在一个高维空间里衡量一个文本的任意两个字之间的相关度",{"2":{"33":1}}],["即把做",{"2":{"29":1}}],["即增加一个维度",{"2":{"29":1,"36":1}}],["即",{"2":{"28":1,"36":2,"78":1,"84":1,"99":1,"122":3,"128":1,"140":1,"145":1,"170":1,"172":2,"178":1,"180":1,"181":1,"189":1,"191":1,"209":1,"225":1,"230":1,"232":1,"301":1,"315":1,"383":1,"393":1,"406":1,"415":1,"446":1,"529":1,"624":1,"684":1,"689":2,"702":1,"704":2,"715":1,"721":1,"736":3,"761":1,"976":1,"1003":1,"1004":1,"1036":1,"1343":1,"1344":1,"1611":1,"1640":1,"2119":1}}],["即四个线性层",{"2":{"23":1}}],["即越来越包含更多的位置信息",{"2":{"20":1}}],["即使没有显式地声明为",{"2":{"1924":1}}],["即使没有显式声明constexpr",{"2":{"1924":1}}],["即使针对的是其他项目",{"2":{"1197":1}}],["即使进行了20次试验",{"2":{"1177":1}}],["即使它不是故意的",{"2":{"1149":1}}],["即使它们的作者具有在应用工作中提供有用建议的经验",{"2":{"1127":1}}],["即使这对于解决主要的实验目标不是必要的",{"2":{"1149":1}}],["即使这些见解与当前目标并不直接相关",{"2":{"1146":1}}],["即使花里胡哨的东西在未来被证明是有用的",{"2":{"1137":1}}],["即使更大的batch",{"2":{"1133":1}}],["即使硬件支持更大的batch",{"2":{"1132":1}}],["即使只是",{"2":{"1130":1}}],["即使您不确定您的模型是否具有训练模式特定的行为",{"2":{"1122":1}}],["即使存在",{"2":{"1120":1}}],["即使通过手动配置",{"2":{"985":1}}],["即使对于像lmsys这样具有有限计算资源的小型研究团队",{"2":{"980":1}}],["即使其中一个取决于另一个的输出",{"2":{"973":1}}],["即使无法减少键值缓存",{"2":{"957":1}}],["即使有更多的flops",{"2":{"946":1,"966":1}}],["即使切分出来极高的维度",{"2":{"712":1}}],["即使是对于自定义任务",{"2":{"1313":1}}],["即使是对于长度为",{"2":{"504":1}}],["即使是早期时间步的信息也可以传递到后续时间步",{"2":{"863":1}}],["即使是单个",{"2":{"504":1}}],["即使是像softmax这样广泛使用的函数",{"2":{"181":1}}],["即使处于叠加状态",{"2":{"477":1}}],["即使我们计算出错误的答案",{"2":{"406":1}}],["即使我们的模型只有有限的维度",{"2":{"137":1}}],["即使上一个词预测错误",{"2":{"406":1}}],["即使在发生异常的情况下也能确保资源被正确释放",{"2":{"1764":1}}],["即使在搜索算法实现发生变化的情况下",{"2":{"1175":1}}],["即使在昂贵的硬件上",{"2":{"980":1}}],["即使在最后一层",{"2":{"306":1}}],["即使在稀疏性约束使得叠加不可能的情况下也是如此",{"2":{"118":1}}],["即使某个子层没有学到有用的信息",{"2":{"304":1}}],["即使某些头被噪声或者不相关的信息干扰",{"2":{"21":1}}],["即使一个事件令人难忘",{"2":{"230":1}}],["即使用自己的任务语料对模型进行",{"2":{"1313":1}}],["即使用模型进行预测时",{"2":{"721":1}}],["即使用多层感知机",{"2":{"228":1}}],["即使用softmax操作将权值进行归一化",{"2":{"173":1}}],["即使使用深度或者非常大的矩阵值记忆时也是如此",{"2":{"230":1}}],["即使使用",{"2":{"222":1}}],["即使使用稀疏或局部注意力掩码",{"2":{"92":1}}],["即使每次只能看几页",{"2":{"216":1}}],["即使每个头工作在较低维度的子空间中",{"2":{"21":1}}],["即使增加维度",{"2":{"193":1}}],["即使目标物品",{"2":{"163":1}}],["即使堆叠多个",{"2":{"117":1}}],["即使尺寸是d",{"2":{"34":1}}],["即使",{"2":{"17":1,"1149":1}}],["即设定一个可学习参数",{"2":{"10":1}}],["即512维的向量长度",{"2":{"10":1}}],["即效率",{"2":{"1":1}}],["即完整性",{"2":{"1":1}}],["是孩子和饼干数量的较大值",{"2":{"2155":1}}],["是做出来的",{"2":{"2056":1}}],["是那些热爱生命并且勇往直前的人",{"2":{"2056":1}}],["是理解的关键",{"2":{"2054":1}}],["是理论上还是用到了很少一点点未来的信息",{"2":{"199":1}}],["是很客观的",{"2":{"2054":1}}],["是图形学中将3d图形转化为2d图像的关键技术",{"2":{"2009":1}}],["是最常用的标准输出流对象",{"2":{"1815":1,"1833":1}}],["是最常用的标准输入流对象",{"2":{"1812":1,"1830":1}}],["是最基本的神经网络模型之一",{"2":{"242":1}}],["是类",{"2":{"1786":1}}],["是类似于金钱豹之类的动物",{"2":{"713":1}}],["是这个联合体的名称",{"2":{"1728":1}}],["是这个枚举类型的名称",{"2":{"1728":1}}],["是这个结构体的名称",{"2":{"1728":1}}],["是这个实体对应的某个属性",{"2":{"122":1}}],["是析构函数",{"2":{"1674":1}}],["是构造函数",{"2":{"1674":1}}],["是与标准输出设备",{"2":{"1673":1}}],["是与标准输入设备",{"2":{"1673":1}}],["是命令行参数的常见形式",{"2":{"1650":1}}],["是递归能够结束的根本保证",{"2":{"1646":1}}],["是小写字母",{"2":{"1619":1}}],["是小批量bb",{"2":{"343":2}}],["是大写字母",{"2":{"1619":1}}],["是学习现代",{"2":{"1603":1}}],["是学习一个特征的技术的集合",{"2":{"1455":1}}],["是用于运行",{"2":{"1589":1}}],["是用来排序的",{"2":{"1087":1}}],["是用来补全长度的",{"2":{"382":1}}],["是进程的唯一标识",{"2":{"1573":1}}],["是整个文件系统的起点",{"2":{"1505":1}}],["是典型的深度学习模型",{"2":{"1457":1}}],["是机器学习的分支",{"2":{"1455":1}}],["是机械式的",{"2":{"676":1}}],["是论文",{"2":{"1341":1}}],["是选择可训练式的还是三角函数式的",{"2":{"1339":1}}],["是位置向量的维度",{"2":{"1336":1}}],["是线性序列",{"2":{"1322":1}}],["是有开销的",{"2":{"1299":1}}],["是有输入的",{"2":{"661":1,"1104":1}}],["是两种不同的计算图表示形式",{"2":{"1287":1}}],["是两个相互独立的随机向量",{"2":{"332":1}}],["是我们在用作迭代调优过程的一部分时优于更高级的黑盒优化工具",{"2":{"1175":1}}],["是我们的llm推理和服务引擎背后的核心技术",{"2":{"983":1}}],["是非常重要的",{"2":{"1169":1}}],["是非零的",{"2":{"694":1}}],["是更好的选择吗",{"2":{"1143":1}}],["是使用尽可能大的batch",{"2":{"1133":1}}],["是具有与之相关联的反向图的张量",{"2":{"1117":1}}],["是不同的条件超参数",{"2":{"1143":1}}],["是不同的张量对象",{"2":{"1114":1}}],["是不是很符合人的本性",{"2":{"2112":1}}],["是不是就完事儿了呢",{"2":{"908":1}}],["是不是",{"2":{"661":1,"1104":1}}],["是需要计算权重并保存梯度的",{"2":{"1107":1}}],["是需要与其他模型参数一起学习的参数",{"2":{"343":1}}],["是另一种学习率自适应的优化算法",{"2":{"1059":1}}],["是另一种深度学习归一化方式",{"2":{"338":1}}],["是介于批量梯度下降",{"2":{"1027":1}}],["是损失函数j关于参数θ的梯度向量",{"2":{"1023":1}}],["是上一层通过relu得到的",{"2":{"1003":1}}],["是上面两种方法的平衡",{"2":{"567":1}}],["是必要的",{"2":{"977":1}}],["是必须有的",{"2":{"325":1}}],["是注意力的变体",{"2":{"971":1}}],["是下投影矩阵",{"2":{"957":1}}],["是键",{"2":{"957":1}}],["是赋值符号",{"2":{"943":1,"961":1}}],["是头维度",{"2":{"941":1,"960":1}}],["是序列长度",{"2":{"941":1,"960":1}}],["是为了使得",{"2":{"934":1}}],["是为了把每个词的softmax值用作似然",{"2":{"184":1}}],["是将四张图片进行随机裁剪",{"2":{"1015":1}}],["是将序列",{"2":{"885":1}}],["是将本来依赖于二元坐标",{"2":{"759":1,"1339":1}}],["是每帧的声音信号",{"2":{"878":1}}],["是每个词向量的维度",{"2":{"700":1}}],["是同一份数据吗",{"2":{"856":1}}],["是前馈神经网络在处理序列数据时的一种自然推广",{"2":{"850":1}}],["是前向传播的输入",{"2":{"485":1}}],["是沿着那个轴进行split",{"2":{"831":1}}],["是torch",{"2":{"825":1}}],["是transformer论文的读书笔记",{"2":{"432":1}}],["是cnn",{"2":{"813":1}}],["是nvidia打造的针对深度神经网络的加速库",{"2":{"796":1}}],["是nlp领域的主要处理对象",{"2":{"678":1}}],["是显卡的核心芯片",{"2":{"795":1}}],["是由两层神经元组成的结构",{"2":{"1461":1}}],["是由子层本身实现的函数",{"2":{"914":1}}],["是由nvidia推出的通用并行计算架构",{"2":{"795":1}}],["是由所有词共同计算得到的",{"2":{"415":1}}],["是增大了计算量还是减小了计算量",{"2":{"773":1}}],["是几维的",{"2":{"773":1}}],["是超参数",{"2":{"765":1}}],["是依照transformer中的通过正余弦频率方式来获取的",{"2":{"760":1}}],["是依赖数据的",{"2":{"301":1}}],["是query",{"2":{"758":1}}],["是二个token",{"2":{"757":1}}],["是编码",{"2":{"745":1}}],["是编码器对输入`",{"2":{"537":1}}],["是编码器的输入",{"2":{"407":1}}],["是第一个可以在",{"2":{"1317":1}}],["是第一个在",{"2":{"1315":1}}],["是第一个基于",{"2":{"1315":1}}],["是第一个token",{"2":{"744":1,"757":1}}],["是第t步的输出",{"2":{"855":1}}],["是第五个token",{"2":{"744":1}}],["是第二个线性层的输入维度和第一个线性层的输出维度",{"2":{"113":1}}],["是比较常见的方式",{"2":{"736":1}}],["是次优的",{"2":{"720":1}}],["是它在句子世界中的社交圈",{"2":{"714":1}}],["是神经网络倒数第一层的参数权重",{"2":{"707":1}}],["是神经网络架构设计的一次飞跃",{"2":{"279":1}}],["是相等的",{"2":{"698":1}}],["是相应的",{"2":{"148":1,"485":1}}],["是和向量化相比",{"2":{"676":1}}],["是后者",{"2":{"665":1}}],["是直接加在",{"2":{"765":1}}],["是直接使用缓存的填充矩阵还是需要拿出缓存数据",{"2":{"656":1}}],["是直到当前时间步所产生的整个输出序列",{"2":{"81":1}}],["是时候重新审视tokenization了",{"2":{"638":1}}],["是固定的",{"2":{"629":1,"2022":1}}],["是通常改变模型结构",{"2":{"624":1}}],["是通过两个权重层和非线性激活函数relu构成的子网络来学习的",{"2":{"301":1}}],["是通过",{"2":{"204":1}}],["是通过将输入数据以及线性层权重",{"2":{"29":1}}],["是任意非线性函数",{"2":{"621":1}}],["是任务相关的变量",{"2":{"164":1}}],["是标量scale",{"2":{"621":1}}],["是标准正态分布的累积分布函数",{"2":{"106":1}}],["是改进的",{"2":{"621":1}}],["是看能否正确预测到下一个单词",{"2":{"612":1}}],["是确定性的",{"2":{"576":1}}],["是自然随机化的过程",{"2":{"576":1}}],["是自由度最大的一项",{"2":{"176":1}}],["是3次",{"2":{"575":1}}],["是否是层层深入的关系我们先不去探讨",{"2":{"2054":1}}],["是否是同一个tensor",{"2":{"1079":1}}],["是否足够",{"2":{"1873":1}}],["是否有效",{"2":{"1695":1}}],["是否有任何试验显示过拟合",{"2":{"1149":1}}],["是否会修改实参",{"2":{"1650":1}}],["是否处于训练模式",{"2":{"1208":1}}],["是否生成模块的补丁列表",{"2":{"1208":1}}],["是否全为",{"2":{"1086":1}}],["是否使用参数高效的微调技术主要取决于使用的数据量",{"2":{"733":1}}],["是否喜欢咖啡等等",{"2":{"690":1}}],["是否需要学习",{"2":{"676":1}}],["是否扩增词表应该基于对领域数据的详细分析和对模型性能需求的准确理解",{"2":{"560":1}}],["是否被多个",{"2":{"135":1}}],["是模型词汇表",{"2":{"700":1}}],["是模型有效理解和管理的单元",{"2":{"548":1}}],["是模型维度",{"2":{"402":1}}],["是包含编码器",{"2":{"540":1}}],["是仅包含解码器的模型",{"2":{"540":1}}],["是仅包含编码器的模型",{"2":{"540":1}}],["是解码器中掩码自注意力机制对输入",{"2":{"537":1}}],["是解码器和编码器之间的桥梁",{"2":{"525":1}}],["是目标序列对输入序列的注意力计算",{"2":{"535":1}}],["是要堆叠的解码器层",{"2":{"532":1}}],["是训练集的真值标签",{"2":{"528":1}}],["是结合了已生成部分序列的信息的一个上下文向量",{"2":{"525":1}}],["是计算理论中的一个核心概念",{"2":{"504":1}}],["是核函数选择与参数化的过程",{"2":{"472":1}}],["是表示向量的维度",{"2":{"445":1}}],["是表征语义的真实数据",{"2":{"164":1}}],["是句子中单词个数",{"2":{"445":1}}],["是因果注意力层的输出向量",{"2":{"444":1}}],["是因为这些工具都是为开发和执行",{"2":{"1589":1}}],["是因为这样会导致矩阵秩的降低",{"2":{"396":1}}],["是因为文本生成类任务还没这么火热",{"2":{"935":1,"951":1}}],["是因为其具有如下特点",{"2":{"689":1}}],["是因为其创造者对ln进行研究和改进",{"2":{"346":1}}],["是因为直接在字节维度上来着眼",{"2":{"612":1}}],["是因为最近有些从非ai领域转过来的新同学来找我询问是否有比较好的学习资料",{"2":{"235":1}}],["是因为u和v矩阵不参与训练",{"2":{"224":1}}],["是因为query和输入的shape相同",{"2":{"199":1}}],["是因为ffn有如下特点",{"2":{"101":1}}],["是所有单词的加权和",{"2":{"415":1}}],["是真值标签",{"2":{"407":1}}],["是慢与快的交叉过程",{"2":{"402":1}}],["是可学习的矩阵",{"2":{"501":1}}],["是可学习的",{"2":{"360":1}}],["是微软",{"2":{"347":1}}],["是已知的",{"2":{"344":1}}],["是个数字",{"2":{"1110":1}}],["是个flag",{"2":{"1110":1}}],["是个node实例",{"2":{"1110":1}}],["是个",{"2":{"344":1}}],["是个实体",{"2":{"122":1}}],["是针对单个",{"2":{"326":1}}],["是瓶颈所在",{"2":{"284":1}}],["是向量value",{"2":{"265":1}}],["是基于",{"2":{"216":1}}],["是特征映射后的维度",{"2":{"210":1}}],["是稀疏注意力的一种",{"2":{"204":1}}],["是经过权重矩阵线性转换过的q",{"2":{"199":1}}],["是希望引入新的token后",{"2":{"194":1}}],["是公因子",{"2":{"191":1}}],["是在原来的sd模型的基础上增加了clip的image",{"2":{"1363":1}}],["是在relu",{"2":{"841":1}}],["是在同一时间工作的",{"2":{"840":1}}],["是在输入序列和输出序列之间进行对齐",{"2":{"536":1}}],["是在模仿传统编码器",{"2":{"525":1}}],["是在更高维的向量空间中的表示",{"2":{"518":1}}],["是在每个神经网络层的输出上进行归一化操作",{"2":{"331":1}}],["是在点乘之后除以一个分母",{"2":{"186":1}}],["是在token内部完成特征映射",{"2":{"101":1}}],["是feature的大小",{"2":{"185":1}}],["是ffn的隐藏层维度",{"2":{"99":1}}],["是如下原因",{"2":{"180":1}}],["是词在高维空间的数值映射",{"2":{"176":1}}],["是早就被预先训练好的了",{"2":{"172":1}}],["是深度学习相关概念",{"2":{"167":1,"259":1}}],["是深拷贝",{"2":{"35":1,"36":1}}],["是蕴含了序列内部关系的全局特征表示",{"2":{"158":1}}],["是主语",{"2":{"145":1}}],["是指每次做选择时",{"2":{"2101":1}}],["是指将具有多头注意力的语言模型转换为具有多查询注意力的模型",{"2":{"938":1,"954":1}}],["是指对已有的模型进行进一步的训练",{"2":{"938":1,"954":1}}],["是指两个softmax函数间的差异来消除注意力噪声",{"2":{"502":1}}],["是指在并发环境中",{"2":{"1412":1}}],["是指在训练过程中",{"2":{"309":1}}],["是指在上投影和下投影之间的中间维数",{"2":{"141":1}}],["是指神经网络中的连接权重",{"2":{"172":1}}],["是指使用模型组件的位置",{"2":{"141":1}}],["是关系",{"2":{"122":1}}],["是split",{"2":{"825":1}}],["是swish激活函数",{"2":{"109":1}}],["是sigmoid激活函数",{"2":{"109":1}}],["是误差函数",{"2":{"106":1}}],["是偏置项",{"2":{"105":1}}],["是权重矩阵",{"2":{"105":1}}],["是输入序列的线性加权表示结果",{"2":{"758":1}}],["是输入序列对自身的注意力计算",{"2":{"535":2}}],["是输入的浓缩摘要",{"2":{"241":1}}],["是输入张量",{"2":{"109":1}}],["是输入",{"2":{"105":1,"106":1}}],["是一门编译型语言",{"2":{"1604":1}}],["是一组电脑",{"2":{"1563":1}}],["是一款优秀的持久层框架",{"2":{"1476":1}}],["是一阶的近似方法",{"2":{"498":1}}],["是一种很正常很常见的事情",{"2":{"2054":1}}],["是一种固定大小的静态数组容器",{"2":{"1802":1}}],["是一种单向链表",{"2":{"1801":1}}],["是一种双向链表",{"2":{"1799":1}}],["是一种变量",{"2":{"1612":1}}],["是一种特殊的指针类型",{"2":{"1611":1}}],["是一种良好的编程习惯",{"2":{"1611":1}}],["是一种深度学习架构",{"2":{"1472":1}}],["是一种能有效的处理序列数据的算法",{"2":{"1472":1}}],["是一种自适应系统",{"2":{"1456":1}}],["是一种模仿生物神经网络",{"2":{"1456":1}}],["是一种以人工神经网络为架构",{"2":{"1455":1}}],["是一种中间表示形式",{"2":{"1290":1}}],["是一种基本的梯度下降优化算法",{"2":{"1025":1}}],["是一种基于随机采样的梯度下降优化算法",{"2":{"1026":1}}],["是一种基于",{"2":{"614":1}}],["是一种常用的优化算法",{"2":{"1023":1}}],["是一种用于处理程序与外部世界",{"2":{"1810":1,"1828":1}}],["是一种用于评估生成图像质量的度量标准",{"2":{"1361":1}}],["是一种用于在不同深度学习框架之间交换数据的标准化格式",{"2":{"1083":2}}],["是一种用于初始化神经网络权重的方法",{"2":{"999":1}}],["是一种用于分析和解释大型语言模型内部机制的方法",{"2":{"147":1}}],["是一种数据结构",{"2":{"986":1}}],["是一种在序列长度维度上的并行化方案",{"2":{"976":1}}],["是一种在平滑零值和正值之间转换的激活函数",{"2":{"103":1}}],["是一种被证明的技术",{"2":{"973":1}}],["是一种attention机制",{"2":{"911":1}}],["是一种时间循环神经网络",{"2":{"862":1}}],["是一种人工神经网络中常用的激励函数",{"2":{"840":1}}],["是一种前馈神经网络",{"2":{"769":1}}],["是一种专门用来处理具有类似网格结构的数据的神经网络",{"2":{"769":1}}],["是一种使用特定提示模板让llm生成句子embedding的方法",{"2":{"736":1}}],["是一种二阶的近似方法",{"2":{"498":1}}],["是一种比较好的数值解",{"2":{"498":1}}],["是一种解释词汇空间中模型组件输出激活的技术",{"2":{"479":1}}],["是一种正则化技术",{"2":{"469":1}}],["是一种引导和加速模型学习过程的方法",{"2":{"404":1}}],["是一种广泛用于机器学习和深度学习的通过给参数增加约束项来限制参数取值范围的方法",{"2":{"393":1}}],["是一种全局操作",{"2":{"276":1}}],["是一种灵活",{"2":{"267":1}}],["是一种非常普遍的现象",{"2":{"220":1}}],["是一种结合了swish和glu",{"2":{"107":1}}],["是一个固定大小的位序列",{"2":{"2062":1}}],["是一个被先贤们整理好的册子",{"2":{"2054":1}}],["是一个由vitepress构建的静态网页",{"2":{"2034":1,"2036":1}}],["是一个由token与数字组成的一个字典",{"2":{"550":1}}],["是一个跨平台的构建系统生成工具",{"2":{"1963":1}}],["是一个文本文件",{"2":{"1917":1}}],["是一个编译期整数序列",{"2":{"1912":1}}],["是一个公共基类",{"2":{"1869":1}}],["是一个存储不重复元素的集合",{"2":{"1806":1}}],["是一个键值对的集合",{"2":{"1725":1}}],["是一个有序集合",{"2":{"1724":1}}],["是一个队列",{"2":{"1723":1}}],["是一个双端队列",{"2":{"1722":1}}],["是一个双向链表",{"2":{"1720":1}}],["是一个单向链表",{"2":{"1721":1}}],["是一个动态数组",{"2":{"1719":1}}],["是一个数组",{"2":{"1705":1}}],["是一个普通的字符",{"2":{"1704":1}}],["是一个对象",{"2":{"1678":1}}],["是一个非常复杂的类",{"2":{"1678":1}}],["是一个成员函数",{"2":{"1674":1}}],["是一个指针",{"2":{"1614":1,"1705":1}}],["是一个指向字符的指针",{"2":{"1715":1}}],["是一个指向包含",{"2":{"1705":1}}],["是一个指向常量整数的常量指针",{"2":{"1614":1}}],["是一个指向常量整数的指针",{"2":{"1614":1}}],["是一个指向整数的常量指针",{"2":{"1614":1}}],["是一个指向",{"2":{"1611":1}}],["是一个已存在变量的别名",{"2":{"1612":1}}],["是一个流行的跨平台构建工具",{"2":{"1605":1}}],["是一个半自动化的orm框架",{"2":{"1479":1}}],["是一个正交矩阵",{"2":{"1343":1}}],["是一个实现各种优化算法的包",{"2":{"1219":1}}],["是一个标志",{"2":{"1117":1}}],["是一个标准的下三角矩阵",{"2":{"89":1}}],["是一个方法",{"2":{"1083":1}}],["是一个用于深层神经网络的gpu加速库",{"2":{"796":1}}],["是一个与qjqjq",{"2":{"766":1}}],["是一个很好的选择",{"2":{"1184":1}}],["是一个很复杂的问题",{"2":{"1123":1}}],["是一个很大的挑战",{"2":{"761":1}}],["是一个很抽象的词语概念",{"2":{"689":1}}],["是一个超参数",{"2":{"739":1}}],["是一个线性代数概念",{"2":{"689":1}}],["是一个二维向量",{"2":{"680":1}}],["是一个静态图",{"2":{"668":1}}],["是一个基于",{"2":{"614":1}}],["是一个具有",{"2":{"614":1}}],["是一个nn",{"2":{"450":2}}],["是一个以warmup",{"2":{"402":1}}],["是一个训练时的超参数",{"2":{"392":1}}],["是一个主观的选择",{"2":{"343":1}}],["是一个无需训练的框架",{"2":{"204":1}}],["是一个可学习的参数",{"2":{"108":1}}],["是一个结合了",{"2":{"103":1}}],["是一个全连接层",{"2":{"83":1,"702":1}}],["是一个词一个词输入的",{"2":{"57":1}}],["是一个",{"2":{"36":1,"1614":2,"1704":1}}],["是一个句子的中心",{"2":{"20":1}}],["是对内存地址的封装",{"2":{"1612":1}}],["是对多层人工神经网络进行梯度下降的算法",{"2":{"1439":1}}],["是对",{"2":{"1052":1}}],["是对每一条句子内部做归一化",{"2":{"323":1}}],["是对每个粒子单独作用的阐述",{"2":{"498":1}}],["是对每个",{"2":{"101":1}}],["是对输入向量空间的一种动态变换",{"2":{"170":1}}],["是对外提供的使用者所需的内容",{"2":{"164":1}}],["是对不同位置的",{"2":{"101":1}}],["是多头注意力的输出",{"2":{"99":1}}],["是encoder的输出",{"2":{"82":1}}],["是怎么来的",{"2":{"82":1}}],["是view的base",{"2":{"1110":1}}],["是v中的所有行基于weight矩阵第i行中的各个权重进行加权平均的结果",{"2":{"71":1}}],["是vanilla",{"2":{"19":1}}],["是分开的",{"2":{"41":1}}],["是把符号空间",{"2":{"722":1}}],["是把",{"2":{"36":1}}],["是等价的",{"2":{"34":1}}],["是三个独立的线性层",{"2":{"26":1}}],["是让所有head都共享映射矩阵",{"2":{"19":1}}],["是明显的",{"2":{"17":1}}],["是激活乘以激活",{"2":{"17":1}}],["是去除非本质信息",{"2":{"4":1}}],["是人类概念的映射",{"2":{"4":1}}],["是",{"2":{"1":1,"17":1,"83":1,"125":1,"246":1,"320":1,"325":1,"511":1,"532":1,"543":1,"702":1,"739":1,"772":1,"1433":1,"1435":1,"1611":1,"1612":1,"1650":2,"1674":1,"1678":1,"1714":1,"1734":1,"1769":1,"1781":4,"1797":1,"1803":1,"1805":1,"1807":1}}],["mkdir",{"2":{"1509":2,"1999":1}}],["mkldnn",{"2":{"1087":2}}],["md文件更新",{"2":{"2043":1}}],["mdr",{"2":{"1344":1}}],["mdwqxm",{"2":{"1343":1}}],["mdwq",{"2":{"1343":1}}],["m−n",{"2":{"1342":4}}],["m2m",{"2":{"1317":2}}],["mn",{"2":{"1443":1}}],["mnasnet1",{"2":{"1308":2}}],["mnasnet0",{"2":{"1308":2}}],["mnist",{"2":{"1215":5,"1253":1,"1255":1,"1258":1,"1259":1,"1262":1,"1263":2,"1266":1,"1269":1,"1283":2}}],["mntp",{"2":{"734":1}}],["msc并回车进入到",{"2":{"2093":1}}],["msvc",{"2":{"1605":1}}],["ms",{"2":{"1569":1}}],["msgs",{"2":{"1214":1}}],["mseloss",{"2":{"1202":1,"1205":1,"1392":1}}],["msort",{"2":{"1087":1}}],["m0=0",{"2":{"1192":1,"1193":1}}],["m0=0v",{"2":{"1191":1}}],["mvapich",{"2":{"1569":1}}],["mvc",{"0":{"1496":1}}],["mvlgamma",{"2":{"1087":2}}],["mv",{"2":{"1087":1,"1510":2}}],["mfu",{"2":{"977":1}}],["m^",{"2":{"970":1,"971":1}}],["m​d​​",{"2":{"1344":1}}],["m​d​​w​q​​x​m​​",{"2":{"1343":1}}],["m​d​​w​q",{"2":{"1343":1}}],["m​t+1​​=β​1​​m​t​​+",{"2":{"1192":1,"1193":1}}],["m​t+1​​=γm​t​​+​√​v​t+1​​+ϵ​​​​​η​t​​​​∇l",{"2":{"1191":1}}],["m​t​r​​​​",{"2":{"944":1}}],["m​0​​=0",{"2":{"1191":1,"1192":1,"1193":1}}],["m​i​​l​i​​",{"2":{"967":1}}],["m​ij​​=rowmax",{"2":{"944":1}}],["m​1​​",{"2":{"944":1}}],["m1",{"2":{"944":1}}],["m4d",{"2":{"944":1}}],["m≠n时",{"2":{"883":1}}],["mm",{"2":{"805":1,"1087":1,"1102":1,"1270":2,"1642":1}}],["mm是用于两个二维矩阵乘法的函数",{"2":{"805":1}}],["mmt16",{"2":{"370":1}}],["mtn",{"2":{"1283":1}}],["mt+1=β1mt+",{"2":{"1192":1,"1193":1}}],["mt+1=γmt+ηtvt+1+ϵ∇l",{"2":{"1191":1}}],["mtrm",{"2":{"944":1}}],["mt",{"2":{"906":1,"1082":1}}],["mteb榜单里nv",{"2":{"735":1}}],["mtp从两个来源来收集数据",{"2":{"725":1}}],["mtp",{"2":{"725":3,"726":2}}],["m|",{"2":{"571":1}}],["mcculloch",{"2":{"1459":1}}],["mcbal",{"2":{"513":1}}],["mcl表示训练期间的最大上下文长度",{"2":{"335":1}}],["mpiexec",{"2":{"1589":1}}],["mpirun",{"2":{"1589":2,"1594":1}}],["mpif90",{"2":{"1589":1}}],["mpi环境",{"0":{"1585":1},"1":{"1586":1,"1587":1,"1588":1,"1589":1}}],["mpi还允许用户定义结构化数据类型",{"2":{"1576":1}}],["mpi支持多种基础数据类型",{"2":{"1576":1}}],["mpi支持两种基本的通信模式",{"2":{"1574":1}}],["mpi传递消息时需要明确数据类型和标签",{"2":{"1576":1}}],["mpi标准中包含了一系列函数",{"2":{"1575":1}}],["mpi提供多种集体通信模式",{"2":{"1573":1}}],["mpi使用消息传递",{"2":{"1573":1}}],["mpi使用这个id来标识进程并确定数据传输的目标",{"2":{"1573":1}}],["mpi使用分布式内存模型",{"2":{"1572":1}}],["mpi程序通常启动多个进程",{"2":{"1573":1}}],["mpi适用于需要跨多个节点运行的任务",{"2":{"1572":1}}],["mpi的优势和劣势",{"0":{"1579":1}}],["mpi的数据类型和消息标签",{"0":{"1576":1}}],["mpi的基本使用",{"0":{"1590":1},"1":{"1591":1,"1592":1,"1593":1,"1594":1}}],["mpi的基本函数",{"0":{"1575":1}}],["mpi的基本原理",{"0":{"1570":1},"1":{"1571":1,"1572":1,"1573":1,"1574":1,"1575":1,"1576":1,"1577":1,"1578":1,"1579":1,"1580":1}}],["mpi的通信模式",{"0":{"1574":1}}],["mpi的架构",{"0":{"1572":1}}],["mpi中的常见通信模式",{"0":{"1578":1}}],["mpi中的默认通信域是mpi",{"2":{"1577":1}}],["mpi中",{"2":{"1569":1}}],["mpi是一个标准化的接口",{"2":{"1569":1}}],["mpi实现",{"2":{"1566":1}}],["mpic++",{"2":{"1589":1}}],["mpich",{"2":{"1561":1,"1569":1,"1585":1,"1589":2}}],["mpicc",{"2":{"1533":1,"1589":3}}],["mpi",{"2":{"1561":2,"1568":1,"1569":6,"1573":4,"1575":11,"1576":1,"1589":24,"1590":15,"1594":19}}],["mpi并行计算",{"0":{"1561":1},"1":{"1562":1,"1563":1,"1564":1,"1565":1,"1566":1,"1567":1,"1568":1,"1569":1,"1570":1,"1571":1,"1572":1,"1573":1,"1574":1,"1575":1,"1576":1,"1577":1,"1578":1,"1579":1,"1580":1,"1581":1,"1582":1,"1583":1,"1584":1,"1585":1,"1586":1,"1587":1,"1588":1,"1589":1,"1590":1,"1591":1,"1592":1,"1593":1,"1594":1,"1595":1,"1596":1,"1597":1,"1598":1},"2":{"1599":1,"2043":1}}],["mps",{"2":{"1087":1,"1215":6}}],["mp3音频的编解码",{"2":{"637":1}}],["mpds",{"2":{"498":1}}],["mp",{"2":{"422":1}}],["mylib",{"2":{"1974":3,"1975":1,"1984":1,"1985":2}}],["myproject",{"2":{"1972":1}}],["myprogram",{"2":{"1917":1}}],["myvar",{"2":{"1926":8}}],["myvector",{"2":{"1615":2}}],["myglobal",{"2":{"1923":3}}],["mytarget",{"2":{"1994":1}}],["mytype",{"2":{"1931":7}}],["mytuple",{"2":{"1912":2}}],["mytest",{"2":{"1481":1,"1986":1}}],["myfriendclass",{"2":{"1770":1}}],["myfriendfunction",{"2":{"1770":1}}],["mystring",{"2":{"1887":16}}],["mystack",{"2":{"1726":3}}],["mysql基础",{"2":{"1492":1}}],["mysql",{"2":{"1474":2,"1481":2,"1492":1}}],["mycontainer",{"2":{"1701":5,"1925":5}}],["myclass",{"2":{"1638":3,"1639":5,"1640":5,"1641":5,"1653":3,"1694":10,"1700":1,"1770":2,"1923":2}}],["myobject",{"2":{"1700":1}}],["myaccount",{"2":{"1677":5}}],["myarray",{"2":{"1634":2,"1700":7,"1706":5}}],["myheader",{"2":{"1628":4}}],["myid",{"2":{"1590":3}}],["mydata",{"2":{"1696":3}}],["mydataset",{"2":{"1250":1,"1295":1,"1296":1}}],["mydog",{"2":{"1655":3,"1674":6}}],["mydir",{"2":{"1535":1}}],["mybatis笔记",{"2":{"2043":1}}],["mybatisutils",{"2":{"1481":2,"1485":1,"1486":1,"1487":1,"1488":1}}],["mybatis第一个程序",{"0":{"1480":1},"1":{"1481":1,"1482":1}}],["mybatis不会对应用程序或者数据库的现有设计强加任何影响",{"2":{"1479":1}}],["mybatis的优点",{"2":{"1479":1}}],["mybatis就是帮助程序猿将数据存入数据库中",{"2":{"1479":1}}],["mybatis官方文档",{"2":{"1476":1}}],["mybatis",{"0":{"1494":1},"2":{"1476":8,"1479":1,"1481":9,"1501":1}}],["mybatis简介",{"0":{"1475":1},"1":{"1476":1,"1477":1,"1478":1,"1479":1}}],["mybatis框架",{"0":{"1474":1},"1":{"1475":1,"1476":1,"1477":1,"1478":1,"1479":1,"1480":1,"1481":1,"1482":1,"1483":1,"1484":1,"1485":1,"1486":1,"1487":1,"1488":1,"1489":1}}],["mymodel",{"2":{"1213":3,"1295":2,"1296":1,"1297":1,"1298":1}}],["mymodule",{"2":{"1211":2,"1212":2}}],["my",{"2":{"387":1,"722":1,"1254":1,"1509":5,"1589":6,"1923":4,"1930":4,"1980":2,"1986":2,"1991":1}}],["mädchen",{"2":{"370":1}}],["männer",{"2":{"370":3,"557":1}}],["mqa多头共用k",{"2":{"652":1}}],["mqa",{"0":{"935":1,"951":1},"2":{"205":1,"935":6,"938":2,"951":6,"954":2,"956":1,"971":1,"976":1}}],["mqa和gqa",{"0":{"652":1},"2":{"47":1,"292":1}}],["m=5",{"2":{"143":1}}],["mla仅需缓存",{"2":{"957":1}}],["mla的性能优于mha",{"2":{"956":1}}],["mla",{"0":{"955":1,"956":1,"957":1},"1":{"956":1,"957":1},"2":{"956":1}}],["mlm",{"2":{"721":1,"1315":6,"1317":1}}],["mlm和nsp",{"2":{"671":1}}],["mlsys2024",{"2":{"361":1}}],["mli⋅vlimil⋅vilm",{"2":{"128":1}}],["mli=f",{"2":{"128":1}}],["mlperf1",{"2":{"945":1,"965":1}}],["mlp中的神经网络层相对应",{"2":{"785":1}}],["mlp就是常规mlp",{"2":{"735":1}}],["mlp层",{"2":{"735":1}}],["mlp层后的激活",{"2":{"137":1}}],["mlp可以看作是计算在受力作用下的按照运动方程的运动轨迹",{"2":{"509":1}}],["mlp可以通过这个分量将残差流引导向目标答案的unembedding",{"2":{"122":1}}],["mlp的第一个矩阵ff1ff1ff",{"2":{"485":1}}],["mlp的输出会擦除或放大残留流中单个磁头的输出",{"2":{"122":1}}],["mlp的输出中",{"2":{"122":1}}],["mlp模块由两个紧密连接的矩阵",{"2":{"485":1}}],["mlp对self",{"2":{"446":1}}],["mlp虽然理论上没有距离长依赖问题",{"2":{"242":1}}],["mlp发挥了主要的中介作用",{"2":{"145":1}}],["mlps",{"2":{"136":1,"155":2}}],["mlp17层则是结合之前组件提供的信息",{"2":{"130":1}}],["mlp17",{"2":{"130":3}}],["mlp14类似的点代表着第14层的mlp层",{"2":{"130":1}}],["mlp将",{"2":{"122":1}}],["mlp是注意力头输出的",{"2":{"122":1}}],["mlp在这个过程中是怎么工作的",{"2":{"122":1}}],["mlp",{"0":{"356":2,"1102":1},"2":{"110":3,"130":1,"136":1,"137":3,"145":5,"148":5,"232":1,"242":1,"293":2,"352":1,"354":2,"356":1,"357":1,"446":1,"461":1,"485":7,"976":1,"994":1,"1457":1,"1464":1}}],["much",{"2":{"2079":1}}],["mutex",{"2":{"1894":1,"1913":1}}],["must",{"2":{"557":1,"702":1,"1216":1,"1218":1}}],["music",{"2":{"122":4}}],["mult",{"2":{"1244":2}}],["multilayer",{"2":{"1457":1,"1464":2}}],["multisteplr",{"0":{"1236":1},"2":{"1231":1,"1236":1}}],["multiscale",{"2":{"47":1}}],["multinomial",{"2":{"1087":1}}],["multi30k得到三个iterator",{"2":{"557":1}}],["multi30k",{"2":{"370":2,"372":3,"375":1,"557":2}}],["multi30k是flickr30k数据集",{"2":{"370":1}}],["multiheaddiffattn",{"2":{"503":1}}],["multihead",{"0":{"289":1},"2":{"289":1,"292":2,"510":1,"698":4,"927":2}}],["multiheadattention",{"2":{"119":1,"330":2,"1216":3,"1217":3,"1218":5}}],["multiheadedattention类中",{"2":{"199":1}}],["multiheadedattention",{"2":{"23":2,"37":1,"449":2,"538":1,"703":1}}],["multiheadedattention的初始化代码如下",{"2":{"23":1}}],["multiprocessing",{"2":{"1306":1,"1307":2,"1308":2}}],["multiplicativelr",{"0":{"1234":1},"2":{"1234":1}}],["multiplier=args",{"2":{"201":1}}],["multiplier",{"2":{"114":4,"201":1}}],["multiply",{"2":{"395":3,"1087":4,"1217":1,"1729":8,"1905":2,"1999":3}}],["multiple",{"0":{"1306":1,"1307":1},"2":{"114":4,"201":2,"601":1,"638":1,"983":1,"1216":1,"1218":1}}],["multipack",{"2":{"90":3}}],["multi",{"0":{"290":1,"649":1,"650":1,"925":1,"930":1,"931":1,"932":1,"935":1,"951":1,"955":1,"1305":1},"1":{"926":1,"927":1,"928":1,"929":1,"933":1,"934":1,"956":1,"957":1,"1306":1,"1307":1},"2":{"5":1,"7":1,"13":1,"19":1,"20":1,"35":1,"38":1,"39":2,"41":3,"42":1,"43":2,"47":5,"71":2,"119":2,"330":1,"420":1,"445":2,"449":2,"498":3,"513":1,"517":1,"525":1,"535":1,"538":1,"698":1,"928":1,"935":2,"938":2,"951":2,"954":2,"985":1,"1305":1,"1308":2,"1464":1}}],["mul",{"2":{"395":2,"829":1,"1085":1,"1087":2,"1097":1,"1099":1}}],["mu",{"2":{"313":2,"343":2,"1361":6}}],["mu和w∑w∑w",{"2":{"209":1}}],["mh",{"2":{"1082":1}}],["mha和ffn",{"2":{"517":1}}],["mha可以使网络在进行预测时对输入句子的不同位置的分配不同的注意力",{"2":{"517":1}}],["mha是对输入序列自身进行的注意力计算",{"2":{"517":1}}],["mha是transformer中最靓的仔",{"2":{"120":1}}],["mha考虑单词在不同位置的语义和依赖关系",{"2":{"120":1}}],["mha允许模型在不同的表示子空间中学习信息",{"2":{"101":1}}],["mha代码",{"2":{"47":1}}],["mha硬把它们放到一个注意力头的qkov里",{"2":{"45":1}}],["mha",{"0":{"206":1},"2":{"43":1,"44":1,"47":2,"157":1,"498":1,"503":1,"517":1,"520":2,"530":4,"935":2,"951":2,"956":1}}],["mhsa",{"2":{"1":1,"97":1,"144":1}}],["migrate",{"2":{"2070":1}}],["mij=rowmax",{"2":{"944":1}}],["mistakes",{"2":{"2075":1}}],["mistral",{"2":{"735":1}}],["misra",{"2":{"1960":1}}],["missing",{"2":{"1214":1}}],["misc",{"2":{"1195":1}}],["mish函数在曲线上几乎所有点上的平滑度都很高",{"2":{"846":1}}],["mishactivation",{"2":{"110":1}}],["mish",{"0":{"846":1},"2":{"110":1,"156":1,"838":1,"846":6}}],["mixup就是将两张图像进行mix操作",{"2":{"1015":1}}],["mixtral",{"2":{"638":1}}],["mixtral指出",{"2":{"560":1}}],["mixture",{"2":{"42":3,"47":1,"737":1,"740":2}}],["mixer指定为注意力或spatial",{"2":{"446":1}}],["mixer的具体设计",{"2":{"446":1}}],["mixer",{"2":{"446":1}}],["microsoft",{"2":{"387":1,"503":1,"1569":1}}],["mit团队用神经元和星形胶质细胞来构建",{"2":{"513":1}}],["mit",{"2":{"370":1}}],["mipsasmzwei",{"2":{"370":1}}],["milestones",{"2":{"1236":1}}],["milestones=",{"2":{"1231":1,"1236":1,"1247":1}}],["milim",{"2":{"967":1}}],["milk",{"2":{"261":3}}],["milora",{"2":{"233":2}}],["middle",{"2":{"1099":1,"1821":1,"1839":1}}],["mid",{"2":{"122":1,"240":1,"903":2,"908":7,"1099":1,"1322":2,"1377":6}}],["mingw",{"2":{"1605":1}}],["ming6383",{"2":{"361":1}}],["minus",{"2":{"1350":7}}],["min=min",{"2":{"1243":1}}],["minlength",{"2":{"1087":1}}],["mint",{"2":{"592":1}}],["minbpe",{"0":{"589":1},"1":{"590":1,"591":1,"592":1},"2":{"591":2,"638":1}}],["minist",{"2":{"1215":1}}],["minibatch",{"2":{"402":2}}],["mini",{"0":{"1027":1},"2":{"316":1,"322":2,"1027":7,"1308":1}}],["minimum",{"0":{"1971":1},"2":{"160":1,"557":1,"1087":1,"1966":1,"1971":1,"1980":1,"1999":1}}],["minimax",{"0":{"214":1},"1":{"215":1,"216":1,"217":1},"2":{"157":1,"214":2,"215":1,"216":1,"217":1,"233":2,"569":1}}],["minimal",{"2":{"111":1,"768":1}}],["minimize",{"2":{"151":1}}],["minh",{"2":{"292":1}}],["minor",{"2":{"233":1}}],["min",{"2":{"76":1,"110":1,"402":2,"557":5,"592":2,"833":2,"944":1,"1087":23,"1243":3,"1244":2,"1245":1,"1339":2}}],["mimim",{"2":{"19":1}}],["m",{"0":{"883":1,"1459":1},"2":{"39":3,"82":4,"194":1,"241":1,"313":1,"373":2,"485":1,"510":2,"529":3,"530":4,"533":3,"538":2,"613":4,"801":4,"802":3,"804":4,"807":3,"809":3,"810":4,"814":3,"815":3,"816":7,"835":1,"839":3,"840":4,"841":1,"842":3,"843":2,"844":1,"845":1,"846":1,"847":1,"883":1,"912":1,"943":22,"944":10,"945":1,"961":22,"963":1,"965":1,"970":1,"1191":4,"1192":3,"1193":3,"1215":1,"1281":1,"1308":2,"1342":11,"1343":11,"1344":1,"1398":2,"1459":1,"1807":1,"1887":43}}],["making",{"2":{"2073":1,"2077":1,"2087":1}}],["makes",{"2":{"2075":1,"2076":1}}],["makesound",{"2":{"1690":3,"1691":4}}],["makemigrations",{"2":{"2070":1}}],["makemessages",{"2":{"2070":1}}],["makeavailable",{"2":{"1990":1}}],["makefiles",{"2":{"1999":1}}],["makefilecxx",{"2":{"1917":2}}],["makefilecalculator",{"2":{"1917":1}}],["makefiletarget",{"2":{"1917":1}}],["makefile",{"0":{"1917":1},"2":{"1917":6,"1918":3,"1963":1}}],["makefile工程管理",{"0":{"1915":1},"1":{"1916":1,"1917":1}}],["make",{"0":{"1911":1},"2":{"74":3,"76":2,"79":1,"83":1,"372":1,"380":2,"382":1,"395":2,"399":1,"423":1,"424":1,"428":2,"448":1,"449":2,"472":1,"529":1,"688":1,"703":2,"1086":1,"1283":2,"1350":1,"1695":4,"1891":3,"1902":1,"1904":1,"1907":2,"1911":6,"1912":4,"1913":2,"1917":2,"1921":1,"1968":1,"1996":1,"1999":1,"2079":1}}],["mammal",{"2":{"1659":3,"1869":6}}],["maven静态资源过滤问题",{"2":{"1482":1}}],["maven",{"2":{"1474":2}}],["maybe",{"2":{"1214":1}}],["may",{"2":{"591":1}}],["mariet",{"2":{"1194":1}}],["mark",{"2":{"399":1,"1616":1}}],["marsggbo",{"2":{"292":1}}],["mainres",{"2":{"1891":5}}],["main",{"2":{"387":1,"423":4,"429":1,"1086":1,"1215":3,"1216":1,"1218":1,"1281":1,"1303":3,"1304":2,"1306":1,"1307":2,"1308":1,"1309":1,"1398":1,"1436":1,"1440":1,"1482":2,"1590":1,"1594":1,"1606":1,"1607":1,"1608":1,"1616":1,"1619":1,"1620":1,"1621":2,"1623":1,"1624":1,"1625":2,"1633":1,"1634":1,"1639":1,"1640":1,"1641":1,"1645":1,"1646":1,"1647":1,"1648":1,"1649":2,"1650":1,"1653":1,"1654":1,"1655":1,"1659":1,"1660":1,"1661":1,"1662":1,"1663":1,"1664":1,"1665":1,"1667":3,"1668":1,"1670":1,"1671":1,"1672":1,"1673":2,"1674":1,"1675":1,"1676":1,"1677":1,"1678":1,"1680":1,"1683":4,"1684":2,"1685":2,"1687":2,"1688":2,"1690":1,"1691":1,"1693":3,"1694":2,"1695":4,"1698":2,"1699":1,"1700":2,"1701":2,"1702":1,"1704":1,"1705":1,"1706":2,"1707":1,"1708":1,"1709":1,"1710":2,"1712":2,"1713":4,"1714":1,"1715":2,"1718":1,"1719":6,"1720":4,"1721":4,"1722":4,"1723":1,"1724":3,"1725":4,"1726":1,"1728":3,"1729":8,"1736":1,"1737":1,"1738":1,"1739":1,"1741":1,"1742":1,"1743":1,"1744":1,"1746":1,"1747":1,"1749":1,"1750":1,"1751":1,"1752":1,"1754":1,"1755":1,"1756":1,"1761":1,"1762":1,"1763":1,"1766":1,"1772":1,"1774":1,"1778":1,"1779":1,"1784":1,"1788":1,"1789":1,"1791":1,"1792":1,"1797":1,"1799":1,"1800":1,"1801":1,"1802":1,"1803":1,"1805":1,"1806":1,"1807":1,"1811":1,"1813":1,"1814":1,"1816":1,"1817":1,"1820":2,"1821":1,"1824":1,"1825":1,"1829":1,"1831":1,"1832":1,"1834":1,"1835":1,"1838":2,"1839":1,"1842":1,"1843":1,"1849":1,"1853":1,"1857":1,"1861":1,"1866":1,"1867":1,"1868":1,"1869":1,"1873":1,"1874":1,"1883":1,"1887":1,"1891":1,"1897":1,"1902":2,"1905":1,"1906":1,"1907":1,"1908":1,"1909":1,"1910":1,"1911":1,"1912":1,"1914":1,"1917":1,"1918":1,"1921":1,"1922":1,"1923":2,"1924":1,"1925":1,"1926":1,"1927":1,"1928":1,"1929":1,"1930":2,"1931":1,"1933":1,"1966":1,"1973":1,"1980":1,"1999":1,"2003":1,"2004":1,"2005":1,"2006":5,"2007":1,"2008":1,"2059":5,"2060":2,"2061":1,"2062":3,"2063":1}}],["manipulators",{"2":{"1817":1,"1835":1}}],["manage",{"2":{"2068":1,"2069":2,"2070":2}}],["manager",{"2":{"1657":1}}],["management",{"2":{"513":1}}],["manually",{"2":{"2076":1}}],["manual",{"2":{"1098":3,"1205":1,"1215":1,"1308":1}}],["man",{"2":{"370":1,"557":1,"685":1,"689":1}}],["many",{"2":{"370":1,"450":1,"557":1,"591":1,"703":1,"945":1,"965":1,"1215":1,"2079":1}}],["mann",{"2":{"370":1}}],["map>",{"2":{"1725":4,"1807":2,"1933":1}}],["map的",{"2":{"1485":1}}],["mapper>",{"2":{"1481":1}}],["mapper",{"2":{"1481":8,"1485":3,"1486":2,"1487":3,"1488":2,"1489":2}}],["mappers>",{"2":{"1481":2}}],["mappings",{"2":{"302":1}}],["mapping",{"2":{"301":1,"688":1,"1214":1,"1479":1}}],["mapping翻译成中文就是恒等映射",{"2":{"299":1}}],["map2",{"2":{"1087":1}}],["map之间是有非常强的相关性的",{"2":{"337":1}}],["map",{"0":{"1725":1,"1807":1},"2":{"337":1,"375":8,"499":1,"591":1,"770":1,"773":1,"1083":2,"1087":1,"1255":1,"1263":1,"1485":7,"1725":9,"1795":1,"1807":11,"1933":3}}],["map是x∈rn×c×h×w𝑥∈𝑅𝑁×𝐶×𝐻×𝑊𝑥∈𝑅^",{"2":{"315":1}}],["maps",{"2":{"212":1}}],["malloc",{"2":{"1668":2}}],["males",{"2":{"370":1,"557":1}}],["mal",{"2":{"229":1,"231":2}}],["magic",{"2":{"1728":1}}],["magikarp",{"2":{"562":1}}],["magnitude",{"2":{"680":1}}],["mag",{"2":{"229":1,"231":2}}],["mac为子",{"2":{"1594":1}}],["mac安装虚拟机linux系统",{"2":{"1582":1}}],["macos系统略有区别",{"2":{"2089":1}}],["macos安装配置openmpi",{"2":{"1594":1}}],["macos",{"0":{"1968":1},"2":{"1215":1,"1594":5,"1605":2}}],["macaron结构是在",{"2":{"498":1}}],["mac",{"2":{"229":1,"231":2,"1582":1,"1594":3,"1605":1}}],["machine",{"2":{"123":1,"139":1,"175":1,"237":1,"257":1,"282":1,"284":1,"285":1,"287":1,"292":7,"429":1,"490":1,"574":1,"605":1,"638":4,"698":1,"906":1,"907":2,"1067":1,"1435":1,"2079":1}}],["machines",{"2":{"104":1,"156":1}}],["master",{"2":{"422":2,"503":1,"1578":1}}],["mas",{"2":{"382":1,"538":1}}],["massive",{"2":{"175":1,"698":1}}],["mask步骤",{"2":{"933":1}}],["mask来调用attention",{"2":{"538":1}}],["mask来调用attenion",{"2":{"538":1}}],["mask来调用到decoder",{"2":{"538":1}}],["mask调用encoder类的forward",{"2":{"538":1}}],["mask在此处的作用是遮挡填充符号",{"2":{"533":1}}],["mask一同传给解码函数decode",{"2":{"450":1}}],["mask传入编码函数encode",{"2":{"450":1}}],["mask则为",{"2":{"399":1}}],["mask则比较复杂",{"2":{"74":1,"382":1}}],["mask需要一个矩阵",{"2":{"382":1}}],["mask类似",{"2":{"380":1}}],["mask类型",{"2":{"77":1}}],["mask有两种",{"2":{"198":2}}],["mask也会对应得发生改变",{"2":{"88":1}}],["masking实现机理",{"0":{"651":1}}],["masking",{"2":{"83":1,"343":1,"532":1,"733":1,"970":1}}],["mask形状是",{"2":{"79":1}}],["mask限定之下的sequence",{"2":{"79":2}}],["mask就是融合掩码",{"2":{"79":1}}],["mask就是padding",{"2":{"79":1}}],["mask中完成",{"2":{"77":1}}],["mask中的每个元素都会被1减",{"2":{"74":1}}],["mask是源序列和目标序列的掩码",{"2":{"528":1}}],["mask是",{"2":{"84":1,"382":1}}],["mask是包含了padding",{"2":{"79":1}}],["mask是可以同时存在的",{"2":{"77":1}}],["mask是合并在一起施加的",{"2":{"75":1}}],["mask进行与操作",{"2":{"74":1}}],["mask略有不同",{"2":{"74":1,"382":1}}],["mask与src",{"2":{"74":1,"382":1}}],["mask实现中解析",{"2":{"74":1}}],["mask吗",{"2":{"72":1}}],["mask操作只针对自回归模型的训练过程和推理时的",{"2":{"69":1}}],["mask都在一起应用",{"2":{"67":1}}],["mask返回的是一个布尔矩阵",{"2":{"66":1}}],["mask要和注意力分数进行掩码计算",{"2":{"66":1,"382":1}}],["mask如何实现",{"2":{"60":1}}],["mask的时候",{"2":{"665":1}}],["mask的时候想到",{"2":{"665":1}}],["mask的每一个时刻都能attendto所有时刻",{"2":{"382":1}}],["mask的shape是",{"2":{"382":1}}],["mask的结合",{"2":{"382":1}}],["mask的作用就是用到上面所说的",{"2":{"82":1}}],["mask的作用是防止预测时看到未来的单词",{"2":{"533":1}}],["mask的作用是屏蔽未来信息",{"2":{"59":1}}],["mask的作用是避免填充符号带来的偏差",{"2":{"59":1}}],["mask的作用是通过构建下三角",{"2":{"50":1}}],["mask的作用是将这些填充符号对应位置的注意力分数设为一个很小的值",{"2":{"50":1}}],["mask的形状是",{"2":{"79":1,"382":1}}],["mask的融合mask",{"2":{"79":1}}],["mask的核心逻辑是",{"2":{"69":1}}],["mask的语句比较简单",{"2":{"66":1,"74":1,"382":1}}],["mask的",{"2":{"50":1}}],["mask叫做casual",{"2":{"50":1}}],["mask和key",{"2":{"84":1}}],["mask和tgt",{"2":{"79":2,"82":1,"85":1,"528":1}}],["mask和sequence",{"2":{"50":1,"67":1,"79":1,"382":1}}],["mask和x的维度都是3",{"2":{"36":1}}],["masks",{"2":{"47":1,"91":1,"95":1}}],["masked语言建模",{"2":{"721":1}}],["masked",{"2":{"39":1,"67":1,"71":4,"72":1,"76":1,"79":1,"82":1,"84":2,"198":1,"199":2,"201":1,"394":1,"450":1,"510":1,"525":1,"528":1,"530":2,"698":1,"703":1,"732":2,"734":3,"740":3,"933":1,"1086":1,"1087":8,"1216":1,"1217":1,"1218":1,"1312":2,"1315":1}}],["mask=mask",{"2":{"36":1}}],["mask=none",{"2":{"36":1,"67":1,"199":1,"394":1,"503":1,"933":1,"1216":3,"1218":6}}],["mask增加一个维度",{"2":{"36":1}}],["mask",{"0":{"60":1,"68":1,"89":1,"650":1,"932":1,"933":1,"934":1},"1":{"61":1,"62":1,"63":1,"64":1,"65":1,"66":1,"67":1,"69":1,"70":1,"71":1,"72":1,"73":1,"74":1,"75":1,"76":1,"933":1,"934":1},"2":{"36":4,"38":4,"39":6,"49":3,"50":4,"55":1,"59":2,"66":3,"67":2,"69":1,"70":2,"72":3,"74":21,"76":18,"77":6,"78":16,"79":15,"80":1,"82":23,"83":18,"84":32,"85":1,"89":7,"90":3,"198":5,"199":5,"201":12,"343":7,"344":6,"380":15,"381":2,"382":15,"385":4,"394":2,"399":5,"409":1,"410":2,"424":2,"428":4,"450":14,"451":4,"472":4,"503":3,"522":4,"523":3,"528":5,"529":16,"532":6,"533":8,"538":4,"650":4,"703":11,"734":2,"735":1,"932":4,"933":3,"934":2,"964":1,"1086":3,"1087":9,"1216":6,"1217":7,"1218":8,"1328":9,"1329":4,"1330":11,"1331":2}}],["matched",{"2":{"1254":2}}],["matches",{"2":{"590":1,"1086":1}}],["matmal",{"2":{"1087":1}}],["matmul等算子如何设置weight和act",{"2":{"1107":1}}],["matmul是通用的矩阵乘法函数",{"2":{"805":1}}],["matmul对应上图的标号5",{"2":{"199":1}}],["matmul",{"0":{"805":1},"2":{"67":2,"198":1,"199":3,"200":6,"201":2,"361":1,"394":2,"503":2,"700":1,"785":1,"805":1,"933":2,"1085":1,"1086":1,"1087":2,"1092":1,"1095":2,"1096":1,"1098":3,"1099":1,"1102":1,"1109":1,"1216":2,"1218":2,"1345":2,"1398":2}}],["mat2",{"2":{"805":4,"1086":2,"1087":4}}],["mat1",{"2":{"805":2,"1086":2,"1087":1}}],["matrices",{"2":{"764":1}}],["matrix",{"2":{"201":1,"674":1,"745":1,"923":1,"1082":1,"1087":2,"1217":1,"1398":2,"1634":2,"1705":24,"1797":2}}],["mata",{"2":{"658":1}}],["matthias",{"2":{"513":1}}],["mathrm",{"2":{"343":2,"640":2,"840":1}}],["mathcal",{"2":{"301":6,"343":8,"499":1,"1188":1,"1189":1,"1190":2,"1191":2,"1192":2,"1193":3}}],["mathematical",{"2":{"233":1,"499":1,"513":2}}],["mathbb",{"2":{"161":7,"499":4,"927":4,"941":3,"960":3}}],["mathbf",{"2":{"71":25,"161":18,"178":2,"192":5,"343":5,"941":6,"960":6,"1360":4}}],["math",{"2":{"67":1,"199":2,"201":1,"361":1,"394":1,"701":1,"933":1,"1345":1,"1825":9,"1843":9,"2076":1}}],["max=total",{"2":{"1243":1}}],["maxnorm",{"2":{"1087":2}}],["maximum",{"2":{"1087":1,"1330":1}}],["maxpool2d",{"2":{"814":2}}],["max",{"0":{"814":1,"1156":1},"2":{"65":2,"71":3,"83":6,"89":1,"90":2,"99":3,"104":1,"110":1,"191":13,"201":5,"326":1,"364":2,"372":1,"375":2,"380":2,"384":3,"423":2,"424":3,"428":1,"451":1,"472":1,"520":1,"529":1,"530":1,"557":4,"558":1,"572":5,"592":1,"694":1,"702":7,"723":2,"749":1,"833":2,"841":3,"943":1,"961":1,"971":1,"1087":23,"1155":1,"1156":1,"1159":1,"1215":2,"1217":2,"1218":7,"1240":1,"1241":1,"1242":1,"1243":2,"1244":1,"1257":1,"1295":1,"1332":1,"1339":2,"1345":1,"1360":1,"1613":1,"1630":2,"1664":1,"1665":14,"1698":8,"1709":3,"1789":2,"1814":1,"1832":1}}],["made",{"2":{"38":1,"39":1,"82":1,"344":1,"523":1,"533":1,"1254":1}}],["meow",{"2":{"1675":3,"1690":1,"1691":1,"1866":2}}],["messages",{"2":{"1539":1}}],["message",{"2":{"1308":1,"1573":1,"1708":2,"1729":2}}],["median",{"2":{"1087":6}}],["medium",{"2":{"429":2,"768":1}}],["meets",{"2":{"711":1,"740":1}}],["member的api",{"2":{"2070":1}}],["member",{"2":{"1778":1,"2070":3}}],["members",{"2":{"1214":2,"1784":1,"1863":3}}],["membership",{"2":{"592":1}}],["memo",{"2":{"1083":1,"1214":1}}],["memoy",{"2":{"538":1}}],["memorize",{"2":{"226":1,"233":1,"428":2}}],["memorization",{"2":{"156":1,"437":1}}],["memories",{"2":{"126":1,"127":1,"156":2,"306":1}}],["memory>",{"2":{"1695":3,"1891":1,"1902":1,"1907":1,"1911":2}}],["memoryleaker",{"2":{"1671":2}}],["memory的次数",{"2":{"972":1}}],["memory会被用作参数传递给decode",{"2":{"538":1}}],["memory相同",{"2":{"125":1}}],["memoryformer通过多表分块和向量分段的方式来控制哈希表的存储规模",{"2":{"153":1}}],["memoryformer",{"0":{"151":1},"1":{"152":1,"153":1},"2":{"96":1,"151":2,"152":1,"153":3,"156":2}}],["memory",{"0":{"154":1,"862":1},"1":{"863":1,"864":1,"865":1,"866":1,"867":1,"868":1,"869":1,"870":1,"871":1},"2":{"17":3,"39":2,"78":2,"82":5,"83":5,"84":3,"96":1,"125":3,"143":1,"145":1,"151":1,"153":3,"154":3,"156":4,"228":1,"229":5,"230":1,"231":3,"233":2,"287":1,"292":2,"343":2,"395":1,"428":2,"450":3,"472":2,"488":1,"497":1,"528":1,"529":8,"532":3,"533":3,"538":1,"620":1,"703":2,"723":1,"820":1,"862":1,"967":1,"970":1,"974":1,"981":2,"1083":2,"1087":21,"1214":1,"1215":1,"1647":1,"1695":1}}],["mec",{"2":{"490":1}}],["mechanics",{"2":{"1067":1,"1088":1}}],["mechanistic",{"2":{"475":1,"513":1}}],["mechanisms",{"2":{"122":1,"156":1,"263":1,"292":1,"437":1}}],["mechanism",{"2":{"122":1,"230":1,"499":1}}],["mech",{"2":{"47":1}}],["mensch",{"2":{"638":1}}],["men",{"2":{"370":2,"557":1}}],["mehrere",{"2":{"370":1}}],["measure",{"2":{"499":1}}],["measures",{"2":{"499":1}}],["measured",{"2":{"160":1}}],["means",{"2":{"591":1,"731":1,"736":2,"827":1}}],["mean=0",{"2":{"503":4}}],["mean的形状为",{"2":{"343":1}}],["mean",{"2":{"313":2,"343":4,"346":3,"361":2,"499":1,"731":1,"735":3,"807":10,"808":4,"809":3,"810":3,"812":2,"833":3,"1047":2,"1087":5,"1211":20}}],["meaning",{"2":{"156":1,"2075":1}}],["met",{"2":{"1078":1}}],["methods",{"2":{"1046":1,"1067":1}}],["method",{"2":{"495":1,"497":1,"558":1,"700":1,"1054":1,"1059":1,"1082":1,"1100":1,"1254":2,"1456":1,"1849":1}}],["method=",{"2":{"423":1}}],["method=lambda",{"2":{"8":4,"114":3,"201":6}}],["metric",{"2":{"230":1}}],["metavar=",{"2":{"1215":7}}],["metadata还可以包含assign",{"2":{"1214":1}}],["metadata为空",{"2":{"1214":1}}],["metadata参数",{"2":{"1214":1}}],["metadata",{"2":{"1214":3}}],["metaclass",{"2":{"1082":2}}],["metaclass=",{"2":{"1082":1}}],["meta提出大概念模型lcm",{"2":{"638":1}}],["meta最新研究",{"2":{"638":1}}],["metaunitech",{"2":{"513":1}}],["metaformer",{"2":{"446":1,"513":1}}],["meta探索大模型记忆层",{"2":{"156":1}}],["meta",{"2":{"142":1,"361":1,"542":1,"543":1,"627":1,"631":1,"1078":6,"1087":1}}],["megatron",{"2":{"89":1,"387":2,"420":1}}],["merges用来决定把哪些单字节合并成一个token",{"2":{"592":1}}],["merges",{"2":{"591":16,"592":11}}],["merge",{"2":{"576":1,"580":1,"590":1,"591":1,"592":8}}],["mergeable",{"2":{"571":3}}],["merge的合并方法",{"2":{"143":1}}],["merged",{"2":{"47":1,"592":1}}],["merging",{"2":{"47":1}}],["monday",{"2":{"1728":2}}],["monotonic",{"2":{"156":1,"838":1}}],["monosemanticity",{"2":{"137":1}}],["mov",{"2":{"1603":1}}],["movecapture",{"2":{"1907":2}}],["move",{"2":{"1302":1,"1695":1,"1869":2,"1887":4,"1911":2}}],["movedim",{"2":{"1087":2}}],["moveaxis",{"2":{"1087":2}}],["moves",{"2":{"1083":1,"1869":1}}],["mover",{"2":{"130":1}}],["mobilenet",{"2":{"1308":3}}],["moments",{"2":{"1059":1}}],["momentum=0",{"2":{"1039":1,"1205":1,"1215":1,"1221":1,"1222":1,"1231":2,"1241":1,"1242":1,"1245":1}}],["momentum",{"0":{"1028":1,"1189":1},"1":{"1029":1,"1030":1,"1031":1,"1032":1},"2":{"807":4,"1034":1,"1035":1,"1036":1,"1130":1,"1143":5,"1144":3,"1180":1,"1308":3}}],["mosaic",{"2":{"1015":1}}],["most",{"2":{"167":1,"259":1,"557":1,"592":1,"1254":1,"1330":1}}],["mooncake",{"2":{"977":2}}],["mountain",{"2":{"588":1}}],["moun",{"2":{"588":1}}],["more",{"2":{"233":1,"399":1,"513":1,"592":1,"1902":1}}],["motivating",{"2":{"160":1}}],["moe模型通过动态路由机制将输入分配给不同的专家",{"2":{"739":1}}],["moee",{"2":{"737":1,"739":5}}],["moe的核心思想是动态地将不同的计算预算分配给不同的输入令牌",{"2":{"150":1}}],["moe",{"0":{"150":1,"1400":1},"2":{"42":1,"96":1,"150":1,"215":1,"222":1,"739":3}}],["modbus",{"2":{"1957":1}}],["modify",{"2":{"2005":2}}],["modifyarray",{"2":{"1667":3}}],["modified",{"2":{"1227":2}}],["modle",{"2":{"1206":1}}],["mod",{"2":{"1085":1}}],["mode",{"0":{"1119":1,"1121":1},"2":{"201":1,"385":4,"1087":11,"1118":2,"1121":1,"1214":2,"1299":1,"1404":1}}],["mode=",{"2":{"83":2,"364":1,"385":2,"399":2,"423":2,"424":2,"1330":1}}],["model非常大时",{"2":{"701":1}}],["model大小也会决定模型最终的参数量",{"2":{"699":1}}],["model详解",{"2":{"638":1}}],["model=512",{"2":{"448":1,"449":1,"703":1}}],["model一致",{"2":{"343":1}}],["model的实例化",{"2":{"1205":1}}],["model的时候无法直接保存整个网络",{"2":{"668":1}}],["model的值",{"2":{"199":1}}],["model的输出维度",{"2":{"23":1}}],["model的输入维度和d",{"2":{"23":1}}],["model赋值",{"2":{"199":1}}],["model维降到了每一个头的",{"2":{"28":1}}],["model可以被h整除",{"2":{"23":1}}],["model是模型的维度",{"2":{"23":1,"198":1}}],["models",{"0":{"1379":1},"2":{"8":1,"76":1,"89":1,"95":1,"122":3,"131":1,"136":1,"137":1,"141":1,"143":1,"147":1,"156":8,"167":1,"204":1,"233":6,"240":1,"259":1,"260":1,"281":1,"292":2,"429":1,"437":2,"450":1,"490":1,"513":5,"542":1,"543":1,"561":1,"562":1,"601":1,"625":2,"637":1,"638":5,"703":1,"716":1,"735":1,"736":2,"740":4,"760":1,"768":3,"1086":1,"1272":1,"1282":3,"1283":1,"1303":1,"2073":1,"2077":1,"2078":1,"2079":2,"2081":1}}],["modeling这可能是最早提出qkv概念的论文",{"2":{"288":1}}],["modeling",{"2":{"8":1,"47":2,"76":1,"105":1,"156":1,"288":1,"292":2,"638":1,"894":1,"906":1,"1312":2,"1315":2}}],["modelargs",{"2":{"8":1,"201":3,"1345":1}}],["model",{"0":{"912":1,"1102":1,"1380":1},"2":{"5":1,"7":4,"8":4,"23":8,"25":1,"28":3,"30":3,"34":1,"35":2,"36":22,"47":1,"83":18,"99":2,"101":2,"113":6,"119":7,"143":1,"144":1,"148":1,"156":4,"167":2,"173":2,"198":2,"199":6,"201":4,"233":1,"235":1,"259":1,"260":1,"316":1,"335":1,"344":1,"361":1,"364":1,"371":1,"372":13,"381":1,"385":3,"399":8,"402":7,"410":1,"419":2,"422":4,"423":14,"424":14,"428":10,"429":3,"437":1,"447":1,"448":1,"449":19,"451":1,"461":2,"472":6,"483":1,"507":1,"513":1,"520":10,"523":1,"528":1,"529":8,"530":12,"532":1,"533":1,"542":1,"559":1,"567":1,"571":8,"572":1,"573":1,"591":13,"616":1,"623":1,"624":3,"634":1,"638":3,"699":1,"700":1,"701":8,"702":2,"703":13,"704":1,"711":1,"718":1,"723":1,"724":1,"726":2,"727":1,"732":1,"737":1,"740":3,"764":1,"926":1,"927":4,"1039":4,"1122":5,"1205":13,"1208":1,"1213":3,"1215":17,"1216":29,"1218":42,"1221":1,"1222":4,"1223":2,"1231":4,"1239":3,"1241":1,"1242":1,"1243":1,"1245":1,"1255":3,"1258":2,"1259":3,"1262":3,"1263":3,"1266":4,"1267":6,"1269":7,"1270":3,"1272":3,"1273":6,"1274":4,"1283":6,"1295":2,"1296":11,"1297":9,"1298":4,"1299":8,"1300":1,"1303":2,"1308":3,"1312":1,"1331":1,"1332":1,"1404":2,"1665":22,"2075":1,"2076":1,"2086":6}}],["module环境问题",{"2":{"2048":1}}],["moduleb",{"2":{"1980":2}}],["modulea",{"0":{"1981":1},"2":{"1980":2,"1981":4}}],["module=false",{"2":{"1214":1}}],["moduledemo",{"2":{"1205":3}}],["module以及属性和方法",{"2":{"664":1}}],["module的初始化方法",{"2":{"522":1}}],["module的构造函数",{"2":{"343":1,"344":1,"523":1}}],["module类",{"2":{"522":1,"533":1,"701":1}}],["modules吗",{"2":{"668":1}}],["modules",{"2":{"370":1,"664":1,"668":1,"1207":2,"1208":1,"1210":1,"1214":2}}],["modulelist",{"2":{"201":1,"522":1,"1216":1,"1217":1,"1218":2}}],["module",{"0":{"662":1,"1122":1,"1200":1,"1207":1,"1208":1,"1209":1,"1214":1},"1":{"1210":1,"1211":1,"1212":1,"1213":1,"1214":1},"2":{"8":1,"23":1,"38":1,"39":1,"82":2,"83":1,"110":1,"113":2,"114":1,"201":3,"228":1,"326":1,"343":3,"344":2,"346":2,"364":1,"385":1,"394":3,"399":1,"423":7,"450":2,"503":1,"522":3,"523":1,"529":3,"532":1,"533":1,"558":1,"662":1,"701":1,"702":1,"703":1,"723":1,"733":1,"808":1,"810":1,"834":1,"1083":1,"1116":1,"1117":2,"1122":2,"1205":3,"1206":1,"1208":1,"1210":2,"1211":7,"1212":7,"1213":4,"1214":24,"1215":3,"1216":4,"1217":3,"1218":5,"1257":1,"1295":1,"1345":1,"1404":1,"2086":1}}],["mohammad",{"2":{"1194":1}}],["moh主要改进如下图所示",{"2":{"42":1}}],["moh的输出是k个选定头的输出的加权和",{"2":{"42":1}}],["moh总体架构如下图右侧所示",{"2":{"42":1}}],["moh具有两个显著优点",{"2":{"42":1}}],["moh",{"0":{"42":1},"2":{"0":1,"42":2,"47":1}}],["mohsa",{"0":{"41":1},"2":{"0":1}}],["解题步骤",{"0":{"2152":1}}],["解题思路",{"2":{"1463":1}}],["解引用行指针",{"2":{"1705":1}}],["解引用来访问指向的值",{"2":{"1612":1}}],["解引用",{"2":{"1611":1,"1705":1}}],["解引用运算符",{"2":{"1611":1,"1635":1}}],["解压缩",{"2":{"1535":1}}],["解包",{"2":{"1535":1}}],["解包操作",{"2":{"1114":1}}],["解除sql与程序代码的耦合",{"2":{"1479":1}}],["解方程",{"0":{"1098":1}}],["解线性方程组",{"2":{"1083":1}}],["解耦合",{"2":{"1645":1}}],["解耦的逻辑如下图所示",{"2":{"764":1}}],["解耦",{"2":{"762":1,"1064":1,"1340":1}}],["解耦了位置和位置表示之间的一对一对应关系",{"2":{"756":1}}],["解耦语言表示与推理",{"2":{"627":1}}],["解铃还须系铃人",{"2":{"513":1}}],["解剖transformer",{"2":{"407":1,"429":1,"543":1}}],["解析transformer如何运作",{"2":{"235":1}}],["解释内存泄漏可能对程序造成的危害",{"2":{"1678":1}}],["解释为三字符序列",{"2":{"1616":1}}],["解释型语言",{"2":{"1602":1,"1604":1}}],["解释",{"0":{"474":1},"1":{"475":1,"476":1,"477":1,"478":1,"479":1,"480":1,"481":1,"482":1,"483":1,"484":1,"485":1,"486":1,"487":1,"488":1,"489":1,"490":1,"491":1,"492":1,"493":1,"494":1,"495":1,"496":1,"497":1,"498":1,"499":1,"500":1,"501":1,"502":1,"503":1,"504":1,"505":1,"506":1,"507":1,"508":1,"509":1},"2":{"1667":1,"1668":1,"1674":1}}],["解释器会一行一行地理解并执行代码",{"2":{"1604":1}}],["解释器",{"2":{"233":1}}],["解释了每个注意头所学习到的知识",{"2":{"131":1}}],["解读小模型",{"2":{"513":1}}],["解读group",{"2":{"361":1}}],["解读",{"0":{"176":1},"2":{"157":1}}],["解决rollup",{"2":{"2048":1}}],["解决实际编程挑战",{"2":{"1727":1}}],["解决菱形继承问题",{"2":{"1693":1}}],["解决之前的问题",{"0":{"1202":1}}],["解决这个问题的一种方法是创建假数据并",{"2":{"1015":1}}],["解决这个问题的一个方法是通过让注意力层访问外部存储器",{"2":{"151":1}}],["解决context长度限制问题",{"2":{"892":1}}],["解决此问题的结构称之为encoder",{"2":{"883":1}}],["解决",{"0":{"880":1,"881":1,"882":1},"2":{"848":1}}],["解决梯度消失问题",{"2":{"848":1}}],["解决思路",{"2":{"838":1,"1109":1}}],["解决的是用更加廉价的设备资源",{"2":{"795":1}}],["解决数据稀缺问题应该是未来工作的重点",{"2":{"561":1}}],["解决长距离依赖关系",{"2":{"434":1}}],["解决起来就简单了",{"2":{"316":1}}],["解决了遗忘和对齐问题",{"2":{"284":1}}],["解决方案相比",{"2":{"977":1}}],["解决方案很容易",{"2":{"679":1}}],["解决方案就是注意力机制在transformer中所模拟的过程",{"2":{"167":1}}],["解决方案",{"0":{"5":1,"55":1,"59":1,"318":1,"2092":1},"1":{"2093":1,"2094":1},"2":{"0":1,"49":2,"293":1}}],["解码为原始字节",{"2":{"614":1}}],["解码序列长度也没有现阶段大模型的要求那么高",{"2":{"935":1,"951":1}}],["解码序列",{"2":{"588":1}}],["解码过程",{"0":{"1330":1}}],["解码过程是编码的逆过程",{"2":{"588":1}}],["解码过程如下所示",{"2":{"573":1}}],["解码",{"0":{"573":1,"588":1},"2":{"886":1}}],["解码函数",{"2":{"450":1}}],["解码端的残差结构有没有把后续未被看见的token信息添加进来",{"2":{"307":1}}],["解码的过程中",{"2":{"886":1}}],["解码的时候会面临歧义问题",{"2":{"595":1}}],["解码的时候掩盖掉当前时刻之后的信息",{"2":{"69":1}}],["解码的每一步都用到了编码器生成的隐向量",{"2":{"282":1}}],["解码中的每一步都用到了编码器生成的上下文向量",{"2":{"281":1}}],["解码输出",{"2":{"170":1}}],["解码新token的过程是先把隐向量用线性层变换",{"2":{"147":1}}],["解码器还插入第三个子层",{"2":{"915":1}}],["解码器同样由n",{"2":{"915":1}}],["解码器生成符号的一个输出序列",{"2":{"912":1}}],["解码器结构",{"2":{"912":1}}],["解码器结果",{"0":{"472":1}}],["解码器进行相反的过程",{"2":{"885":1}}],["解码器进行解码",{"2":{"445":1}}],["解码器负责生成下一个句子的嵌入",{"2":{"635":1}}],["解码器基于来自编码器的源语言特征以及解码器之前生成的tokens来生成目标语言中的tokens",{"2":{"540":1}}],["解码器可能会询问编码器",{"2":{"536":1}}],["解码器可以更准确地预测目标序列的下一个词",{"2":{"536":1}}],["解码器可以对源句子的每个",{"2":{"525":1}}],["解码器可以使用该词之前的目标词以及该词之后的目标词",{"2":{"58":1}}],["解码器交叉注意力",{"2":{"536":1}}],["解码器层",{"2":{"530":1}}],["解码器层内部",{"2":{"530":6}}],["解码器第二次推理输出",{"2":{"529":1}}],["解码器采用的是自回归模式",{"2":{"529":1}}],["解码器采用了sequence",{"2":{"59":1}}],["解码器有两种输入",{"2":{"528":1}}],["解码器在对当前时刻进行解码输出时",{"2":{"537":1}}],["解码器在推理时的工作流程相对简单多了",{"2":{"529":1}}],["解码器在t",{"2":{"528":1}}],["解码器在训练时采用teacher",{"2":{"528":1}}],["解码器在解码过程中产生的隐向量",{"2":{"416":1}}],["解码器就是在高维空间内对向量进行操作",{"2":{"526":1}}],["解码器就是购买者",{"2":{"524":1}}],["解码器最终会输出一个实数向量",{"2":{"526":1}}],["解码器最初的输入是中间语义上下文向量ccc",{"2":{"241":1}}],["解码器也是层叠的栈结构",{"2":{"526":1}}],["解码器也是按照类似方式调用sublayerconnection",{"2":{"523":1}}],["解码器之前预测的输出结果",{"2":{"525":1}}],["解码器之前预测输出结果的拼接",{"2":{"525":1,"526":1}}],["解码器使用解码器的新输出token和先前生成的",{"2":{"515":1}}],["解码器栈内部通过掩码自注意力机制完成了对目标序列特征的提取",{"2":{"515":1}}],["解码器以标记和编码器的输出作为起点",{"2":{"515":1}}],["解码器会把该组向量和编码器的输出结合起来",{"2":{"528":1}}],["解码器会把这个中间语义上下文向量ccc解码成输出句子y=",{"2":{"241":1}}],["解码器会基于这两个输入来预测下一个输出token",{"2":{"524":1}}],["解码器会循环执行",{"2":{"515":1}}],["解码器已经处理结束",{"2":{"473":1}}],["解码器并不能一次性全部输出",{"2":{"453":1}}],["解码器对象",{"2":{"450":1}}],["解码器实现",{"2":{"450":1}}],["解码器预测出第二个单词",{"2":{"445":1}}],["解码器把自己的输入序列和编码器的输出进行融合转换",{"2":{"445":1}}],["解码器把过去所有的上下文信息压缩到一个固定大小低维向量",{"2":{"273":1}}],["解码器首先进入masked",{"2":{"445":1}}],["解码器中的每个位置都可以关注解码器前一层的所有位置",{"2":{"443":1}}],["解码器中的lstm替换为lstmn",{"2":{"287":1}}],["解码器处理输出",{"2":{"427":1}}],["解码器处理输入",{"2":{"427":1}}],["解码器将生成的概念解码为子词",{"2":{"628":1}}],["解码器将目标嵌入与编码器的编码表示一起处理",{"2":{"427":2}}],["解码器将上下文向量转换成目标语言",{"2":{"281":1}}],["解码器解码",{"2":{"427":1}}],["解码器解码时",{"2":{"252":1}}],["解码器需要",{"2":{"536":1}}],["解码器需要两种隐向量",{"2":{"416":1}}],["解码器需要将预测输出序列的最后一个字符和真实的结果作比较",{"2":{"380":1}}],["解码器输出可以使用self",{"2":{"450":1}}],["解码器输出",{"2":{"405":1,"407":1,"427":1}}],["解码器输出后面接了一个模块generator",{"2":{"397":1}}],["解码器输入2",{"2":{"405":1,"407":1,"427":1}}],["解码器输入1",{"2":{"405":1,"407":1,"427":1}}],["解码器计算隐状态的流程不同",{"2":{"285":1}}],["解码器做成一个端到端的模型",{"2":{"282":1}}],["解码器用rnn实现",{"2":{"281":1}}],["解码器框架",{"2":{"281":1}}],["解码器当前状态和解码器当前时刻的上下文语义向量context这三者作为输入",{"2":{"267":1}}],["解码器的模型",{"2":{"540":1}}],["解码器的目标是在给定某源语言序列时产生正确的目标语言输出序列",{"2":{"536":1}}],["解码器的每个解码层都会使用同一个隐向量编码矩阵c",{"2":{"532":1}}],["解码器的每一步输入和输出具体如下表所示",{"2":{"445":1}}],["解码器的流程需要区分训练阶段和推理阶段",{"2":{"527":1}}],["解码器的本质就是自回归的",{"2":{"525":1}}],["解码器的掩码多头自注意力与编码器的多头自注意力不同之处的终极原因在于训练和推理的不同",{"2":{"525":1}}],["解码器的掩码自注意力层要用的掩码",{"2":{"451":1}}],["解码器的结构如下图所示",{"2":{"525":1}}],["解码器的结构图",{"2":{"249":1}}],["解码器的输出",{"2":{"398":1}}],["解码器的输入是一个长度变化的张量y",{"2":{"530":1}}],["解码器的输入之一",{"2":{"525":1}}],["解码器的输入有两个",{"2":{"524":1}}],["解码器的输入其实有两种",{"2":{"453":1}}],["解码器的输入就是真值标签",{"2":{"408":1}}],["解码器的输入要除掉最后一个",{"2":{"381":1}}],["解码器的输入输出不同",{"2":{"285":1}}],["解码器的基础上增加了注意力机制",{"2":{"284":1}}],["解码器的不同阶段都使用这个隐状态",{"2":{"273":1}}],["解码器的参数是src",{"2":{"82":1}}],["解码器接着用新的隐状态和第一个输出词作为联合输入来计算第二个输出词",{"2":{"241":1}}],["解码器依据ccc计算出第一个输出词和新的隐状态",{"2":{"241":1}}],["解码器都是自回归的",{"2":{"241":1}}],["解码器架构进行训练",{"2":{"631":1}}],["解码器架构的性能进行了研究",{"2":{"542":1}}],["解码器架构一样",{"2":{"541":1}}],["解码器架构做出了重大贡献",{"2":{"540":1}}],["解码器架构中仅取编码器和解码器组件",{"2":{"540":1}}],["解码器架构",{"2":{"241":1,"450":1}}],["解码器模型那样捕获丰富的语义",{"2":{"729":1}}],["解码器模型仍然很有前景",{"2":{"540":1}}],["解码器模型流行",{"2":{"540":1}}],["解码器模型引入",{"2":{"540":1}}],["解码器模型的解码过程",{"2":{"525":1}}],["解码器模型在整个解码过程中保持相关性的能力可能会减弱",{"2":{"244":1}}],["解码器模型",{"0":{"241":1},"2":{"241":1,"539":1}}],["解码器模型要面对的问题",{"2":{"240":1}}],["解码器cross",{"2":{"77":1}}],["解码器masked",{"2":{"77":1}}],["解码器内部的带有mask的multiheadattention的qkv向量输入来自目标单词嵌入或者前一个解码器输出",{"2":{"72":1}}],["解码器不应该提前知道下文的信息",{"2":{"58":1}}],["解码器这种运行方式导致其在训练时候需要做特殊处理",{"2":{"57":1}}],["解码器",{"0":{"39":1,"282":1,"416":1,"514":1,"524":1,"915":1},"1":{"515":1,"516":1,"517":1,"518":1,"519":1,"520":1,"521":1,"522":1,"523":1,"524":1,"525":2,"526":2,"527":2,"528":2,"529":2,"530":2,"531":2,"532":2,"533":2,"534":1,"535":1,"536":1,"537":1,"538":1,"539":1,"540":1,"541":1,"542":1,"543":1},"2":{"0":1,"78":1,"241":1,"436":1,"530":1,"535":2,"541":1}}],["调试难度",{"2":{"1709":1}}],["调试困难",{"2":{"1632":1}}],["调试等功能",{"2":{"1605":1}}],["调试",{"2":{"1605":1}}],["调节阀的总数可以成千上万甚至更多",{"2":{"1466":1}}],["调节不同组件对模型行为的影响",{"2":{"224":1}}],["调整",{"2":{"1083":1}}],["调整绝对位置编码公式的第三项和第四项",{"2":{"760":1}}],["调整绝对位置编码公式的第二项",{"2":{"760":1}}],["调整方案",{"0":{"667":1,"1229":1}}],["调整之后",{"2":{"399":1}}],["调整了",{"2":{"355":1}}],["调整到了整个序列",{"2":{"274":1}}],["调整维度",{"0":{"31":1},"2":{"0":1}}],["调用animal的move",{"2":{"1869":1}}],["调用函数时可以省略有默认值的参数",{"2":{"1708":1}}],["调用函数f",{"2":{"249":1,"267":1}}],["调用第三个",{"2":{"1707":1}}],["调用第二个",{"2":{"1707":1}}],["调用第一个",{"2":{"1707":1}}],["调用的是基类的函数",{"2":{"1688":1}}],["调用的复杂程序中",{"2":{"985":1}}],["调用参数化构造函数",{"2":{"1675":1}}],["调用默认构造函数",{"2":{"1675":1}}],["调用析构函数",{"2":{"1674":1}}],["调用公有成员函数",{"2":{"1674":1}}],["调用构造函数",{"2":{"1674":1}}],["调用派生类的成员函数",{"2":{"1654":1}}],["调用基类的成员函数",{"2":{"1654":1}}],["调用传入的回调函数",{"2":{"1645":1}}],["调用者不需要知道被调用者具体做什么",{"2":{"1645":1}}],["调用训练函数",{"2":{"1296":1}}],["调用之前执行预处理操作",{"2":{"1227":1}}],["调用以上三种to",{"2":{"1214":1}}],["调用父类初始化方法",{"2":{"701":1}}],["调用父类nn",{"2":{"343":1,"344":1,"522":1,"523":1}}],["调用merge函数",{"2":{"592":1}}],["调用multiheadedattention",{"2":{"38":1,"39":1}}],["调用分词器进行分词",{"2":{"557":1}}],["调用分词模型tokenizer对text进行分词",{"2":{"557":1}}],["调用pytorch函数build",{"2":{"557":1}}],["调用datasets",{"2":{"557":1}}],["调用loss",{"2":{"385":1}}],["调用run",{"2":{"364":1}}],["调用sublayerconnection时候",{"2":{"344":1}}],["调用torch",{"2":{"65":1}}],["调用",{"0":{"37":1,"703":1},"1":{"38":1,"39":1},"2":{"0":1,"1083":2,"1085":1,"1214":1,"1227":2,"1663":2,"1688":4,"1707":3,"1729":5,"1997":1}}],["函数抛出异常时",{"2":{"1762":1}}],["函数并异常终止",{"2":{"1762":1}}],["函数通过返回特定的值来表示错误状态",{"2":{"1761":1}}],["函数判断输入的年份是否为闰年",{"2":{"1729":1}}],["函数用于判断一个年份是否为闰年",{"2":{"1729":1}}],["函数接收一个",{"2":{"1729":1}}],["函数接收两个",{"2":{"1729":1}}],["函数根据这些参数进行计算或操作",{"2":{"1729":1}}],["函数体内部可以包含任何有效的",{"2":{"1729":1}}],["函数体",{"2":{"1729":2}}],["函数定义",{"2":{"1729":1}}],["函数定义的示例",{"2":{"1729":1}}],["函数定义的语法",{"2":{"1729":1}}],["函数定义了数据加载器",{"2":{"375":1}}],["函数定义了标准注意力机制的操作过程",{"2":{"197":1}}],["函数包含了所有的逻辑",{"2":{"1729":1}}],["函数就像一个小型",{"2":{"1729":1}}],["函数就是对下面公式的实现",{"2":{"402":1}}],["函数与指针的应用",{"2":{"1709":1}}],["函数与指针的深度应用",{"0":{"1706":1}}],["函数作为参数传递给",{"2":{"1706":1}}],["函数指针可以存储函数的地址",{"2":{"1706":1}}],["函数指针的声明需要指定函数的返回类型和参数列表",{"2":{"1706":1}}],["函数指针的声明方式",{"2":{"1645":1}}],["函数指针存储的是函数的入口地址",{"2":{"1706":1}}],["函数指针",{"2":{"1706":1}}],["函数模板",{"0":{"1699":1},"2":{"1698":1,"1699":1}}],["函数查找优先级",{"2":{"1698":1}}],["函数重载与类型转换",{"0":{"2004":1}}],["函数重载通过不同的参数列表区分同名函数",{"2":{"1707":1}}],["函数重载允许在同一个作用域内定义多个同名函数",{"2":{"1707":1}}],["函数重载时",{"2":{"1687":1}}],["函数重写",{"2":{"1654":1}}],["函数外部",{"2":{"1667":1}}],["函数名应具有描述性",{"2":{"1729":1}}],["函数名称",{"2":{"1699":1}}],["函数名称相同",{"2":{"1663":1}}],["函数名",{"2":{"1693":1,"1729":3}}],["函数名在大多数情况下会被隐式转换为函数指针",{"2":{"1645":1}}],["函数内部确实抛出了异常且未被捕获",{"2":{"1764":1}}],["函数内部通过指针操作数组元素",{"2":{"1667":1}}],["函数内部",{"2":{"1667":1}}],["函数内部对形参的修改会直接影响到实参的值",{"2":{"1650":1}}],["函数内部对形参的修改不会影响到实参的值",{"2":{"1650":1}}],["函数内部可以通过解引用指针来修改实参的值",{"2":{"1650":1}}],["函数内获取状态前执行",{"2":{"1208":1}}],["函数内获取状态后执行",{"2":{"1208":1}}],["函数进阶与内存管理",{"0":{"1643":1},"1":{"1644":1,"1645":1,"1646":1,"1647":1,"1648":1,"1649":1,"1650":1}}],["函数进行具体训练",{"2":{"385":1}}],["函数调用链深时",{"2":{"1761":1}}],["函数调用的示例",{"2":{"1729":1}}],["函数调用的语法",{"2":{"1729":1}}],["函数调用运算符重载",{"2":{"1712":1}}],["函数调用信息",{"2":{"1648":1}}],["函数调用或圆括号",{"2":{"1635":1}}],["函数调用模型的前向传播函数",{"2":{"538":1}}],["函数替代简单的宏函数",{"2":{"1632":1}}],["函数参数是否相同不影响覆盖",{"2":{"1663":1}}],["函数参数传递",{"2":{"1612":2}}],["函数参数",{"2":{"1607":1,"1648":1}}],["函数返回",{"2":{"1891":1}}],["函数返回类型推导",{"0":{"1905":1},"2":{"1904":1,"1913":1}}],["函数返回类型",{"2":{"1699":1}}],["函数返回值",{"2":{"1607":1,"1629":1}}],["函数返回的就是编码之后的向量",{"2":{"522":1}}],["函数如图下图右所示",{"2":{"1460":1}}],["函数f沿负梯度方向减小",{"2":{"1440":1}}],["函数f沿梯度方向增加",{"2":{"1440":1}}],["函数y",{"2":{"1377":1}}],["函数时传入的参数类型",{"2":{"1699":1}}],["函数时",{"2":{"1227":1}}],["函数时触发",{"2":{"1085":1}}],["函数或在交互式环境中直接输出对象时被调用",{"2":{"1214":1}}],["函数计算输出之后被调用",{"2":{"1214":1}}],["函数计算梯度",{"2":{"423":1}}],["函数之前被调用",{"2":{"1214":1}}],["函数之中",{"2":{"198":1}}],["函数初始化",{"2":{"1206":1}}],["函数有一个参数",{"2":{"1099":1}}],["函数永远无法真正预测0",{"2":{"1016":1}}],["函数和明确目标的最大似然学习可能永远不会收敛",{"2":{"1016":1}}],["函数和线性函数特性的激活函数",{"2":{"103":1}}],["函数传递",{"2":{"865":1}}],["函数当",{"2":{"840":1}}],["函数图如下",{"2":{"839":1}}],["函数得到了广泛的应用",{"2":{"839":1}}],["函数得到注意力权重",{"2":{"355":1}}],["函数w",{"2":{"770":1}}],["函数x",{"2":{"770":1}}],["函数实际上调用到了vocab对象的forward",{"2":{"558":1}}],["函数其次会调用multiheadedattention",{"2":{"538":1}}],["函数首先会调用multiheadedattention",{"2":{"538":1}}],["函数利用memory",{"2":{"538":1}}],["函数利用src",{"2":{"538":1}}],["函数执行代码",{"2":{"1699":1}}],["函数执行完毕后",{"2":{"1648":1}}],["函数执行解码功能",{"2":{"538":1}}],["函数执行编码功能",{"2":{"538":1}}],["函数完成了这个功能",{"2":{"557":1}}],["函数完成了编码和解码的工作",{"2":{"450":1}}],["函数完成注意力计算功能",{"2":{"538":1}}],["函数最终调用到attention",{"2":{"538":1}}],["函数最终调用到multiheadedattention",{"2":{"538":1}}],["函数又会调用encoderlayer",{"2":{"538":1}}],["函数克隆了n个decoderlayer",{"2":{"532":1}}],["函数将字符串转换为整数",{"2":{"1683":1}}],["函数将layer克隆n份",{"2":{"522":1}}],["函数将",{"2":{"473":1}}],["函数负责把输入编码成隐状态",{"2":{"451":1}}],["函数负责加载模型",{"2":{"372":1}}],["函数构建了顺序容器",{"2":{"449":1}}],["函数代码如下",{"2":{"385":1}}],["函数在训练时",{"2":{"385":1}}],["函数是一段独立的代码块",{"2":{"1729":1}}],["函数是解决这个问题的重要工具",{"2":{"1729":1}}],["函数是编码时使用到的基础函数",{"2":{"590":1}}],["函数是德语分词函数",{"2":{"557":1}}],["函数是英语分词函数",{"2":{"557":1}}],["函数是transformer模型的构建函数",{"2":{"447":1}}],["函数是dataloader类的collate",{"2":{"384":1,"558":1}}],["函数是核心所在",{"2":{"375":1}}],["函数data",{"2":{"383":1}}],["函数对象或仿函数",{"2":{"1712":1}}],["函数对应的代码如下",{"2":{"382":1}}],["函数对processed",{"2":{"65":1}}],["函数加载数据",{"2":{"375":1}}],["函数具体代码如下",{"2":{"374":1}}],["函数来计算每个批次的损失",{"2":{"385":1}}],["函数来处理",{"2":{"384":1,"558":1}}],["函数来定义构建batch功能",{"2":{"375":1}}],["函数来训练一个模型",{"2":{"372":1}}],["函数来生成未来词汇相关的掩码",{"2":{"74":1,"382":1}}],["函数迭代运行训练步",{"2":{"364":1}}],["函数做矩阵乘法",{"2":{"199":1}}],["函数应用",{"2":{"122":3}}],["函数含有一个可学习的参数",{"2":{"107":1}}],["函数可以看到",{"2":{"82":1}}],["函数产生一个上三角阵",{"2":{"74":1}}],["函数会从数据集中通过迭代器来读取数据",{"2":{"557":1}}],["函数会调用decoderlayer",{"2":{"538":1}}],["函数会调用self",{"2":{"538":1}}],["函数会返回memory",{"2":{"538":1}}],["函数会开辟一个新内存并将源实例完全复制过来",{"2":{"449":1}}],["函数会依据配置选择是进行分布式训练还是单机训练",{"2":{"422":1}}],["函数会加载词表",{"2":{"374":1}}],["函数会把大部分概率分布分配给最大的元素",{"2":{"191":1}}],["函数会退化为",{"2":{"191":1}}],["函数会将所有负数输入直接归零",{"2":{"107":1}}],["函数会生成未来词汇相关的掩码",{"2":{"74":1,"380":1}}],["函数会为目标句子加入掩码",{"2":{"65":1}}],["函数的所有要求",{"2":{"1924":1}}],["函数的第二个参数是一个泛型lambda表达式",{"2":{"1914":1}}],["函数的第一个参数可以是任意类型的容器",{"2":{"1914":1}}],["函数的应用场景",{"2":{"1729":1}}],["函数的应用",{"2":{"1729":1}}],["函数的",{"2":{"1729":1}}],["函数的调用",{"2":{"1729":1}}],["函数的标识符",{"2":{"1729":1}}],["函数的定义",{"2":{"1729":1}}],["函数的概念",{"2":{"1729":1}}],["函数的魔力",{"0":{"1729":1}}],["函数的默认参数值",{"0":{"1708":1}}],["函数的重载",{"0":{"1707":1},"2":{"1709":1}}],["函数的地址赋值给",{"2":{"1706":1}}],["函数的头文件",{"2":{"1704":1}}],["函数的指针传参和引用传参",{"0":{"1650":1}}],["函数的指针传递给",{"2":{"1645":2}}],["函数的递归调用",{"0":{"1646":1}}],["函数的模型",{"2":{"1016":1}}],["函数的主要思路就是用从小到大搭建积木的方式来构建transformer",{"2":{"449":1}}],["函数的参数有如下7个",{"2":{"448":1}}],["函数的功能是加载德语分词模型和英语分词模型",{"2":{"373":1}}],["函数的输入是",{"2":{"423":1}}],["函数的输入参数如下",{"2":{"198":1}}],["函数的输出有两个",{"2":{"198":1}}],["函数的梯度将变得非常小",{"2":{"187":1}}],["函数的源码如下",{"2":{"74":2}}],["函数的逻辑如下",{"2":{"74":1,"382":1}}],["函数中执行以下操作",{"2":{"1873":1}}],["函数中调用",{"2":{"1702":1}}],["函数中调用该函数",{"2":{"1678":1}}],["函数中的测试代码实现sum",{"2":{"1710":1}}],["函数中的原始数组",{"2":{"1667":1}}],["函数中的全局变量",{"2":{"1649":1}}],["函数中创建一个",{"2":{"1664":1}}],["函数中会变得难以管理和维护",{"2":{"1729":1}}],["函数中会利用embedding类来对输入和输出序列进行编码",{"2":{"703":1}}],["函数中会把词表传入进去",{"2":{"558":1}}],["函数中会调用self",{"2":{"538":1}}],["函数中会调用损失函数",{"2":{"399":1}}],["函数中使用了build",{"2":{"557":1}}],["函数中展示出来",{"2":{"449":1}}],["函数中做了一定的抽象",{"2":{"449":1}}],["函数中还要将多头的结果进行合并",{"2":{"199":1}}],["函数中",{"2":{"67":1,"74":1,"344":1,"866":2,"868":2,"1667":2,"1690":1,"1698":1,"1700":1,"1701":1,"1761":1,"1766":3}}],["函数增加一维度",{"2":{"66":1,"382":1}}],["函数",{"0":{"36":1},"2":{"0":1,"36":1,"106":1,"191":1,"198":2,"344":1,"346":1,"375":2,"423":1,"538":11,"557":1,"558":1,"1083":1,"1099":2,"1114":1,"1214":1,"1227":1,"1246":1,"1649":1,"1651":1,"1657":2,"1664":5,"1690":3,"1693":1,"1698":2,"1701":1,"1706":2,"1707":6,"1713":1,"1729":5,"1763":2,"1914":2,"1933":1}}],["793",{"2":{"1910":1}}],["78",{"2":{"1779":1,"2086":1}}],["784",{"2":{"1373":1,"2086":3}}],["71",{"2":{"1698":1,"1699":1}}],["7106",{"2":{"557":1}}],["7x",{"2":{"1398":1}}],["7x7",{"2":{"816":1}}],["755",{"2":{"1513":1}}],["75",{"2":{"1308":1,"1607":3,"1680":1,"1921":1}}],["7580993408473766",{"2":{"395":1}}],["7a",{"2":{"1182":2}}],["7gb的空间",{"2":{"981":1}}],["77",{"2":{"941":1,"960":1}}],["7796",{"2":{"557":1}}],["7us和1",{"2":{"935":1,"951":1}}],["70",{"2":{"1825":3,"1843":3,"1953":1,"2131":1}}],["702",{"2":{"844":1}}],["702x",{"2":{"844":2}}],["70b",{"2":{"561":1}}],["7b展示了以5",{"2":{"1182":1}}],["7bit",{"2":{"1075":1}}],["7b在nvidia",{"2":{"980":1}}],["7b",{"2":{"735":1,"1182":1}}],["768x768",{"2":{"1363":1}}],["768",{"2":{"976":21}}],["7628",{"2":{"557":1}}],["7647",{"2":{"557":1}}],["7是实际语句内容",{"2":{"380":1}}],["7来防止数据精度溢出",{"2":{"346":1}}],["7个单词",{"2":{"343":1}}],["72",{"2":{"1330":1,"1331":1}}],["729",{"2":{"1246":1,"1247":1}}],["7276",{"2":{"315":4}}],["72kg",{"2":{"313":1}}],["7",{"0":{"21":1,"287":1,"346":1,"347":1,"348":1,"349":1,"350":1,"358":1,"413":1,"421":1,"511":1,"512":1,"654":1,"673":1,"734":1,"762":1,"775":1,"823":1,"845":1,"859":1,"872":1,"873":1,"874":1,"875":1,"905":1,"906":1,"907":1,"908":1,"909":1,"931":1,"965":1,"974":1,"975":1,"1001":1,"1002":1,"1003":1,"1004":1,"1005":1,"1006":1,"1007":1,"1020":1,"1051":1,"1052":1,"1053":1,"1054":1,"1083":1,"1098":1,"1110":1,"1123":1,"1239":1,"1284":1,"1318":1,"1398":1,"1450":1,"1471":1,"1497":1,"1525":1,"1555":1,"1578":1,"1673":1,"1724":1,"1757":1,"1825":1,"1843":1,"1911":1,"1927":1,"1977":1,"1992":1,"1993":1,"1994":1,"2153":1},"1":{"351":1,"352":1,"353":1,"354":1,"355":1,"356":1,"357":1,"359":1,"360":1,"414":1,"415":1,"416":1,"417":1,"418":1,"419":1,"420":1,"422":1,"423":1,"424":1,"674":1,"675":1,"676":1,"677":1,"678":1,"679":1,"680":1,"681":1,"682":1,"683":1,"684":1,"685":1,"686":1,"687":1,"688":1,"689":1,"690":1,"691":1,"692":1,"693":1,"694":1,"695":1,"696":1,"697":1,"698":1,"699":1,"700":1,"701":1,"702":1,"703":1,"704":1,"705":1,"706":1,"707":1,"708":1,"709":1,"710":1,"711":1,"712":1,"713":1,"714":1,"715":1,"716":1,"717":1,"718":1,"719":1,"720":1,"721":1,"722":1,"723":1,"724":1,"725":1,"726":1,"727":1,"728":1,"729":1,"730":1,"731":1,"732":1,"733":1,"734":1,"735":1,"736":1,"737":1,"738":1,"739":1,"740":1,"873":1,"874":1,"875":1,"906":1,"907":1,"908":1,"909":1,"975":1,"1002":1,"1003":1,"1004":1,"1005":1,"1006":2,"1007":2,"1052":1,"1053":1,"1054":1,"1526":1,"1527":1,"1528":1,"1556":1,"1557":1,"1758":1,"1993":1,"1994":1},"2":{"0":1,"83":1,"156":1,"167":1,"217":1,"224":1,"293":6,"315":1,"343":3,"380":1,"424":1,"427":1,"428":1,"453":2,"472":1,"529":1,"545":1,"582":3,"583":2,"741":1,"816":3,"828":1,"832":4,"970":1,"986":1,"1059":1,"1215":3,"1316":1,"1330":13,"1389":1,"1391":1,"1392":3,"1393":3,"1394":1,"1395":7,"1398":3,"1607":2,"1625":3,"1630":1,"1635":1,"1672":1,"1724":3,"1737":1,"1738":1,"1744":1,"1751":1,"1752":1,"1764":1,"1789":1,"1933":1,"2006":1}}],["分发饼干",{"2":{"2148":1}}],["分发饼干问题",{"0":{"2146":1},"1":{"2147":1,"2148":1,"2149":1,"2150":1,"2151":1,"2152":1,"2153":1,"2154":1,"2155":1,"2156":1,"2157":1},"2":{"2043":1}}],["分而治之",{"2":{"2104":1}}],["分隔",{"2":{"1922":1}}],["分隔的代码块",{"2":{"1641":1}}],["分钟",{"2":{"1728":3,"1729":4,"1732":7}}],["分屏切换",{"2":{"1557":1}}],["分屏操作",{"0":{"1557":1}}],["分页查看文件内容",{"2":{"1510":1}}],["分桶",{"2":{"1340":1}}],["分数最大",{"2":{"1330":1}}],["分数越高",{"2":{"1324":1}}],["分支",{"0":{"1315":1,"1316":1,"1317":1}}],["分离错误处理代码和正常逻辑代码",{"2":{"1761":1}}],["分离张量y",{"2":{"1094":1}}],["分离了隐层大小和词嵌入大小",{"2":{"698":1}}],["分离了在许多不同背景下反复出现的神经元激活模式",{"2":{"137":1}}],["分母会不断积累使",{"2":{"1045":1}}],["分母上梯度平方的累加将会越来越大",{"2":{"1045":1}}],["分母较大",{"2":{"1044":1}}],["分母较小",{"2":{"1044":1}}],["分母缩放顺序调节避免了非矩阵的",{"2":{"970":1}}],["分块管道并行",{"2":{"977":1}}],["分块计算",{"2":{"216":1}}],["分为卡数那么多份",{"2":{"974":1}}],["分为",{"2":{"944":1}}],["分成块",{"2":{"944":1,"963":1}}],["分组查询注意力",{"2":{"937":1,"953":1}}],["分组卷积",{"0":{"775":1}}],["分解",{"2":{"1083":1,"1930":1}}],["分解后的卷积计算过程如下图",{"2":{"777":1}}],["分解为若干独立的组件",{"2":{"224":1}}],["分开刻画",{"2":{"764":1}}],["分层",{"2":{"628":1}}],["分层softmax使用输出层的二叉树表示",{"2":{"184":1}}],["分配包含",{"2":{"1714":1}}],["分配速度",{"2":{"1648":1}}],["分配和释放",{"2":{"1648":1}}],["分配了内存",{"2":{"1647":1}}],["分配数组的内存",{"2":{"1647":1}}],["分配单个对象的内存",{"2":{"1647":1}}],["分配一个整数",{"2":{"1668":1}}],["分配一个包含",{"2":{"1647":1,"1668":1}}],["分配一个",{"2":{"1647":1}}],["分配的内存都有对应的",{"2":{"1672":1}}],["分配的内存",{"2":{"1668":1,"1669":1}}],["分配的数组的内存",{"2":{"1647":1,"1669":1}}],["分配的单个对象的内存",{"2":{"1647":1,"1669":1}}],["分配到足够的计算资源",{"2":{"613":1}}],["分配更多的计算资源",{"2":{"612":1}}],["分段",{"2":{"671":1}}],["分段为长度为m",{"2":{"613":1}}],["分段常数衰减",{"2":{"402":1}}],["分界",{"2":{"542":1}}],["分到不同的device上",{"2":{"420":1}}],["分词的速度和内存消耗也是重要的考量因素",{"2":{"568":1}}],["分词的本质其实就是一个字符到数字的映射",{"2":{"549":1}}],["分词结果的可解释性也是一个重要因素",{"2":{"568":1}}],["分词结果",{"2":{"564":1}}],["分词结果应该可以无损还原为输入",{"2":{"563":1}}],["分词通常分成三大类",{"2":{"564":1}}],["分词粒度",{"0":{"564":1},"1":{"565":1,"566":1,"567":1,"568":1}}],["分词策略非常重要",{"2":{"560":1}}],["分词流程主要分为如下几个阶段",{"2":{"551":1}}],["分词流程",{"0":{"551":1},"1":{"552":1,"553":1,"554":1,"555":1}}],["分词产生的结果就是一个列表",{"2":{"547":1}}],["分词会将输入文本拆分为模型或机器可以处理的小单位",{"2":{"547":1}}],["分词是把一句话分成一个个词",{"2":{"547":1}}],["分词是将输入文本分解为更小",{"2":{"456":1}}],["分词与word",{"2":{"545":1}}],["分词和embedding化",{"2":{"545":1}}],["分词器无法识别的单词会被归为`",{"2":{"557":1}}],["分词器",{"2":{"456":1,"549":1}}],["分词",{"0":{"456":1,"547":1},"2":{"363":1,"431":1,"455":1,"545":1,"563":1,"568":1}}],["分割为",{"2":{"944":2}}],["分割和视频",{"2":{"338":1}}],["分割操作将张量中每一行",{"2":{"28":1}}],["分摊",{"2":{"194":1}}],["分布的陡峭程度就和dkdkd",{"2":{"187":1}}],["分布的方差大",{"2":{"187":1}}],["分布集中在绝对值大的区域",{"2":{"187":1}}],["分布式一致性算法",{"2":{"1952":1}}],["分布式存储系统",{"2":{"1951":1}}],["分布式存储和记忆聚合",{"0":{"129":1},"2":{"96":1}}],["分布式实现",{"0":{"1594":1}}],["分布式内存模型",{"2":{"1568":1}}],["分布式计算",{"2":{"1566":1}}],["分布式系统",{"2":{"1563":1}}],["分布式训练",{"2":{"422":1}}],["分布式训练的进程数",{"2":{"201":1}}],["分布式机器学习",{"2":{"48":1}}],["分裂",{"2":{"137":1}}],["分类结果按一定的比例分配",{"2":{"1015":1}}],["分类之后才会基于输入元素之间的相似性进行加权组合",{"2":{"174":1}}],["分类",{"0":{"141":1,"439":1,"535":1,"540":1},"2":{"96":1,"1675":1}}],["分类逻辑等",{"2":{"12":1,"33":1}}],["分类逻辑等多种维度",{"2":{"4":1}}],["分别从最小的开始处理",{"2":{"2152":1}}],["分别从原始编码器",{"2":{"540":1}}],["分别输出所有的奇数和偶数",{"2":{"1759":1}}],["分别处理",{"2":{"1698":1}}],["分别用于存款和取款",{"2":{"1766":1}}],["分别用于发送和接收消息",{"2":{"1573":1}}],["分别用来存储加载的分词器",{"2":{"371":1}}],["分别表示两个分布的特征向量的协方差矩阵",{"2":{"1361":1}}],["分别表示两个分布的特征向量的均值",{"2":{"1361":1}}],["分别表示查询矩阵",{"2":{"507":1}}],["分别46us和3",{"2":{"935":1,"951":1}}],["分别耗时1",{"2":{"935":1,"951":1}}],["分别显示在上图的左边和右边",{"2":{"912":1}}],["分别为0",{"2":{"739":1}}],["分别产出token和word",{"2":{"545":1}}],["分别进行向前传播",{"2":{"542":1}}],["分别进行归一化",{"2":{"315":1}}],["分别得到了中间变量z1和z2",{"2":{"519":1}}],["分别与query",{"2":{"517":1}}],["分别称为query",{"2":{"463":1}}],["分别对应self",{"2":{"533":1}}],["分别对应了两种不同的独立的输入",{"2":{"453":1}}],["分别对应g等于c和g等于1",{"2":{"341":1}}],["分别计算hs和rw嵌入的相似度得分",{"2":{"739":1}}],["分别计算",{"2":{"420":1}}],["分别是多少呢",{"2":{"1462":1}}],["分别是位置",{"2":{"1336":1}}],["分别是k和v的上投影矩阵",{"2":{"957":1}}],["分别是几维的",{"2":{"773":1}}],["分别是",{"2":{"479":1,"522":2,"567":1,"650":1,"783":1,"932":1}}],["分别是构建词表时和训练构建batch",{"2":{"370":1}}],["分别是消除输入和权重零奇点",{"2":{"305":1}}],["分别学习线性和非线性变换部分",{"2":{"298":1}}],["分别采用相同的线性变换",{"2":{"101":1}}],["分别在",{"2":{"84":1}}],["分别如下",{"2":{"80":1}}],["分别乘以三个权重矩阵",{"2":{"71":1}}],["分别负责基础组合",{"2":{"46":1}}],["分别传到线性层中进行遍历",{"2":{"36":1}}],["分析性变换",{"2":{"1389":1}}],["分析模型的结构",{"2":{"1291":1}}],["分析文本的情感倾向",{"2":{"906":1}}],["分析transformer模型的参数量",{"2":{"768":1}}],["分析完上述相对位置编码之后",{"2":{"766":1}}],["分析其可以解决问题的复杂度",{"2":{"480":1}}],["分析",{"0":{"20":1,"904":1},"2":{"0":1}}],["分治是对数据进行有差别的对待",{"2":{"11":1}}],["分治",{"0":{"12":1},"2":{"0":1}}],["68",{"2":{"1608":2}}],["662866",{"2":{"1396":3}}],["6×0",{"2":{"1389":2}}],["6b",{"2":{"1316":2}}],["6gf",{"2":{"1308":2}}],["6m",{"2":{"1262":1}}],["6f",{"2":{"1215":1}}],["6ni^",{"2":{"1007":1}}],["6ni",{"2":{"1007":1}}],["6nj+nj+1",{"2":{"1000":1}}],["6倍的加速",{"2":{"940":1,"962":1}}],["6倍",{"2":{"940":1,"962":1}}],["671",{"2":{"846":1}}],["6732λ≈1",{"2":{"843":1}}],["6732",{"2":{"843":2}}],["6hardswish",{"2":{"845":1}}],["653",{"2":{"1910":1}}],["653688",{"2":{"1395":2}}],["653688w",{"2":{"1395":1}}],["6538",{"2":{"833":1}}],["6561",{"2":{"1246":1}}],["65",{"2":{"1102":1,"1395":3,"1398":2}}],["6548",{"2":{"833":1}}],["6370",{"2":{"557":1}}],["6330",{"2":{"315":2}}],["6循环",{"2":{"515":1}}],["6视为一个整体",{"2":{"315":1}}],["6000",{"2":{"1874":1}}],["60",{"2":{"1235":2,"1315":1,"1316":1,"1632":2,"1825":3,"1843":3,"1938":1,"1958":1,"1977":1,"1991":1}}],["60个token",{"2":{"637":1}}],["604739354",{"2":{"156":1}}],["60k",{"2":{"87":1,"89":1}}],["64×32×3×3=1843264",{"2":{"1003":1}}],["6420",{"2":{"833":1}}],["64千米",{"2":{"713":1}}],["64k",{"2":{"87":1}}],["64",{"2":{"36":1,"781":1,"816":3,"945":1,"965":1,"976":16,"1003":1,"1004":1,"1168":1,"1215":5,"1216":1,"1218":1,"1257":1,"1283":4,"1607":1,"1611":1,"1678":1,"2086":2}}],["6",{"0":{"20":1,"195":1,"242":1,"286":1,"343":1,"344":1,"358":1,"404":1,"446":1,"475":1,"481":1,"486":1,"491":1,"506":1,"521":1,"544":1,"551":1,"610":1,"616":1,"625":1,"637":1,"652":1,"653":1,"693":1,"728":1,"761":1,"774":1,"812":1,"818":1,"819":1,"820":1,"821":1,"822":1,"844":1,"858":1,"862":1,"863":1,"864":1,"865":1,"866":1,"867":1,"868":1,"869":1,"870":1,"871":1,"900":1,"901":1,"902":1,"903":1,"904":1,"930":1,"946":1,"964":1,"973":1,"1000":1,"1019":1,"1046":1,"1047":1,"1048":1,"1049":1,"1050":1,"1082":1,"1097":1,"1109":1,"1118":1,"1119":1,"1120":1,"1121":1,"1122":1,"1214":1,"1238":1,"1271":1,"1272":1,"1273":1,"1274":1,"1283":1,"1317":1,"1354":1,"1362":1,"1363":1,"1364":1,"1365":1,"1375":1,"1397":1,"1444":1,"1445":1,"1446":1,"1447":1,"1448":1,"1449":1,"1468":1,"1469":1,"1470":1,"1489":1,"1496":1,"1521":1,"1552":1,"1577":1,"1596":1,"1597":1,"1598":1,"1634":1,"1639":1,"1640":1,"1672":1,"1703":1,"1723":1,"1748":1,"1753":1,"1754":1,"1755":1,"1756":1,"1822":1,"1823":1,"1824":1,"1840":1,"1841":1,"1842":1,"1862":1,"1863":1,"1864":1,"1865":1,"1897":1,"1898":1,"1899":1,"1910":1,"1926":1,"1976":1,"1988":1,"1989":1,"1990":1,"1991":1,"2062":1,"2152":1},"1":{"359":1,"360":1,"405":1,"406":1,"407":1,"408":1,"409":1,"410":1,"411":1,"412":1,"476":1,"477":1,"478":1,"479":1,"480":1,"482":1,"483":1,"484":1,"485":1,"487":1,"488":1,"489":1,"490":1,"492":1,"493":1,"494":1,"495":1,"496":1,"497":1,"498":1,"499":1,"500":1,"501":1,"502":1,"503":1,"504":1,"505":1,"507":1,"508":1,"509":1,"522":1,"523":1,"545":1,"546":1,"547":1,"548":1,"549":1,"550":1,"551":1,"552":2,"553":2,"554":2,"555":2,"556":1,"557":1,"558":1,"559":1,"560":1,"561":1,"562":1,"563":1,"564":1,"565":1,"566":1,"567":1,"568":1,"569":1,"570":1,"571":1,"572":1,"573":1,"574":1,"575":1,"576":1,"577":1,"578":1,"579":1,"580":1,"581":1,"582":1,"583":1,"584":1,"585":1,"586":1,"587":1,"588":1,"589":1,"590":1,"591":1,"592":1,"593":1,"594":1,"595":1,"596":1,"597":1,"598":1,"599":1,"600":1,"601":1,"602":1,"603":1,"604":1,"605":1,"606":1,"607":1,"608":1,"609":1,"610":1,"611":2,"612":2,"613":2,"614":2,"615":2,"616":1,"617":2,"618":2,"619":2,"620":2,"621":2,"622":2,"623":2,"624":2,"625":1,"626":2,"627":2,"628":2,"629":2,"630":2,"631":2,"632":2,"633":2,"634":2,"635":2,"636":2,"637":1,"638":1,"694":1,"695":1,"696":1,"729":1,"730":1,"731":1,"732":1,"733":1,"819":1,"820":1,"821":1,"822":1,"863":1,"864":1,"865":2,"866":2,"867":2,"868":2,"869":2,"870":1,"871":1,"901":1,"902":1,"903":1,"904":1,"1048":1,"1049":1,"1050":1,"1119":1,"1120":1,"1121":1,"1122":1,"1272":1,"1273":1,"1274":1,"1363":1,"1364":1,"1445":1,"1446":1,"1447":1,"1448":1,"1449":1,"1469":1,"1470":1,"1522":1,"1523":1,"1524":1,"1553":1,"1554":1,"1704":1,"1705":1,"1706":1,"1707":1,"1708":1,"1709":1,"1710":1,"1749":1,"1750":1,"1751":1,"1752":1,"1754":1,"1755":1,"1756":1,"1823":1,"1824":1,"1841":1,"1842":1,"1863":1,"1864":1,"1866":1,"1867":1,"1868":1,"1869":1,"1898":1,"1899":1,"1989":1,"1990":1,"1991":1},"2":{"0":1,"20":1,"34":1,"83":1,"157":1,"167":1,"293":3,"315":2,"341":2,"343":2,"346":2,"380":2,"399":2,"424":1,"428":1,"429":1,"453":2,"460":1,"472":1,"517":1,"520":1,"529":1,"578":1,"579":1,"582":5,"583":5,"638":1,"741":1,"768":1,"810":9,"828":1,"832":4,"833":2,"834":1,"841":3,"845":1,"914":1,"915":1,"986":2,"1000":2,"1007":4,"1078":2,"1082":1,"1094":1,"1102":1,"1116":1,"1143":1,"1181":1,"1215":1,"1389":1,"1394":1,"1395":1,"1398":3,"1624":1,"1630":3,"1635":1,"1645":1,"1667":1,"1668":1,"1672":1,"1705":1,"1712":1,"1713":3,"1714":1,"1723":5,"1728":1,"1729":1,"1736":1,"1751":1,"1752":1,"1754":1,"1755":1,"1764":1,"1788":1,"1797":4,"1799":1,"1800":1,"1821":1,"1839":1,"1883":1,"1905":1,"1917":1,"1924":1,"1933":1,"2006":1,"2059":2,"2139":1}}],["5再取整",{"2":{"2019":1}}],["5年经验区间",{"2":{"1961":1}}],["589",{"2":{"1910":1}}],["592",{"2":{"1910":1}}],["59049",{"2":{"1246":1}}],["5905",{"2":{"833":1}}],["5>",{"2":{"1802":1}}],["5397",{"2":{"1779":1}}],["5y",{"2":{"1398":1}}],["5∗0",{"2":{"1395":2}}],["5×0",{"2":{"1389":2}}],["5×一个+0",{"2":{"170":2}}],["5plus数据集上采用cfg以512x512尺寸训练595",{"2":{"1363":1}}],["5plus数据集上采用cfg以512x512尺寸训练225",{"2":{"1363":1}}],["5plus数据集上继续以512x512尺寸训练195",{"2":{"1363":1}}],["5plus数据集上laion2b",{"2":{"1363":1}}],["5plus数据集上以512x512尺寸训练515",{"2":{"1363":1}}],["5bit",{"2":{"1075":1}}],["5倍",{"2":{"980":1}}],["5us",{"2":{"935":1,"951":1}}],["557448",{"2":{"1395":3}}],["553629",{"2":{"1395":3}}],["55",{"2":{"1102":1,"1398":1}}],["5596",{"2":{"833":1}}],["558937247",{"2":{"156":1}}],["5745",{"2":{"833":1}}],["5x7",{"2":{"816":1}}],["511881",{"2":{"1396":3}}],["51cto博客",{"2":{"768":1}}],["512的二维矩阵",{"2":{"704":2}}],["512",{"2":{"36":7,"99":1,"116":1,"119":1,"137":1,"423":1,"460":1,"473":2,"532":2,"533":2,"674":1,"700":1,"704":1,"709":1,"914":1,"945":1,"965":1,"1207":4,"1317":1,"1341":2}}],["5f76f8656ea9",{"2":{"768":1}}],["54",{"2":{"739":1}}],["54米",{"2":{"713":1}}],["5个单词被切分成6个token",{"2":{"460":1}}],["5⋅min",{"2":{"402":1}}],["5model⋅min",{"2":{"402":1}}],["5时",{"2":{"396":1}}],["5时效果最好",{"2":{"396":1}}],["5000",{"2":{"1874":1}}],["5000多个中文常用字基本能组合出所有文本序列",{"2":{"566":1}}],["50182",{"2":{"1396":3}}],["50的特定配置",{"2":{"1150":1}}],["5080",{"2":{"833":1}}],["5027",{"2":{"833":1}}],["50",{"2":{"387":1,"727":1,"801":1,"802":1,"814":1,"815":1,"1086":1,"1150":1,"1177":1,"1184":1,"1623":1,"1633":2,"1634":1,"1648":1,"1715":1,"1943":1,"2131":1}}],["5到1e",{"2":{"346":1}}],["5上",{"2":{"320":1}}],["5视为一个整体",{"2":{"315":1}}],["56层的深层网络在训练集和测试集上的表现都不如20层的浅层网络",{"2":{"299":1}}],["5学习到了关于实体关系的的共同参考",{"2":{"131":1}}],["5找到了输入句子中介词",{"2":{"131":1}}],["5",{"0":{"19":1,"85":1,"87":1,"91":1,"120":1,"150":1,"151":1,"154":1,"155":1,"186":1,"241":1,"279":1,"285":1,"328":1,"337":1,"338":1,"339":1,"350":1,"388":1,"403":1,"472":1,"473":1,"531":1,"589":1,"593":1,"597":1,"601":1,"605":1,"646":1,"647":1,"648":1,"649":1,"650":1,"651":2,"652":1,"680":1,"688":1,"724":1,"760":1,"773":1,"797":1,"798":1,"799":1,"811":1,"817":1,"843":1,"857":1,"861":1,"869":1,"899":1,"925":1,"926":1,"927":1,"928":1,"929":1,"944":1,"963":1,"968":1,"969":1,"970":1,"971":1,"972":1,"997":1,"998":1,"999":1,"1018":1,"1040":1,"1041":1,"1042":1,"1043":1,"1044":1,"1045":2,"1046":1,"1081":1,"1096":1,"1108":1,"1117":1,"1209":1,"1210":1,"1211":1,"1212":1,"1213":1,"1237":1,"1268":1,"1269":1,"1270":1,"1282":1,"1299":1,"1316":1,"1326":1,"1327":1,"1328":1,"1329":1,"1330":1,"1331":1,"1353":1,"1359":1,"1360":1,"1361":1,"1374":1,"1396":1,"1443":1,"1449":1,"1457":1,"1467":1,"1488":1,"1495":1,"1517":1,"1518":1,"1519":1,"1520":1,"1551":1,"1576":1,"1591":1,"1592":1,"1593":1,"1594":1,"1633":1,"1663":1,"1671":1,"1722":1,"1748":1,"1749":1,"1750":1,"1751":1,"1752":1,"1776":1,"1803":1,"1818":1,"1819":1,"1820":1,"1821":1,"1836":1,"1837":1,"1838":1,"1839":1,"1858":1,"1859":1,"1860":1,"1861":1,"1893":1,"1894":1,"1895":1,"1896":1,"1909":1,"1925":1,"1955":1,"1975":1,"1983":1,"1984":1,"1985":1,"1986":1,"1987":1,"2079":1,"2087":1,"2095":1,"2108":1,"2140":1,"2151":1},"1":{"88":1,"89":1,"90":1,"92":1,"93":1,"94":1,"152":1,"153":1,"187":1,"188":1,"189":1,"190":1,"191":1,"192":1,"193":1,"194":1,"329":1,"330":1,"331":1,"332":1,"333":1,"334":1,"335":1,"340":1,"341":1,"351":1,"352":1,"353":1,"354":1,"355":1,"356":1,"357":1,"389":1,"390":1,"391":1,"392":1,"393":1,"394":1,"395":1,"396":1,"397":1,"398":1,"399":1,"400":1,"401":1,"402":1,"403":1,"404":1,"405":1,"406":1,"407":1,"408":1,"409":1,"410":1,"411":1,"412":1,"413":1,"414":1,"415":1,"416":1,"417":1,"418":1,"419":1,"420":1,"421":1,"422":1,"423":1,"424":1,"425":1,"426":1,"427":1,"428":1,"429":1,"532":1,"533":1,"590":1,"591":1,"592":1,"594":1,"595":1,"598":1,"599":1,"600":1,"602":1,"603":1,"604":1,"606":1,"607":1,"608":1,"647":1,"648":1,"649":1,"650":1,"651":1,"652":1,"681":1,"682":1,"683":1,"684":1,"685":1,"686":1,"687":1,"689":1,"690":1,"691":1,"692":1,"725":1,"726":1,"727":1,"798":1,"799":1,"926":1,"927":1,"928":1,"929":1,"969":1,"970":1,"971":1,"972":1,"998":1,"999":1,"1041":1,"1042":1,"1043":1,"1044":1,"1045":1,"1046":1,"1210":1,"1211":1,"1212":1,"1213":1,"1214":1,"1269":1,"1270":1,"1328":1,"1329":1,"1330":1,"1331":1,"1360":1,"1361":1,"1518":1,"1519":1,"1520":1,"1594":1,"1749":1,"1750":1,"1751":1,"1752":1,"1819":1,"1820":1,"1821":1,"1837":1,"1838":1,"1839":1,"1859":1,"1860":1,"1861":1,"1894":1,"1895":1,"1896":1,"1956":1,"1957":1,"1958":1,"1959":1,"1984":1,"1985":1,"1986":1,"1987":1},"2":{"0":1,"36":2,"47":1,"49":3,"74":2,"80":2,"83":3,"89":1,"96":5,"157":1,"170":2,"204":1,"224":2,"267":2,"293":5,"313":1,"315":4,"326":2,"380":2,"399":6,"402":11,"405":1,"407":1,"424":3,"427":2,"428":1,"451":1,"453":2,"472":1,"503":2,"529":1,"545":1,"569":1,"571":2,"578":1,"579":2,"582":8,"583":9,"638":1,"702":2,"741":1,"801":2,"802":1,"805":2,"807":1,"808":3,"809":1,"810":1,"816":1,"822":1,"828":1,"831":1,"832":3,"833":2,"834":3,"968":1,"976":1,"986":1,"1029":1,"1069":1,"1070":3,"1075":1,"1076":1,"1078":1,"1082":1,"1083":1,"1087":2,"1092":1,"1093":6,"1095":2,"1096":2,"1097":2,"1098":9,"1101":1,"1102":4,"1114":2,"1116":1,"1143":1,"1179":1,"1202":2,"1205":2,"1211":2,"1212":1,"1215":6,"1237":1,"1238":1,"1255":2,"1257":1,"1283":2,"1295":2,"1308":3,"1315":1,"1332":1,"1350":2,"1363":2,"1388":1,"1389":1,"1394":1,"1395":2,"1398":8,"1413":1,"1425":1,"1481":1,"1486":1,"1488":1,"1515":4,"1603":1,"1607":1,"1608":3,"1611":1,"1621":1,"1623":9,"1630":2,"1632":3,"1633":4,"1634":6,"1635":1,"1641":1,"1645":2,"1646":1,"1647":4,"1655":2,"1663":2,"1665":1,"1667":5,"1668":6,"1669":1,"1670":5,"1672":1,"1673":1,"1687":4,"1693":1,"1698":1,"1699":1,"1700":1,"1704":1,"1705":3,"1706":4,"1707":4,"1708":3,"1709":2,"1710":1,"1712":2,"1713":3,"1714":6,"1715":2,"1718":1,"1719":2,"1720":2,"1721":2,"1722":6,"1724":1,"1728":2,"1729":3,"1732":1,"1737":1,"1738":1,"1739":1,"1741":1,"1742":1,"1743":1,"1744":1,"1746":1,"1747":1,"1749":1,"1751":2,"1752":1,"1754":1,"1755":1,"1756":1,"1759":1,"1764":1,"1774":3,"1779":1,"1789":3,"1792":1,"1797":5,"1799":1,"1800":1,"1802":1,"1816":2,"1817":2,"1820":1,"1821":4,"1834":2,"1835":2,"1838":1,"1839":4,"1879":1,"1883":2,"1897":1,"1898":1,"1906":3,"1907":1,"1912":3,"1914":1,"1917":1,"1922":1,"1924":2,"1925":3,"1999":1,"2004":2,"2005":2,"2006":3,"2007":1,"2019":1,"2059":1,"2060":1,"2062":1,"2135":1,"2137":1,"2139":1}}],["4321",{"2":{"2125":1}}],["432553711",{"2":{"156":1}}],["4>",{"2":{"2062":5}}],["47",{"2":{"1404":1}}],["462",{"2":{"1910":1}}],["46",{"2":{"1404":1}}],["4644",{"2":{"833":1}}],["44",{"2":{"1404":1}}],["44826203",{"2":{"768":1}}],["42",{"2":{"1404":1,"1778":1,"1907":2,"1911":4,"1923":2,"1927":1,"1928":1}}],["41",{"2":{"1404":1}}],["4102",{"2":{"557":1}}],["4x",{"2":{"1398":1}}],["4−0",{"2":{"1395":2}}],["4×0",{"2":{"1389":2}}],["4f",{"2":{"1215":1,"1218":1,"1295":1}}],["4倍的挂钟速度提升",{"2":{"964":1}}],["494",{"2":{"846":1}}],["4|>",{"2":{"571":1}}],["4视为一个整体",{"2":{"315":1}}],["4个维度分别是",{"2":{"313":1}}],["4维的样本",{"2":{"313":1}}],["4=softmax",{"2":{"267":2}}],["456",{"2":{"1824":1,"1842":1}}],["458137",{"2":{"1395":2}}],["458137w",{"2":{"1395":1}}],["453383",{"2":{"1395":3}}],["45",{"2":{"267":2,"807":1,"809":1,"1102":1,"1398":1,"1404":1}}],["408知识点",{"2":{"2001":1}}],["408知识",{"0":{"1427":1},"1":{"1428":1,"1429":1,"1430":1,"1431":1,"1432":1},"2":{"2049":1}}],["4096",{"2":{"1317":1}}],["409287967",{"2":{"156":1}}],["40gb",{"2":{"980":1}}],["4000",{"2":{"1440":1,"1671":1}}],["400mf",{"2":{"1308":2}}],["400",{"2":{"572":1,"1619":1,"1729":6}}],["400万上下文",{"2":{"233":1}}],["40",{"2":{"385":1,"1315":1,"1404":1,"1623":1,"1633":2,"1634":1,"1728":1,"1806":3}}],["4d",{"2":{"113":4}}],["4k",{"2":{"87":3,"233":1}}],["48972",{"2":{"1396":3}}],["48个头中的",{"2":{"20":1}}],["48",{"2":{"20":1}}],["4",{"0":{"18":1,"41":1,"42":1,"43":1,"78":1,"79":1,"81":1,"84":2,"85":1,"102":1,"119":1,"122":1,"123":1,"132":1,"138":1,"146":2,"155":1,"170":1,"177":1,"203":1,"208":1,"240":1,"256":1,"272":1,"281":1,"282":1,"283":1,"284":2,"285":1,"286":1,"287":1,"288":1,"289":1,"290":1,"291":1,"303":1,"316":1,"318":1,"319":1,"320":1,"321":2,"328":1,"349":1,"376":1,"379":1,"384":1,"385":1,"400":1,"445":1,"451":1,"462":1,"466":1,"467":1,"491":1,"506":1,"514":1,"520":1,"530":1,"538":1,"540":1,"541":1,"542":1,"550":1,"575":1,"576":1,"577":1,"586":2,"589":1,"593":1,"637":1,"645":1,"650":1,"661":1,"670":1,"671":1,"679":1,"719":1,"751":1,"759":1,"772":1,"793":1,"794":1,"795":1,"796":1,"810":1,"813":1,"814":1,"815":1,"816":1,"822":1,"828":1,"842":1,"856":1,"860":1,"868":1,"871":1,"893":1,"894":1,"895":1,"896":1,"897":1,"898":1,"904":1,"909":1,"916":1,"917":1,"918":1,"919":1,"920":1,"921":1,"922":1,"923":2,"924":2,"929":1,"943":1,"958":1,"959":1,"960":1,"961":1,"962":2,"963":1,"964":1,"965":1,"966":1,"967":1,"972":1,"983":1,"993":1,"994":1,"995":1,"996":1,"1005":1,"1006":1,"1007":1,"1017":1,"1037":1,"1038":1,"1039":1,"1044":1,"1062":1,"1072":1,"1077":1,"1078":1,"1079":1,"1080":1,"1095":1,"1107":1,"1112":1,"1113":1,"1114":1,"1115":1,"1116":2,"1117":1,"1118":1,"1119":1,"1120":1,"1121":1,"1122":2,"1123":1,"1208":1,"1213":1,"1236":1,"1248":1,"1252":1,"1253":1,"1260":1,"1265":1,"1266":1,"1267":1,"1281":1,"1293":1,"1298":1,"1303":1,"1315":1,"1319":1,"1320":1,"1321":1,"1322":1,"1323":1,"1324":1,"1325":2,"1326":1,"1331":1,"1345":1,"1352":1,"1355":1,"1356":1,"1357":1,"1358":1,"1373":1,"1378":1,"1384":1,"1395":1,"1405":1,"1410":1,"1411":1,"1412":1,"1413":1,"1424":1,"1426":2,"1442":1,"1448":1,"1456":1,"1464":1,"1465":1,"1466":1,"1479":1,"1487":1,"1494":1,"1514":1,"1546":1,"1566":1,"1575":1,"1582":1,"1583":1,"1584":1,"1585":2,"1586":2,"1587":2,"1588":2,"1589":3,"1632":1,"1638":1,"1641":1,"1656":1,"1662":1,"1670":1,"1701":1,"1715":1,"1721":1,"1735":1,"1739":1,"1740":1,"1744":1,"1745":1,"1746":1,"1747":1,"1752":1,"1775":1,"1787":1,"1788":1,"1789":1,"1802":1,"1815":1,"1816":1,"1817":1,"1833":1,"1834":1,"1835":1,"1854":1,"1855":1,"1856":1,"1857":1,"1865":1,"1869":1,"1889":1,"1890":1,"1891":1,"1892":1,"1908":1,"1924":1,"1950":1,"1974":1,"1978":1,"1979":1,"1980":1,"1981":1,"1982":1,"1987":1,"2020":1,"2025":1,"2030":1,"2061":2,"2063":1,"2078":1,"2084":1,"2085":1,"2086":1,"2092":1,"2093":1,"2094":1,"2102":1,"2107":1,"2120":1,"2126":1,"2136":1,"2137":1,"2138":1,"2139":1,"2150":1},"1":{"44":1,"45":1,"46":1,"80":1,"82":1,"83":1,"103":1,"104":1,"105":1,"106":1,"107":1,"108":1,"109":1,"110":1,"111":1,"124":1,"125":1,"126":1,"127":1,"128":1,"129":1,"130":1,"131":1,"133":1,"134":1,"135":1,"136":1,"137":1,"139":1,"140":1,"141":1,"142":1,"143":1,"144":1,"145":1,"147":2,"148":2,"178":1,"179":1,"180":1,"181":1,"182":1,"183":1,"184":1,"185":1,"204":1,"205":1,"206":1,"207":1,"209":1,"210":1,"211":1,"212":1,"213":1,"214":1,"215":1,"216":1,"217":1,"218":1,"219":1,"220":1,"221":1,"222":1,"223":1,"224":1,"225":1,"226":1,"227":1,"228":1,"229":1,"230":1,"231":1,"232":1,"273":1,"274":1,"275":1,"276":1,"277":1,"278":1,"304":1,"305":1,"306":1,"307":1,"322":2,"323":2,"324":2,"325":2,"326":2,"327":2,"329":1,"330":1,"331":1,"332":1,"333":1,"334":1,"335":1,"377":1,"378":1,"380":1,"381":1,"382":1,"383":1,"401":1,"402":1,"463":1,"464":1,"465":1,"468":1,"469":1,"470":1,"492":1,"493":1,"494":1,"495":1,"496":1,"497":1,"498":1,"499":1,"500":1,"501":1,"502":1,"503":1,"504":1,"505":1,"507":1,"508":1,"509":1,"515":1,"516":1,"517":1,"518":1,"519":1,"520":1,"521":1,"522":1,"523":1,"524":1,"525":1,"526":1,"527":1,"528":1,"529":1,"530":1,"531":1,"532":1,"533":1,"534":1,"535":1,"536":1,"537":1,"538":1,"539":1,"540":1,"541":1,"542":1,"543":1,"578":1,"579":1,"580":1,"581":1,"582":1,"583":1,"584":1,"585":1,"587":2,"588":2,"590":1,"591":1,"592":1,"594":1,"595":1,"720":1,"721":1,"722":1,"723":1,"794":1,"795":1,"796":1,"814":1,"815":1,"816":1,"894":1,"895":1,"896":2,"897":2,"898":1,"917":1,"918":1,"919":1,"920":2,"921":2,"922":2,"923":2,"924":1,"959":1,"960":1,"961":1,"962":1,"963":1,"964":1,"965":1,"966":1,"967":1,"994":1,"995":1,"996":1,"1006":1,"1007":1,"1038":1,"1039":1,"1078":1,"1079":1,"1080":1,"1081":1,"1113":1,"1114":1,"1115":1,"1116":1,"1117":1,"1118":1,"1119":2,"1120":2,"1121":2,"1122":2,"1123":1,"1253":1,"1254":1,"1266":1,"1267":1,"1304":1,"1305":1,"1306":1,"1307":1,"1308":1,"1320":1,"1321":1,"1322":2,"1323":2,"1324":2,"1325":1,"1326":3,"1356":1,"1357":1,"1358":1,"1406":1,"1407":1,"1408":1,"1409":1,"1410":1,"1411":2,"1412":2,"1413":2,"1414":1,"1415":1,"1416":1,"1417":1,"1418":1,"1419":1,"1420":1,"1421":1,"1422":1,"1423":1,"1424":1,"1425":1,"1465":1,"1466":1,"1515":1,"1516":1,"1547":1,"1548":1,"1549":1,"1550":1,"1586":2,"1587":2,"1588":2,"1589":2,"1736":1,"1737":1,"1738":1,"1739":1,"1741":1,"1742":1,"1743":1,"1744":1,"1746":1,"1747":1,"1788":1,"1789":1,"1816":1,"1817":1,"1834":1,"1835":1,"1855":1,"1856":1,"1857":1,"1866":1,"1867":1,"1868":1,"1869":1,"1890":1,"1891":1,"1892":1,"1951":1,"1952":1,"1953":1,"1954":1,"1979":1,"1980":2,"1981":2,"1982":1,"2085":1,"2086":1,"2093":1,"2094":1,"2137":1,"2138":1,"2139":1},"2":{"0":4,"23":1,"49":6,"74":1,"83":1,"89":5,"96":9,"113":1,"119":1,"147":1,"157":4,"267":1,"292":1,"293":9,"315":5,"344":1,"399":1,"405":1,"407":1,"424":1,"427":2,"428":1,"450":2,"453":2,"472":1,"529":1,"569":1,"590":2,"668":1,"674":1,"679":1,"680":1,"741":2,"801":2,"802":1,"805":4,"810":2,"819":1,"820":4,"827":2,"828":1,"829":1,"831":1,"832":4,"833":10,"834":4,"840":1,"885":1,"944":2,"986":3,"1069":1,"1070":3,"1072":1,"1075":1,"1076":1,"1078":1,"1082":1,"1083":1,"1092":2,"1097":1,"1101":1,"1102":1,"1116":1,"1130":1,"1178":1,"1202":1,"1205":3,"1215":2,"1216":1,"1218":1,"1237":1,"1238":1,"1240":1,"1246":1,"1247":1,"1262":1,"1296":1,"1298":2,"1308":2,"1363":1,"1364":1,"1373":1,"1389":1,"1394":1,"1395":2,"1398":4,"1412":1,"1425":1,"1440":1,"1589":2,"1594":1,"1607":3,"1611":2,"1619":1,"1623":2,"1633":4,"1634":5,"1635":1,"1645":1,"1653":2,"1665":1,"1667":3,"1668":1,"1670":1,"1671":1,"1672":1,"1694":1,"1695":2,"1705":1,"1707":2,"1710":1,"1712":2,"1713":1,"1714":3,"1715":6,"1718":1,"1719":4,"1720":3,"1721":6,"1722":2,"1724":1,"1725":1,"1728":1,"1729":6,"1736":1,"1739":1,"1741":1,"1742":1,"1743":1,"1746":1,"1749":1,"1751":1,"1752":4,"1754":2,"1755":2,"1756":1,"1764":8,"1774":1,"1788":2,"1789":2,"1791":2,"1792":1,"1797":6,"1799":1,"1800":1,"1801":1,"1802":1,"1808":1,"1820":1,"1838":1,"1883":1,"1897":1,"1898":1,"1906":1,"1914":1,"1916":1,"1917":1,"1922":1,"2006":1,"2059":1,"2060":1,"2061":1,"2123":1,"2125":2,"2135":1}}],["3>",{"2":{"1912":1}}],["3i",{"2":{"1712":1}}],["375711",{"2":{"1396":3}}],["379850",{"2":{"1396":3}}],["3781",{"2":{"713":1}}],["396402",{"2":{"1396":3}}],["391829",{"2":{"1389":2}}],["391829e",{"2":{"1389":1}}],["3970",{"2":{"557":1}}],["3e",{"2":{"1332":1}}],["3所示那样",{"2":{"1177":1}}],["31",{"2":{"1332":1}}],["3101",{"2":{"1098":1}}],["31分数夺得榜首",{"2":{"735":1}}],["3=1843264×32×3×3=18432",{"2":{"1003":1}}],["384",{"2":{"976":8,"1332":1}}],["3862",{"2":{"557":1}}],["3节",{"2":{"894":1}}],["348993",{"2":{"1395":2}}],["348993w",{"2":{"1395":1}}],["34",{"2":{"844":1}}],["3rd",{"2":{"820":1}}],["35​​​​1​​=0",{"2":{"1388":1}}],["35=0",{"2":{"1388":1}}],["35net",{"2":{"1388":1}}],["35×1=2",{"2":{"1388":2}}],["35",{"2":{"807":1,"809":1,"1102":1,"1388":4,"1395":3,"1398":2,"2131":2}}],["358",{"2":{"95":1}}],["3x1",{"2":{"777":1}}],["3x3",{"2":{"232":1,"777":1}}],["33>",{"2":{"2062":1}}],["333",{"2":{"1607":1}}],["3330",{"2":{"833":1}}],["3306",{"2":{"1481":1}}],["33",{"2":{"740":1,"801":3,"802":2}}],["3a",{"2":{"713":1}}],["3ffw",{"2":{"638":1}}],["3d场景需要经过投影转化为2d图像",{"2":{"2009":1}}],["3d图形渲染",{"2":{"1936":1,"2009":1}}],["3d卷积核有三个维度",{"2":{"783":1}}],["3dpt",{"2":{"638":1}}],["3d",{"0":{"783":1},"2":{"638":2}}],["3|>",{"2":{"571":1}}],["3b",{"2":{"561":1}}],["3blue1brown",{"2":{"513":1}}],["3081",{"2":{"1215":1}}],["30",{"2":{"804":2,"1205":3,"1231":1,"1233":1,"1235":2,"1236":3,"1302":1,"1303":1,"1315":1,"1404":1,"1481":2,"1614":3,"1623":2,"1630":1,"1633":2,"1634":1,"1649":1,"1714":2,"1725":4,"1729":1,"1732":1,"1750":2,"1784":1,"1806":3,"1853":1,"1857":1,"1861":1,"2043":1,"2131":1}}],["300",{"2":{"700":4,"1102":1,"1279":1,"1684":2}}],["3000",{"2":{"372":1,"1874":1}}],["3011",{"2":{"557":1}}],["32位",{"2":{"1607":1}}],["323201",{"2":{"1396":3}}],["3233",{"2":{"557":1}}],["32x4d",{"2":{"1308":1}}],["32x8d",{"2":{"1308":1}}],["32gf",{"2":{"1308":2}}],["3270",{"2":{"833":1}}],["32",{"2":{"781":2,"814":1,"815":1,"976":12,"1003":4,"1086":1,"1215":2,"1218":1,"1257":2,"1308":2,"1608":3,"1611":1,"1678":1,"1704":1,"2086":3}}],["32对于每个字符都采用4位字节",{"2":{"607":1}}],["32k",{"2":{"561":1}}],["3243",{"2":{"557":1}}],["32828",{"2":{"557":1}}],["3的引入",{"2":{"540":1}}],["3的维度就足够捕捉绝大部分的信息了",{"2":{"19":1}}],["3在解码器端就分别被做了深度拷贝",{"2":{"449":1}}],["3小节position",{"2":{"419":1}}],["360968",{"2":{"1395":5}}],["360968w",{"2":{"1395":1}}],["360968=0",{"2":{"1394":3}}],["36",{"2":{"399":1}}],["3架构及源码解析",{"2":{"233":1}}],["3×苹果一个",{"2":{"170":1}}],["3×苹果一个=0",{"2":{"170":1}}],["3倍",{"2":{"147":1}}],["3通过助动词",{"2":{"131":1}}],["3",{"0":{"5":1,"15":1,"23":1,"24":1,"37":2,"43":1,"69":1,"73":1,"76":2,"81":1,"101":1,"116":1,"117":1,"118":2,"119":1,"120":1,"132":1,"138":1,"154":1,"162":1,"174":1,"197":1,"201":1,"239":1,"248":1,"258":1,"262":1,"266":2,"272":1,"279":1,"283":1,"300":1,"311":1,"313":1,"314":1,"315":2,"316":1,"320":1,"339":1,"348":1,"362":1,"372":1,"373":1,"374":2,"384":1,"385":1,"397":1,"428":1,"438":1,"450":1,"453":1,"454":1,"455":2,"467":1,"486":1,"519":1,"527":1,"535":1,"536":1,"537":2,"538":1,"542":1,"549":1,"559":1,"564":1,"569":1,"570":2,"577":1,"605":1,"625":1,"644":1,"649":1,"660":1,"668":1,"669":1,"678":1,"705":1,"711":1,"712":1,"716":2,"719":1,"724":1,"728":1,"734":1,"735":1,"736":1,"737":1,"746":1,"750":1,"753":1,"757":1,"758":2,"759":1,"760":1,"761":1,"762":1,"763":1,"764":1,"765":1,"766":1,"767":1,"771":1,"787":1,"792":1,"796":1,"806":1,"807":1,"808":1,"809":2,"810":1,"811":1,"812":1,"816":1,"821":1,"827":1,"841":1,"852":1,"853":1,"854":1,"855":2,"856":1,"857":1,"858":1,"859":1,"867":1,"870":1,"875":1,"882":1,"883":1,"889":1,"890":1,"891":1,"892":1,"898":1,"903":1,"908":1,"913":1,"914":1,"915":1,"919":1,"922":1,"928":1,"942":1,"955":1,"956":1,"957":1,"961":1,"971":1,"982":1,"992":1,"996":1,"1004":1,"1013":1,"1014":1,"1015":1,"1016":2,"1017":1,"1018":1,"1019":1,"1020":1,"1027":1,"1032":1,"1033":1,"1034":1,"1035":1,"1036":2,"1043":1,"1050":1,"1054":1,"1061":1,"1071":1,"1074":1,"1075":1,"1076":1,"1080":1,"1087":1,"1094":1,"1103":1,"1104":1,"1105":1,"1106":2,"1107":1,"1108":1,"1109":1,"1110":1,"1111":1,"1115":1,"1121":1,"1207":1,"1212":1,"1223":1,"1227":1,"1228":1,"1232":1,"1233":1,"1234":1,"1235":2,"1236":1,"1237":1,"1238":1,"1239":1,"1240":1,"1241":1,"1242":1,"1243":1,"1244":1,"1245":1,"1246":1,"1247":1,"1251":1,"1259":1,"1261":1,"1262":1,"1263":1,"1264":2,"1274":1,"1277":1,"1278":1,"1279":1,"1280":2,"1288":1,"1292":1,"1294":1,"1295":1,"1296":1,"1297":2,"1302":1,"1311":1,"1312":1,"1313":1,"1314":2,"1315":1,"1316":1,"1317":1,"1318":1,"1324":1,"1330":1,"1334":1,"1335":1,"1336":1,"1337":1,"1338":1,"1339":1,"1340":1,"1341":2,"1342":2,"1343":2,"1344":3,"1345":2,"1348":1,"1349":1,"1350":1,"1351":2,"1352":1,"1353":1,"1354":1,"1372":1,"1383":1,"1390":1,"1391":1,"1392":1,"1393":1,"1394":1,"1409":1,"1413":1,"1419":1,"1420":1,"1421":1,"1422":1,"1423":2,"1424":1,"1425":1,"1441":1,"1447":1,"1455":1,"1461":1,"1462":1,"1463":1,"1478":1,"1484":1,"1485":1,"1486":2,"1487":1,"1488":1,"1489":1,"1493":1,"1511":1,"1520":1,"1545":1,"1565":1,"1571":1,"1574":1,"1580":1,"1584":1,"1588":1,"1593":1,"1594":1,"1598":1,"1621":1,"1631":1,"1655":1,"1661":1,"1669":1,"1700":1,"1714":1,"1720":1,"1738":1,"1740":1,"1741":1,"1742":1,"1743":2,"1744":1,"1745":1,"1751":1,"1753":1,"1756":1,"1757":1,"1774":1,"1780":1,"1782":1,"1783":1,"1784":1,"1786":2,"1801":1,"1807":1,"1812":1,"1813":1,"1814":1,"1821":1,"1830":1,"1831":1,"1832":1,"1839":1,"1850":1,"1851":1,"1852":1,"1853":2,"1857":1,"1861":1,"1868":1,"1880":1,"1884":1,"1885":1,"1886":1,"1887":1,"1888":2,"1892":1,"1896":1,"1907":1,"1923":1,"1945":1,"1970":1,"1971":1,"1972":1,"1973":2,"1974":1,"1975":1,"1976":1,"1977":1,"1986":1,"1991":1,"2019":1,"2021":1,"2022":1,"2023":1,"2024":2,"2025":1,"2026":1,"2027":1,"2028":1,"2029":2,"2030":1,"2031":1,"2059":1,"2060":1,"2062":1,"2068":1,"2069":1,"2070":1,"2077":1,"2080":1,"2081":1,"2082":1,"2083":2,"2091":1,"2101":1,"2103":1,"2104":1,"2105":1,"2106":2,"2116":1,"2117":1,"2118":1,"2119":2,"2125":1,"2131":1,"2132":1,"2133":1,"2134":1,"2135":2,"2139":1,"2149":1},"1":{"16":1,"17":1,"25":1,"26":1,"27":1,"28":1,"29":1,"30":1,"31":1,"32":1,"33":1,"34":1,"35":1,"36":1,"38":2,"39":2,"44":1,"45":1,"46":1,"70":1,"71":1,"72":1,"74":1,"75":1,"82":1,"83":1,"133":1,"134":1,"135":1,"136":1,"137":1,"139":1,"140":1,"141":1,"142":1,"143":1,"144":1,"145":1,"163":1,"164":1,"165":1,"166":1,"167":1,"168":1,"169":1,"175":1,"176":1,"198":1,"199":1,"200":1,"249":1,"250":1,"251":1,"252":1,"253":1,"254":1,"255":1,"259":1,"260":1,"261":1,"263":1,"264":1,"265":1,"267":2,"268":2,"269":2,"270":2,"271":2,"273":1,"274":1,"275":1,"276":1,"277":1,"278":1,"301":1,"302":1,"340":1,"341":1,"363":1,"364":1,"365":1,"366":1,"367":1,"368":1,"369":1,"370":1,"371":1,"372":1,"373":1,"374":1,"375":1,"376":1,"377":1,"378":1,"379":1,"380":1,"381":1,"382":1,"383":1,"384":1,"385":1,"386":1,"387":1,"398":1,"399":1,"439":1,"440":1,"441":1,"442":1,"443":1,"444":1,"456":2,"457":2,"458":2,"459":2,"460":2,"468":1,"469":1,"470":1,"487":1,"488":1,"489":1,"490":1,"528":1,"529":1,"560":1,"561":1,"562":1,"565":1,"566":1,"567":1,"568":1,"571":2,"572":2,"573":2,"578":1,"579":1,"580":1,"581":1,"582":1,"583":1,"584":1,"585":1,"606":1,"607":1,"608":1,"626":1,"627":1,"628":1,"629":1,"630":1,"631":1,"632":1,"633":1,"634":1,"635":1,"636":1,"706":1,"707":1,"708":1,"709":1,"713":1,"714":1,"715":1,"717":2,"718":2,"720":1,"721":1,"722":1,"723":1,"725":1,"726":1,"727":1,"729":1,"730":1,"731":1,"732":1,"733":1,"738":1,"739":1,"754":1,"755":1,"756":1,"807":1,"808":1,"809":1,"810":1,"811":1,"812":1,"853":1,"854":1,"855":1,"856":1,"857":1,"858":1,"859":1,"892":1,"914":1,"915":1,"920":1,"921":1,"922":1,"923":1,"956":1,"957":1,"1014":1,"1015":1,"1016":1,"1017":1,"1018":1,"1019":1,"1020":1,"1034":1,"1035":1,"1036":1,"1075":1,"1076":1,"1104":1,"1105":1,"1106":1,"1107":1,"1108":1,"1109":1,"1110":1,"1111":1,"1262":1,"1263":1,"1264":1,"1278":1,"1279":1,"1280":1,"1295":1,"1296":1,"1297":1,"1312":1,"1313":1,"1314":1,"1315":1,"1316":1,"1317":1,"1318":1,"1335":1,"1336":2,"1337":2,"1338":1,"1339":2,"1340":2,"1341":1,"1342":3,"1343":3,"1344":3,"1345":3,"1349":1,"1350":1,"1351":1,"1352":1,"1353":1,"1354":1,"1391":1,"1392":1,"1393":1,"1394":1,"1421":1,"1422":1,"1423":1,"1424":1,"1462":1,"1463":1,"1512":1,"1513":1,"1572":1,"1573":1,"1574":1,"1575":1,"1576":1,"1577":1,"1578":1,"1579":1,"1594":1,"1741":1,"1742":1,"1743":1,"1744":1,"1746":1,"1747":1,"1754":1,"1755":1,"1756":1,"1758":1,"1781":1,"1783":1,"1784":1,"1785":1,"1786":1,"1813":1,"1814":1,"1831":1,"1832":1,"1851":1,"1852":1,"1853":1,"1886":1,"1887":1,"1888":1,"1946":1,"1947":1,"1948":1,"1949":1,"1971":1,"1972":1,"1973":1,"1974":1,"1975":1,"1976":1,"1977":1,"2022":1,"2023":1,"2024":1,"2025":1,"2027":1,"2028":1,"2029":1,"2030":1,"2069":1,"2070":1,"2081":1,"2082":1,"2083":1,"2104":1,"2105":1,"2106":1,"2117":1,"2118":1,"2119":1,"2133":1,"2134":1,"2135":1},"2":{"0":7,"20":1,"36":6,"37":1,"49":5,"66":1,"74":4,"80":1,"83":1,"88":1,"89":2,"96":10,"109":2,"113":1,"114":1,"122":1,"145":1,"156":1,"157":4,"170":2,"199":1,"201":1,"224":2,"267":10,"293":10,"315":8,"334":1,"380":1,"382":1,"385":1,"399":6,"405":1,"407":1,"424":1,"427":2,"428":1,"429":1,"450":2,"472":1,"515":1,"529":1,"533":1,"569":3,"571":1,"578":1,"579":1,"582":7,"583":7,"590":5,"595":1,"611":2,"664":1,"668":1,"679":1,"680":1,"702":2,"704":2,"741":15,"776":3,"783":1,"801":4,"802":4,"805":5,"807":2,"808":2,"809":2,"810":5,"814":2,"815":2,"819":1,"820":2,"821":1,"822":1,"825":1,"826":2,"827":2,"828":1,"832":3,"834":10,"839":1,"842":1,"844":1,"847":1,"855":1,"973":1,"986":2,"987":1,"1003":5,"1004":2,"1069":2,"1070":3,"1071":1,"1072":2,"1076":1,"1078":2,"1083":1,"1092":2,"1094":1,"1095":3,"1096":3,"1097":1,"1099":1,"1102":2,"1116":2,"1137":1,"1143":1,"1157":1,"1177":1,"1202":3,"1205":3,"1207":1,"1215":6,"1222":2,"1237":1,"1238":1,"1240":1,"1246":1,"1247":1,"1254":1,"1257":2,"1272":1,"1282":1,"1283":2,"1308":3,"1316":6,"1363":1,"1364":2,"1394":1,"1395":1,"1398":1,"1412":1,"1425":1,"1474":1,"1476":2,"1481":5,"1589":1,"1604":2,"1607":5,"1611":2,"1612":1,"1614":1,"1615":2,"1623":4,"1633":3,"1634":7,"1635":1,"1645":1,"1667":1,"1670":1,"1672":1,"1673":2,"1674":1,"1680":1,"1691":1,"1693":1,"1694":1,"1695":2,"1698":1,"1699":1,"1700":2,"1701":1,"1705":10,"1706":1,"1707":6,"1708":2,"1709":1,"1710":1,"1712":3,"1713":6,"1714":26,"1715":1,"1718":1,"1719":11,"1720":12,"1721":8,"1722":8,"1723":2,"1724":7,"1725":1,"1728":2,"1737":1,"1738":1,"1739":1,"1741":1,"1742":1,"1743":1,"1746":1,"1749":1,"1751":2,"1752":1,"1754":1,"1755":1,"1756":6,"1761":1,"1762":1,"1763":2,"1764":1,"1774":1,"1779":1,"1788":1,"1789":2,"1791":2,"1792":2,"1797":6,"1799":1,"1800":1,"1801":1,"1802":1,"1808":1,"1817":1,"1820":1,"1821":3,"1825":1,"1835":1,"1838":1,"1839":3,"1843":1,"1853":1,"1857":1,"1861":1,"1879":2,"1891":1,"1897":1,"1898":1,"1905":1,"1906":1,"1908":4,"1910":1,"1914":1,"1916":3,"1917":2,"1918":1,"1922":2,"1925":4,"1926":2,"1928":1,"1966":1,"1971":1,"1980":1,"1999":1,"2006":1,"2043":1,"2059":1,"2086":1,"2123":1,"2125":3,"2131":1,"2135":3,"2139":2,"2154":1}}],["2个垂直领域深入",{"2":{"1961":1}}],["2个",{"2":{"1918":1}}],["2个字词同时连续出现",{"2":{"582":1}}],["2个字词不是完全同时连续出现",{"2":{"582":1}}],["2y",{"2":{"1398":1}}],["2=outo1",{"2":{"1393":1}}],["2=11+e−net−1",{"2":{"1393":1}}],["2=1+e−net−1",{"2":{"1393":1}}],["2=0",{"2":{"1389":1}}],["2−1∗−1+0=−",{"2":{"1393":1}}],["2−82−82^",{"2":{"765":1}}],["2e",{"2":{"1393":1}}],["2extract",{"2":{"1302":1}}],["2x",{"2":{"1364":2}}],["2x2",{"2":{"1178":1}}],["2为初始权重",{"2":{"1363":3}}],["2为pad",{"2":{"450":1,"451":1}}],["2i​​",{"2":{"1336":1}}],["2i​​=sin",{"2":{"1336":1}}],["2i+12i",{"2":{"1336":2}}],["2i+1​​",{"2":{"1336":1}}],["2i+1​​=cos",{"2":{"1336":1}}],["2i+1p",{"2":{"1336":1}}],["2i+1",{"2":{"1336":2}}],["2i+1=cos",{"2":{"1336":1}}],["2i",{"2":{"1336":3}}],["2i=sin",{"2":{"1336":1}}],["2gf",{"2":{"1308":2}}],["2f",{"2":{"1295":1}}],["2fanout",{"2":{"1006":1}}],["2fanin",{"2":{"1006":1}}],["2v",{"2":{"1191":1,"1192":1,"1193":1,"2038":1}}],["2var",{"2":{"1002":2,"1003":1}}],["2β​2​​",{"2":{"1174":1}}],["2+12",{"2":{"1389":1}}],["2+",{"2":{"1003":1}}],["2+var",{"2":{"1002":1}}],["2+b",{"2":{"99":1}}],["2倍",{"2":{"983":1}}],["2算法",{"2":{"973":1}}],["2算法遵循简化的同步模型",{"2":{"973":1}}],["2以利用这些硬件特性",{"2":{"973":1}}],["2节的方程中",{"2":{"964":1}}],["2节中回顾了hopper在这些方向上提供的功能",{"2":{"973":1}}],["2节中",{"2":{"941":1,"960":1}}],["2b数据集上训练的clip",{"2":{"1363":1}}],["2b",{"2":{"943":1,"961":1}}],["2在pytorch实现的注意力机制",{"2":{"940":1,"962":1}}],["2上最高可达7",{"2":{"940":1,"962":1}}],["2π",{"2":{"844":1}}],["29",{"2":{"1404":1,"1623":1}}],["299497",{"2":{"1395":3}}],["2959",{"2":{"833":1}}],["29左右",{"2":{"739":1}}],["28=784维",{"2":{"1370":1}}],["28",{"2":{"781":4,"1207":4,"1215":2,"1259":2,"1262":2,"1263":2,"1267":2,"1269":2,"1270":2,"1370":1,"1373":2,"1404":1,"1905":1,"2137":1,"2139":1}}],["28591",{"2":{"557":1}}],["27",{"0":{"1915":1},"1":{"1916":1,"1917":1},"2":{"1244":1,"1404":1}}],["27590277",{"2":{"768":1}}],["2706",{"2":{"557":1}}],["2^",{"2":{"765":1,"1192":1,"1193":1}}],["2^8",{"2":{"765":1}}],["2^3",{"2":{"765":1}}],["2^2",{"2":{"765":1}}],["2^1",{"2":{"765":1}}],["2^tk",{"2":{"175":1}}],["2k×d",{"2":{"763":1}}],["2|>",{"2":{"571":1}}],["21",{"0":{"1809":1,"1827":1},"1":{"1810":1,"1811":1,"1812":1,"1813":1,"1814":1,"1815":1,"1816":1,"1817":1,"1818":1,"1819":1,"1820":1,"1821":1,"1822":1,"1823":1,"1824":1,"1825":1,"1826":1,"1828":1,"1829":1,"1830":1,"1831":1,"1832":1,"1833":1,"1834":1,"1835":1,"1836":1,"1837":1,"1838":1,"1839":1,"1840":1,"1841":1,"1842":1,"1843":1,"1844":1},"2":{"1404":1,"2043":3,"2059":1}}],["214925",{"2":{"1396":3}}],["2147",{"2":{"557":1}}],["212",{"2":{"1007":1}}],["2127",{"2":{"315":4}}],["210021002^",{"2":{"698":1}}],["216k",{"2":{"561":1}}],["262855",{"2":{"1396":3}}],["26",{"2":{"1244":1,"1404":1,"1725":1,"2043":1}}],["264",{"2":{"1611":1}}],["264×3×3",{"2":{"1004":1}}],["2641",{"2":{"557":1}}],["2667",{"2":{"156":1}}],["2编码的信息向新目标的embedding",{"2":{"485":1}}],["2则将其映射回",{"2":{"485":1}}],["25开始",{"2":{"2056":1}}],["255",{"2":{"592":2,"1817":1,"1835":1}}],["256",{"2":{"571":1,"591":4,"592":4,"606":1,"614":1,"1215":3,"1254":4,"1308":1}}],["25",{"2":{"477":1,"572":1,"839":1,"1102":1,"1215":1,"1257":1,"1266":1,"1398":1,"1404":1,"1436":1,"1607":3,"1661":1,"1662":1,"1725":4,"1728":1,"1729":1,"1921":1,"1924":1,"2043":1,"2046":1,"2059":1,"2086":2,"2139":1}}],["2500",{"2":{"399":4}}],["2月4日拉取其代码",{"2":{"432":1}}],["2d",{"2":{"423":1,"700":1,"1082":2}}],["23标准",{"2":{"1960":1}}],["238",{"2":{"1910":1}}],["232",{"2":{"1611":1}}],["232×3×3",{"2":{"1003":1}}],["231",{"2":{"1607":2}}],["2310",{"2":{"387":1}}],["23",{"2":{"1177":1,"1404":1,"1474":1,"1481":1,"2043":2}}],["2309",{"2":{"387":1,"768":1}}],["2303",{"2":{"387":1}}],["2307",{"2":{"233":1,"740":1}}],["2是",{"2":{"380":1}}],["2进行缩放和偏移",{"2":{"343":1}}],["2和b",{"2":{"343":1}}],["2和一个对角矩阵diag",{"2":{"46":1}}],["2对应beta",{"2":{"343":1}}],["2对应gamma",{"2":{"343":1}}],["2u",{"2":{"289":1}}],["224",{"2":{"1254":3,"1272":2,"1282":2,"1283":2}}],["2203",{"2":{"638":1}}],["2292",{"2":{"557":1}}],["2212",{"2":{"543":1}}],["22",{"0":{"2057":1},"1":{"2058":1,"2059":1,"2060":1,"2061":1,"2062":1},"2":{"233":1,"1404":1,"2042":1,"2043":1}}],["2可以有助于对序列中两个元素",{"2":{"209":1}}],["2已经发生了某种程度上的信息交换",{"2":{"172":1}}],["2×我+0",{"2":{"170":2}}],["24年最新版本的有一些bug",{"2":{"2064":1}}],["246422",{"2":{"1395":3}}],["241207",{"2":{"2088":1}}],["2412",{"2":{"638":1}}],["241013",{"2":{"2050":1}}],["2410",{"2":{"513":1}}],["241108",{"2":{"2051":1}}],["241125",{"2":{"2050":1}}],["241111",{"2":{"2050":1}}],["2411",{"2":{"156":1}}],["240910",{"2":{"2043":1,"2064":1}}],["2405",{"2":{"557":1}}],["2406",{"2":{"361":1}}],["2401",{"2":{"156":1,"233":1}}],["24",{"2":{"233":1,"1404":1,"1774":1,"2086":1}}],["2nin+nout",{"2":{"1000":1}}],["2nd",{"2":{"820":1}}],["2n",{"2":{"155":1}}],["2的梯度则与新的目标嵌入相同",{"2":{"485":1}}],["2的梯度则旨在将ff2ff2ff",{"2":{"485":1}}],["2的梯度则使用δiδi",{"2":{"485":1}}],["2的神经元中减去",{"2":{"148":1}}],["2的输出进行调整",{"2":{"485":1}}],["2的输出移向新目标",{"2":{"148":1}}],["2的输出",{"2":{"148":1}}],["2的mlp层缩减到1",{"2":{"147":1}}],["2神经元的激活程度",{"2":{"148":1}}],["2一致",{"2":{"147":1}}],["2差大约1",{"2":{"147":1}}],["2∈r^",{"2":{"101":1}}],["20倍",{"2":{"964":1}}],["2035",{"2":{"833":1}}],["2041",{"2":{"557":1}}],["2048",{"2":{"99":1,"698":1}}],["2063",{"2":{"557":1}}],["200w+",{"2":{"1948":1}}],["2000",{"2":{"1729":2,"1874":1}}],["2001",{"2":{"1661":1,"1662":1}}],["200",{"2":{"1279":2,"1677":1}}],["2003",{"2":{"429":1,"557":1}}],["2006",{"2":{"429":1,"768":1}}],["201",{"2":{"1779":1}}],["2013年11月迁移到github",{"2":{"1476":1}}],["2011",{"2":{"1042":1}}],["2010年这个项目由apache",{"2":{"1476":1}}],["2010s",{"2":{"907":1}}],["2010",{"2":{"840":1}}],["2012",{"2":{"775":1,"840":1,"1048":1,"1176":1}}],["2016",{"2":{"543":2,"844":1}}],["2019",{"2":{"513":1,"956":1,"1969":1}}],["2015",{"2":{"470":1}}],["2015年",{"2":{"125":1}}],["2018",{"2":{"432":1,"638":1,"1131":1,"1133":1,"1137":1,"1186":1}}],["2017年",{"2":{"845":1}}],["2017",{"2":{"429":1,"956":1,"1312":1,"1472":1}}],["2014",{"2":{"370":1,"1059":1}}],["2022",{"2":{"347":1,"429":2,"543":1,"638":1,"937":1,"953":1}}],["2025年",{"2":{"432":1}}],["2025年三月",{"2":{"48":1}}],["2025",{"0":{"2047":1},"1":{"2048":1,"2049":1},"2":{"233":1}}],["2021",{"2":{"233":1,"429":1,"513":1}}],["2020",{"2":{"156":1,"768":1}}],["2023001",{"2":{"1728":1}}],["2023",{"2":{"156":1,"387":1,"429":1,"557":1,"956":1,"1195":1,"1729":2}}],["20241112173201855",{"2":{"2021":2}}],["2024中获得国际认可",{"2":{"2011":1}}],["2024最佳论文",{"2":{"156":1,"513":1}}],["2024",{"0":{"2041":1,"2044":1},"1":{"2042":1,"2043":1,"2045":1,"2046":1},"2":{"95":1,"156":4,"233":3,"361":1,"429":1,"1729":2,"2034":1,"2051":1,"2053":2,"2056":1}}],["20",{"0":{"1767":1},"1":{"1768":1,"1769":1,"1770":1,"1771":1,"1772":1,"1773":1,"1774":1,"1775":1,"1776":1,"1777":1,"1778":1,"1779":1,"1780":1,"1781":1,"1782":1,"1783":1,"1784":1,"1785":1,"1786":1,"1787":1,"1788":1,"1789":1,"1790":1,"1791":1,"1792":1,"1793":1},"2":{"83":2,"326":2,"399":2,"424":2,"801":1,"802":1,"804":3,"807":1,"808":1,"809":1,"810":1,"814":1,"815":1,"835":1,"1205":2,"1218":1,"1231":2,"1244":3,"1404":1,"1481":1,"1608":2,"1612":2,"1613":1,"1614":2,"1623":2,"1630":1,"1633":2,"1634":1,"1639":1,"1640":1,"1648":1,"1649":1,"1650":1,"1651":1,"1654":1,"1663":2,"1672":1,"1714":4,"1715":1,"1728":4,"1732":2,"1750":2,"1784":1,"1785":2,"1806":8,"1853":2,"1857":2,"1861":2,"2003":1,"2005":1,"2131":2,"2139":1}}],["2",{"0":{"4":1,"7":1,"11":2,"15":1,"18":1,"19":1,"20":1,"21":1,"24":1,"42":1,"56":1,"61":1,"64":2,"73":1,"79":1,"91":1,"100":1,"113":1,"114":2,"117":1,"123":1,"151":1,"161":1,"172":1,"173":2,"174":1,"177":1,"186":1,"195":1,"201":1,"208":1,"238":1,"244":1,"247":2,"248":1,"256":1,"262":1,"282":1,"297":1,"309":1,"310":2,"311":1,"314":1,"319":1,"338":1,"344":1,"347":1,"366":1,"370":2,"373":1,"379":1,"392":1,"426":1,"427":2,"428":1,"432":1,"435":1,"448":1,"449":2,"450":1,"451":1,"454":1,"464":1,"466":1,"473":1,"481":1,"512":1,"518":1,"525":1,"526":2,"527":1,"530":1,"531":1,"536":1,"541":1,"548":1,"557":1,"558":2,"559":1,"569":1,"576":1,"601":1,"616":1,"641":1,"642":1,"643":2,"648":1,"659":1,"662":1,"665":1,"666":1,"667":2,"668":1,"671":1,"677":1,"698":1,"699":2,"705":1,"712":1,"745":1,"748":1,"749":2,"750":1,"751":1,"757":1,"770":1,"782":1,"786":1,"789":1,"790":1,"791":2,"795":1,"799":1,"802":1,"803":1,"804":1,"805":2,"808":1,"815":1,"820":1,"826":1,"832":1,"840":1,"851":1,"854":1,"864":1,"865":1,"866":2,"867":1,"868":1,"869":1,"874":1,"879":1,"880":1,"881":2,"882":1,"884":1,"885":1,"886":2,"887":2,"888":3,"889":2,"890":1,"895":1,"896":1,"897":2,"902":1,"907":1,"912":1,"915":1,"918":1,"920":1,"921":2,"922":1,"923":1,"927":1,"934":1,"938":1,"941":1,"947":1,"952":1,"953":1,"954":2,"957":1,"960":1,"968":1,"970":1,"981":1,"986":1,"989":1,"990":1,"991":1,"995":1,"999":1,"1003":1,"1007":1,"1012":1,"1015":1,"1024":1,"1025":1,"1026":2,"1027":1,"1028":1,"1029":1,"1030":2,"1031":2,"1032":1,"1035":1,"1039":1,"1042":1,"1049":1,"1053":1,"1057":1,"1060":1,"1065":1,"1070":1,"1073":1,"1076":1,"1079":1,"1086":1,"1090":1,"1091":1,"1092":1,"1093":2,"1094":1,"1095":1,"1096":1,"1097":1,"1098":1,"1099":1,"1100":1,"1101":1,"1102":1,"1105":1,"1114":1,"1120":1,"1203":1,"1204":1,"1205":1,"1206":2,"1211":1,"1222":1,"1224":1,"1225":1,"1226":2,"1227":1,"1230":1,"1231":1,"1234":1,"1250":1,"1256":1,"1257":1,"1258":2,"1259":1,"1260":1,"1263":1,"1267":1,"1270":1,"1273":1,"1276":1,"1279":1,"1287":1,"1289":1,"1290":1,"1291":2,"1292":1,"1293":1,"1296":1,"1301":1,"1310":1,"1313":1,"1321":1,"1322":1,"1323":2,"1324":1,"1325":1,"1326":1,"1329":1,"1333":1,"1337":1,"1338":1,"1339":1,"1340":2,"1343":2,"1347":1,"1350":1,"1357":1,"1358":1,"1361":1,"1364":1,"1371":1,"1376":1,"1377":1,"1382":1,"1387":1,"1388":1,"1389":2,"1393":1,"1394":1,"1408":1,"1412":1,"1414":1,"1415":1,"1416":2,"1417":2,"1418":3,"1419":2,"1420":1,"1421":1,"1422":2,"1423":1,"1424":1,"1436":1,"1439":1,"1440":1,"1446":1,"1454":1,"1458":1,"1459":1,"1460":2,"1463":1,"1466":1,"1470":1,"1477":1,"1481":1,"1482":2,"1485":1,"1492":1,"1508":1,"1519":1,"1542":1,"1564":1,"1568":1,"1569":2,"1573":1,"1580":1,"1583":1,"1587":1,"1592":1,"1597":1,"1620":1,"1624":1,"1630":1,"1654":1,"1660":1,"1668":1,"1684":1,"1688":1,"1699":1,"1713":1,"1719":1,"1735":1,"1736":1,"1737":2,"1738":1,"1739":1,"1742":1,"1747":1,"1750":1,"1755":1,"1773":1,"1777":1,"1778":1,"1779":2,"1780":1,"1784":1,"1789":1,"1792":1,"1798":1,"1799":1,"1800":2,"1801":1,"1802":1,"1803":1,"1804":1,"1805":1,"1806":2,"1807":1,"1811":1,"1814":1,"1817":1,"1820":1,"1824":1,"1829":1,"1832":1,"1835":1,"1838":1,"1842":1,"1847":1,"1848":1,"1849":2,"1852":1,"1856":1,"1860":1,"1864":1,"1867":1,"1879":1,"1881":1,"1882":1,"1883":2,"1884":1,"1887":1,"1891":1,"1895":1,"1899":1,"1906":1,"1922":1,"1940":1,"1965":1,"1966":1,"1967":2,"1968":2,"1969":3,"1972":1,"1981":1,"1982":1,"1985":1,"1990":1,"1994":1,"2014":1,"2015":1,"2016":2,"2017":2,"2018":3,"2019":2,"2020":2,"2021":1,"2022":1,"2023":2,"2024":1,"2025":1,"2026":1,"2027":1,"2028":2,"2029":1,"2030":1,"2055":1,"2059":1,"2067":1,"2070":1,"2074":1,"2075":1,"2076":2,"2077":1,"2078":1,"2079":1,"2082":1,"2086":1,"2090":1,"2094":1,"2098":1,"2099":1,"2100":2,"2101":1,"2102":1,"2105":1,"2113":1,"2114":1,"2115":2,"2118":1,"2124":1,"2128":1,"2129":1,"2130":2,"2131":1,"2134":1,"2138":1,"2148":1},"1":{"8":1,"9":1,"10":1,"12":2,"13":2,"14":2,"16":1,"17":1,"25":1,"26":1,"27":1,"28":1,"29":1,"30":1,"31":1,"32":1,"33":1,"34":1,"35":1,"36":1,"57":1,"58":1,"59":1,"62":1,"63":1,"65":2,"66":2,"67":2,"74":1,"75":1,"80":1,"92":1,"93":1,"94":1,"124":1,"125":1,"126":1,"127":1,"128":1,"129":1,"130":1,"131":1,"152":1,"153":1,"175":1,"176":1,"178":1,"179":1,"180":1,"181":1,"182":1,"183":1,"184":1,"185":1,"187":1,"188":1,"189":1,"190":1,"191":1,"192":1,"193":1,"194":1,"209":1,"210":1,"211":1,"212":1,"213":1,"214":1,"215":1,"216":1,"217":1,"218":1,"219":1,"220":1,"221":1,"222":1,"223":1,"224":1,"225":1,"226":1,"227":1,"228":1,"229":1,"230":1,"231":1,"232":1,"245":1,"246":1,"249":1,"250":1,"251":1,"252":1,"253":1,"254":1,"255":1,"263":1,"264":1,"265":1,"298":1,"299":1,"367":1,"368":1,"369":1,"380":1,"381":1,"382":1,"383":1,"393":1,"394":1,"395":1,"396":1,"436":1,"437":1,"482":1,"483":1,"484":1,"485":1,"528":1,"529":1,"532":1,"533":1,"560":1,"561":1,"562":1,"602":1,"603":1,"604":1,"617":1,"618":1,"619":1,"620":1,"621":1,"622":1,"623":1,"624":1,"642":1,"643":1,"700":2,"701":2,"702":2,"703":2,"704":2,"706":1,"707":1,"708":1,"709":1,"713":1,"714":1,"715":1,"790":1,"791":1,"804":1,"805":1,"865":1,"866":1,"867":1,"868":1,"869":1,"880":1,"881":1,"882":1,"887":2,"888":2,"889":2,"890":2,"896":1,"897":1,"953":1,"969":1,"970":1,"971":1,"972":1,"990":1,"991":1,"1025":1,"1026":1,"1027":1,"1029":1,"1030":1,"1031":1,"1032":1,"1092":1,"1093":1,"1094":1,"1095":1,"1096":1,"1097":1,"1098":1,"1099":1,"1100":1,"1101":1,"1102":1,"1205":1,"1206":1,"1225":1,"1226":1,"1227":1,"1231":1,"1232":1,"1233":1,"1234":1,"1235":1,"1236":1,"1237":1,"1238":1,"1239":1,"1240":1,"1241":1,"1242":1,"1243":1,"1244":1,"1245":1,"1246":1,"1247":1,"1248":1,"1257":1,"1258":1,"1259":1,"1260":1,"1290":1,"1291":1,"1292":1,"1293":1,"1322":1,"1323":1,"1324":1,"1326":1,"1339":1,"1340":1,"1377":1,"1388":1,"1389":1,"1415":1,"1416":1,"1417":3,"1418":3,"1419":3,"1420":1,"1421":2,"1422":2,"1423":2,"1424":2,"1459":1,"1460":1,"1509":1,"1510":1,"1543":1,"1544":1,"1736":1,"1737":1,"1738":1,"1739":1,"1778":1,"1779":1,"1780":1,"1781":2,"1799":1,"1800":1,"1801":1,"1802":1,"1803":1,"1805":1,"1806":1,"1807":1,"1848":1,"1849":1,"1882":1,"1883":1,"1884":1,"1941":1,"1942":1,"1943":1,"1944":1,"1966":1,"1967":1,"1968":3,"1969":3,"2015":1,"2016":1,"2017":3,"2018":3,"2019":3,"2020":3,"2021":1,"2022":2,"2023":2,"2024":2,"2025":2,"2026":1,"2027":2,"2028":2,"2029":2,"2030":2,"2075":1,"2076":1,"2077":1,"2078":1,"2079":1,"2099":1,"2100":1,"2101":1,"2102":1,"2114":1,"2115":1,"2129":1,"2130":1,"2131":1},"2":{"0":11,"20":1,"36":9,"39":1,"49":7,"54":1,"66":5,"67":1,"74":6,"79":1,"80":1,"82":1,"83":1,"88":1,"96":7,"99":1,"106":2,"113":3,"114":1,"119":1,"122":1,"125":1,"135":4,"145":1,"147":1,"156":1,"157":10,"167":2,"170":2,"178":2,"199":6,"201":6,"210":2,"230":1,"233":1,"267":3,"293":10,"315":7,"332":2,"334":3,"343":4,"344":1,"346":2,"349":1,"372":1,"380":5,"382":3,"385":1,"394":2,"395":1,"399":18,"405":1,"407":1,"424":1,"427":2,"428":2,"429":1,"449":1,"450":9,"451":2,"472":2,"485":2,"501":1,"502":2,"503":14,"504":1,"507":2,"513":1,"519":3,"523":3,"529":3,"532":1,"533":2,"543":1,"569":2,"578":1,"579":1,"582":4,"583":5,"590":9,"592":1,"595":2,"615":1,"664":1,"668":1,"671":1,"679":1,"700":1,"702":5,"721":1,"741":7,"758":1,"768":1,"801":4,"802":2,"805":1,"807":2,"808":2,"809":2,"810":3,"814":2,"815":2,"819":3,"820":3,"821":1,"822":2,"825":1,"826":2,"827":1,"828":1,"831":2,"832":5,"833":1,"834":5,"835":1,"839":3,"840":2,"841":1,"842":3,"843":2,"844":4,"845":2,"846":1,"847":1,"855":1,"879":2,"899":1,"903":2,"933":1,"941":1,"943":23,"945":2,"947":1,"960":1,"961":23,"965":2,"976":1,"986":1,"987":1,"1000":3,"1002":4,"1003":21,"1004":2,"1006":2,"1007":1,"1069":2,"1070":2,"1071":1,"1072":1,"1075":1,"1076":1,"1078":2,"1082":2,"1085":1,"1086":1,"1087":3,"1092":4,"1094":2,"1097":1,"1098":2,"1099":1,"1101":1,"1102":5,"1114":1,"1116":4,"1130":1,"1132":1,"1137":1,"1143":1,"1150":1,"1175":1,"1176":1,"1179":1,"1192":2,"1193":2,"1202":10,"1205":9,"1215":4,"1216":7,"1217":1,"1218":6,"1222":2,"1227":1,"1237":1,"1238":1,"1240":1,"1243":1,"1244":1,"1246":1,"1247":2,"1250":1,"1254":4,"1257":1,"1279":1,"1295":2,"1307":2,"1308":2,"1309":1,"1315":1,"1316":5,"1317":1,"1322":2,"1329":1,"1330":10,"1331":3,"1332":1,"1336":4,"1342":1,"1343":7,"1344":4,"1345":11,"1361":5,"1363":8,"1364":1,"1373":1,"1388":4,"1389":6,"1393":8,"1394":1,"1395":2,"1398":20,"1404":1,"1411":1,"1425":1,"1440":4,"1462":1,"1481":2,"1589":1,"1607":5,"1608":5,"1611":1,"1612":1,"1613":1,"1614":1,"1615":1,"1621":2,"1623":3,"1625":2,"1629":1,"1630":2,"1632":3,"1633":5,"1634":7,"1635":1,"1645":1,"1646":1,"1647":1,"1651":2,"1665":1,"1667":3,"1668":2,"1670":1,"1672":1,"1675":1,"1687":4,"1691":1,"1693":2,"1694":2,"1695":2,"1698":1,"1699":1,"1705":13,"1706":1,"1707":6,"1710":1,"1712":4,"1713":7,"1714":13,"1715":1,"1718":1,"1719":13,"1720":9,"1721":6,"1722":6,"1723":2,"1724":7,"1725":1,"1726":1,"1728":2,"1729":1,"1736":2,"1737":1,"1738":1,"1739":2,"1741":1,"1742":1,"1743":4,"1746":1,"1749":1,"1751":1,"1752":1,"1754":1,"1755":1,"1756":1,"1761":1,"1762":4,"1763":1,"1764":1,"1774":1,"1788":1,"1789":5,"1791":2,"1792":1,"1797":6,"1799":1,"1800":1,"1801":1,"1802":1,"1808":1,"1820":1,"1821":1,"1838":1,"1839":1,"1853":1,"1857":1,"1861":1,"1879":1,"1883":1,"1897":1,"1898":1,"1905":1,"1906":1,"1907":2,"1912":3,"1914":2,"1916":2,"1917":2,"1918":2,"1922":1,"1925":6,"1928":1,"2006":1,"2038":1,"2043":1,"2059":3,"2061":2,"2063":3,"2123":1,"2125":4,"2135":2,"2139":1,"2154":1}}],["1变0",{"2":{"2063":1}}],["1变为false",{"2":{"74":1}}],["1ystep=1",{"2":{"2018":1}}],["1或0",{"2":{"1817":1,"1835":1}}],["1或1附近",{"2":{"995":1}}],["1mb",{"2":{"1532":1}}],["1×5+0",{"2":{"1388":2}}],["1×吃了+0",{"2":{"170":2}}],["1为什么要提出",{"0":{"1407":1}}],["1为初始权重",{"2":{"1363":1}}],["1为eos",{"2":{"450":1,"451":1}}],["1时",{"2":{"1233":1,"1234":1}}],["1时刻会预测t时刻的输出",{"2":{"528":1}}],["1时刻的输出",{"2":{"426":1}}],["1β​1​​",{"2":{"1174":2}}],["1bit",{"2":{"1075":2}}],["1b模型干翻70b",{"2":{"638":1}}],["1层的输出来求",{"2":{"1003":1}}],["1中的图表显示错误率",{"2":{"1147":1}}],["1中提供了对mha",{"2":{"956":1}}],["1中被使用",{"2":{"603":1}}],["1表示单个组",{"2":{"937":1,"953":1}}],["1表示从第一个item到序列中所有其他item的注意力或重要性的分配",{"2":{"209":1}}],["1−0",{"2":{"1394":2}}],["1−out​o2​​",{"2":{"1394":1}}],["1−out​o1​​",{"2":{"1392":1,"1393":2,"1394":1}}],["1−outo2",{"2":{"1394":1}}],["1−outo1",{"2":{"1392":1,"1393":2,"1394":1}}],["1−​max​epochs​​​​epoch​​",{"2":{"1240":1}}],["1−epochmaxepochs",{"2":{"1240":1}}],["1−β​2​​",{"2":{"1192":1,"1193":1}}],["1−β​1​​",{"2":{"1192":1,"1193":2}}],["1−β2",{"2":{"1192":1,"1193":1}}],["1−β1",{"2":{"1192":1,"1193":2}}],["1−ρ",{"2":{"1191":2}}],["1−sigmoid",{"2":{"839":2}}],["1−ϵr",{"2":{"93":2}}],["1dk",{"2":{"924":1}}],["1dk√d",{"2":{"647":1}}],["1d",{"2":{"783":1}}],["1xstep=1",{"2":{"2018":1}}],["1x3",{"2":{"777":1}}],["1x1",{"0":{"774":1},"2":{"232":1,"774":2,"776":3,"1178":1}}],["1到1",{"2":{"692":1}}],["1来编码",{"2":{"681":1}}],["1来强化类似的输入",{"2":{"148":1}}],["1个字节有256种表示",{"2":{"607":1}}],["1个时间步预测的单词",{"2":{"413":1}}],["1个时间步的输出向量的一部分来计算下一个词的分布表示",{"2":{"288":1}}],["1个时间步的信息",{"2":{"71":1}}],["1|>",{"2":{"571":1}}],["1900",{"2":{"1729":2}}],["19",{"2":{"1623":1,"2043":1}}],["1979",{"2":{"1603":1}}],["198211",{"2":{"1395":2}}],["198211w",{"2":{"1395":1}}],["19ugnloisklj9ibwbzfgldw",{"2":{"1302":1}}],["1964",{"2":{"1028":1}}],["1990s",{"2":{"907":1}}],["1992",{"2":{"833":1}}],["1950s",{"2":{"907":1}}],["1943",{"2":{"595":1,"1459":2}}],["1944",{"2":{"557":1}}],["19341",{"2":{"387":1}}],["1神经元的激活程度",{"2":{"485":1}}],["1试图将在前向传播过程中遇到的信息合并在模型的权重",{"2":{"485":1}}],["1的直线进行讲解",{"2":{"2014":1}}],["1的神经元中",{"2":{"485":1}}],["1的梯度使用",{"2":{"485":1}}],["1的矩阵收缩",{"2":{"446":1}}],["1将输入从",{"2":{"485":1}}],["1ff1ff021cd5",{"2":{"429":1}}],["1代表作用到第一个维度上",{"2":{"399":1}}],["1代表自适应维度",{"2":{"36":1}}],["1也在算法中要变成",{"2":{"399":1}}],["1是在sd",{"2":{"1363":1}}],["1是",{"2":{"380":1}}],["1e20",{"2":{"1217":1}}],["1e",{"2":{"346":1,"807":1,"808":2,"809":1,"810":1,"1087":5,"1211":1,"1222":3,"1308":1,"1331":6}}],["1e9来替换它",{"2":{"67":1}}],["1e9",{"2":{"62":1,"63":1,"67":1,"199":2,"394":1,"933":2}}],["1号标签和2号标签可以观察到类似的趋势是",{"2":{"306":1}}],["1部分",{"2":{"288":1}}],["15次授课",{"0":{"1794":1},"1":{"1795":1,"1796":1,"1797":1,"1798":1,"1799":1,"1800":1,"1801":1,"1802":1,"1803":1,"1804":1,"1805":1,"1806":1,"1807":1,"1808":1}}],["15×10+0",{"2":{"1388":2}}],["1572",{"2":{"1098":1}}],["15595",{"2":{"768":1}}],["15556",{"2":{"638":1}}],["1535",{"2":{"702":1}}],["15365",{"2":{"557":1}}],["150w",{"2":{"1938":1,"1953":1}}],["150",{"2":{"1665":1}}],["1508",{"2":{"574":1}}],["150b",{"2":{"387":1}}],["15",{"0":{"837":1,"950":1,"1247":1,"1917":1},"2":{"233":1,"361":1,"545":1,"727":1,"768":1,"1087":1,"1102":1,"1388":1,"1398":1,"1404":1,"1607":1,"1635":1,"1729":2,"1732":3,"1933":1,"2053":1,"2131":2}}],["1和1附近的gradient都接近0",{"2":{"995":1}}],["1和",{"2":{"485":1}}],["1和a2a2a",{"2":{"209":1}}],["1和x2x2x",{"2":{"172":1}}],["1分出去的部分",{"2":{"191":1}}],["1804",{"2":{"638":1}}],["18068",{"2":{"557":1}}],["1803",{"2":{"429":1}}],["1801",{"2":{"429":1}}],["18223",{"2":{"387":1}}],["182cm等",{"2":{"313":1}}],["18",{"0":{"1730":1},"1":{"1731":1,"1732":1,"1733":1,"1734":1,"1735":1,"1736":1,"1737":1,"1738":1,"1739":1,"1740":1,"1741":1,"1742":1,"1743":1,"1744":1,"1745":1,"1746":1,"1747":1,"1748":1,"1749":1,"1750":1,"1751":1,"1752":1,"1753":1,"1754":1,"1755":1,"1756":1,"1757":1,"1758":1,"1759":1},"2":{"169":10,"429":1,"781":1,"2046":1}}],["18746838",{"2":{"768":1}}],["18741857",{"2":{"740":1}}],["18732939",{"2":{"638":1}}],["18730583",{"2":{"429":1}}],["18727704",{"2":{"543":1}}],["18722830",{"2":{"387":1}}],["18706134",{"2":{"513":1}}],["18705809",{"2":{"292":1}}],["18774865",{"2":{"361":1}}],["18765884",{"2":{"156":1}}],["18751758",{"2":{"233":1}}],["18758992",{"2":{"95":1}}],["18759167",{"2":{"47":1}}],["14f",{"2":{"1728":1}}],["141",{"2":{"1910":1}}],["1416",{"2":{"1673":1}}],["141592653589793",{"2":{"1908":1}}],["1415926535897932385",{"2":{"1908":2}}],["14159265358979323846",{"2":{"1817":1,"1835":1}}],["1415926535",{"2":{"1673":1}}],["14159",{"2":{"1604":2,"1693":1,"1779":1,"1908":1}}],["14模型",{"2":{"1363":1}}],["14995",{"2":{"233":1}}],["14",{"0":{"293":1,"784":1,"836":1,"949":1,"1246":1,"1711":1},"1":{"294":1,"295":1,"296":1,"297":1,"298":1,"299":1,"300":1,"301":1,"302":1,"303":1,"304":1,"305":1,"306":1,"307":1,"308":1,"309":1,"310":1,"311":1,"312":1,"313":1,"314":1,"315":1,"316":1,"317":1,"318":1,"319":1,"320":1,"321":1,"322":1,"323":1,"324":1,"325":1,"326":1,"327":1,"328":1,"329":1,"330":1,"331":1,"332":1,"333":1,"334":1,"335":1,"336":1,"337":1,"338":1,"339":1,"340":1,"341":1,"342":1,"343":1,"344":1,"345":1,"346":1,"347":1,"348":1,"349":1,"350":1,"351":1,"352":1,"353":1,"354":1,"355":1,"356":1,"357":1,"358":1,"359":1,"360":1,"361":1,"1712":1,"1713":1,"1714":1,"1715":1},"2":{"156":1,"293":1,"1215":1,"1283":4,"1363":1,"1398":5,"1404":1,"1607":1,"1611":1,"1615":1,"1635":1,"1698":1,"1699":1,"1700":1,"1701":1,"1879":1,"1905":1,"1925":1,"1926":2}}],["1𝐴^",{"2":{"122":2}}],["1𝐴𝑙",{"2":{"122":2}}],["1+0=",{"2":{"1393":1}}],["1+",{"2":{"1243":1}}],["1+cos",{"2":{"1243":2,"1244":3}}],["1+e​−net​​",{"2":{"1393":3}}],["1+e​x​​",{"2":{"846":1}}],["1+e−net",{"2":{"1393":3}}],["1+e^",{"2":{"846":1,"1388":2,"1389":2,"1393":6}}],["1+ex",{"2":{"846":1}}],["1+erf",{"2":{"106":2}}],["1+tanh",{"2":{"844":2}}],["1+p",{"2":{"748":1}}],["1+σ^2",{"2":{"332":1}}],["1+b",{"2":{"99":2}}],["1∈r^",{"2":{"101":1}}],["130w",{"2":{"1958":1}}],["1307",{"2":{"1215":1}}],["1301",{"2":{"713":1}}],["13b中",{"2":{"981":1}}],["13b在nvidia",{"2":{"980":1}}],["13米高的树上或悬崖上跳下",{"2":{"713":1}}],["131",{"2":{"1318":1}}],["1310毫米",{"2":{"713":1}}],["1311440131",{"2":{"429":1}}],["13",{"0":{"96":1,"783":1,"835":1,"948":1,"987":1,"1245":1,"1692":1},"1":{"97":1,"98":1,"99":1,"100":1,"101":1,"102":1,"103":1,"104":1,"105":1,"106":1,"107":1,"108":1,"109":1,"110":1,"111":1,"112":1,"113":1,"114":1,"115":1,"116":1,"117":1,"118":1,"119":1,"120":1,"121":1,"122":1,"123":1,"124":1,"125":1,"126":1,"127":1,"128":1,"129":1,"130":1,"131":1,"132":1,"133":1,"134":1,"135":1,"136":1,"137":1,"138":1,"139":1,"140":1,"141":1,"142":1,"143":1,"144":1,"145":1,"146":1,"147":1,"148":1,"149":1,"150":1,"151":1,"152":1,"153":1,"154":1,"155":1,"156":1,"1693":1,"1694":1,"1695":1},"2":{"96":1,"1404":1,"1635":1,"1910":2,"2042":1,"2043":1,"2049":1,"2053":1}}],["164",{"2":{"2086":1}}],["16gf",{"2":{"1308":2}}],["1603",{"2":{"840":1}}],["16645",{"2":{"740":1}}],["1655",{"2":{"702":1}}],["1672",{"2":{"557":1}}],["1675",{"2":{"557":1}}],["1690",{"2":{"557":1}}],["16",{"0":{"1697":1},"1":{"1698":1,"1699":1,"1700":1,"1701":1},"2":{"89":1,"801":4,"802":8,"810":1,"814":1,"815":1,"820":1,"835":1,"976":29,"1003":2,"1282":1,"1308":2,"1330":13,"1331":1,"1607":1,"1969":1,"2049":2}}],["1k",{"2":{"89":1,"945":1,"965":1}}],["1以前时刻的key",{"2":{"70":1}}],["1二值",{"2":{"50":1}}],["118",{"2":{"2086":1}}],["11817d47e1ba7a4959b025eb1ca308572e0e3963",{"2":{"361":1}}],["11001",{"2":{"2059":1}}],["1100",{"2":{"2059":1}}],["110",{"2":{"1317":1,"2059":2}}],["1155",{"2":{"429":1}}],["1137",{"2":{"429":1}}],["11",{"0":{"49":1,"291":1,"766":1,"779":1,"833":1,"849":1,"939":1,"940":1,"941":1,"942":1,"943":1,"944":1,"946":1,"979":1,"980":1,"981":1,"982":1,"983":1,"1066":1,"1102":1,"1243":1,"1536":1,"1677":1,"1931":1,"2044":1,"2157":1},"1":{"50":1,"51":1,"52":1,"53":1,"54":1,"55":1,"56":1,"57":1,"58":1,"59":1,"60":1,"61":1,"62":1,"63":1,"64":1,"65":1,"66":1,"67":1,"68":1,"69":1,"70":1,"71":1,"72":1,"73":1,"74":1,"75":1,"76":1,"77":1,"78":1,"79":1,"80":1,"81":1,"82":1,"83":1,"84":1,"85":1,"86":1,"87":1,"88":1,"89":1,"90":1,"91":1,"92":1,"93":1,"94":1,"95":1,"940":1,"941":1,"942":1,"943":1,"944":1,"945":1,"946":1,"980":1,"981":1,"982":1,"983":1,"1537":1,"2045":1,"2046":1},"2":{"49":1,"83":1,"424":1,"428":2,"472":2,"529":2,"560":3,"582":1,"595":1,"741":1,"1389":1,"1393":2,"1395":1,"1398":5,"1404":1,"1635":1,"2042":1,"2043":2,"2045":3,"2046":6,"2051":2,"2053":1,"2056":1,"2059":1}}],["10>",{"2":{"2063":1}}],["10分钟",{"2":{"1917":1}}],["104",{"2":{"1183":1}}],["10倍",{"2":{"1245":1}}],["10倍大小的",{"2":{"1183":1}}],["10倍的峰值学习率来训练模型中产生的训练损失是如何通过双重检查的",{"2":{"1182":1}}],["10bit",{"2":{"1075":1}}],["10x7",{"2":{"816":1}}],["106264402",{"2":{"768":1}}],["10个向量",{"2":{"702":1}}],["10959",{"2":{"638":1}}],["105",{"2":{"1183":1}}],["10559",{"2":{"543":1}}],["1050624",{"2":{"119":2}}],["10305",{"2":{"387":1}}],["103",{"2":{"315":1,"1183":1,"1807":2}}],["1010",{"2":{"2062":1}}],["10101",{"2":{"2059":1}}],["1011",{"2":{"2062":1}}],["1019206​​​​1​​=0",{"2":{"1389":1}}],["1019206=0",{"2":{"1389":1}}],["1019206",{"2":{"1389":3}}],["1019206net",{"2":{"1389":1}}],["101",{"2":{"315":1,"1807":2,"2059":1}}],["1024",{"2":{"700":1,"733":1,"1210":1,"1867":1}}],["10268",{"2":{"557":1}}],["102",{"2":{"315":1,"1807":4}}],["10250219d5ce42e8b465087c383a034e",{"2":{"233":1}}],["1001",{"2":{"1661":1,"1662":1,"2062":1}}],["100以内除以",{"2":{"1625":1}}],["100以内的偶数",{"2":{"1625":1}}],["100kb",{"2":{"945":1,"965":1}}],["100k",{"2":{"768":1}}],["100257",{"2":{"591":1}}],["1000次时对应的权重为",{"2":{"1396":1}}],["1000000000",{"2":{"1910":1}}],["1000000",{"2":{"1651":1,"1702":1}}],["100000",{"2":{"1594":1}}],["10000^",{"2":{"1336":2}}],["10000",{"2":{"700":2,"1345":1}}],["10000100",{"2":{"694":2}}],["1000",{"2":{"361":1,"1215":1,"1218":2,"1396":1,"1398":1,"1671":1,"1677":1,"1874":1}}],["100",{"2":{"88":1,"302":1,"315":1,"399":1,"595":1,"698":2,"801":1,"802":1,"807":3,"809":3,"1072":1,"1087":2,"1098":3,"1099":1,"1102":1,"1177":1,"1202":1,"1205":2,"1215":2,"1231":1,"1233":1,"1234":1,"1235":1,"1236":1,"1237":1,"1238":1,"1240":1,"1243":1,"1246":1,"1247":1,"1250":2,"1295":3,"1316":1,"1317":3,"1398":2,"1613":1,"1619":1,"1621":1,"1623":2,"1625":4,"1648":1,"1671":1,"1677":1,"1728":1,"1729":6,"1825":1,"1843":1,"1857":1,"1861":1,"2131":1}}],["10",{"0":{"157":1,"290":1,"737":1,"765":1,"778":1,"830":1,"831":1,"832":1,"848":1,"936":1,"937":1,"938":1,"978":1,"1010":1,"1063":1,"1064":1,"1065":1,"1101":1,"1242":1,"1500":1,"1534":1,"1676":1,"1914":1,"1930":1,"2041":1,"2156":1},"1":{"158":1,"159":1,"160":1,"161":1,"162":1,"163":1,"164":1,"165":1,"166":1,"167":1,"168":1,"169":1,"170":1,"171":1,"172":1,"173":1,"174":1,"175":1,"176":1,"177":1,"178":1,"179":1,"180":1,"181":1,"182":1,"183":1,"184":1,"185":1,"186":1,"187":1,"188":1,"189":1,"190":1,"191":1,"192":1,"193":1,"194":1,"195":1,"196":1,"197":1,"198":1,"199":1,"200":1,"201":1,"202":1,"203":1,"204":1,"205":1,"206":1,"207":1,"208":1,"209":1,"210":1,"211":1,"212":1,"213":1,"214":1,"215":1,"216":1,"217":1,"218":1,"219":1,"220":1,"221":1,"222":1,"223":1,"224":1,"225":1,"226":1,"227":1,"228":1,"229":1,"230":1,"231":1,"232":1,"233":1,"738":1,"739":1,"831":1,"832":1,"937":1,"1064":1,"1065":1,"1535":1,"2042":1,"2043":1},"2":{"36":2,"110":3,"135":1,"157":1,"326":3,"372":2,"383":1,"428":5,"453":2,"472":2,"529":2,"543":1,"595":1,"692":2,"702":1,"704":2,"741":1,"805":4,"808":1,"810":2,"816":2,"831":1,"832":2,"834":3,"1070":1,"1102":1,"1183":1,"1202":1,"1205":3,"1207":1,"1210":1,"1211":1,"1212":2,"1213":2,"1215":3,"1216":1,"1218":1,"1241":1,"1242":1,"1245":1,"1250":1,"1253":1,"1257":1,"1266":1,"1295":2,"1296":1,"1297":2,"1298":2,"1299":4,"1303":1,"1308":1,"1331":6,"1364":2,"1388":1,"1393":1,"1395":1,"1398":1,"1404":1,"1607":4,"1611":6,"1612":2,"1614":2,"1615":1,"1620":3,"1621":6,"1623":5,"1624":1,"1629":2,"1630":3,"1633":2,"1634":1,"1635":1,"1639":1,"1640":1,"1641":1,"1645":1,"1646":2,"1647":1,"1648":1,"1649":1,"1650":1,"1663":3,"1667":1,"1668":1,"1672":1,"1673":2,"1676":1,"1694":7,"1695":8,"1698":1,"1699":1,"1700":1,"1701":1,"1704":1,"1706":1,"1709":1,"1713":3,"1714":6,"1728":1,"1729":2,"1736":1,"1743":1,"1762":1,"1772":1,"1784":1,"1785":1,"1806":4,"1853":2,"1857":2,"1861":2,"1907":4,"1922":2,"1924":1,"1926":2,"1933":1,"1966":1,"1971":1,"1980":1,"1999":2,"2003":1,"2005":1,"2006":2,"2007":1,"2031":2,"2042":4,"2043":18,"2053":2,"2059":1,"2063":1,"2086":2,"2137":1,"2139":1}}],["1750",{"2":{"1316":1}}],["1727018183608193393",{"2":{"768":1}}],["1722",{"2":{"557":1}}],["1799",{"2":{"557":2}}],["1706",{"2":{"432":1,"557":1,"1176":1}}],["17",{"0":{"1717":1},"1":{"1718":1,"1719":1,"1720":1,"1721":1,"1722":1,"1723":1,"1724":1,"1725":1},"2":{"20":1,"545":1,"1982":1,"2043":1,"2049":1}}],["1^",{"2":{"1192":1,"1193":1}}],["1^tq+w",{"2":{"175":1}}],["1^v",{"2":{"9":1}}],["1^k",{"2":{"9":1}}],["1^q",{"2":{"9":1}}],["1",{"0":{"3":2,"4":1,"5":1,"7":1,"23":1,"41":1,"52":2,"56":1,"61":1,"69":1,"78":1,"87":1,"99":2,"100":1,"101":1,"102":1,"113":1,"116":1,"122":1,"150":1,"160":2,"161":1,"162":1,"170":1,"172":1,"197":1,"203":1,"234":1,"237":2,"238":1,"239":1,"240":1,"241":1,"242":1,"244":1,"258":1,"281":1,"296":2,"297":1,"300":1,"303":1,"309":1,"313":1,"318":1,"337":1,"343":1,"346":1,"366":1,"372":1,"376":1,"391":2,"392":1,"397":1,"400":1,"403":1,"404":1,"413":1,"421":1,"426":1,"431":1,"434":2,"435":1,"438":1,"445":1,"446":1,"448":1,"453":1,"462":1,"463":1,"472":1,"475":1,"511":1,"517":2,"518":1,"519":1,"520":1,"521":1,"525":1,"535":1,"540":1,"547":2,"548":1,"549":1,"550":1,"551":1,"557":1,"564":1,"575":1,"597":1,"610":1,"640":1,"642":1,"647":1,"658":2,"659":1,"660":1,"661":1,"662":1,"664":2,"665":1,"666":1,"669":1,"670":1,"676":2,"677":1,"678":1,"679":1,"680":1,"688":1,"693":1,"698":1,"711":1,"744":2,"745":1,"746":1,"748":1,"753":1,"769":1,"781":1,"785":1,"788":1,"790":1,"794":1,"798":1,"800":1,"801":2,"802":1,"804":1,"807":1,"814":1,"819":1,"825":1,"831":1,"839":1,"853":1,"863":1,"865":1,"873":1,"877":1,"878":2,"879":1,"880":2,"881":2,"882":2,"883":1,"885":1,"887":1,"892":1,"894":1,"896":1,"901":1,"906":1,"911":1,"914":1,"917":1,"920":1,"926":1,"933":1,"937":1,"940":1,"951":1,"953":1,"956":1,"959":1,"962":1,"969":1,"975":1,"980":1,"985":1,"988":1,"990":1,"991":1,"994":1,"998":1,"1002":1,"1006":1,"1011":1,"1014":1,"1022":1,"1023":2,"1024":1,"1025":2,"1026":1,"1027":1,"1029":1,"1034":1,"1038":1,"1041":1,"1048":1,"1052":1,"1056":1,"1059":1,"1064":1,"1068":1,"1069":2,"1070":1,"1071":1,"1072":1,"1075":1,"1078":1,"1085":1,"1088":1,"1089":2,"1090":1,"1092":1,"1104":1,"1113":1,"1119":1,"1201":1,"1202":2,"1203":1,"1205":1,"1210":1,"1215":1,"1220":1,"1221":2,"1222":1,"1223":1,"1225":1,"1231":1,"1233":1,"1249":1,"1253":1,"1255":1,"1257":1,"1262":1,"1266":1,"1269":1,"1272":1,"1275":1,"1278":1,"1285":1,"1286":2,"1287":1,"1288":1,"1290":1,"1295":1,"1300":1,"1309":1,"1312":1,"1320":1,"1322":1,"1328":1,"1332":1,"1335":1,"1336":2,"1337":1,"1339":1,"1342":1,"1346":1,"1349":1,"1356":1,"1360":1,"1363":1,"1369":1,"1370":2,"1371":1,"1372":1,"1373":1,"1374":1,"1375":1,"1377":1,"1381":1,"1386":1,"1388":1,"1391":1,"1392":2,"1393":1,"1405":1,"1406":1,"1407":1,"1408":1,"1409":1,"1410":1,"1411":2,"1412":1,"1413":1,"1415":1,"1417":1,"1421":1,"1435":1,"1437":1,"1438":2,"1439":1,"1445":1,"1452":1,"1453":2,"1454":1,"1455":1,"1456":1,"1457":1,"1459":1,"1462":1,"1465":1,"1469":1,"1476":2,"1477":1,"1478":1,"1479":1,"1481":1,"1484":1,"1491":1,"1504":1,"1518":1,"1541":1,"1563":2,"1564":1,"1565":1,"1566":1,"1568":1,"1571":1,"1572":1,"1582":1,"1586":1,"1591":1,"1594":1,"1596":1,"1619":1,"1623":1,"1629":1,"1637":1,"1653":1,"1659":1,"1667":1,"1680":1,"1683":1,"1687":1,"1698":1,"1712":1,"1718":1,"1733":2,"1736":1,"1741":1,"1746":1,"1749":1,"1754":1,"1771":1,"1772":2,"1773":1,"1774":1,"1775":1,"1776":1,"1778":1,"1783":1,"1788":1,"1791":1,"1796":1,"1797":2,"1798":1,"1799":2,"1800":1,"1801":1,"1802":1,"1803":1,"1805":1,"1810":1,"1813":1,"1816":1,"1819":1,"1823":1,"1828":1,"1831":1,"1834":1,"1837":1,"1841":1,"1846":1,"1848":1,"1851":1,"1855":1,"1859":1,"1863":1,"1866":1,"1877":1,"1878":2,"1879":1,"1880":1,"1882":1,"1886":1,"1890":1,"1894":1,"1898":1,"1905":1,"1921":1,"1935":1,"1963":1,"1964":2,"1966":1,"1968":1,"1971":1,"1979":1,"1980":2,"1981":1,"1984":1,"1989":1,"1993":1,"1996":1,"2013":1,"2015":1,"2017":1,"2022":1,"2027":1,"2053":1,"2054":2,"2055":1,"2058":2,"2065":1,"2066":2,"2067":1,"2068":1,"2069":2,"2070":1,"2073":1,"2075":1,"2081":1,"2085":1,"2089":1,"2093":1,"2097":1,"2099":1,"2104":1,"2112":1,"2114":1,"2117":1,"2122":1,"2123":2,"2124":1,"2125":1,"2126":1,"2129":1,"2133":1,"2137":1,"2147":1},"1":{"8":1,"9":1,"10":1,"53":2,"54":2,"55":2,"57":1,"58":1,"59":1,"62":1,"63":1,"70":1,"71":1,"72":1,"88":1,"89":1,"90":1,"103":1,"104":1,"105":1,"106":1,"107":1,"108":1,"109":1,"110":1,"111":1,"163":1,"164":1,"165":1,"166":1,"167":1,"168":1,"169":1,"198":1,"199":1,"200":1,"204":1,"205":1,"206":1,"207":1,"235":1,"236":1,"237":1,"238":1,"239":1,"240":1,"241":1,"242":1,"243":1,"244":1,"245":2,"246":2,"247":1,"248":1,"249":1,"250":1,"251":1,"252":1,"253":1,"254":1,"255":1,"256":1,"257":1,"258":1,"259":2,"260":2,"261":2,"262":1,"263":1,"264":1,"265":1,"266":1,"267":1,"268":1,"269":1,"270":1,"271":1,"272":1,"273":1,"274":1,"275":1,"276":1,"277":1,"278":1,"279":1,"280":1,"281":1,"282":1,"283":1,"284":1,"285":1,"286":1,"287":1,"288":1,"289":1,"290":1,"291":1,"292":1,"298":1,"299":1,"301":1,"302":1,"304":1,"305":1,"306":1,"307":1,"367":1,"368":1,"369":1,"377":1,"378":1,"393":1,"394":1,"395":1,"396":1,"398":1,"399":1,"401":1,"402":1,"405":1,"406":1,"407":1,"408":1,"409":1,"410":1,"411":1,"412":1,"414":1,"415":1,"416":1,"417":1,"418":1,"419":1,"420":1,"422":1,"423":1,"424":1,"436":1,"437":1,"439":1,"440":1,"441":1,"442":1,"443":1,"444":1,"463":1,"464":1,"465":1,"476":1,"477":1,"478":1,"479":1,"480":1,"522":1,"523":1,"552":1,"553":1,"554":1,"555":1,"565":1,"566":1,"567":1,"568":1,"598":1,"599":1,"600":1,"611":1,"612":1,"613":1,"614":1,"615":1,"681":1,"682":1,"683":1,"684":1,"685":1,"686":1,"687":1,"689":1,"690":1,"691":1,"692":1,"694":1,"695":1,"696":1,"754":1,"755":1,"756":1,"801":1,"802":1,"878":1,"879":1,"880":2,"881":2,"882":2,"883":1,"1023":1,"1024":1,"1025":2,"1026":2,"1027":2,"1060":1,"1061":1,"1062":1,"1069":1,"1070":1,"1071":1,"1072":1,"1089":1,"1090":1,"1202":1,"1203":1,"1221":1,"1222":1,"1223":1,"1286":1,"1287":1,"1288":1,"1336":1,"1337":1,"1370":1,"1371":1,"1372":1,"1373":1,"1374":1,"1375":1,"1406":1,"1407":2,"1408":2,"1409":2,"1410":2,"1411":3,"1412":3,"1413":3,"1414":1,"1415":1,"1416":1,"1417":1,"1418":1,"1419":1,"1420":1,"1421":1,"1422":1,"1423":1,"1424":1,"1425":1,"1438":1,"1439":1,"1453":1,"1454":1,"1455":1,"1456":1,"1457":1,"1505":1,"1506":1,"1507":1,"1572":1,"1573":1,"1574":1,"1575":1,"1576":1,"1577":1,"1578":1,"1579":1,"1734":2,"1772":1,"1773":1,"1774":1,"1775":1,"1776":1,"1797":1,"1798":1,"1799":2,"1800":2,"1801":2,"1802":2,"1803":2,"1878":1,"1879":1,"1880":1,"1936":1,"1937":1,"1938":1,"1939":1,"1964":1,"1980":1,"1981":1,"2054":1,"2055":1,"2069":1,"2070":1,"2123":1,"2124":1,"2125":1,"2126":1},"2":{"0":7,"17":5,"23":2,"29":1,"36":19,"38":1,"39":1,"46":1,"49":7,"54":1,"57":1,"58":1,"66":5,"67":3,"70":56,"74":9,"76":6,"79":5,"80":2,"82":2,"83":12,"84":5,"89":1,"93":4,"96":9,"99":2,"101":1,"106":2,"108":2,"109":2,"113":4,"114":1,"116":1,"119":1,"122":3,"125":1,"147":1,"148":1,"155":1,"156":1,"157":8,"167":6,"170":2,"178":3,"183":2,"188":2,"189":1,"191":3,"192":3,"198":4,"199":8,"201":7,"217":1,"224":3,"230":1,"240":4,"241":4,"267":6,"285":1,"289":1,"292":1,"293":11,"304":1,"309":1,"314":2,"315":21,"332":4,"334":4,"341":8,"343":7,"344":2,"346":2,"349":1,"352":4,"364":1,"372":1,"380":7,"381":2,"382":4,"383":2,"384":1,"385":6,"394":5,"395":10,"398":3,"399":19,"402":5,"405":1,"407":1,"410":3,"418":2,"422":1,"423":3,"424":4,"427":2,"428":11,"429":2,"448":1,"449":2,"450":5,"451":1,"460":1,"472":13,"473":3,"485":1,"499":1,"501":1,"502":2,"503":20,"507":5,"511":2,"519":3,"523":5,"529":11,"532":3,"533":5,"543":1,"557":2,"558":1,"560":2,"569":1,"571":2,"582":1,"584":1,"590":14,"591":1,"592":1,"595":2,"608":1,"613":1,"661":1,"664":1,"668":1,"671":1,"679":1,"681":2,"692":7,"694":2,"698":3,"700":6,"701":3,"703":1,"713":1,"721":1,"723":5,"741":6,"748":1,"765":4,"768":1,"776":1,"778":1,"801":3,"802":2,"807":2,"808":4,"810":2,"814":1,"815":1,"816":4,"819":2,"820":5,"821":1,"822":1,"825":1,"827":3,"828":1,"831":1,"832":3,"833":12,"834":5,"839":6,"840":1,"842":2,"843":2,"844":5,"846":1,"847":1,"856":1,"865":2,"866":4,"879":2,"886":2,"899":2,"903":5,"912":3,"924":1,"927":1,"933":3,"943":24,"944":7,"945":2,"961":24,"965":2,"970":1,"974":1,"975":2,"976":2,"986":1,"995":2,"1000":4,"1002":2,"1003":14,"1004":1,"1016":3,"1039":1,"1069":2,"1070":2,"1071":1,"1072":2,"1075":1,"1076":1,"1078":3,"1082":3,"1086":3,"1087":43,"1092":2,"1097":2,"1098":3,"1099":3,"1102":4,"1104":1,"1110":1,"1115":3,"1116":2,"1130":1,"1137":1,"1143":1,"1147":1,"1157":1,"1176":1,"1177":1,"1178":1,"1191":2,"1192":6,"1193":9,"1195":1,"1202":8,"1205":11,"1208":1,"1211":14,"1212":1,"1213":4,"1215":19,"1216":15,"1217":4,"1218":22,"1227":1,"1231":1,"1235":2,"1236":2,"1237":2,"1238":2,"1239":2,"1240":3,"1241":2,"1242":1,"1243":2,"1244":1,"1245":1,"1246":3,"1247":5,"1250":1,"1254":2,"1257":4,"1259":2,"1262":2,"1263":2,"1267":2,"1269":2,"1270":2,"1272":1,"1273":2,"1283":2,"1284":1,"1295":1,"1297":2,"1303":1,"1306":2,"1307":2,"1308":3,"1316":1,"1322":7,"1323":2,"1328":5,"1329":9,"1330":54,"1343":4,"1345":9,"1350":3,"1361":2,"1363":3,"1373":1,"1388":10,"1389":5,"1392":1,"1393":10,"1394":7,"1395":2,"1396":8,"1398":15,"1404":1,"1408":1,"1425":1,"1440":1,"1455":1,"1460":3,"1462":1,"1481":3,"1485":1,"1487":1,"1513":1,"1564":1,"1589":1,"1594":6,"1607":9,"1608":4,"1611":1,"1612":1,"1613":1,"1614":1,"1615":1,"1620":4,"1621":7,"1623":3,"1624":1,"1625":1,"1633":7,"1634":4,"1635":1,"1645":2,"1646":5,"1651":3,"1667":1,"1670":1,"1672":1,"1687":4,"1691":1,"1693":2,"1694":2,"1695":3,"1700":3,"1705":18,"1708":6,"1710":2,"1712":7,"1713":7,"1714":12,"1715":1,"1718":1,"1719":10,"1720":6,"1721":6,"1722":6,"1723":4,"1724":4,"1725":1,"1726":1,"1728":2,"1729":1,"1737":1,"1738":1,"1739":1,"1741":1,"1742":1,"1743":1,"1746":1,"1747":1,"1749":2,"1751":1,"1752":1,"1754":1,"1755":1,"1756":2,"1761":7,"1762":1,"1763":1,"1764":1,"1788":1,"1789":6,"1792":1,"1797":6,"1799":1,"1800":1,"1801":1,"1802":1,"1805":2,"1808":1,"1813":4,"1814":1,"1817":1,"1820":1,"1821":1,"1825":2,"1831":4,"1832":1,"1835":1,"1838":1,"1839":1,"1843":2,"1853":1,"1857":1,"1861":1,"1879":1,"1883":1,"1887":3,"1895":1,"1897":1,"1898":1,"1906":1,"1910":2,"1912":3,"1914":1,"1916":2,"1917":2,"1918":1,"1921":1,"1922":2,"1923":1,"1924":1,"1925":7,"1928":2,"1933":3,"1972":1,"1977":1,"1990":1,"1991":1,"2006":1,"2018":8,"2019":1,"2027":1,"2031":1,"2059":5,"2061":2,"2062":4,"2063":1,"2123":1,"2125":4,"2135":2,"2137":1,"2139":2,"2153":2,"2154":4}}],["120w",{"2":{"1943":1}}],["125",{"2":{"1774":1}}],["125858248",{"2":{"95":1}}],["127",{"2":{"1306":1,"1684":2}}],["12nl^var",{"2":{"1004":1}}],["12nlvar",{"2":{"1003":1}}],["1264",{"2":{"833":1}}],["129255185",{"2":{"768":1}}],["12992",{"2":{"156":1}}],["123456",{"2":{"1481":1,"1485":1}}],["12307",{"2":{"768":1}}],["123",{"2":{"765":2,"1607":1,"1673":1,"1683":2,"1701":1,"1824":1,"1842":1,"1921":2}}],["12356",{"2":{"422":1}}],["122",{"2":{"765":2}}],["121",{"2":{"765":2}}],["128gf",{"2":{"1308":1}}],["128",{"2":{"343":1,"765":2,"776":1,"804":1,"945":1,"965":1,"976":1,"1086":2,"1215":7,"1216":1,"1218":1,"1254":1,"1257":2,"1318":1,"1332":1,"1684":2,"2086":2}}],["12",{"0":{"0":1,"767":1,"780":1,"781":1,"782":1,"834":1,"947":1,"984":1,"985":1,"986":1,"1067":1,"1244":1,"1538":1,"1932":1},"1":{"1":1,"2":1,"3":1,"4":1,"5":1,"6":1,"7":1,"8":1,"9":1,"10":1,"11":1,"12":1,"13":1,"14":1,"15":1,"16":1,"17":1,"18":1,"19":1,"20":1,"21":1,"22":1,"23":1,"24":1,"25":1,"26":1,"27":1,"28":1,"29":1,"30":1,"31":1,"32":1,"33":1,"34":1,"35":1,"36":1,"37":1,"38":1,"39":1,"40":1,"41":1,"42":1,"43":1,"44":1,"45":1,"46":1,"47":1,"781":1,"782":1,"985":1,"986":1,"1539":1,"1933":1},"2":{"0":1,"156":1,"361":1,"451":1,"545":1,"741":1,"802":2,"1007":1,"1318":1,"1332":1,"1393":1,"1395":1,"1404":1,"1635":1,"2043":1,"2045":1,"2046":1,"2059":1}}]],"serializationVersion":2}';export{e as default};
