# å›é¡¾å’Œå¤ä¹ æ•´ç†DL



## 00. pythonåŸºç¡€ï¼ˆæœªè¡¥ï¼‰

> æŠ½ç©ºè¡¥å……

- è¿­ä»£å™¨
- è£…é¥°å™¨
- callable
- 



## 01. æ¿€æ´»å‡½æ•°ğŸŒŸğŸŒŸğŸŒŸ

### 1. æˆ‘çš„è®°å½•ï¼š

- ä¸ºå•¥éœ€è¦Activationï¼Ÿï¼ˆæ·±åº¦å­¦ä¹ å¾€å¾€åªè¯´éçº¿æ€§Attentionï¼‰

    > éƒ½æ˜¯Linearï¼ˆæˆ–è€…Covnï¼‰çš„è¯ç½‘ç»œè¿˜æ˜¯çº¿æ€§çš„ï¼Œå¤šå±‚å’Œä¸€å±‚æ²¡å•¥å¤ªå¤§åŒºåˆ«ã€‚

- æœ‰å“ªäº›ç±»å‹ï¼Ÿï¼ˆå·¥ç¨‹ä¸Šä¸»è¦ç±»åˆ«ï¼‰

    > ä¸€ç§æ˜¯point wiseçš„æ“ä½œï¼ˆæˆ–è€…å«åšelement wiseæ“ä½œï¼‰ï¼Œä¹Ÿå°±æ˜¯é€å…ƒç´ æ“ä½œï¼›å¦ä¸€ç§æ˜¯å…·æœ‰ç›¸å…³æ€§æ“ä½œ
    
    > > Note: å…¬å¼ã€å›¾åƒã€å¯¼æ•°å›¾åƒã€ä¼˜ç¼ºç‚¹â€¦â€¦
    >
    > Aç±»ï¼šæ•°æ®é—´ç‹¬ç«‹
    >
    > 1. Så‹ï¼š
    >
    >     - sigmoidï¼ˆ0.5çš„äºŒåˆ†ç±»é—®é¢˜ï¼‰
    >
    >     - Tanhï¼ˆNLPçš„é€’å½’ç³»åˆ—å¸¸ç”¨ï¼‰
    >
    > 2. ==ReLU==ï¼šAlexNeté‡Œé¢æå‡ºã€‚
    >
    >     - ReLU
    >
    >     - ç¼“è§£æ¿€æ´»å€¼æŒ‡æ•°æ€§å¢é•¿=>ReLU6
    >     - ç¼“è§£ç¥ç»å…ƒåæ­»ç°è±¡=>Leakey ReLU=>PReLU/RReLU
    >     - x=0å¤„å¹³æ»‘è¿‡æ¸¡=>ELU/SELU
    >     - ==GeLU==ï¼ˆå·¥ç¨‹ä¸Šæœ‰ç‚¹ç®€åŒ–ï¼Œç°åœ¨å¤§æ¨¡å‹éƒ½ä¼šç”¨ä»–çš„å˜ç§ï¼‰
    >
    > 3. Swishï¼š
    >
    >     - Switchï¼ˆå¯¹sigmoidè¿›è¡Œä¿®æ”¹ï¼‰
    >     - Hard Swishï¼ˆå¯¹ReLU6åšä¿®æ”¹ï¼Œè®¡ç®—æ›´ç®€å•ï¼‰ã€‚â€œå‡‘å›¾åƒâ€
    >
    > 4. mishï¼šå¹³æ»‘æ€§æ›´é«˜ï¼ˆçœ‹ä¸€é˜¶å’ŒäºŒé˜¶å¯¼å›¾åƒï¼‰
    >
    > Bç±»ï¼šæ•°æ®é—´æœ‰ç›¸äº’ä½œç”¨
    >
    > 1. ==softmax==ï¼›æƒ³æƒ³sigmoidã€‚è¿™ä¸ªæ˜¯å¤šåˆ†ç±»ï¼Œå•è°ƒéè´Ÿå½’ä¸€
    >
    >     ç±»ä¼¼æ“ä½œè¿˜æœ‰çŸ©é˜µä¹˜æ³•ç­‰
    
    $$
    sigmoid = \frac{1}{1 + e^{-x}} \\
    
    \frac{d(sigmoid)}{dx} = sigmoid(x) Â· (1 - sigmoid(x))
    $$
    
    ![image-20250408113044493](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250408113044493.png)
    
    
    $$
    Tanh = \frac{e^x - e^{-x}}{e^x + e^{-x}} \\
    \frac{dTanh}{dx} = 1 - Tanh^2
    $$
    
    ![image-20250408114522622](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250408114522622.png)
    
    ![image-20250408150852517](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250408150852517.png)
    $$
    ReLU = max(0, x)\\\\
    \frac{d(ReLU)}{dx} = \left\{\begin{matrix}
                          1,\ \ x > 0 \\
                          \ \ 0,\ \ x <= 0
    					 \end{matrix}\right.
    $$
    
    
    ![image-20250408115631449](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250408115631449.png)
    $$
    ReLU6 = \left\{\begin{matrix}
              x,\ \ 0 <= x <= 6 \\
              \ \ 6,\ \ x < 0 \ or \ x >6
             \end{matrix}\right.
    $$

$$
Leakey \ ReLU = \left\{\begin{matrix}
                  x,\ \ x >= 0 \\
                  ax,\ \ x < 0
                 \end{matrix}\right. \ aä¸ºå¸¸æ•°
\\
PReLU = \left\{\begin{matrix}
                  x,\ \ x >= 0 \\
                  ax,\ \ x < 0
                 \end{matrix}\right. \ aä¸ºè®­ç»ƒå‚æ•°
\\
RReLU =\left\{\begin{matrix}
                  x,\ \ x >= 0 \\
                  ax,\ \ x < 0
                 \end{matrix}\right. \ aä¸ºéšæœºæ•°
$$

$$
ELU = \left\{\begin{matrix}
                  &x,&x >= 0 \\
                  &a(e^x-1),&x < 0
                 \end{matrix}\right. \ aä¸ºè¶…å‚æ•°
\\\\
SELU =\lambda Â·\left\{\begin{matrix}
                  &x,&x >= 0 \\
                  &a(e^x-1),&x < 0
                 \end{matrix}\right. \ \lambdaå’Œaä¸ºè¶…å‚æ•°
$$

![image-20250408152548266](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250408152548266.png)

> è¶…å‚æ•°a=1æ—¶


$$
GELU(x)=xP(Xâ‰¤x)\\
\ \ \ \ \ \ \ \ =xÎ¦(x)
$$

$$
xÎ¦(x)â‰ˆxÏƒ(1.702x)
\\
xÎ¦(x)â‰ˆ\frac{1}{2} Ã—[1+tanh(\sqrt{\frac{Ï€}{2}}(x+0.044715x^3))]
$$

![image-20250408175314814](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250408175314814.png)![image-20250408175719134](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250408175719134.png)

![](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/op-activation-figure9.jpg)



$$
Swish = xÂ·Sigmoid(\beta x)
$$

$$
Hard\ Swish = xÂ·\frac{ReLU6(x + 3)}{6}
$$

![](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/op-activation-figure13.jpg)

$$
\text{Softmax}(x_{i}) =  \frac{e^{x_i}}{\sum_je^{x_j}} \\
\frac{\partial y_i}{\partial x_j} =

\begin{cases}

y_i (1 - y_i), & \text{if } i = j \\

- y_i y_j,     & \text{if } i \ne j

\end{cases}
\\
\\
J = 
\begin{bmatrix}
y_1(1 - y_1) & -y_1 y_2 & \cdots & -y_1 y_n \\
-y_2 y_1 & y_2(1 - y_2) & \cdots & -y_2 y_n \\
\vdots & \vdots & \ddots & \vdots \\
-y_n y_1 & -y_n y_2 & \cdots & y_n(1 - y_n)
\end{bmatrix}
$$

æºç é‡Œé¢æœ‰æ‰€ä¼˜åŒ–ï¼š
$$
\text{Softmax}(x_{i}) =  \frac{e^{x_i - max(x)}}{\sum_je^{x_j - max(x)}}
$$


ğŸ’¡ å®é™…æ„ä¹‰ï¼š

- Softmax çš„å¯¼æ•°ä¸æ˜¯ç‹¬ç«‹çš„ï¼Œæ¯ä¸ªè¾“å‡ºå€¼çš„æ¢¯åº¦éƒ½è·Ÿå…¶å®ƒçš„æœ‰å…³ã€‚

###  2. ç®€å•æ€»ç»“ï¼ˆæœªä¼˜åŒ–ï¼‰ï¼š

|    æ¿€æ´»å‡½æ•°    |                          æ•°å­¦è¡¨è¾¾å¼                          |             ä¼˜ç‚¹             |             ç¼ºç‚¹             |         é€‚ç”¨åœºæ™¯         |
| :------------: | :----------------------------------------------------------: | :--------------------------: | :--------------------------: | :----------------------: |
|  **Sigmoid**   |                    $\frac{1}{1 + e^{-x}}$                    |  è¾“å‡ºåœ¨ (0,1)ï¼Œé€‚åˆæ¦‚ç‡è¾“å‡º  | æ¢¯åº¦æ¶ˆå¤±ã€éé›¶å‡å€¼ã€è®¡ç®—è¾ƒæ…¢ |       äºŒåˆ†ç±»è¾“å‡ºå±‚       |
|  **Softmax**   |               $\frac{e^{x_i}}{\sum_j e^{x_j}}$               | å¤šåˆ†ç±»æ¦‚ç‡åˆ†å¸ƒï¼Œè¾“å‡ºæ€»å’Œä¸º 1 |   å¯¹æç«¯å€¼æ•æ„Ÿï¼Œè®¡ç®—æˆæœ¬é«˜   |       å¤šåˆ†ç±»è¾“å‡ºå±‚       |
|    **Tanh**    |             $\frac{e^x - e^{-x}}{e^x + e^{-x}}$              |    è¾“å‡ºåœ¨ (-1,1)ï¼Œé›¶å‡å€¼     |         æ¢¯åº¦æ¶ˆå¤±é—®é¢˜         |    RNNã€æ•°æ®å¯¹ç§°åœºæ™¯     |
|    **ReLU**    |                         $\max(0, x)$                         |    è®¡ç®—é«˜æ•ˆï¼Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±    |  ç¥ç»å…ƒæ­»äº¡ï¼ˆè´Ÿå€¼è¾“å‡ºä¸º 0ï¼‰  |   CNNã€é»˜è®¤éšè—å±‚æ¿€æ´»    |
| **Leaky ReLU** | $\begin{cases} x & \text{if } x \geq 0 \\ \alpha x & \text{if } x < 0 \end{cases}$ |      ç¼“è§£ç¥ç»å…ƒæ­»äº¡é—®é¢˜      |   éœ€æ‰‹åŠ¨è°ƒå‚ï¼ˆå¦‚ Î±=0.01ï¼‰    |    æ·±å±‚ç½‘ç»œæ›¿ä»£ ReLU     |
|   **PReLU**    |                   Leaky ReLUï¼Œä½†$Î±$å¯å­¦ä¹                     |      è‡ªé€‚åº”æ–œç‡ï¼Œæ›´çµæ´»      |          å¢åŠ å‚æ•°é‡          |    å¤æ‚ä»»åŠ¡ã€æ·±å±‚ç½‘ç»œ    |
|    **ELU**     | $\begin{cases} x & \text{if } x \geq 0 \\ \alpha(e^x - 1) & \text{if } x < 0 \end{cases}$ |    å¹³æ»‘è´Ÿå€¼ï¼Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±    |    è®¡ç®—å¤æ‚ï¼ˆå«æŒ‡æ•°è¿ç®—ï¼‰    |    éœ€è¦å¤„ç†è´Ÿå€¼çš„åœºæ™¯    |
|    **GELU**    |             $x \cdot \Phi(x)$  Î¦ ä¸ºæ ‡å‡†æ­£æ€ CDF              |    å¹³æ»‘æŸ”å’Œï¼Œé€‚åˆæ·±åº¦ç½‘ç»œ    |           è®¡ç®—å¤æ‚           | Transformerã€BERT ç­‰æ¨¡å‹ |
|   **Swish**    |             $x \cdot \sigma(\beta x)$ï¼ˆ$Î²$å¯è°ƒï¼‰             |  éå•è°ƒï¼Œå®éªŒæ€§èƒ½ä¼˜äº ReLU   |          è®¡ç®—è¾ƒå¤æ‚          | EfficientNet ç­‰å…ˆè¿›ç½‘ç»œ  |
|    **Mish**    |                $x \cdot \tanh(\ln(1 + e^x))$                 |  å¹³æ»‘ã€æ— ä¸Šç•Œï¼Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±  |          è®¡ç®—æˆæœ¬é«˜          |      è®¡ç®—æœºè§†è§‰ä»»åŠ¡      |
|    **Step**    | $\begin{cases} 1 & \text{if } x \geq 0 \\ 0 & \text{if } x < 0 \end{cases}$ |           äºŒå…ƒè¾“å‡º           |   ä¸å¯å¾®ï¼Œæ— æ³•ç”¨äºæ¢¯åº¦ä¸‹é™   |  æ—©æœŸæ„ŸçŸ¥æœºï¼ˆç°å·²å°‘ç”¨ï¼‰  |

### 3. æˆ‘çš„æ€è€ƒï¼š

- å¯¹äºç±»ä¼¼softmaxè¿™ç§åŒä¸€æ¡æ•°æ®çš„åˆ†æ¯ç›¸åŒçš„ï¼Œæ¯æ¬¡æ˜¯å¦éœ€è¦é‡æ–°è®¡ç®—åˆ†æ¯ï¼Ÿï¼ˆåº•å±‚æ˜¯å¦ä¼šåšcacheï¼Ÿï¼‰
- PyTorchæºç é‡Œé¢çš„`Softmax`æ¿€æ´»æåŠäº†`NLLLoss`ï¼Œè¿™æ˜¯ä»€ä¹ˆï¼Ÿä¸äº¤å‰ç†µåˆæ˜¯ä»€ä¹ˆå…³ç³»ï¼Ÿ
- softmaxçš„å¯¼æ•°æ¨å¯¼ã€‚
- softmaxè¾“å…¥çš„shapeå’Œæ¢¯åº¦çš„shapeä¸ä¸€æ ·å¤§å—ï¼Ÿå‚æ•°æ›´æ–°çš„æ—¶å€™åˆæ˜¯æ€æ ·å­çš„ï¼Ÿ

---





## 02. å¸¸ç”¨Torchç®—å­

torch.nn

- [Containers](https://pytorch.org/docs/stable/nn.html#containers)
- [Convolution Layers](https://pytorch.org/docs/stable/nn.html#convolution-layers)
- [Pooling layers](https://pytorch.org/docs/stable/nn.html#pooling-layers)
- [Padding Layers](https://pytorch.org/docs/stable/nn.html#padding-layers)
- [Non-linear Activations (weighted sum, nonlinearity)](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)
- [Non-linear Activations (other)](https://pytorch.org/docs/stable/nn.html#non-linear-activations-other)
- [Normalization Layers](https://pytorch.org/docs/stable/nn.html#normalization-layers)
- [Recurrent Layers](https://pytorch.org/docs/stable/nn.html#recurrent-layers)
- [Transformer Layers](https://pytorch.org/docs/stable/nn.html#transformer-layers)
- [Linear Layers](https://pytorch.org/docs/stable/nn.html#linear-layers)
- [Dropout Layers](https://pytorch.org/docs/stable/nn.html#dropout-layers)
- [Sparse Layers](https://pytorch.org/docs/stable/nn.html#sparse-layers)
- [Distance Functions](https://pytorch.org/docs/stable/nn.html#distance-functions)
- [Loss Functions](https://pytorch.org/docs/stable/nn.html#loss-functions)
- [Vision Layers](https://pytorch.org/docs/stable/nn.html#vision-layers)
- [Shuffle Layers](https://pytorch.org/docs/stable/nn.html#shuffle-layers)
- [DataParallel Layers (multi-GPU, distributed)](https://pytorch.org/docs/stable/nn.html#module-torch.nn.parallel)
- [Utilities](https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils)
- [Quantized Functions](https://pytorch.org/docs/stable/nn.html#quantized-functions)
- [Lazy Modules Initialization](https://pytorch.org/docs/stable/nn.html#lazy-modules-initialization)
    - [Aliases](https://pytorch.org/docs/stable/nn.html#aliases)

### 1. Convolutionï¼ˆæœªè¡¥ï¼‰

### 2. çº¿æ€§å˜æ¢å±‚

- Linear/Gemm

    > Note: Linearçš„weightæ˜¯è½¬ç½®å­˜æ”¾çš„

- Matmul

    > typeä¸Linearçš„ä¸åŒ
    >
    > è¦æ»¡è¶³å¹¿æ’­æœºåˆ¶

### 3. Normlizationï¼ˆğŸŒŸğŸŒŸğŸŒŸğŸŒŸï¼‰

- ç±»å‹
    $$
    Batch Normï¼ŒLayer Normï¼ŒInstance Normï¼ŒGroup Normï¼ŒRMS Norm
    $$
    

![figure4](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/op-figure4.jpg)



- å…¬å¼
    $$
    y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta
    $$



- åŠŸèƒ½

    > - å»é‡çº²ï¼ŒæŠŠæ•°æ®è°ƒæ•´åˆ°æ›´å¼ºçƒˆçš„æ•°æ®åˆ†å¸ƒ
    > - å‡å°‘æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸
    > - ä¸»è¦æ˜¯æœ‰ä¸€ä¸ªè®¡ç®—æœŸæœ›å’Œæ–¹å·®çš„è¿‡ç¨‹
    > - åšNormçš„ç²’åº¦ä¸åŒï¼Œåº”ç”¨åœºæ™¯ä¸åŒ
    >
    > - å…¶ä»–èµ„æ–™ï¼š[Batch Normçš„æŠ€æœ¯åšå®¢](https://blog.csdn.net/LoseInVain/article/details/86476010)

- ç‰¹å¾å’Œä¸åŒ

    >- ç²’åº¦ä¸åŒï¼ˆç»´åº¦ä¸åŒï¼‰ï¼Œå¯¹åº”åº”ç”¨é¢†åŸŸä¸åŒ
    >
    >**Batch Normæ˜¯é€channelï¼ˆæ¯ä¸ªbatchçš„åŒä¸€ä¸ªchannelï¼‰è¿›è¡Œæ ‡å‡†åŒ–**ï¼Œä¹Ÿå°±æ˜¯å®batchçš„ã€‚å›¾ç‰‡æ°å¥½éœ€è¦è¿™ç§æ–¹å¼ã€‚
    >
    >LNæ˜¯é€batchè¿›è¡Œæ ‡å‡†åŒ–çš„ã€‚NLPä¸­å¾€å¾€æ˜¯ä¸€ä¸ªä¸€ä¸ªçš„seqè¿›è¡Œè®­ç»ƒçš„ï¼Œè€Œä¸”é•¿åº¦ä¸åŒï¼Œæ›´é€‚åˆè¿™ç§ã€‚**è¿™è®©æˆ‘æƒ³èµ·äº†Attentionçš„soft maxæ“ä½œæ˜¯å¯¹ä¸€ä¸ªè¡Œå‘é‡è¿›è¡Œå½’ä¸€åŒ–çš„**
    >
    >LayerNormæœ‰åŠ©äºç¨³å®šè®­ç»ƒè¿‡ç¨‹å¹¶æé«˜æ”¶æ•›æ€§ã€‚å®ƒçš„å·¥ä½œåŸç†æ˜¯å¯¹è¾“å…¥çš„å„ä¸ªç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–ï¼Œç¡®ä¿æ¿€æ´»çš„å‡å€¼å’Œæ–¹å·®ä¸€è‡´ã€‚**æ™®éè®¤ä¸ºè¿™ç§å½’ä¸€åŒ–æœ‰åŠ©äºç¼“è§£ä¸å†…éƒ¨åå˜é‡åç§»ç›¸å…³çš„é—®é¢˜ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å­¦ä¹ å¹¶é™ä½å¯¹åˆå§‹æƒé‡çš„æ•æ„Ÿæ€§ã€‚**ä»æ¶æ„å›¾ä¸Šçœ‹ï¼ŒLayerNormåœ¨æ¯ä¸ªTransformer å—ä¸­åº”ç”¨ä¸¤æ¬¡ï¼Œä¸€æ¬¡åœ¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¹‹åï¼Œä¸€æ¬¡åœ¨FFNå±‚ä¹‹åï¼Œä½†æ˜¯åœ¨å®é™…å·¥ä½œä¸­ä¸ä¸€å®šå¦‚æ­¤ã€‚
    >
    >æ–‡æœ¬é•¿åº¦ä¸ç¡®å®šï¼Œè€Œåœ¨LNå±‚å¯ä»¥ã€‚
    >
    >åº”ç”¨åœºæ™¯ç¡®å®šLN

- BNæœŸæœ›å’Œæ–¹å·®è®¡ç®—ç­–ç•¥

    > `é‡‡ç”¨ç§»åŠ¨æŒ‡æ•°å¹³å‡`ï¼Œä¼šæœ‰å†å²ä¿¡æ¯åœ¨ï¼Œæœ‰ç‚¹ç±»ä¼¼RNNäº†
    >
    > $E_n = \alpha E + (1- \alpha)E_{n-1}$
    >
    > Var åŒç†

- åœ¨è®­ç»ƒå’Œæ¨ç†æ—¶æœ‰ä½•ä¸åŒï¼Ÿ

    > pytorchçš„æ¨¡å‹æœ‰ä¸¤ç§æ¨¡å¼ï¼Œåœ¨moduleæ¨¡å—é‡Œé¢æœ‰ä¸ª`training`å±æ€§ï¼Œä¹Ÿæœ‰å¯¹åº”çš„APIï¼Œé‡Œé¢æ˜ç¡®æŒ‡å‡ºäº†è¿™ä¸ª
    >
    > åœ¨BatchNormé‡‡ç”¨è®­ç»ƒæ—¶è®¡ç®—çš„ç»“æœï¼ˆEå’ŒVarï¼‰ï¼Œåº”ç”¨åˆ°æµ‹è¯•æˆ–è€…æ¨ç†çš„æ—¶å€™
    >
    > åœ¨Dropoutåç»­ä¼šè¯´ï¼Œè®­ç»ƒä¼šdropæ‰ï¼Œä½†æ¨ç†ä¸ä¼šï¼Œä¼šæ”¹æˆï¼ˆ1-rateï¼‰

    ```python
    def train(self: T, mode: bool = True) -> T:
        r"""Set the module in training mode.
    
        This has any effect only on certain modules. See documentations of
        particular modules for details of their behaviors in training/evaluation
        mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,
        etc.
    
        Args:
            mode (bool): whether to set training mode (``True``) or evaluation
                         mode (``False``). Default: ``True``.
    ```

    

- **RMS Norm**(å¤§æ¨¡å‹ä½¿ç”¨)ğŸŒŸğŸŒŸğŸŒŸ

    > æ¥æºäºLNï¼Œç®€åŒ–äº†LN
    >
    > å‡å‡å€¼ç›¸å½“äºå¹³ç§»ï¼Œè¿™é‡Œç›´æ¥å»æ‰å¹³ç§»ï¼Œåªä¿ç•™ç¼©æ”¾
    >
    > æŠŠä¹˜æ³•ç›´æ¥æ”¾è¿›æ¥äº†ï¼Œ

å¯¹LNåšç®€åŒ–ï¼Œå¯¹äºNLPï¼Œå¯¹ç¼©æ”¾æ•æ„Ÿï¼Œå¯¹å¹³ç§»ä¸æ•æ„Ÿï¼Œæ‰€ä»¥åˆ†å­ä¸å‡$E_x$ï¼Œå‡å°‘äº†å¾ˆå¤§è®¡ç®—é‡

![image-20250326224954810](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250326224954810.png)



**è¡¥å……éƒ¨åˆ†ï¼š**

- **DyT**(Transformers without normlization)

    [Transformers without normlization](https://yiyibooks.cn/arxiv/2503.10622v1/index.html)
    $$
    Norm:\ \gamma * \frac{(_x - E_{_x})}{\sqrt (Var_{_x})} + \beta
    $$

    $$
    DyT:\ \gamma * Tanh(\alpha * x) + \beta
    $$

    ![image-20250410151122973](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250410151122973.png)

- **Pre-Normï¼ˆå¤§æ¨¡å‹ä½¿ç”¨ï¼‰å’ŒPost-Norm**

    > ç»“åˆTransformeré‚£å—çŸ¥è¯†ï¼Œä¸€ä¸ªæ˜¯åœ¨æ®‹å·®å‰ï¼Œä¸€ä¸ªåœ¨æ®‹å·®å

    $$
    Output_{post}=LayerNorm(x+SubLayer(x))
    \\
    Output_{pre}=x+SubLayer(LayerNorm(x))
    $$

    

$$
å…¬å¼1:PostNorm(x)=x+LayerNorm(FeedForward(x+LayerNorm(Attention(x))))
\\\\
å…¬å¼2:PreNorm(x)=x+FeedForward(LayerNorm(x))+Attention(LayerNorm(x))
$$



| ç‰¹æ€§     |                Post-Norm                 |             Pre-Norm             |
| -------- | :--------------------------------------: | :------------------------------: |
| å…¬å¼     |                  å…¬å¼1                   |              å…¬å¼2               |
| ä½ç½®     |                  æ®‹å·®å                  |              æ®‹å·®å‰              |
| å‡ºç°æ—¶é—´ | åŸå§‹ Transformerï¼ˆVaswani et al., 2017ï¼‰ |     ä¹‹åå‘å±•ï¼ˆå¦‚ GPT-2 ç­‰ï¼‰      |
| ä¼˜ç‚¹     |        æ”¶æ•›åè¡¨ç°ç•¥å¥½ï¼ˆæŸäº›ä»»åŠ¡ï¼‰        | æ›´ç¨³å®šï¼Œè®­ç»ƒæ·±å±‚æ¨¡å‹ä¸æ˜“æ¢¯åº¦æ¶ˆå¤± |
| ç¼ºç‚¹     |       æ·±å±‚æ¨¡å‹ä¸­å®¹æ˜“æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸        |  å¯èƒ½æœ€ç»ˆæ€§èƒ½ç•¥ä½ï¼Œä½†æ›´å®¹æ˜“è®­ç»ƒ  |
| åº”ç”¨æƒ…å†µ |          BERTã€åˆç‰ˆTransformer           |    GPTç³»åˆ—ã€T5ã€LLamaç­‰å¤§æ¨¡å‹    |



|         æ¨¡å‹         | å½’ä¸€åŒ–ç±»å‹ |
| :------------------: | :--------: |
|     **DeepSeek**     |  Pre-Norm  |
|    **GPT-2/3/4**     |  Pre-Norm  |
|       **BERT**       | Post-Norm  |
|        **T5**        |  Pre-Norm  |
|      **LLaMA**       |  Pre-Norm  |
|  **Transformer XL**  |  Pre-Norm  |
| **åŸå§‹ Transformer** | Post-Norm  |

- DeepNorm

    DeepNorm æ˜¯å¾®è½¯åœ¨ 2022 å¹´æå‡ºçš„æ”¹è¿›æ–¹æ³•ï¼ˆè®ºæ–‡ *"[DeepNet: Scaling Transformers to 1,000 Layers](https://arxiv.org/abs/2203.00555)"*ï¼‰ï¼Œ**åŸºäº Post-Norm ä½†å¤§å¹…æå‡äº†æ·±å±‚è®­ç»ƒçš„ç¨³å®šæ€§**ï¼Œå¯æ”¯æŒè¶…æ·±å±‚ï¼ˆå¦‚ 1000 å±‚ï¼‰Transformer çš„è®­ç»ƒã€‚

    ![image-20250409164334392](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250409164334392.png)

    ![image-20250409162034019](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250409162034019.png)

    åŸå§‹æ®‹å·®ç»“æ„:
    $$
    x_{l+1} = LayerNorm(x_l + F(x_l))
    $$
    DeepNorm:
    $$
    x_{l+1} = \text{LN}(\alpha \cdot x_l + G_l(x_l, \theta_l))
    $$
    ![image-20250409164837814](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250409164837814.png)

**æ€è€ƒï¼š**DeepNormä¸­çš„$\beta$æ˜¯å“ªé‡Œçš„å‚æ•°ï¼Ÿ



> Noteï¼šæ•´ä½“å›é¡¾ä¸€ä¸‹ç®—å­éƒ¨åˆ†ï¼Œå¼•å…¥åé¢çš„ç®—å­ï¼Œä¸ç„¶å¤§è„‘è¿˜åœç•™åœ¨norméƒ¨åˆ†



### 4. Polling(CV)

- ä½œç”¨

> å¢å¤§æ„Ÿå—é‡
>
> å‡å°‘ç‰¹å¾å›¾å°ºå¯¸ï¼Œä¿ç•™é‡è¦ä¿¡æ¯ï¼Œé™ä½è®¡ç®—é‡
>
> é™é‡‡æ ·ï¼Œå‡å°‘å™ªéŸ³
>
> ä½ç½®å˜åŒ–é²æ£’æ€§å¢å¼º

- å…¶ä»–

> å¯¹åº”æœ‰ä¸ªout positionçŸ©é˜µ

- ç±»å‹

> Maxã€Avgç­‰

### 5. activationsï¼ˆçœ‹01éƒ¨åˆ†ï¼‰

> è¿™é‡Œé¢å†…å®¹è¾ƒå¤šï¼Œç›´æ¥çœ‹01ç« èŠ‚

[01. æ¿€æ´»å‡½æ•°ğŸŒŸğŸŒŸğŸŒŸ](#01. æ¿€æ´»å‡½æ•°ğŸŒŸğŸŒŸğŸŒŸ)

### 6. å…¶ä»–

### 7. ç‰¹åˆ«çš„ä¸€äº›Operator

$$
reshapeã€viewã€permuteã€transpose
$$

> å¯èƒ½éœ€è¦è¡¥å……ä¸€ä¸‹PyTorchçš„TensorçŸ¥è¯†
>
> æ¯”å¦‚ï¼šmatedataå’Œstorageï¼›dataã€storageã€data_ptrã€strideã€contiguousï¼›stateã€state_dictç­‰

- reshape

    > åŸå§‹æ•°æ®å†…å­˜æ’å¸ƒä¸å˜ï¼Œåªå˜shape

- view

    > ç±»ä¼¼reshapeï¼ŒåŸå§‹æ•°æ®ä¸å˜

- permute

    > permuteä¼šå¯¹æ•°æ®åº•å±‚é‡æ’ï¼Œæ”¯æŒå¤šä¸ªè½´è¿›è¡Œäº¤æ¢

- transpose

    > ç±»ä¼¼permuteï¼Œä¼šå¯¹æ•°æ®é‡æ’ï¼Œæ”¯æŒä¸¤è½´äº¤æ¢

**Tips:**

> 1. view()ï¼šå½“tensorè¿ç»­æ—¶tensor.view()ä¸æ”¹å˜å­˜å‚¨åŒºçš„çœŸå®æ•°æ®ï¼Œåªæ”¹å˜å…ƒæ•°æ®ï¼ˆMetadataï¼‰ä¸­çš„ä¿¡æ¯, è°ƒç”¨viewæ–¹æ³•å¼ é‡å¿…é¡»è¿ç»­çš„ã€‚ 
>
> 2.  reshape()ï¼šå½“tensorè¿ç»­æ—¶å’Œview()ç›¸åŒï¼Œä¸è¿ç»­æ—¶ç­‰ä»·äºcontiguous().view() 
>
> 3. permute()ï¼šé€šè¿‡æ”¹å˜å¼ é‡çš„æ­¥é•¿ï¼ˆstrideï¼‰é‡æ–°æ’åˆ—å¼ é‡çš„ç»´åº¦ï¼Œä½†ä¼šå¯¼è‡´å¼ é‡åœ¨å†…å­˜ä¸­çš„å­˜å‚¨å˜å¾—ä¸è¿ç»­ 
>
> 4. contiguous()ï¼šå¼€è¾Ÿæ–°çš„å­˜å‚¨åŒºï¼Œç¡®ä¿å¼ é‡åœ¨å†…å­˜ä¸­æ˜¯è¿ç»­å­˜å‚¨ï¼Œåœ¨permute()æ“ä½œåéœ€è¦æ¥contiguous()æ‰èƒ½æ¥view()
>
> 5. stride()ï¼šåœ¨æŒ‡å®šç»´åº¦ï¼ˆdimï¼‰ä¸Šï¼Œå­˜å‚¨åŒºä¸­çš„æ•°æ®å…ƒç´ ï¼Œä»ä¸€ä¸ªå…ƒç´ è·³åˆ°ä¸‹ä¸€ä¸ªå…ƒç´ æ‰€å¿…é¡»çš„æ­¥é•¿ 
>
> 6. pytorchçš„å­˜å‚¨æ–¹å¼ï¼šmetadata+storage
>
>     metadataä¿å­˜ï¼šsize,dimension,strideç­‰å…ƒä¿¡æ¯
>
>     storageä¿å­˜ï¼šä»¥ä¸€ç»´æ•°ç»„ä¿å­˜å¯¹åº”çš„å¼ é‡æ•°æ®
>
>     <img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250409174658632.png" alt="image-20250409174658632" style="zoom:50%;" />


$$
squeezeã€unsqueeze
$$

- æ˜¯å¯¹ç»´åº¦çš„å‹ç¼©å’Œæ‰©å……

    > å¢åŠ ä¸€ä¸ªç»´åº¦ï¼Œå‡å°‘ä¸€ä¸ªç»´åº¦

$$
concatã€stackã€expandã€flatten
$$

- concat

    > ä¸splitç›¸åæ“ä½œï¼Œå¯ä»¥æŒ‡å®šæŸè½´

- stack

    > æ”¯æŒæ–°å¢ä¸€ä¸ªè½´è¿›è¡Œæ‹¼æ¥

- expand

    > æ”¯æŒå¹¿æ’­æœºåˆ¶

- flatten

    > æ‹‰å¹³æˆä¸€ç»´

$$
pointwizeç±»å‹
$$

â€¦â€¦
$$
splitã€slice
$$

$$
reduceç±»å‹
$$

### 8. Embeddingï¼ˆå¯èƒ½éœ€è¦å•ç‹¬ä¸€èŠ‚ï¼‰

> ç»“åˆTokenizer

- å¯¹åˆ†è¯å™¨åˆ†åˆ°çš„ç»“æœè¿›è¡ŒEmbedding

- æœ‰ä¸€ä¸ªEmbeddingè¡¨ï¼Œç›´æ¥æ ¹æ®indexæŸ¥åˆ°
- è®¡ç®—åŸç†ï¼šä¼ å…¥weightå¤§å°ï¼ŒæŠŠEmbedding tableé‡Œé¢çš„æå–

### 9. Dropoutï¼ˆğŸŒŸğŸŒŸğŸŒŸï¼‰

- åŠŸèƒ½

- åŸç†

    > ä¸¢å¼ƒ => ç½®é›¶
    >
    > éšæœºæ€§ï¼Œä¸ç„¶ä¼šç¥ç»å…ƒåæ­»

- è®­ç»ƒå’Œæ¨ç†æœ‰å•¥ä¸åŒ

    > æ¨ç†ç›´æ¥å»æ‰ï¼ŒæŠŠè¿æ¥weightä¹˜ä»¥ï¼ˆ1-pï¼‰

### 10 . æˆ‘çš„æ€è€ƒï¼š

- ä¸åŒçš„Normçš„å‚æ•°é‡

    > å¯å­¦ä¹ å‚æ•°å’Œå‡å€¼æ–¹å·®

- ä¸åŒNormæ“ä½œç»´åº¦ï¼ŒConvæ“ä½œç»´åº¦ï¼ŒPollingæ“ä½œç»´åº¦

- å…¶ä»–å“ªäº›ç®—å­çš„åº•å±‚æ˜¯copyè¿˜æ˜¯in-place

- å¦‚ä½•å®ç°è®­ç»ƒå’Œæ¨ç†ä¸åŒçš„æƒ…å†µï¼Ÿï¼ˆç›¸å½“äºåŠ é”æˆ–è€…if elseï¼‰

---





## 03. BPç¥ç»ç½‘ç»œ&BaseLine

