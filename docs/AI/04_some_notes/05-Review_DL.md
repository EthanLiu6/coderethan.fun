# å›é¡¾å’Œå¤ä¹ æ•´ç†DL



## 00. pythonåŸºç¡€ï¼ˆæœªè¡¥ï¼‰

> æŠ½ç©ºè¡¥å……

- è¿­ä»£å™¨

- è£…é¥°å™¨

    | åœºæ™¯             | è£…é¥°å™¨åŠŸèƒ½                     | ç¤ºä¾‹                           |
    | :--------------- | :----------------------------- | :----------------------------- |
    | **è®­ç»ƒæ—¶é—´ç»Ÿè®¡** | è®°å½•å‡½æ•°è¿è¡Œæ—¶é—´               | `@timer`                       |
    | **æ¢¯åº¦è£å‰ª**     | é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸                   | `@gradient_clip(max_norm=1.0)` |
    | **æ¨¡å‹éªŒè¯æ¨¡å¼** | è‡ªåŠ¨åˆ‡æ¢ `eval()` å’Œ `train()` | `@eval_mode`                   |
    | **æ—¥å¿—è®°å½•**     | ä¿å­˜è®­ç»ƒæŒ‡æ ‡åˆ°æ–‡ä»¶             | `@log_metrics(log_file="...")` |
    | **æ•°æ®é¢„å¤„ç†**   | è‡ªåŠ¨æ ‡å‡†åŒ–/å¢å¼ºè¾“å…¥æ•°æ®        | `@normalize_data(mean, std)`   |

- callable

- 

---



## 00. æ•°å­¦è¡¥å……ï¼ˆæœªå®Œå…¨æ¨å¯¼ï¼‰

- æ¦‚ç‡ã€å‡ ç‡ã€æ¡ä»¶æ¦‚ç‡ã€è”åˆæ¦‚ç‡ç­‰

    > $P(A)$ï¼šäº‹æƒ…Aå‘ç”Ÿçš„â€œå¯èƒ½æ€§â€æ˜¯å¤šå°‘ã€‚
    >
    > $Odds$ï¼šæˆåŠŸçš„æœºä¼šä¸å¤±è´¥çš„æœºä¼šçš„â€œæ¯”å€¼â€ï¼ˆæˆ–è€…å¯¹åº”æ¦‚ç‡çš„æ¯”å€¼ï¼‰
    >
    > $P(Aâˆ£B)$ï¼šåœ¨å·²çŸ¥Bå‘ç”Ÿçš„å‰æä¸‹ï¼ŒAå‘ç”Ÿçš„æ¦‚ç‡ã€‚
    >
    > $P(Aâˆ©B)$ï¼šAå’ŒB**åŒæ—¶å‘ç”Ÿ**çš„æ¦‚ç‡ã€‚

- æ ¹æ®ä¸Šé¢å†…å®¹å¼•å‡ºsigmoidå’Œsoftmax

- ç†µã€äº¤å‰ç†µã€è”åˆç†µç­‰

    > $H(P)$ï¼šä¸€ä¸ªåˆ†å¸ƒæœ‰å¤šâ€œä¸å¯é¢„æµ‹â€ï¼Ÿç®€å•è¯´å°±æ˜¯ä¸€äº‹ä»¶çš„ä¸ç¡®å®šç¨‹åº¦ã€‚ç†µè¶Šå¤§ï¼Œè¯´æ˜è¶Šâ€œæ··ä¹±â€ï¼Œè¶Šä¸ç¡®å®šã€‚
    >
    > $H(P,Q)$ï¼šç”¨å¦ä¸€ä¸ªåˆ†å¸ƒ $Q$ å»é¢„æµ‹çœŸå®åˆ†å¸ƒ $P$ æ—¶ï¼Œæˆ‘ä»¬â€œçŠ¯äº†å¤šå¤§é”™â€ã€‚
    >
    > $è”åˆç†µ$ï¼šä¸¤ä¸ªå˜é‡ä¸€èµ·çš„â€œä¸ç¡®å®šæ€§â€ã€‚

- KLè·ç¦»å’ŒKLæ•£åº¦

    > **KLæ•£åº¦è¡¡é‡â€œä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒå·®äº†å¤šå°‘â€**ã€‚

- æå¤§ä¼¼ç„¶ä¼°è®¡

    > æå¤§ä¼¼ç„¶ä¼°è®¡æ˜¯ä¸€ç§â€œåæ¨â€æ¨¡å‹å‚æ•°çš„åŠæ³•ï¼Œå®ƒè®¤ä¸ºä½ è§‚å¯Ÿåˆ°çš„æ•°æ®å°±æ˜¯æœ€æœ‰å¯èƒ½çš„ç°å®ï¼Œç„¶åå»æ‰¾é‚£ä¸ªæœ€å¯èƒ½è§£é‡Šç°å®çš„å‚æ•°ã€‚ï¼ˆæœ‰ç‚¹ç±»ä¼¼æœ€å°äºŒä¹˜ï¼‰
    >
    > - 1.ä¼¼ç„¶å‡½æ•°ï¼šåœ¨æŸä¸ªç‰¹å®šå‚æ•°ä¸‹ï¼Œ**æˆ‘ä»¬è§‚æµ‹åˆ°çš„æ•°æ®æœ‰å¤šâ€œå¯èƒ½â€å‡ºç°**ã€‚
    > - 2.å–å¯¹æ•°
    > - 3.å¯¹2ä½œä¸ºç›®æ ‡å‡½æ•°ï¼Œæ±‚æå¤§å€¼ï¼ˆæˆ–è€…è´Ÿå¯¹æ•°çš„æå°å€¼ï¼‰
    > - 4.å‚æ•°è§£æå¼çš„æ±‚æ³•ï¼ša.ç›´æ¥æ±‚è§£ï¼ˆæœ‰å¯¹æ•°ï¼Œä¸æ–¹ä¾¿ï¼‰ï¼Œb.é‡‡ç”¨æ¢¯åº¦ä¸‹é™æ€æƒ³
    > - 5.å‚æ•°ç¡®å®šåï¼Œä¸Šæ¸¸ä»»åŠ¡ç¡®å®š





## 01. ActivationğŸŒŸğŸŒŸğŸŒŸ

### 1. æˆ‘çš„è®°å½•ï¼š

- ä¸ºå•¥éœ€è¦Activationï¼Ÿï¼ˆæ·±åº¦å­¦ä¹ å¾€å¾€åªè¯´éçº¿æ€§Attentionï¼‰

    > éƒ½æ˜¯Linearï¼ˆæˆ–è€…Covnï¼‰çš„è¯ç½‘ç»œè¿˜æ˜¯çº¿æ€§çš„ï¼Œå¤šå±‚å’Œä¸€å±‚æ²¡å•¥å¤ªå¤§åŒºåˆ«ã€‚

- æœ‰å“ªäº›ç±»å‹ï¼Ÿï¼ˆå·¥ç¨‹ä¸Šä¸»è¦ç±»åˆ«ï¼‰

    > ä¸€ç§æ˜¯point wiseçš„æ“ä½œï¼ˆæˆ–è€…å«åšelement wiseæ“ä½œï¼‰ï¼Œä¹Ÿå°±æ˜¯é€å…ƒç´ æ“ä½œï¼›å¦ä¸€ç§æ˜¯å…·æœ‰ç›¸å…³æ€§æ“ä½œ
    
    > > Note: å…¬å¼ã€å›¾åƒã€å¯¼æ•°å›¾åƒã€ä¼˜ç¼ºç‚¹â€¦â€¦
    >
    > Aç±»ï¼šæ•°æ®é—´ç‹¬ç«‹
    >
    > 1. Så‹ï¼š
    >
    >     - sigmoidï¼ˆ0.5çš„äºŒåˆ†ç±»é—®é¢˜ï¼‰
    >
    >         > æ¢¯åº¦é“¾å¼æ³•åˆ™ç´¯ä¹˜ä¼šè¶‹è¿‘äº0ï¼Œä¸¤å±‚çš„æœ€å€¼ä¹Ÿå°±æ˜¯0.25ï¼Œå¤šå±‚å¯æƒ³è€ŒçŸ¥
    >
    >     - Tanhï¼ˆNLPçš„é€’å½’ç³»åˆ—å¸¸ç”¨ï¼‰
    >
    > 2. ==ReLU==ï¼šAlexNeté‡Œé¢æå‡ºã€‚
    >
    >     - ReLU
    >
    >         > ä½†æ˜¯ç¥ç»å…ƒæˆ–è€…é“¾è·¯å¤ªå°‘ï¼Œä¼šæœ‰0çš„å¯¼æ•°ï¼Œæ›´æ–°ä¸æ˜“
    >
    >     - ç¼“è§£æ¿€æ´»å€¼æŒ‡æ•°æ€§å¢é•¿=>ReLU6
    >
    >     - ç¼“è§£ç¥ç»å…ƒåæ­»ç°è±¡=>Leakey ReLU=>PReLU/RReLU
    >
    >     - x=0å¤„å¹³æ»‘è¿‡æ¸¡=>ELU/SELU
    >
    >     - ==GeLU==ï¼ˆå·¥ç¨‹ä¸Šæœ‰ç‚¹ç®€åŒ–ï¼Œç°åœ¨å¤§æ¨¡å‹éƒ½ä¼šç”¨ä»–çš„å˜ç§ï¼‰
    >
    > 3. Swishï¼š
    >
    >     - Switchï¼ˆå¯¹sigmoidè¿›è¡Œä¿®æ”¹ï¼‰
    >     - Hard Swishï¼ˆå¯¹ReLU6åšä¿®æ”¹ï¼Œè®¡ç®—æ›´ç®€å•ï¼‰ã€‚â€œå‡‘å›¾åƒâ€
    >
    > 4. mishï¼šå¹³æ»‘æ€§æ›´é«˜ï¼ˆçœ‹ä¸€é˜¶å’ŒäºŒé˜¶å¯¼å›¾åƒï¼‰
    >
    > Bç±»ï¼šæ•°æ®é—´æœ‰ç›¸äº’ä½œç”¨
    >
    > 1. ==softmax==ï¼›æƒ³æƒ³sigmoidã€‚è¿™ä¸ªæ˜¯å¤šåˆ†ç±»ï¼Œå•è°ƒéè´Ÿå½’ä¸€
    >
    >     ç±»ä¼¼æ“ä½œè¿˜æœ‰çŸ©é˜µä¹˜æ³•ç­‰
    
    $$
    sigmoid = \frac{1}{1 + e^{-x}}
    $$
    
    $$
    \frac{d(sigmoid)}{dx} = sigmoid(x) Â· (1 - sigmoid(x))
    $$
    
    
    
    ![image-20250408113044493](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250408113044493.png)
    
    $$
    Tanh = \frac{e^x - e^{-x}}{e^x + e^{-x}}
    $$
    
    $$
    \frac{dTanh}{dx} = 1 - Tanh^2
    $$
    
    
    
    ![image-20250408114522622](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250408114522622.png)
    
    ![image-20250408150852517](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250408150852517.png)
    $$
    ReLU = max(0, x)
    $$
    
    $$
    \frac{d(ReLU)}{dx} = \left\{\begin{matrix}
                          1, x > 0 \\
                          0, x <= 0
    					 \end{matrix}\right.
    $$
    
    
    
    ![image-20250408115631449](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250408115631449.png)
    $$
    ReLU6 = \left\{\begin{matrix}
              x,\ \ 0 <= x <= 6 \\
              \ \ 6,\ \ x < 0 \ or \ x >6
             \end{matrix}\right.
    $$

$$
Leakey \ ReLU = \left\{\begin{matrix}
                  x,\ \ x >= 0 \\
                  ax,\ \ x < 0
                 \end{matrix}\right. \ aä¸ºå¸¸æ•°
$$

$$
PReLU = \left\{\begin{matrix}
                  x,\ \ x >= 0 \\
                  ax,\ \ x < 0
                 \end{matrix}\right. \ aä¸ºè®­ç»ƒå‚æ•°
$$

$$
RReLU =\left\{\begin{matrix}
                  x,\ \ x >= 0 \\
                  ax,\ \ x < 0
                 \end{matrix}\right. \ aä¸ºéšæœºæ•°
$$


$$
ELU = \left\{\begin{matrix}
                  &x,&x >= 0 \\
                  &a(e^x-1),&x < 0
                 \end{matrix}\right. \ aä¸ºè¶…å‚æ•°
$$

$$
SELU =\lambda Â·\left\{\begin{matrix}
                  &x,&x >= 0 \\
                  &a(e^x-1),&x < 0
                 \end{matrix}\right. \ \lambdaå’Œaä¸ºè¶…å‚æ•°
$$



![image-20250408152548266](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250408152548266.png)

> è¶…å‚æ•°a=1æ—¶


$$
GELU(x)=xP(Xâ‰¤x)\\
\ \ \ \ \ \ \ \ =xÎ¦(x)
$$

$$
xÎ¦(x)â‰ˆxÏƒ(1.702x)
$$

$$
xÎ¦(x)â‰ˆ\frac{1}{2} Ã—[1+tanh(\sqrt{\frac{Ï€}{2}}(x+0.044715x^3))]
$$



![image-20250408175314814](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250408175314814.png)![image-20250408175719134](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250408175719134.png)

![](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/op-activation-figure9.jpg)



$$
Swish = xÂ·Sigmoid(\beta x)
$$

$$
Hard\ Swish = xÂ·\frac{ReLU6(x + 3)}{6}
$$

![](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/op-activation-figure13.jpg)

$$
\text{Softmax}(x_{i}) =  \frac{e^{x_i}}{\sum_je^{x_j}}
$$

$$
\frac{\partial y_i}{\partial x_j} =

\begin{cases}

y_i (1 - y_i), & \text{if } i = j \\

- y_i y_j,     & \text{if } i \ne j

\end{cases}
$$

$$
J = 
\begin{bmatrix}
y_1(1 - y_1) & -y_1 y_2 & \cdots & -y_1 y_n \\
-y_2 y_1 & y_2(1 - y_2) & \cdots & -y_2 y_n \\
\vdots & \vdots & \ddots & \vdots \\
-y_n y_1 & -y_n y_2 & \cdots & y_n(1 - y_n)
\end{bmatrix}
$$



æºç é‡Œé¢æœ‰æ‰€ä¼˜åŒ–ï¼š
$$
\text{Softmax}(x_{i}) =  \frac{e^{x_i - max(x)}}{\sum_je^{x_j - max(x)}}
$$


ğŸ’¡ å®é™…æ„ä¹‰ï¼š

- Softmax çš„å¯¼æ•°ä¸æ˜¯ç‹¬ç«‹çš„ï¼Œæ¯ä¸ªè¾“å‡ºå€¼çš„æ¢¯åº¦éƒ½è·Ÿå…¶å®ƒçš„æœ‰å…³ã€‚

###  2. ç®€å•æ€»ç»“ï¼ˆæœªä¼˜åŒ–ï¼‰ï¼š

|    æ¿€æ´»å‡½æ•°    |                          æ•°å­¦è¡¨è¾¾å¼                          |             ä¼˜ç‚¹             |             ç¼ºç‚¹             |         é€‚ç”¨åœºæ™¯         |
| :------------: | :----------------------------------------------------------: | :--------------------------: | :--------------------------: | :----------------------: |
|  **Sigmoid**   |                    $\frac{1}{1 + e^{-x}}$                    |  è¾“å‡ºåœ¨ (0,1)ï¼Œé€‚åˆæ¦‚ç‡è¾“å‡º  | æ¢¯åº¦æ¶ˆå¤±ã€éé›¶å‡å€¼ã€è®¡ç®—è¾ƒæ…¢ |       äºŒåˆ†ç±»è¾“å‡ºå±‚       |
|  **Softmax**   |               $\frac{e^{x_i}}{\sum_j e^{x_j}}$               | å¤šåˆ†ç±»æ¦‚ç‡åˆ†å¸ƒï¼Œè¾“å‡ºæ€»å’Œä¸º 1 |   å¯¹æç«¯å€¼æ•æ„Ÿï¼Œè®¡ç®—æˆæœ¬é«˜   |       å¤šåˆ†ç±»è¾“å‡ºå±‚       |
|    **Tanh**    |             $\frac{e^x - e^{-x}}{e^x + e^{-x}}$              |    è¾“å‡ºåœ¨ (-1,1)ï¼Œé›¶å‡å€¼     |         æ¢¯åº¦æ¶ˆå¤±é—®é¢˜         |    RNNã€æ•°æ®å¯¹ç§°åœºæ™¯     |
|    **ReLU**    |                         $\max(0, x)$                         |    è®¡ç®—é«˜æ•ˆï¼Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±    |  ç¥ç»å…ƒæ­»äº¡ï¼ˆè´Ÿå€¼è¾“å‡ºä¸º 0ï¼‰  |   CNNã€é»˜è®¤éšè—å±‚æ¿€æ´»    |
| **Leaky ReLU** | $\begin{cases} x & \text{if } x \geq 0 \\ \alpha x & \text{if } x < 0 \end{cases}$ |      ç¼“è§£ç¥ç»å…ƒæ­»äº¡é—®é¢˜      |   éœ€æ‰‹åŠ¨è°ƒå‚ï¼ˆå¦‚ Î±=0.01ï¼‰    |    æ·±å±‚ç½‘ç»œæ›¿ä»£ ReLU     |
|   **PReLU**    |                   Leaky ReLUï¼Œä½†$Î±$å¯å­¦ä¹                     |      è‡ªé€‚åº”æ–œç‡ï¼Œæ›´çµæ´»      |          å¢åŠ å‚æ•°é‡          |    å¤æ‚ä»»åŠ¡ã€æ·±å±‚ç½‘ç»œ    |
|    **ELU**     | $\begin{cases} x & \text{if } x \geq 0 \\ \alpha(e^x - 1) & \text{if } x < 0 \end{cases}$ |    å¹³æ»‘è´Ÿå€¼ï¼Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±    |    è®¡ç®—å¤æ‚ï¼ˆå«æŒ‡æ•°è¿ç®—ï¼‰    |    éœ€è¦å¤„ç†è´Ÿå€¼çš„åœºæ™¯    |
|    **GELU**    |             $x \cdot \Phi(x)$  Î¦ ä¸ºæ ‡å‡†æ­£æ€ CDF              |    å¹³æ»‘æŸ”å’Œï¼Œé€‚åˆæ·±åº¦ç½‘ç»œ    |           è®¡ç®—å¤æ‚           | Transformerã€BERT ç­‰æ¨¡å‹ |
|   **Swish**    |             $x \cdot \sigma(\beta x)$ï¼ˆ$Î²$å¯è°ƒï¼‰             |  éå•è°ƒï¼Œå®éªŒæ€§èƒ½ä¼˜äº ReLU   |          è®¡ç®—è¾ƒå¤æ‚          | EfficientNet ç­‰å…ˆè¿›ç½‘ç»œ  |
|    **Mish**    |                $x \cdot \tanh(\ln(1 + e^x))$                 |  å¹³æ»‘ã€æ— ä¸Šç•Œï¼Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±  |          è®¡ç®—æˆæœ¬é«˜          |      è®¡ç®—æœºè§†è§‰ä»»åŠ¡      |
|    **Step**    | $\begin{cases} 1 & \text{if } x \geq 0 \\ 0 & \text{if } x < 0 \end{cases}$ |           äºŒå…ƒè¾“å‡º           |   ä¸å¯å¾®ï¼Œæ— æ³•ç”¨äºæ¢¯åº¦ä¸‹é™   |  æ—©æœŸæ„ŸçŸ¥æœºï¼ˆç°å·²å°‘ç”¨ï¼‰  |

### 3. æˆ‘çš„æ€è€ƒï¼š

- å¯¹äºç±»ä¼¼softmaxè¿™ç§åŒä¸€æ¡æ•°æ®çš„åˆ†æ¯ç›¸åŒçš„ï¼Œæ¯æ¬¡æ˜¯å¦éœ€è¦é‡æ–°è®¡ç®—åˆ†æ¯ï¼Ÿï¼ˆåº•å±‚æ˜¯å¦ä¼šåšcacheï¼Ÿï¼‰
- PyTorchæºç é‡Œé¢çš„`Softmax`æ¿€æ´»æåŠäº†`NLLLoss`ï¼Œè¿™æ˜¯ä»€ä¹ˆï¼Ÿä¸äº¤å‰ç†µåˆæ˜¯ä»€ä¹ˆå…³ç³»ï¼Ÿ
- softmaxçš„å¯¼æ•°æ¨å¯¼ã€‚
- softmaxè¾“å…¥çš„shapeå’Œæ¢¯åº¦çš„shapeä¸ä¸€æ ·å¤§å—ï¼Ÿå‚æ•°æ›´æ–°çš„æ—¶å€™åˆæ˜¯æ€æ ·å­çš„ï¼Ÿ

---





## 02. å¸¸ç”¨Torchç®—å­

torch.nn

- [Containers](https://pytorch.org/docs/stable/nn.html#containers)
- [Convolution Layers](https://pytorch.org/docs/stable/nn.html#convolution-layers)
- [Pooling layers](https://pytorch.org/docs/stable/nn.html#pooling-layers)
- [Padding Layers](https://pytorch.org/docs/stable/nn.html#padding-layers)
- [Non-linear Activations (weighted sum, nonlinearity)](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)
- [Non-linear Activations (other)](https://pytorch.org/docs/stable/nn.html#non-linear-activations-other)
- [Normalization Layers](https://pytorch.org/docs/stable/nn.html#normalization-layers)
- [Recurrent Layers](https://pytorch.org/docs/stable/nn.html#recurrent-layers)
- [Transformer Layers](https://pytorch.org/docs/stable/nn.html#transformer-layers)
- [Linear Layers](https://pytorch.org/docs/stable/nn.html#linear-layers)
- [Dropout Layers](https://pytorch.org/docs/stable/nn.html#dropout-layers)
- [Sparse Layers](https://pytorch.org/docs/stable/nn.html#sparse-layers)
- [Distance Functions](https://pytorch.org/docs/stable/nn.html#distance-functions)
- [Loss Functions](https://pytorch.org/docs/stable/nn.html#loss-functions)
- [Vision Layers](https://pytorch.org/docs/stable/nn.html#vision-layers)
- [Shuffle Layers](https://pytorch.org/docs/stable/nn.html#shuffle-layers)
- [DataParallel Layers (multi-GPU, distributed)](https://pytorch.org/docs/stable/nn.html#module-torch.nn.parallel)
- [Utilities](https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils)
- [Quantized Functions](https://pytorch.org/docs/stable/nn.html#quantized-functions)
- [Lazy Modules Initialization](https://pytorch.org/docs/stable/nn.html#lazy-modules-initialization)
    - [Aliases](https://pytorch.org/docs/stable/nn.html#aliases)

### 1. Convolutionï¼ˆæœªè¡¥ï¼‰

### 2. çº¿æ€§å˜æ¢å±‚

- Linear/Gemm

    > Note: Linearçš„weightæ˜¯è½¬ç½®å­˜æ”¾çš„
    >
    > ```python
    > self.weight = Parameter(
    >             torch.empty((out_features, in_features), **factory_kwargs)
    >         )
    > ```
    >
    > 

- Matmul

    > typeä¸Linearçš„ä¸åŒ
    >
    > è¦æ»¡è¶³å¹¿æ’­æœºåˆ¶

### 3. NormlizationğŸŒŸğŸŒŸğŸŒŸğŸŒŸ

- ç±»å‹
    $$
    Batch Normï¼ŒLayer Normï¼ŒInstance Normï¼ŒGroup Normï¼ŒRMS Norm
    $$
    

![figure4](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/op-figure4.jpg)



- å…¬å¼
    $$
    y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta
    $$



- åŠŸèƒ½

    > - å»é‡çº²ï¼ŒæŠŠæ•°æ®è°ƒæ•´åˆ°æ›´å¼ºçƒˆçš„æ•°æ®åˆ†å¸ƒ
    > - å‡å°‘æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸
    > - ä¸»è¦æ˜¯æœ‰ä¸€ä¸ªè®¡ç®—æœŸæœ›å’Œæ–¹å·®çš„è¿‡ç¨‹
    > - åšNormçš„ç²’åº¦ä¸åŒï¼Œåº”ç”¨åœºæ™¯ä¸åŒ
    >
    > - å…¶ä»–èµ„æ–™ï¼š[Batch Normçš„æŠ€æœ¯åšå®¢](https://blog.csdn.net/LoseInVain/article/details/86476010)

- ç‰¹å¾å’Œä¸åŒ

    >- ç²’åº¦ä¸åŒï¼ˆç»´åº¦ä¸åŒï¼‰ï¼Œå¯¹åº”åº”ç”¨é¢†åŸŸä¸åŒ
    >
    >**Batch Normæ˜¯é€channelï¼ˆæ¯ä¸ªbatchçš„åŒä¸€ä¸ªchannelï¼‰è¿›è¡Œæ ‡å‡†åŒ–**ï¼Œä¹Ÿå°±æ˜¯å®batchçš„ã€‚å›¾ç‰‡æ°å¥½éœ€è¦è¿™ç§æ–¹å¼ã€‚
    >
    >LNæ˜¯é€batchè¿›è¡Œæ ‡å‡†åŒ–çš„ã€‚NLPä¸­å¾€å¾€æ˜¯ä¸€ä¸ªä¸€ä¸ªçš„seqè¿›è¡Œè®­ç»ƒçš„ï¼Œè€Œä¸”é•¿åº¦ä¸åŒï¼Œæ›´é€‚åˆè¿™ç§ã€‚**è¿™è®©æˆ‘æƒ³èµ·äº†Attentionçš„soft maxæ“ä½œæ˜¯å¯¹ä¸€ä¸ªè¡Œå‘é‡è¿›è¡Œå½’ä¸€åŒ–çš„**
    >
    >LayerNormæœ‰åŠ©äºç¨³å®šè®­ç»ƒè¿‡ç¨‹å¹¶æé«˜æ”¶æ•›æ€§ã€‚å®ƒçš„å·¥ä½œåŸç†æ˜¯å¯¹è¾“å…¥çš„å„ä¸ªç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–ï¼Œç¡®ä¿æ¿€æ´»çš„å‡å€¼å’Œæ–¹å·®ä¸€è‡´ã€‚**æ™®éè®¤ä¸ºè¿™ç§å½’ä¸€åŒ–æœ‰åŠ©äºç¼“è§£ä¸å†…éƒ¨åå˜é‡åç§»ç›¸å…³çš„é—®é¢˜ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å­¦ä¹ å¹¶é™ä½å¯¹åˆå§‹æƒé‡çš„æ•æ„Ÿæ€§ã€‚**ä»æ¶æ„å›¾ä¸Šçœ‹ï¼ŒLayerNormåœ¨æ¯ä¸ªTransformer å—ä¸­åº”ç”¨ä¸¤æ¬¡ï¼Œä¸€æ¬¡åœ¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¹‹åï¼Œä¸€æ¬¡åœ¨FFNå±‚ä¹‹åï¼Œä½†æ˜¯åœ¨å®é™…å·¥ä½œä¸­ä¸ä¸€å®šå¦‚æ­¤ã€‚
    >
    >æ–‡æœ¬é•¿åº¦ä¸ç¡®å®šï¼Œè€Œåœ¨LNå±‚å¯ä»¥ã€‚
    >
    >åº”ç”¨åœºæ™¯ç¡®å®šLN

- BNæœŸæœ›å’Œæ–¹å·®è®¡ç®—ç­–ç•¥

    > è®­ç»ƒæ—¶ï¼šä¸€ä¸ªbatchå†…è®¡ç®—
    >
    > æ¨ç†æ—¶ï¼š`é‡‡ç”¨ç§»åŠ¨æŒ‡æ•°å¹³å‡`ï¼Œä¸ä¾èµ–äºbatchã€‚ä¼šæœ‰å†å²ä¿¡æ¯åœ¨ï¼Œæœ‰ç‚¹ç±»ä¼¼RNNäº†
    >
    > - $E_n = \alpha E + (1- \alpha)E_{n-1}$
    >
    > - Var åŒç†
    >
    > | é˜¶æ®µ | ä½¿ç”¨çš„ç»Ÿè®¡é‡                | æ˜¯å¦æ›´æ–° running_mean/var | æ˜¯å¦ä¾èµ– batch æ•°æ® |
    > | :--- | :-------------------------- | :------------------------ | :------------------ |
    > | è®­ç»ƒ | å½“å‰ batch çš„ Î¼, ÏƒÂ²         | âœ… æ›´æ–°                    | âœ… ä¾èµ–              |
    > | æ¨ç† | è®­ç»ƒç´¯ç§¯çš„ running_mean/var | âŒ ä¸æ›´æ–°                  | âŒ ä¸ä¾èµ–            |

- åœ¨è®­ç»ƒå’Œæ¨ç†æ—¶æœ‰ä½•ä¸åŒï¼Ÿ

    > pytorchçš„æ¨¡å‹æœ‰ä¸¤ç§æ¨¡å¼ï¼Œåœ¨moduleæ¨¡å—é‡Œé¢æœ‰ä¸ª`training`å±æ€§ï¼Œä¹Ÿæœ‰å¯¹åº”çš„APIï¼Œé‡Œé¢æ˜ç¡®æŒ‡å‡ºäº†è¿™ä¸ª
    >
    > åœ¨BatchNormé‡‡ç”¨è®­ç»ƒæ—¶è®¡ç®—çš„ç»“æœï¼ˆEå’ŒVarï¼‰ï¼Œåº”ç”¨åˆ°æµ‹è¯•æˆ–è€…æ¨ç†çš„æ—¶å€™
    >
    > åœ¨Dropoutåç»­ä¼šè¯´ï¼Œè®­ç»ƒä¼šdropæ‰ï¼Œä½†æ¨ç†ä¸ä¼šï¼Œä¼šæ”¹æˆï¼ˆ1-rateï¼‰

    ```python
    def train(self: T, mode: bool = True) -> T:
        r"""Set the module in training mode.
    
        This has any effect only on certain modules. See documentations of
        particular modules for details of their behaviors in training/evaluation
        mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,
        etc.
    
        Args:
            mode (bool): whether to set training mode (``True``) or evaluation
                         mode (``False``). Default: ``True``.
    ```


- **å®˜æ–¹æ–‡æ¡£è¯´æ˜ï¼š**

[BachNorm1d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d)

- **RMS Norm**(å¤§æ¨¡å‹ä½¿ç”¨)ğŸŒŸğŸŒŸğŸŒŸ

    > æ¥æºäºLNï¼Œç®€åŒ–äº†LN
    >
    > å‡å‡å€¼ç›¸å½“äºå¹³ç§»ï¼Œè¿™é‡Œç›´æ¥å»æ‰å¹³ç§»ï¼Œåªä¿ç•™ç¼©æ”¾
    >
    > æŠŠä¹˜æ³•ç›´æ¥æ”¾è¿›æ¥äº†ï¼Œ

å¯¹LNåšç®€åŒ–ï¼Œå¯¹äºNLPï¼Œå¯¹ç¼©æ”¾æ•æ„Ÿï¼Œå¯¹å¹³ç§»ä¸æ•æ„Ÿï¼Œæ‰€ä»¥åˆ†å­ä¸å‡$E_x$ï¼Œå‡å°‘äº†å¾ˆå¤§è®¡ç®—é‡

![image-20250326224954810](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250326224954810.png)



**è¡¥å……éƒ¨åˆ†ï¼š**

- **DyT**(Transformers without normlization)

    [Transformers without normlization](https://yiyibooks.cn/arxiv/2503.10622v1/index.html)
    $$
    Norm:\ \gamma * \frac{(_x - E_{_x})}{\sqrt (Var_{_x})} + \beta
    $$

    $$
    DyT:\ \gamma * Tanh(\alpha * x) + \beta
    $$

    ![image-20250410151122973](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250410151122973.png)

- **Pre-Normï¼ˆå¤§æ¨¡å‹ä½¿ç”¨ï¼‰å’ŒPost-Norm**

    > ç»“åˆTransformeré‚£å—çŸ¥è¯†ï¼Œä¸€ä¸ªæ˜¯åœ¨æ®‹å·®å‰ï¼Œä¸€ä¸ªåœ¨æ®‹å·®å

    $$
    Output_{post}=LayerNorm(x+SubLayer(x))
    \\
    Output_{pre}=x+SubLayer(LayerNorm(x))
    $$

    

$$
å…¬å¼1:PostNorm(x)=x+LayerNorm(FeedForward(x+LayerNorm(Attention(x))))
\\\\
å…¬å¼2:PreNorm(x)=x+FeedForward(LayerNorm(x))+Attention(LayerNorm(x))
$$



| ç‰¹æ€§     |                Post-Norm                 |             Pre-Norm             |
| -------- | :--------------------------------------: | :------------------------------: |
| å…¬å¼     |                  å…¬å¼1                   |              å…¬å¼2               |
| ä½ç½®     |                  æ®‹å·®å                  |              æ®‹å·®å‰              |
| å‡ºç°æ—¶é—´ | åŸå§‹ Transformerï¼ˆVaswani et al., 2017ï¼‰ |     ä¹‹åå‘å±•ï¼ˆå¦‚ GPT-2 ç­‰ï¼‰      |
| ä¼˜ç‚¹     |        æ”¶æ•›åè¡¨ç°ç•¥å¥½ï¼ˆæŸäº›ä»»åŠ¡ï¼‰        | æ›´ç¨³å®šï¼Œè®­ç»ƒæ·±å±‚æ¨¡å‹ä¸æ˜“æ¢¯åº¦æ¶ˆå¤± |
| ç¼ºç‚¹     |       æ·±å±‚æ¨¡å‹ä¸­å®¹æ˜“æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸        |  å¯èƒ½æœ€ç»ˆæ€§èƒ½ç•¥ä½ï¼Œä½†æ›´å®¹æ˜“è®­ç»ƒ  |
| åº”ç”¨æƒ…å†µ |          BERTã€åˆç‰ˆTransformer           |    GPTç³»åˆ—ã€T5ã€LLamaç­‰å¤§æ¨¡å‹    |



|         æ¨¡å‹         | å½’ä¸€åŒ–ç±»å‹ |
| :------------------: | :--------: |
|     **DeepSeek**     |  Pre-Norm  |
|    **GPT-2/3/4**     |  Pre-Norm  |
|       **BERT**       | Post-Norm  |
|        **T5**        |  Pre-Norm  |
|      **LLaMA**       |  Pre-Norm  |
|  **Transformer XL**  |  Pre-Norm  |
| **åŸå§‹ Transformer** | Post-Norm  |

- DeepNorm

    DeepNorm æ˜¯å¾®è½¯åœ¨ 2022 å¹´æå‡ºçš„æ”¹è¿›æ–¹æ³•ï¼ˆè®ºæ–‡ *"[DeepNet: Scaling Transformers to 1,000 Layers](https://arxiv.org/abs/2203.00555)"*ï¼‰ï¼Œ**åŸºäº Post-Norm ä½†å¤§å¹…æå‡äº†æ·±å±‚è®­ç»ƒçš„ç¨³å®šæ€§**ï¼Œå¯æ”¯æŒè¶…æ·±å±‚ï¼ˆå¦‚ 1000 å±‚ï¼‰Transformer çš„è®­ç»ƒã€‚

    ![image-20250409164334392](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250409164334392.png)

    ![image-20250409162034019](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250409162034019.png)

    åŸå§‹æ®‹å·®ç»“æ„:
    $$
    x_{l+1} = LayerNorm(x_l + F(x_l))
    $$
    DeepNorm:
    $$
    x_{l+1} = \text{LN}(\alpha \cdot x_l + G_l(x_l, \theta_l))
    $$
    ![image-20250409164837814](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250409164837814.png)

**æ€è€ƒï¼š**DeepNormä¸­çš„$\beta$æ˜¯å“ªé‡Œçš„å‚æ•°ï¼Ÿ



> Noteï¼šæ•´ä½“å›é¡¾ä¸€ä¸‹ç®—å­éƒ¨åˆ†ï¼Œå¼•å…¥åé¢çš„ç®—å­ï¼Œä¸ç„¶å¤§è„‘è¿˜åœç•™åœ¨norméƒ¨åˆ†



### 4. Polling(CV)

- ä½œç”¨

> å¢å¤§æ„Ÿå—é‡
>
> å‡å°‘ç‰¹å¾å›¾å°ºå¯¸ï¼Œä¿ç•™é‡è¦ä¿¡æ¯ï¼Œé™ä½è®¡ç®—é‡
>
> é™é‡‡æ ·ï¼Œå‡å°‘å™ªéŸ³
>
> ä½ç½®å˜åŒ–é²æ£’æ€§å¢å¼º

- å…¶ä»–

> å¯¹åº”æœ‰ä¸ªout positionçŸ©é˜µ

- ç±»å‹

> Maxã€Avgç­‰

### 5. activationsï¼ˆçœ‹01éƒ¨åˆ†ï¼‰

> è¿™é‡Œé¢å†…å®¹è¾ƒå¤šï¼Œç›´æ¥çœ‹01ç« èŠ‚

[01. æ¿€æ´»å‡½æ•°ğŸŒŸğŸŒŸğŸŒŸ](#01. æ¿€æ´»å‡½æ•°ğŸŒŸğŸŒŸğŸŒŸ)

### 6. å…¶ä»–

### 7. ç‰¹åˆ«çš„ä¸€äº›OperatorğŸŒŸ

$$
reshapeã€viewã€permuteã€transpose
$$

> å¯èƒ½éœ€è¦è¡¥å……ä¸€ä¸‹PyTorchçš„TensorçŸ¥è¯†
>
> æ¯”å¦‚ï¼šmatedataå’Œstorageï¼›dataã€storageã€data_ptrã€strideã€contiguousï¼›stateã€state_dictç­‰

- reshape

    > è¿”å›å…·æœ‰ä¸è¾“å…¥ç›¸åŒæ•°æ®å’Œå…ƒç´ æ•°é‡çš„å¼ é‡ï¼Œä½†å…·æœ‰æŒ‡å®šçš„å½¢çŠ¶ã€‚
    >
    > åŸå§‹æ•°æ®å†…å­˜æ’å¸ƒä¸å˜ï¼Œåªå˜shape

- view

    > The returned tensor shares the same data and must have the same number
    >
    > ç±»ä¼¼reshapeï¼ŒåŸå§‹æ•°æ®ä¸å˜
    >
    > **è¦æ±‚å¼ é‡æ˜¯**è¿ç»­çš„**ï¼ˆcontiguousï¼‰ï¼Œå¦åˆ™ä¼šæŠ¥é”™ï¼ˆå¯å…ˆè°ƒç”¨ `.contiguous()` è§£å†³ï¼‰ã€‚**

- permute

    > è¿”å›åŸå§‹å¼ é‡è¾“å…¥çš„è§†å›¾ï¼Œå…¶ç»´åº¦è¿›è¡Œäº†æ’åˆ—ã€‚
    >
    > permuteä¼šå¯¹æ•°æ®åº•å±‚é‡æ’ï¼Œæ”¯æŒå¤šä¸ªè½´è¿›è¡Œäº¤æ¢

- transpose

    > è¿”å›ä¸€ä¸ªå¼ é‡ï¼Œå®ƒæ˜¯è¾“å…¥çš„è½¬ç½®ç‰ˆæœ¬ã€‚
    >
    > ç±»ä¼¼permuteï¼Œä¼šå¯¹æ•°æ®é‡æ’ï¼Œæ”¯æŒä¸¤è½´äº¤æ¢

**Tips:**

> 1. view()ï¼šå½“tensorè¿ç»­æ—¶tensor.view()ä¸æ”¹å˜å­˜å‚¨åŒºçš„çœŸå®æ•°æ®ï¼Œåªæ”¹å˜å…ƒæ•°æ®ï¼ˆMetadataï¼‰ä¸­çš„ä¿¡æ¯, è°ƒç”¨viewæ–¹æ³•å¼ é‡å¿…é¡»è¿ç»­çš„ã€‚ 
>
> 2.  reshape()ï¼šå½“tensorè¿ç»­æ—¶å’Œview()ç›¸åŒï¼Œä¸è¿ç»­æ—¶ç­‰ä»·äºcontiguous().view() 
>
> 3. permute()ï¼šé€šè¿‡æ”¹å˜å¼ é‡çš„æ­¥é•¿ï¼ˆstrideï¼‰é‡æ–°æ’åˆ—å¼ é‡çš„ç»´åº¦ï¼Œä½†ä¼šå¯¼è‡´å¼ é‡åœ¨å†…å­˜ä¸­çš„å­˜å‚¨å˜å¾—ä¸è¿ç»­ 
>
> 4. contiguous()ï¼šå¼€è¾Ÿæ–°çš„å­˜å‚¨åŒºï¼Œç¡®ä¿å¼ é‡åœ¨å†…å­˜ä¸­æ˜¯è¿ç»­å­˜å‚¨ï¼Œåœ¨permute()æ“ä½œåéœ€è¦æ¥contiguous()æ‰èƒ½æ¥view()
>
> 5. stride()ï¼šåœ¨æŒ‡å®šç»´åº¦ï¼ˆdimï¼‰ä¸Šï¼Œå­˜å‚¨åŒºä¸­çš„æ•°æ®å…ƒç´ ï¼Œä»ä¸€ä¸ªå…ƒç´ è·³åˆ°ä¸‹ä¸€ä¸ªå…ƒç´ æ‰€å¿…é¡»çš„æ­¥é•¿ 
>
> 6. pytorchçš„å­˜å‚¨æ–¹å¼ï¼šmetadata+storage
>
>     metadataä¿å­˜ï¼šsize,dimension,strideç­‰å…ƒä¿¡æ¯
>
>     storageä¿å­˜ï¼šä»¥ä¸€ç»´æ•°ç»„ä¿å­˜å¯¹åº”çš„å¼ é‡æ•°æ®
>
>     <img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250409174658632.png" alt="image-20250409174658632" style="zoom:50%;" />



| æ“ä½œ        | ç”¨é€”                       | æ˜¯å¦å…±äº«å†…å­˜ | è¿ç»­æ€§è¦æ±‚     | çµæ´»æ€§         |
| :---------- | :------------------------- | :----------- | :------------- | :------------- |
| `view`      | æ”¹å˜å½¢çŠ¶                   | æ˜¯           | å¿…é¡»è¿ç»­       | éœ€å…ƒç´ æ•°é‡ä¸€è‡´ |
| `reshape`   | æ”¹å˜å½¢çŠ¶ï¼ˆè‡ªåŠ¨å¤„ç†è¿ç»­æ€§ï¼‰ | å¯èƒ½å¦       | æ—              | åŒ `view`      |
| `transpose` | äº¤æ¢ä¸¤ä¸ªç»´åº¦               | æ˜¯           | å¯èƒ½ç ´åè¿ç»­æ€§ | ä»…äº¤æ¢ä¸¤ä¸ªç»´åº¦ |
| `permute`   | é‡æ–°æ’åˆ—æ‰€æœ‰ç»´åº¦           | æ˜¯           | å¯èƒ½ç ´åè¿ç»­æ€§ | å¯ä»»æ„è°ƒæ•´é¡ºåº |



$$
squeezeã€unsqueeze
$$

- æ˜¯å¯¹ç»´åº¦çš„å‹ç¼©å’Œæ‰©å……

    > å¢åŠ ä¸€ä¸ªç»´åº¦ï¼Œå‡å°‘ä¸€ä¸ªç»´åº¦

$$
concatã€stackã€expandã€flatten
$$

- concat

    > ä¸splitç›¸åæ“ä½œï¼Œå¯ä»¥æŒ‡å®šæŸè½´

- stack

    > æ”¯æŒæ–°å¢ä¸€ä¸ªè½´è¿›è¡Œæ‹¼æ¥

- expand

    > æ”¯æŒå¹¿æ’­æœºåˆ¶

- flatten

    > æ‹‰å¹³æˆä¸€ç»´


$$
pointwizeç±»å‹
$$

â€¦â€¦


$$
splitã€slice
$$




$$
reduceç±»å‹
$$

### 8. Embeddingï¼ˆå¯èƒ½éœ€è¦å•ç‹¬ä¸€èŠ‚ï¼‰

> ç»“åˆTokenizer

- å¯¹åˆ†è¯å™¨åˆ†åˆ°çš„ç»“æœè¿›è¡ŒEmbedding

- æœ‰ä¸€ä¸ªEmbeddingè¡¨ï¼Œç›´æ¥æ ¹æ®indexæŸ¥åˆ°
- è®¡ç®—åŸç†ï¼šä¼ å…¥weightå¤§å°ï¼ŒæŠŠEmbedding tableé‡Œé¢çš„æå–

### 9. DropoutğŸŒŸğŸŒŸğŸŒŸ

- åŠŸèƒ½

- åŸç†

    > ä¸¢å¼ƒ => ç½®é›¶
    >
    > éšæœºæ€§ï¼Œä¸ç„¶ä¼šç¥ç»å…ƒåæ­»

- è®­ç»ƒå’Œæ¨ç†æœ‰å•¥ä¸åŒ

    > æ¨ç†ç›´æ¥å»æ‰ï¼ŒæŠŠè¿æ¥weightä¹˜ä»¥ï¼ˆ1-pï¼‰

### 10 . æˆ‘çš„æ€è€ƒï¼š

- ä¸åŒçš„Normçš„å‚æ•°é‡

    > å¯å­¦ä¹ å‚æ•°å’Œå‡å€¼æ–¹å·®

- ä¸åŒNormæ“ä½œç»´åº¦ï¼ŒConvæ“ä½œç»´åº¦ï¼ŒPollingæ“ä½œç»´åº¦

    > | æ–¹æ³•       | å¯è®­ç»ƒå‚æ•°é‡ | å…³é”®åŒºåˆ«                      |
    > | :--------- | :----------- | :---------------------------- |
    > | Batch Norm | 2C2*C*       | å¯¹é€šé“å½’ä¸€åŒ–ï¼Œä¾èµ– Batch ç»Ÿè®¡ |
    > | Layer Norm | 2D2*D*       | å¯¹æ ·æœ¬å½’ä¸€åŒ–ï¼Œå« Î³,Î²*Î³*,*Î²*   |
    > | RMS Norm   | D*D*         | ä»…ä¿ç•™ Î³*Î³*ï¼Œæ—  Î²*Î²*          |

- å…¶ä»–å“ªäº›ç®—å­çš„åº•å±‚æ˜¯copyè¿˜æ˜¯in-place

- å¦‚ä½•å®ç°è®­ç»ƒå’Œæ¨ç†ä¸åŒçš„æƒ…å†µï¼Ÿï¼ˆç›¸å½“äºåŠ é”æˆ–è€…if elseï¼‰

---





## 03. BPç¥ç»ç½‘ç»œ&BaseLine

### 1. ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ 

- åº”ç”¨åœºæ™¯ï¼ˆå¤šçœ‹çœ‹æ‹›è˜ç½‘ç«™å’Œå…¬å¸å®˜ç½‘ï¼‰
- å›é¡¾æœºå™¨å­¦ä¹ ï¼ˆæ„é€ ç‰¹å¾å±æ€§åˆ°ç›®æ ‡æ ‡ç­¾çš„æ˜ å°„å…³ç³»ï¼‰
- ç”±äººçš„ç»éªŒå»åˆ¤æ–­çš„ï¼Œå¤§éƒ¨åˆ†éƒ½å¯ä»¥ç”¨æ¨¡å‹å®ç°

### 2. BPç¥ç»ç½‘ç»œ

```
```



### 3. BaseLine



**æ€è€ƒï¼š**

- çŸ©é˜µå®ç°ï¼ŸçŸ©é˜µä¹˜æ³•æ±‚æ¢¯åº¦ï¼Ÿ

---





## 04. Loss Function

> - å…¥æ‰‹æŸå¤±å‡½æ•°ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å›æ¸©ä¸€ä¸‹æ•´ä¸ªæ¨¡å‹è®­ç»ƒæµç¨‹ï¼Œå¤§æ–¹å‘è¦æœ‰ï¼ˆæˆ–è€…è¯´ä¸ºå•¥è¦æŸå¤±å‡½æ•°ã€ä½œç”¨æ˜¯å•¥ï¼‰
>
> - è¡¥å……ä¸€ä¸‹æ•°å­¦ç›¸å…³å†…å®¹ï¼ˆåœ¨å¤§æ–¹å‘ä¸Šè®²ï¼Œç„¶åå…¥ç»†èŠ‚ï¼‰

### 1. ç±»å‹

[PyTorchå®˜ç½‘](https://pytorch.org/docs/stable/nn.html#loss-functions)

- ç»å¯¹è¯¯å·®

$$
\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
        l_n = \left| x_n - y_n \right|
$$

- å¹³æ–¹è¯¯å·®

$$
\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
        l_n = \left( x_n - y_n \right)^2
$$



- ç»“åˆMAEå’ŒMSE

$$
\ell(x, y) = L = \{l_1, ..., l_N\}^T
$$

$$
    l_n = \begin{cases}
    0.5 (x_n - y_n)^2, & \text{if } |x_n - y_n| < delta \\
    delta * (|x_n - y_n| - 0.5 * delta), & \text{otherwise }
    \end{cases}
$$



- äº¤å‰ç†µ

$$
\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
          l_n = - w_{y_n} \log \frac{\exp(x_{n,y_n})}{\sum_{c=1}^C \exp(x_{n,c})}
          \cdot \mathbb{1}\{y_n \not= \text{ignore\_index}\}
$$

- è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆä¸åŒåˆ†å¸ƒï¼‰

    > é€»è¾‘å›å½’ï¼ˆæˆ–è€…softmaxå›å½’ï¼‰çš„æŸå¤±å‡½æ•°

- KLæ•£åº¦
    $$
    L(y_{\text{pred}},\ y_{\text{true}})
                = y_{\text{true}} \cdot \log \frac{y_{\text{true}}}{y_{\text{pred}}}
                = y_{\text{true}} \cdot (\log y_{\text{true}} - \log y_{\text{pred}})
    $$
    
- ç­‰ç­‰ç­‰

### 2. æ­£åˆ™åŒ–

> æ•°å­¦åŸºç¡€ï¼šæ‹‰æ ¼æœ—æ—¥å‡½æ•°ã€KK Tæ¡ä»¶ç­‰

- L1

    > lasso å›å½’

- L2

    > ridge å›å½’

### 3. åº”ç”¨åœºæ™¯

**å›å½’ä»»åŠ¡ï¼š**

| æŸå¤±å‡½æ•°               | å…¬å¼                           | åº”ç”¨åœºæ™¯                                                     |
| :--------------------- | :----------------------------- | :----------------------------------------------------------- |
| **å‡æ–¹è¯¯å·® (MSE)**     | $\frac{1}{n}âˆ‘(y_iâˆ’\hat y_i)^2$ | é»˜è®¤é€‰æ‹©ï¼Œå¯¹å¼‚å¸¸å€¼æ•æ„Ÿï¼ˆå› ä¸ºå¹³æ–¹æ”¾å¤§è¯¯å·®ï¼‰ï¼Œè¦æ±‚æ•°æ®é«˜æ–¯åˆ†å¸ƒã€‚ |
| **å¹³å‡ç»å¯¹è¯¯å·® (MAE)** | $\frac{1}{n}âˆ‘âˆ¥y_iâˆ’ \hat y_iâˆ¥$  | ç¨€ç–å¼‚å¸¸å€¼åœºæ™¯ã€‚                                             |
| **Huber Loss**         | åˆ†æ®µ                           | å¹³è¡¡MSEå’ŒMAEï¼Œå¯¹å¼‚å¸¸å€¼é²æ£’ä¸”å…‰æ»‘ï¼Œéœ€æ‰‹åŠ¨è®¾å®šé˜ˆå€¼Î´ã€‚          |

**åˆ†ç±»ä»»åŠ¡ï¼š**

| æŸå¤±å‡½æ•°                       | å…¬å¼/ç‰¹ç‚¹                                                    | åº”ç”¨åœºæ™¯                                                  |
| ------------------------------ | ------------------------------------------------------------ | --------------------------------------------------------- |
| **äº¤å‰ç†µæŸå¤± (Cross-Entropy)** | $-\sum y_i \log(\hat{y}_i)$ï¼ˆäºŒåˆ†ç±»/å¤šåˆ†ç±»ï¼‰                 | åˆ†ç±»ä»»åŠ¡é»˜è®¤é€‰æ‹©ï¼Œå¯¹é”™è¯¯é¢„æµ‹æƒ©ç½šé«˜ï¼ˆæ¢¯åº¦éšè¯¯å·®å¢å¤§ï¼‰ã€‚    |
| **äºŒå…ƒäº¤å‰ç†µ (BCE)**           | $-\sum [y_i \log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i)]$     | äºŒåˆ†ç±»ä»»åŠ¡ï¼ˆå¦‚åƒåœ¾é‚®ä»¶æ£€æµ‹ï¼‰ã€‚                            |
| **Focal Loss**                 | $-\sum \alpha_t (1-\hat{y}_i)^\gamma \log(\hat{y}_i)$        | ç±»åˆ«ä¸å¹³è¡¡åœºæ™¯ï¼ˆå¦‚ç›®æ ‡æ£€æµ‹ï¼‰ï¼Œé€šè¿‡Î³é™ä½æ˜“åˆ†ç±»æ ·æœ¬çš„æƒé‡ã€‚ |
| **Hinge Loss**                 | $\sum \max(0, 1 - y_i \hat{y}_i)$                            | æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰ï¼Œæœ€å¤§åŒ–åˆ†ç±»é—´éš”ã€‚                       |
| **KLæ•£åº¦**                     | $ y_{\text{true}} \cdot \log \frac{y_{\text{true}}}{y_{\text{pred}}} = y_{\text{true}} \cdot (\log y_{\text{true}} - \log y_{\text{pred}})$ | æ¦‚ç‡åˆ†å¸ƒåŒ¹é…ï¼ˆå¦‚ç”Ÿæˆæ¨¡å‹ã€è¿ç§»å­¦ä¹ ï¼‰ã€‚                    |



## 05. Tokenization

> æ‰€å¤„çš„ä½ç½®ï¼ˆå“ªä¸€æ­¥éœ€è¦åˆ†è¯ï¼‰
>
> Tokenizer

### 1. ç±»å‹

- è¯ç²’åº¦
- å­—ç²’åº¦
- Subwordç²’åº¦
- Unigram Language Model
- WordPiece
- å­—èŠ‚å¯¹ç¼–ç ï¼ˆBPEï¼‰
- BBPEï¼ˆDeepSeekç­‰ï¼‰

### 2.é—®é¢˜

- BPEå¦‚ä½•ä¿è¯æ¨ç†æ—¶ä¸å‡ºç°OOVå‘¢ï¼Ÿ
- 

## 06. Optim

> æ‰€å¤„ä½ç½®ï¼Œä½œç”¨ã€‚
