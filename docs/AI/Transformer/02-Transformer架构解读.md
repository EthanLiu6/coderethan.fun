## 02-Transformer架构解读

> 整理来源于作者：https://www.cnblogs.com/rossiXYZ，已获得许可

### 2.1 整体结构

从网络结构来分析，Transformer 包括了四个主体模块。

> - 输入模块，对应下图的绿色圈。
> - 编码器（Encoder），对应下图的蓝色圈。
> - 解码器（Decoder），对应下图的红色圈。编码器和解码器都有自己的输入和输出，编码器的输出会作为解码器输入的一部分（位于解码器的中间的橙色圈）。
> - 输出模块，对应下图的紫色圈。

确切的说，蓝色圈是编码器层（Encoder layer），红色圈是解码器层（Decoder layer）。图中的 N×代表把若干具有相同结构的层堆叠起来，这种将同一结构重复多次的分层机制就是栈。为了避免混淆，我们后续把单个层称为编码器层或解码器层，把堆叠的结果称为编码器或解码器。在Transformer论文中，Transformer使用了6层堆叠来进行学习。

![img](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/1850883-20250209144200970-309684502.jpg)

### 2.2 Attention结构

在Transformer中有三种注意力结构：全局自注意力，掩码自注意力和交叉注意力，具体如下图所示。

![img](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/1850883-20250209144227590-945674250.jpg)

![img](https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209144236400-880372719.jpg)

论文解读三个Attention：

> Transformer 模型以三种不同的方式使用多头注意力机制：
>
> - 在“编码器 - 解码器注意力”层中，quary来自前一个解码器层，而记忆键和值来自编码器的输出。这使得解码器中的每个位置都能关注输入序列中的所有位置。这模仿了诸如[38, 2, 9]等序列到序列模型中典型的编码器 - 解码器注意力机制。
> - 编码器包含自注意力层。在自注意力层中，所有的键、值和查询都来自同一个地方，在这种情况下，来自编码器前一层的输出。编码器中的每个位置都可以关注到编码器前一层中的所有位置。
> - 同样，解码器中的自注意力层允许解码器中的每个位置关注到解码器中该位置之前的所有位置。我们需要防止解码器中的左向信息流以保持自回归属性。我们在缩放点积注意力内部通过掩码（设置为 -inf）来实现这一点，掩码掉 softmax 输入中对应于非法连接的所有值。

![img](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/1850883-20250209144246216-890349384.jpg)

##### 1. 全局自注意力层

全局自注意力层（Global self attention layer）位于编码器中，它负责处理整个输入序列。在全局自注意力机制中，序列中的每个元素都可以直接访问序列中的其它元素，从而与序列中的其他元素建立动态的关联，这样可以使模型更好地捕捉序列中的重要信息。自注意力的意思就是关注于序列内部关系的注意力机制，那么是如何实现让模型关注序列内部之间的关系呢？自注意力将query、key、value设置成相同的东西，都是输入的序列，就是让注意力机制在序列的本身中寻找关系，注意到不同部分之间的相关性。

对于全局自注意力来说，Q、K、V有如下可能：

- Q、K、V都是输入序列。
- Q、K、V都来自编码器中前一层的输出。编码器中的每个位置都可以关注编码器前一层输出的所有位置。

再细化来说，Q是序列中当前位置的词向量，K和V是序列中的所有位置的词向量。

##### 2. 掩码自注意力

掩码自注意力层或者说因果自注意力层（Causal attention layer）可以在解码阶段捕获当前词与已经解码的词之间的关联。它是对解码器的输入序列执行类似全局自注意力层的工作，但是又有不同之处。

Transformer是自回归模型，它逐个生成文本，然后将当前输出文本附加到之前输入上变成新的输入，后续的输出依赖于前面的输出词，具备因果关系。这种串行操作会极大影响训练模型的时间。**为了并行提速，人们引入了掩码**，这样在计算注意力时，通过掩码可以确保后面的词不会参与前面词的计算。

对于掩码自注意力来说，Q、K、V有如下可能：

- Q、K、V都是解码器的输入序列。
- Q、K、V都来自解码器中前一层的输出。解码器中的每个位置都可以关注解码器前一层的所有位置。

再细化来说，Q是序列中当前位置的词向量，K和V是序列中的所有位置的词向量。

##### 3. 交叉注意力层

交叉注意力层（Cross attention layer）其实就是传统的注意力机制。交叉注意力层位于解码器中，但是其连接了编码器和解码器，这样可以刻画输入序列和输出序列之间的全局依赖关系，完成输入和输出序列之间的对齐。因此它需要将目标序列作为Q，将上下文序列作为K和V。

对于交叉注意力来说，Q、K、V来自如下：

- Q来自前一个解码器层，是因果注意力层的输出向量。
- K和V来自编码器输出的注意力向量。

这使得解码器中的每个位置都能关注输入序列中的所有位置。另外，编码器并非只传递最后一步的隐状态，而是把所有时刻（对应每个位置）产生的所有隐状态都传给解码器，这就解决了中间语义编码上下文的长度是固定的问题。

![img](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/1850883-20250218212308574-176454824-20250318112643142.jpg)

### 2.3 执行流程

我们再来结合模型结构图来简述推理阶段的计算流程，具体如下图所示。

![img](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/1850883-20250209144319485-2071213191.jpg)

假设我们进行机器翻译工作，把中文”我吃了一个苹果“翻译成英文”I ate an apple“，在假设模型只有一层，执行步骤如下：

1. 处理输入。用户输入自然语言句子”我吃了一个苹果“；tokenizer先把序列转换成token序列；然后Input Embedding层对每个token进行embedding编码，再加入Positional Encoding（位置编码），最终形成带有位置信息的embedding编码矩阵。编码矩阵用 Xn∗dXn∗d 表示， n 是句子中单词个数，d 是表示向量的维度（论文中 d=512）。注：原论文图上的输入是token，本篇为了更好的说明，把输入设置为自然语言句子。
2. 编码器进行编码。编码矩阵首先进入MHA（Multi-Head Attention，多头注意力）模块，在这里每个token会依据一定权重把自己的信息和其它token的信息进行交换融合；融合结果会进入FFN（Feed Forward Network）模块做进一步处理，最终得到整个句子的数学表示，句子中每个字都会带上其它字的信息。整个句子的数学表示就是Encoder的输出。
3. 通过输入翻译开始符来启动解码器。
4. 解码器进行解码。解码器首先进入Masked Multi-Head Attention模块，在这里解码器的输入序列会进行内部信息交换；然后在Multi-Head Attention模块中，解码器把自己的输入序列和编码器的输出进行融合转换，最终输出一个概率分布，表示词表中每个单词作为下一个输出单词的概率；最终依据某种策略输出一个最可能的单词。这里会预测出第一个单词”I“。
5. 把预测出的第一个单词”I“和一起作为解码器的输入，进行再次解码。
6. 解码器预测出第二个单词”ate“。

针对本例，解码器的每一步输入和输出具体如下表所示。

![img](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/1850883-20250209144327875-1762767210.jpg)

## 3. 补充

**论文"Attention is not all you need"指出如果没有skip connection（residual connection-残差链接）和MLP，自注意力网络的输出会朝着一个rank-1的矩阵收缩。即，skip connection和MLP可以很好地阻止自注意力网络的这种”秩坍塌（秩坍塌）退化“。这揭示了skip connection，MLP对self-attention的不可或缺的作用**

