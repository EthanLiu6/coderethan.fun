# ä¸€. DL_Base_Notes

## 1. NormalizationğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸ

Batch Normï¼ŒLayer Normï¼ŒInstance Normï¼ŒGroup Norm
$$
y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta
$$
LayerNormæœ‰åŠ©äºç¨³å®šè®­ç»ƒè¿‡ç¨‹å¹¶æé«˜æ”¶æ•›æ€§ã€‚å®ƒçš„å·¥ä½œåŸç†æ˜¯å¯¹è¾“å…¥çš„å„ä¸ªç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–ï¼Œç¡®ä¿æ¿€æ´»çš„å‡å€¼å’Œæ–¹å·®ä¸€è‡´ã€‚æ™®éè®¤ä¸ºè¿™ç§å½’ä¸€åŒ–æœ‰åŠ©äºç¼“è§£ä¸å†…éƒ¨åå˜é‡åç§»ç›¸å…³çš„é—®é¢˜ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å­¦ä¹ å¹¶é™ä½å¯¹åˆå§‹æƒé‡çš„æ•æ„Ÿæ€§ã€‚ä»æ¶æ„å›¾ä¸Šçœ‹ï¼ŒLayerNormåœ¨æ¯ä¸ªTransformer å—ä¸­åº”ç”¨ä¸¤æ¬¡ï¼Œä¸€æ¬¡åœ¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¹‹åï¼Œä¸€æ¬¡åœ¨FFNå±‚ä¹‹åï¼Œä½†æ˜¯åœ¨å®é™…å·¥ä½œä¸­ä¸ä¸€å®šå¦‚æ­¤ã€‚

å…¶ä»–èµ„æ–™ï¼š[Batch Normçš„æŠ€æœ¯åšå®¢](https://blog.csdn.net/LoseInVain/article/details/86476010)

**æ€è€ƒï¼šåœ¨è®­ç»ƒå’Œæ¨ç†æ—¶æœ‰ä½•ä¸åŒï¼Ÿï¼Ÿï¼Ÿ**

### 1.2 ä¸ºå•¥ä¸ç”¨BNæ¥åšNLPï¼Ÿ

æ–‡æœ¬é•¿åº¦ä¸ç¡®å®šï¼Œè€Œåœ¨LNå±‚å¯ä»¥

## 2. ActivationğŸŒŸğŸŒŸğŸŒŸ

### 2.1 Non-linear Activationsçš„ä¸¤ç§ç±»å‹

ä¸€ç§æ˜¯é€å…ƒç´ æ“ä½œï¼ˆElement wise æˆ–è€…Point wiseï¼‰ï¼Œeg:ReLU,Sigmoid,Tanh,ç­‰ï¼Œå¦ä¸€ç§æ˜¯æ“ä½œå¯¹è±¡ï¼ˆå…ƒç´ ï¼‰ä¹‹é—´å…·æœ‰ç›¸å…³æ€§ï¼Œeg.Softmax

### 2.2

Â·Â·Â·Â·Â·Â·



## 3. Loss FunctionğŸŒŸ



## 4. OptimizerğŸŒŸğŸŒŸğŸŒŸğŸŒŸ

> åŠ¨é‡åé¢çš„Admwé‚£äº›æ®ä¼°è®¡å¿˜äº†



## 5. TransformerğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸ

æ·±å…¥ç†è§£è¯·é˜…è¯»Transformerç³»åˆ—æ–‡ç« [Transformer](/AI/Transformer/)

### 5.1 ä¸ºå•¥Attentionçš„æ—¶å€™è¦é™¤ä»¥$\sqrt{d_k}$ï¼Ÿ

$$
Attention(Q, K, V ) = softmax(\frac{QÂ·K^T}{\sqrt{d_k}})Â·V
$$

å½“ $d_k$ çš„å€¼æ¯”è¾ƒå°çš„æ—¶å€™ï¼Œä¸¤ç§ç‚¹ç§¯æœºåˆ¶(additive å’Œ Dot-Product)çš„æ€§èƒ½ç›¸å·®ç›¸è¿‘ï¼Œå½“ dk*d**k* æ¯”è¾ƒå¤§æ—¶ï¼Œadditive attention æ¯”ä¸å¸¦scale çš„ç‚¹ç§¯attentionæ€§èƒ½å¥½ã€‚ æˆ‘ä»¬æ€€ç–‘ï¼Œå¯¹äºå¾ˆå¤§çš„ dk*d**k* å€¼ï¼Œç‚¹ç§¯å¤§å¹…åº¦å¢é•¿ï¼Œå°†softmaxå‡½æ•°æ¨å‘å…·æœ‰æå°æ¢¯åº¦çš„åŒºåŸŸã€‚ ä¸ºäº†æŠµæ¶ˆè¿™ç§å½±å“ï¼Œæˆ‘ä»¬ç¼©å°ç‚¹ç§¯ 1dkâˆš*d**k*1 å€ã€‚

### 5.2 ä¸ºå•¥æ‹†å¤šå¤´ï¼Ÿä¸ºå•¥æ•ˆæœå¥½äº†ï¼Ÿ

- æå–åˆ°äº†æ›´å¤šçš„ä¿¡æ¯ï¼ˆç±»ä¼¼CNNçš„multi- kernelï¼‰ï¼Œæ•°æ®åˆ†å¸ƒç»„ä¸ç»„ï¼ˆå­ç©ºé—´ï¼‰ä¹‹é—´ç‹¬ç«‹

  > Multi-head attentionå…è®¸æ¨¡å‹çš„ä¸åŒè¡¨ç¤ºå­ç©ºé—´è”åˆ**å…³æ³¨ä¸åŒä½ç½®**çš„ä¿¡æ¯ã€‚ å¦‚æœåªæœ‰ä¸€ä¸ªattention headï¼Œå®ƒçš„å¹³å‡å€¼ä¼šå‰Šå¼±è¿™ä¸ªä¿¡æ¯ã€‚
- å‡å°‘è®¡ç®—é‡ï¼ˆåº”è¯¥å¯ä»¥åœ¨è¿™ä¸€å±‚å‡å°‘åŸæ¥çš„$\frac{1}{Num_{head}}$å€ï¼‰

### 5.3 Cross Multi-Head Attentionï¼Ÿ

é¦–å…ˆï¼ŒSelf- Attentionä¸ä¼ ç»Ÿçš„Attentionæœºåˆ¶éå¸¸çš„ä¸åŒï¼šä¼ ç»Ÿçš„Attentionæ˜¯åŸºäºsourceç«¯å’Œtargetç«¯çš„éšå˜é‡ï¼ˆhidden stateï¼‰è®¡ç®—Attentionçš„ï¼Œå¾—åˆ°çš„ç»“æœæ˜¯æºç«¯ï¼ˆsourceç«¯ï¼‰çš„æ¯ä¸ªè¯ä¸ç›®æ ‡ç«¯ï¼ˆtargetç«¯ï¼‰æ¯ä¸ªè¯ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚
    å…¶æ¬¡ï¼ŒSelf-Attentioné¦–å…ˆåˆ†åˆ«åœ¨sourceç«¯å’Œtargetç«¯è¿›è¡Œè‡ªèº«çš„attentionï¼Œä»…ä¸source inputæˆ–è€…target inputè‡ªèº«ç›¸å…³çš„Self -Attentionï¼Œä»¥æ•æ‰sourceç«¯æˆ–targetç«¯è‡ªèº«çš„è¯ä¸è¯ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼›ç„¶åå†æŠŠsourceç«¯çš„å¾—åˆ°çš„self -AttentionåŠ å…¥åˆ°targetç«¯å¾—åˆ°çš„Attentionä¸­ï¼Œç§°ä½œä¸º**Cross-Attention**ï¼Œä»¥æ•æ‰sourceç«¯å’Œtargetç«¯è¯ä¸è¯ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚

###  5.4 Mask Multi-Head Attention

â€‹    ä¸Encoderçš„Multi-Head Attentionè®¡ç®—åŸç†ä¸€æ ·ï¼Œåªæ˜¯å¤šåŠ äº†ä¸€ä¸ªmaskç ã€‚mask è¡¨ç¤ºæ©ç ï¼Œå®ƒå¯¹æŸäº›å€¼è¿›è¡Œæ©ç›–ï¼Œä½¿å…¶åœ¨å‚æ•°æ›´æ–°æ—¶ä¸äº§ç”Ÿæ•ˆæœã€‚Transformer æ¨¡å‹é‡Œé¢æ¶‰åŠä¸¤ç§ maskï¼Œåˆ†åˆ«æ˜¯ padding mask å’Œ sequence maskã€‚

### 5.5 Maskingå®ç°æœºç†

å…·ä½“çš„åšæ³•æ˜¯ï¼ŒæŠŠ**è¿™äº›ä½ç½®**çš„å€¼**åŠ ä¸Šä¸€ä¸ªéå¸¸å¤§çš„è´Ÿæ•°(è´Ÿæ— ç©·)**ï¼Œè¿™æ ·çš„è¯ï¼Œç»è¿‡ softmaxï¼Œè¿™äº›ä½ç½®çš„æ¦‚ç‡å°±ä¼šæ¥è¿‘0ï¼

### 5.6 MQAå’ŒGQA

MQAå¤šå¤´å…±ç”¨Kï¼ŒV

GQAå°†å¤´åˆ†ç»„ï¼Œç»„å†…å…±ç”¨KV

## 5.7 ä½ç½®ç¼–ç 

- attentionå¹¶ä¸ä¼šè·å–åˆ°ä½ç½®ä¿¡æ¯ï¼Œåªæœ‰ä¿©ä¿©çš„ç›¸å…³æ€§

### x.x å…¶ä»–

- å·¥ç¨‹ä¸­å°†QKVçš„æƒé‡çŸ©é˜µç›´æ¥æ”¾åœ¨ä¸€å—ï¼Œshapeå°±æ˜¯åŸæ¥$(embedding\_dim, embedding\_dim)$åˆ° $(embedding\_dim, embedding\_dim \times 3)$

  ![img](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/1850883-20250209144655019-1620461538.jpg)



- Attentionçš„æ—¶å€™æ˜¯å¦éœ€è¦å¯¹è‡ªèº«åšï¼Ÿè‡ªå›å½’çš„æ—¶å€™åº”å½“ä¸‹ä¸€æ¬¡tokenå°½å¯èƒ½ä¸æ˜¯ä¸Šä¸€ä¸ªè¯ï¼Œæ‰€ä»¥çŸ©é˜µå¯¹è§’çº¿æ˜¯å¦åº”å½“æ˜¯è¶‹äºé›¶çš„ï¼Ÿ

- å¥å­é—´çš„ç›¸ä¼¼åº¦è®¡ç®—æ–¹å¼æœ‰å“ªäº›ï¼ŸAttentionä¸ºå•¥é‡‡ç”¨ç‚¹ç§¯ï¼Ÿ

  > addivation
  >
  > dot-dart ï¼ˆä¸ºå•¥ç‚¹ç§¯è¿ç®—å¯ä»¥è®¡ç®—ç›¸ä¼¼åº¦ï¼‰
  >
  > ![image-20250325155249034](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250325155249034.png)

- FFNçš„æ—¶å€™ä¸ºå•¥dimè¦âœ–ï¸4æ”¾å¤§çº¬åº¦
- encoderå…¶å®æ˜¯ç‰¹å¾æå–çš„ä¸€ä¸ªè¿‡ç¨‹
- ä¸ºå•¥encoderçš„inputå’Œoutï¼ˆcontext vectorï¼‰çš„çº¬åº¦å¤§å°è¦ä¸€æ ·ï¼Ÿï¼ˆå› ä¸ºæœ‰åŸå§‹è®ºæ–‡æœ‰6ä¸ªencoder Layeræˆ–è€…cellï¼‰
- encoderå±‚çš„attentionä¼šæ³¨æ„åˆ°å‰é¢çš„è¯å—ï¼Ÿ
- encoderçš„ç‰¹å¾èšåˆæ˜¯åœ¨ä»€ä¹ˆæ—¶å€™
- ç‰¹å¾å‹ç¼©
- softmaxçš„æ—¶å€™æ˜¯å¯¹attentionç»“æœçš„å“ªä¸ªçº¬åº¦è¿›è¡Œå½’ä¸€åŒ–ï¼ˆè¯ä¸è¯ç›¸å…³æ€§è€ƒè™‘ï¼‰ï¼Œä¸ºå•¥å½’ä¸€åŒ–ï¼Ÿï¼ˆè¯ä¸è¯ä¹‹é—´çš„è”ç³»æ€§ï¼Œå› ä¸ºè¿˜è¦ç‚¹ä¹˜valueï¼Œæ‰€ä»¥åº”å½“æ˜¯ä¸€ä¸ªæƒé‡ï¼‰
- maskçš„å¤§å°ï¼Ÿï¼ˆbatch_size, seq_len, seq_lenï¼‰ï¼Œåº”è¯¥å’Œattention scoreä¸€æ ·ï¼Œå› ä¸ºè¦ç è°å°±è·Ÿè°ä¸€æ ·

## 6. ğŸŒŸğŸŒŸğŸŒŸK-V Cache



## 7. ğŸŒŸğŸŒŸğŸŒŸå¸¸è§çš„æ­£åˆ™åŒ–æ–¹æ³•

- Dropout
- 

## 8. BertğŸŒŸğŸŒŸğŸŒŸğŸŒŸ

è¾“å…¥åçš„å¯¹15%çš„ä¸‰ä¸ªå¤„ç†

bertæœ‰ä¸‰ä¸ªç¼–ç ï¼Œåˆ†åˆ«æ˜¯

# äºŒ. è¯¾å ‚è®°å½•

## 0301-0302 TransformerğŸŒŸğŸŒŸğŸŒŸ

- 0301:inputåºåˆ—é•¿åº¦å¤§äºembeddingæ—¶å€™çš„seq_lenæ—¶, inputçš„è¾“å…¥åºåˆ—ä¼šæŒ‰ç…§seq_lenè¿›è¡Œåˆ‡å‰²æ‹¼æ¥åˆ°batchä¸Šå—? (è€å¸ˆè®²äº†encoderæ—¶å€™inputä¸è¶³seq_lenæ—¶å€™ä½¿ç”¨maskç„¶åæƒ³é—®çš„å¦ä¸€ä¸ªé—®é¢˜)
- 0302:`K-V cache`æ—¶å€™å½“é¢„æµ‹ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥çš„æ—¶å€™ä¸ä¹‹å‰çš„åšAttentionçš„æ—¶å€™, ä¸­é€”ä¼šå–å‡ºcacheé‡Œçš„Kâ€”Vå—è¿˜æ˜¯åªå–å‡ºé‡Œé¢çš„Kè¿˜æ˜¯åªåœ¨æœ€åä¸€ä¸ªç»“æŸåæ‰æ•´ä½“å–ä¸€æ¬¡ (æˆ‘æƒ³é—®çš„ä¹Ÿå°±æ˜¯åœ¨ä¸€ä¸ªbatchæˆ–è€…ä¸€ä¸ªseqçš„è®¿å­˜æƒ…å†µ, æ¯ä¸€ä¸ªæ—¶é—´æ­¥éƒ½éœ€è¦è®¿é—®cacheä¸€æ¬¡å—)
- æ˜¯ç›´æ¥ä½¿ç”¨ç¼“å­˜çš„å¡«å……çŸ©é˜µè¿˜æ˜¯éœ€è¦æ‹¿å‡ºç¼“å­˜æ•°æ®(è¯»è¿˜æ˜¯å–)



## 0308-0309â€”â€”PyTorch


> æåŠ: æ··åˆç²¾åº¦è®­ç»ƒ

### 1.1 Tensor ä¸­æ•°æ®çš„è¿ç»­æ€§

reshape, transpose, view, T(è½¬ç½®), permute

transposeä¼šè®©raw dataä¸å˜(å…±ç”¨),  mata dataçš„strideå’Œshapeç­‰å±æ€§å°±å˜äº† is_contiguous()ä¸è¿ç»­, ä½†reshapeå’Œpermuteè¿™äº›æ˜¯ä¸ä¼šå˜çš„,å› ä¸ºä»–ä»¬ä¼šå‘ç”Ÿdata copy,  contiguous()ä¼šå‘ç”Ÿcopy raw dataæ•°æ®

viewå’Œreshapeçš„åŒºåˆ«

viewæ›´åŠ å®‰å…¨, ä¸ä¼šé‡æ–°æ‹·è´æ•°æ®, ä½†æ•°æ®ä¸è¿ç»­ä¸èƒ½ä½¿ç”¨view,ä¹Ÿå°±æ˜¯strideä¸åè°ƒ, reshapeä¸ä¼šé”™è¯¯, ä¼šé‡æ–°æ‹·è´æ•°æ®, æ•°æ®ä¹Ÿè¿ç»­

permuteå’Œtransposeä¼šè®©strideå±æ€§æ”¹å˜, ä»è€Œå‘ç”Ÿæ•°æ®ä¸è¿ç»­, é€šå¸¸ä½¿ç”¨åè¦åŠ ä¸€ä¸ªcontiguous()è®©æ•°æ®è¿ç»­

### 1.2 pytorch autograd

<img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250308163009045.png" alt="image-20250308163009045" style="zoom:36%;" />

â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦

å¶å­ç»“ç‚¹+requests_grad=Trueæ‰æœ‰æœ€ç»ˆçš„grad, éå¶å­ç»“ç‚¹ä¸­é€”å¯èƒ½ä¼šè®¡ç®—grad, ä½†ç”¨äº†å°±ä¼šä¸¢å¼ƒ(requests_grad=Trueçš„)



æ¢¯åº¦ç´¯åŠ ä¹Ÿæœ‰å¯èƒ½, å¤šä¸ªstepçš„æ¢¯åº¦ç´¯åŠ , éšå¼å¢åŠ batch

è‹¥æ²¡è¿›è¡Œxxx.grad.zero_()æˆ–è€…xxx.grad = None, åˆ™ä¼šè¿›è¡Œaccumulate()ç´¯åŠ grad, è¿™ä¸¤ç§æ–¹æ³•æœ‰ä¸€ç‚¹åŒºåˆ«, zero__()ä¼šç½®é›¶,ä¼šå ç”¨æ˜¾å­˜, ä½†=Noneçš„è¯ä¼šé‡Šæ”¾æ˜¾å­˜, ä¸¤è€…å„æœ‰å¥½å



### 1.3 inplace-op

![image-20250309102559857](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250309102559857.png)

å¶å­ç»“ç‚¹çš„Tensorå˜é‡ä¸èƒ½è¿›è¡Œin-placeæ“ä½œ, å› ä¸ºè¦æ›´æ–°æ¢¯åº¦çš„æ—¶å€™è¦ç”¨å¶å­ç»“ç‚¹

<img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250309113212792.png" alt="image-20250309113212792" style="zoom: 50%;" />

<img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250309114217344.png" alt="image-20250309114217344" style="zoom:50%;" />

no_grad()åº•å±‚æ˜¯åŸºäºset_grad_enable(Flase)çš„

### 1.4 è‡ªåŠ¨å¾®åˆ†æœºåˆ¶(auto grad) é‡ç‚¹ï¼š

- pytorchä¸­ æ­£å‘forward å¯¹æˆ‘ä»¬ç”¨æˆ·æ˜¯å¯è§çš„ï¼Œä½†æ˜¯backwardå¯¹æˆ‘ä»¬ç”¨æˆ·æ˜¯ä¸å¯è§çš„ï¼›
- ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œæ¯ä¸€ä¸ªæ­£å‘çš„å‡½æ•°ï¼Œéƒ½å¯¹åº”ä¸€ä¸ªåå‘çš„å‡½æ•°ï¼ˆgrad_fn--> Tensorä¸­ï¼‰ï¼›
- tensorï¼šrequires_grad = True
- tensor: grad --> tensor ä¸­å­˜å‚¨gradçš„åœ°æ–¹ï¼›
- tensor: grad_fn --> å­˜å‚¨æˆ‘ä»¬åå‘å‡½æ•°çš„åœ°æ–¹
- tesnor: is_leaf --> è¿™ä¸ªtensor æ˜¯ä¸æ˜¯ å¶å­èŠ‚ç‚¹ï¼›
- net::all weight --> éƒ½æ˜¯leaf
- å¶å­èŠ‚ç‚¹çš„æ¢¯åº¦ä¼šè‡ªåŠ¨ä¿å­˜ä¸‹æ¥çš„ï¼ˆweightï¼‰ï¼›
- ä¸­é—´çš„ activation çš„æ¢¯åº¦ä¼šè®¡ç®—ï¼Œä½†æ˜¯ä¸ä¿ç•™ï¼›
- pytorch åŠ¨æ€å›¾ vs tensorflow é™æ€å›¾ï¼›
- æˆ‘ä»¬ä¸èƒ½æ”¹å˜ä¸€ä¸ªéå¶å­èŠ‚ç‚¹çš„ requires_grad;
- éå¶å­ï¼ˆä¸€ä¸ªå‡½æ•°çš„outputï¼‰èŠ‚ç‚¹å®ƒçš„ requires_grad è‡ªåŠ¨æ¨å¯¼çš„ï¼›
- éå¶å­èŠ‚ç‚¹å¯¹åº”å‡½æ•°çš„inputs ä¸­åªè¦æœ‰ä¸€ä¸ª requires_grad = True, é‚£ä¹ˆè¿™ä¸ªéå¶å­èŠ‚ç‚¹çš„requires_grad = True;
- torch.no_grad() ä¼šä½¿å¾—é‡Œé¢çš„æ–°çš„tensor requires_grad = False
- inplaceçš„æ“ä½œï¼Œéå¸¸å¤§çš„é£é™©ï¼šè¦†ç›–äº†åŸæ¥çš„å€¼ï¼Œå¯¼è‡´åå‘ä¼ æ’­æ—¶è®¡ç®—ä¸å‡†ç¡®ï¼›
- æ ‡é‡çš„æ¢¯åº¦æ‰èƒ½è¢«éšå¼åˆ›å»ºï¼Œéšå¼åˆ›å»ºï¼ˆ.backward(1)ï¼‰ï¼›
- ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œ.backward(gradient)æ˜¯æœ‰è¾“å…¥çš„: ;



### 2.1 torch.nn.Module

trainæ¨¡å¼å’Œvealæ¨¡å¼ä¸ä¼šå¯¹gradçš„æƒ…å†µåšä¿®æ”¹,åªæ˜¯å¯¹è®­ç»ƒå’Œæ¨ç†çš„å¯¹åº”çš„ç®—å­åšä¸åŒçš„å¤„ç†(ç­‰ä»·å¤„ç†)

å¸¸ç”¨ç®—å­dropoutå’ŒBachNorm 

xxx.cuda()çš„æ—¶å€™æ¬è¿çš„æ˜¯_parametersåˆ°cuda, è¿˜æœ‰bufferä¹Ÿæ¬è¿åˆ°cuda, å¹¶æ²¡æœ‰å°†æ¨¡å‹ç»“æ„è¿›è¡Œæ¬è¿.

æŒ‰ç…§æ·±åº¦ä¼˜å…ˆéå†sub module,å°†é‡Œé¢çš„_parameterså’Œbufferåˆ°cuda, æ•°æ®ç±»å‹è½¬æ¢ä¹Ÿæ˜¯ä¸€æ ·çš„æ“ä½œ

c++åº•å±‚å®ç°äº†ä¸€ä¸ªdispatheråˆ†å‘æœºåˆ¶,æŒ‰ç…§deviceå±æ€§åˆ†å‘, å¯¹åº”deviceä¼šè°ƒç”¨å¯¹åº”çš„fnç®—å­, è®¡ç®—éƒ¨åˆ†æ‰æ‰§è¡Œ

_parameters()é€å‚æ•°ç»™ä¼˜åŒ–å™¨çš„æ—¶å€™å°†æ‰€æœ‰çš„parametersé€åˆ°optim, ä½†æ•°æ®å…±ç”¨, åŒæ—¶æ›´æ–°

é’©å­å‡½æ•°(æ²¡å¤ªæ‡‚)

---



## 0315-0316ï¼ˆç»­PyTorchï¼‰

### 1.1 å›é¡¾

1.Tensorç±»å’Œé‡è¦å±æ€§
2.autogradï¼ŒåŠ¨æ€å›¾
3.Moduleä»¥åŠå±æ€§å’Œæ–¹æ³•

> training,_parameters,_buffers,_modules(hooksæ˜¯ä¸»è¦ç”¨äºŒæ¬¡å¼€å‘ç­‰æƒ…å†µ)

å­æ¨¡å—å•¥æ—¶å€™å®šä¹‰çš„å‘¢ï¼Ÿ

_parameters,_bufferså“ªäº›æœ‰å“ªäº›æ²¡æœ‰

å°†moduleé‡Œçš„parametersä¼ ç»™optimï¼Œä¼šé€šè¿‡è°ƒç”¨parameters()è¿›è¡Œ

ä¸€ç³»åˆ—æ–¹æ³•å…·ä½“æƒ…å†µ

### 1.2 é—®é¢˜åˆé›†

1. åœ¨è®²transformerçš„padding maskçš„æ—¶å€™æƒ³åˆ°ï¼Œå¦‚æœè¾“å…¥seq_lenå¤§äºäº†å®šä¹‰çš„seq_lenï¼Œä¼šç›´æ¥æˆªæ–­è¿˜æ˜¯æˆªæ–­å†æ‹¼æ¥åˆ°ä¸‹ä¸€ä¸ªbatch
2. åœ¨sequence maskçš„æ—¶å€™ï¼Œå¿˜äº†è¦é—®å•¥äº†
3. åœ¨normalizationå±‚çš„æ—¶å€™ä¸æ˜¯æœ‰ä¸¤ä¸ªå­¦ä¹ çš„å‚æ•°å—ï¼Œè¿™ä¿©å‚æ•°æ˜¯ä¸€æ¬¡forwardè®­ç»ƒä¸€æ¬¡è¿˜æ˜¯å•ç‹¬æœ‰è‡ªå·±çš„è®­ç»ƒï¼Ÿè¿˜æœ‰ï¼Œè¿™ä¿©å‚æ•°æ˜¯å’‹æ›´æ–°çš„ï¼Ÿ
4. datasetä¼šè¿­ä»£çš„å°†æ‰€æœ‰æ•°æ®åŠ è½½åˆ°å†…å­˜å—ï¼Œç„¶ådataloaderå†ä¸€æ‰¹æ¬¡çš„æå–å—

with  torch.no_grad():
	evalæ—¶å€™ç”¨ï¼Œè®¡ç®—å›¾ä¸å†è¿›è¡Œï¼Œå¯¹require_grads=Trueçš„ä¸è¿›è¡Œæ¢¯åº¦è®¡ç®—ï¼Œæ˜¾å­˜å ç”¨é‡ä¼šå‡å°‘ï¼Œactivationçš„å°±ä¼šä¸¢å¼ƒ

datasetä¼šè¿­ä»£çš„å°†æ‰€æœ‰æ•°æ®åŠ è½½åˆ°å†…å­˜å—ï¼Œç„¶ådataloaderå†ä¸€æ‰¹æ¬¡ä¸€æ‰¹æ¬¡çš„æå–å—ï¼Ÿè¿˜æ˜¯è¯´dataloaderå‡†å¤‡æ‹¿ä¸€ä¸ªbatchï¼Œç„¶ådatasetæ ¹æ®batch_sizeè¿­ä»£è·å–sizeæ¡ã€‚

> æ˜¯åè€…ï¼Œä¹Ÿå°±æ˜¯I/Oçš„æ—¶å€™ï¼Œbatch_sizeå¤ªå°çš„è¯ä¼šå¢åŠ I/Oè´Ÿæ‹…

### 2.1 torch.optim

å‚æ•°ä¼ paramçš„æ—¶å€™çš„ä¼ é€’å’Œæ‰“åŒ…æ–¹å¼

self.param_groups

==self.state==ï¼šè®­ç»ƒæ—¶å€™æ˜¾å­˜æ¶ˆè€—çš„ä¸»è¦é¡¹ï¼ˆä¼˜åŒ–å™¨çš„åŠ¨é‡é¡¹æœ‰å…³ï¼‰
ä»–æ˜¯ä¸€ä¸ªdictï¼Œkeysæ˜¯tensorï¼Œvaluesä¹Ÿæ˜¯
æ¨¡å‹

>ç§»åŠ¨æŒ‡æ•°å¹³å‡æ˜¯å•¥å¿˜äº†

def load_state_dict

### 2.2 learning rate è°ƒæ•´æ–¹æ¡ˆ

Torch.optim.lr_scheduler

éœ‡è¡ç±»å‹çš„å­¦ä¹ ç‡è°ƒæ•´æ˜¯å‡å°‘è¿›å…¥å±€éƒ¨æœ€ä¼˜è§£çš„æƒ…å†µ

==çŠ¶æ€å­—å…¸==ï¼Œä¸‰ä¸ªåœ°æ–¹è§è¿‡ï¼Œéƒ½ç±»ä¼¼ï¼Œæ¨¡å‹ä¿å­˜æ—¶å€™éœ€è¦æœ‰

### 2.3 æ¨¡å‹ä¿å­˜å’ŒåŠ è½½

==åŠ¨æ€å›¾==

1.save state_dictçš„æ—¶å€™åªæœ‰å‚æ•°ï¼Œsave modelçš„æ—¶å€™æ— æ³•ç›´æ¥ä¿å­˜æ•´ä¸ªç½‘ç»œï¼Œä½†æ˜¯ä»–çš„ææ–™ï¼ˆinitï¼‰çš„é‚£äº›ä¼šä¿å­˜ï¼Œæ¨¡å‹åŠ è½½çš„æ—¶å€™èƒ½é€šè¿‡ï¼Œä½†runing timeæ—¶å€™ï¼Œforwardå¹¶æ²¡æœ‰ï¼Œå¿…é¡»å¯¼å…¥æˆ–è€…è‡ªå·±å®ç°ï¼Œéœ€è¦åŸæ¥Netçš„ç­¾åï¼ˆå…·ä½“å®šä¹‰å¯ä»¥ä¸ä¸€è‡´ï¼Œä¼šæ”¾å…¥_modulesï¼‰

2.å¦‚æœæ˜¯è‡ªå·±å†™çš„ç®—å­ï¼Œåœ¨initæ—¶å€™ä¹Ÿæ”¾å…¥_moduleså—ï¼Ÿ

![img](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/%7B6fbad3cc-1899-4404-b3b3-d91f7da5cb95%7D.png)

3.==onnx==æ¨¡å‹ä¿å­˜å¿…é¡»è¾“å…¥å¯¹åº”çš„inputï¼Œè‡ªå·±runä¸€éï¼Œæ˜¯ä¸€ä¸ªé™æ€å›¾

4.è®­ç»ƒä¸­çš„ä¿å­˜å’ŒåŠ è½½ï¼ˆcheck pointï¼‰==æ¨¡å‹ä¿å­˜çš„å‡ ç§å‚æ•°ç±»å‹==ï¼‰

![image-20250316113313785](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250316113313785.png)

### 3.1 Dataset and Dataloader

> åªå­¦ä¹ pytorchçš„ï¼Œåç»­è‡ªå·±è¡¥hfçš„é‚£äº›

### 4.1 NLP

GPTï¼šè‡ªç›‘ç£è®­ç»ƒå¾—åˆ°é¢„è®­ç»ƒæ¨¡å‹ï¼ˆé‡‡ç”¨è¿ç§»å­¦ä¹ ï¼‰

Bertï¼šå®Œå½¢å¡«ç©º

è¿ç§»å­¦ä¹ ï¼šé¢„è®­ç»ƒ+å¾®è°ƒï¼ˆå¾®è°ƒçš„æ•°æ®é›†å°±æ˜¯ä¸“ä¸šé¢†åŸŸçš„æ•°æ®é›†ï¼‰

### 4.2 Bert

1.ä¸¤ä¸ªä»»åŠ¡ï¼šMLMå’ŒNSP

2.Embeddingï¼Œè¯åµŒå…¥

è¯ï¼Œå¥å­ï¼ˆåˆ†æ®µï¼‰ï¼Œä½ç½® åµŒå…¥

![image-20250316165604714](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250316165604714.png)

> transformerçš„è¯åµŒå…¥å¼ç”¨ä¸‰è§’ä½ç½®åµŒå…¥

![image-20250316175009793](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250316175009793.png)



æœªè®²çŸ¥è¯†ï¼šåˆ†è¯å™¨tokenizer



## 0322

### 1.1 å›é¡¾

`bert4torch`çš„neré¡¹ç›®è®²è§£å’Œdebug

### 2.1 T5è®²è§£

### 2.2 position embeddingğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸ

ç»å¯¹ä½ç½®ç¼–ç 

- ä¸‰è§’å‡½æ•°å¼(Sinusoidal)
- å¯å­¦ä¹ (Learnable)

ç›¸å¯¹ä½ç½®ç¼–ç 

- **æ˜¯åœ¨Attentionçš„æ—¶å€™æ‰ä½ç½®ç¼–ç **
- åªå¯¹qå’Œkåšä½ç½®ç¼–ç ï¼Œå¯¹valueä¸åšï¼Œvalueæ˜¯ç»“æœæˆ–è€…è¯´æ˜¯tokenæœ¬èº«çš„ç‰¹å¾ä¿¡æ¯
- T5çš„åˆ†æ¡¶æ€æƒ³

==æ—‹è½¬ä½ç½®ç¼–ç ==ï¼ˆå¤§æ¨¡å‹ä½¿ç”¨çš„æ–¹æ³•ï¼‰

- æ ¹æ®æ•°å­¦åŸç†æ¨å¯¼

![image-20250322144241104](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250322144241104.png)

- æƒ³è¦å¾—åˆ°çš„æ•ˆæœ=>åæ¨

![image-20250322144600625](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250322144600625.png)

![image-20250322145503812](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250322145503812.png)

### 3.1 GPT

GPT-1 å·²ç»å‡ºç°zero-shotè¿¹è±¡ï¼Œå±‚å½’ä¸€åŒ–è¿˜æ˜¯ä¹‹å‰çš„post-norm

GPT-2 é›¶æ ·æœ¬å­¦ä¹ ï¼Œå³zero-shotï¼Œå±‚å½’ä¸€åŒ–æœ‰ç‚¹å˜åŒ–ï¼Œæ”¹æˆper-Norm

> ç›¸å½“äºçº¯é¢„è®­ç»ƒ

![image-20250322170432945](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250322170432945.png)

GPT-3 few-shotï¼ˆç»™æ¡ˆä¾‹ï¼‰ï¼Œå‘ç°æ¨¡å‹è§„æ¨¡å¯ä»¥æé«˜èƒ½åŠ›ï¼Œæœ€åå®ç°äº†æ— éœ€å¾®è°ƒåˆ°è¾¾ä¸€äº›è¾ƒå¥½çš„ä»»åŠ¡å¤„ç†ï¼Œæ¶æ„åŸºæœ¬å’ŒGPT-2ä¸€è‡´ï¼Œä½†åŠ äº†ä¸€ä¸ªæ–°çš„â€˜äº¤æ›¿çš„ç¨ å¯†å’Œç¨€ç–çš„â€™Attentionï¼Œä½™å¼¦è¡°å‡çš„å­¦ä¹ ç‡ç­–ç•¥ï¼Œbatch-sizeä»å°å˜å¤§ï¼Œå†åŠ ä¸Š0.1çš„æƒé‡è¡°å‡æ­£åˆ™åŒ–

> Few-shot, one-shot, zero-shot
>
> â€¢ **Few-Shotï¼ˆFSï¼‰ï¼š** æ¨¡å‹åœ¨æ¨ç†æ—¶ç»™å‡ºKä¸ªä»»åŠ¡ç¤ºä¾‹ä½œä¸ºä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒåŒæ—¶æä¾›ä»»åŠ¡çš„è‡ªç„¶è¯­è¨€æè¿°ï¼Œä½†ä¸å…è®¸æ¨¡å‹è¿›è¡Œæƒé‡æ›´æ–°ã€‚é€šå¸¸å°†Kè®¾ç½®åœ¨10åˆ°100çš„èŒƒå›´å†…ï¼Œä»¥é€‚åº”æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£ã€‚
>
> â€¢ **One-Shotï¼ˆ1Sï¼‰ï¼š** æ¨¡å‹åœ¨æ¨ç†æ—¶é€šè¿‡æä¾›ä¸€ä¸ªä»»åŠ¡ç¤ºä¾‹ä½œä¸ºä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒåŒæ—¶è¿˜æœ‰ä»»åŠ¡çš„è‡ªç„¶è¯­è¨€æè¿°ã€‚è¿™ç§æ–¹å¼æœ€æ¥è¿‘äºäººç±»åœ¨è§£å†³æŸäº›ä»»åŠ¡æ—¶æ‰€ä½¿ç”¨çš„æ–¹å¼ã€‚
>
> â€¢ **Zero-Shotï¼ˆ0Sï¼‰ï¼š** ä¸æä¾›ä»»ä½•ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæ¨¡å‹åªç»™å‡ºä¸€ä¸ªæè¿°ä»»åŠ¡çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ã€‚





## 0323

### 1.1 è¯¾å‰å‡†å¤‡

- T5æ¨¡å‹æ•°æ®é›†ä¸‹è½½å¹¶ä¿®æ”¹ä»£ç 

- tumxå’Œç»ˆç«¯ä¸åå°ä»è®­ç»ƒ

>  sh Train.sh > ./xxx.log &

> tumx



### 2.1 Scaling Laws

> tipï¼šæ¨¡å‹è§„æ¨¡æå¤§å¯ä»¥æé«˜è‡ªå·±çš„èƒ½åŠ›ï¼Ÿ

- å®éªŒå˜é‡ï¼š

> C, D, N
>
> ![image-20250323103615212](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250323103615212.png)

- æ•°æ®è§„æ¨¡ä¸æ¨¡å‹è§„æ¨¡æ‰©å¤§æ¯”ï¼š5/8
- ä¸€äº›è¶…å‚æ•°çš„è®¾å®š



----

### 3.1 åˆ†å¸ƒå¼è®­ç»ƒ

1. å¹¶è¡Œå¯ä»¥å¹¶è¡Œå“ªäº›ï¼Ÿæ‹†å“ªäº›ï¼Ÿ
2. å¤šå¡å¹¶è¡ŒèŒƒå¼

- **æ•°æ®å¹¶è¡Œæ€§**(DP)ï¼šå°†æ¨¡å‹ï¼ˆæ‰€æœ‰weightï¼‰å¤åˆ¶åˆ°åˆ«çš„Workerä¸­ï¼Œæ‰€ä»¥æ¨¡å‹å¤§äºå•ä¸ªæ˜¾å­˜çš„æ—¶å€™ä½¿ç”¨è¿™ç§æ–¹å¼æ— æ³•å¾ˆå¥½å·¥ä½œ

- æ¨¡å‹å¹¶è¡Œæ€§(MP)ï¼Œå­˜åœ¨Bubbleé—®é¢˜

- MPä¼˜åŒ–ï¼šç®¡çº¿å¹¶è¡Œæ€§ï¼ˆMP --> PPï¼‰ï¼Œåˆå«**æµæ°´çº¿å¹¶è¡Œ**

  > ppä¼ æ’­çš„æ˜¯activationï¼ˆå‰å‘ï¼‰å’Œå¯¹åº”çš„gradï¼ˆåå‘ï¼‰
  >
  > GPipeçš„ä¸è¶³ï¼šæœ€åä¸€ä¸ªæ‰§è¡Œå®Œæ‰èƒ½backward
  >
  > PipeDreamï¼šå‰åå‘ç©¿æ’ï¼Œè°ƒåº¦é—®é¢˜å¾ˆéš¾ï¼Œå·¥ç¨‹ä¸Š=>Pipeline flashã€‚å®ç°one F one B

- **å¼ é‡å¹¶è¡Œæ€§ï¼ˆTPï¼‰**

  > å‰é¢æ˜¯çºµå‘åˆ†å‰²ï¼Œç°åœ¨æå‡ºç”¨æ¨ªå‘åˆ†å‰²
  >
  > å°†ä¸€ä¸ªç®—å­çš„Tenseråˆ†åˆ°å¤šèŠ‚ç‚¹è®¡ç®—

- **ä¸“å®¶æ··åˆï¼ˆEPï¼ŒMoEï¼‰**

  > G shard
  >
  > switch Transformer

  ![image-20250323144057238](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250323144057238.png)

- åé¢è¿˜æœ‰CPï¼ŒxxxxP

3. åˆ†å¸ƒå¼æ¡†æ¶

   > pytorchçš„
   >
   > deepspeed



### 4. æ˜¾å­˜å ç”¨é—®é¢˜

#### 4.1 è§£å†³æ–¹æ¡ˆ

> ä¹‹å‰æœ‰å¤šä¸ªbatchçš„gradç´¯åŠ 

1. é‡è®¡ç®—ï¼ˆrecomputeï¼‰

Pytorch2.6å¼€å§‹æ›´åŠ æ–°çš„é‡è®¡ç®—

2. offload ï¼šç”¨å®Œå°±æ”¾åˆ°CPU

   eg:

   ![image-20250323152635318](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250323152635318.png)

3. gradient accumulate

#### 4.2 æ˜¾å­˜åˆ†æ

1.API

<img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250323152133484.png" alt="image-20250323152133484" style="zoom:50%;" />

2.æ˜¾å­˜é«˜å³°æœŸ

åœ¨ç¬¬ä¸€ä¸ªstepä¸ä¼šï¼Œç†è®ºä¸Šæ˜¯åœ¨ç¬¬äºŒä¸ªstepçš„forwardä¹‹å

### 5. æ··åˆç²¾åº¦è®­ç»ƒï¼ˆAMPï¼‰ğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸ

![alt text](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image.png)

å¤§æ¨¡å‹å¿…ç”¨ï¼ŒåŠ é€Ÿè®­ç»ƒ

> ==ä¸‹ä¸€ä¸ªçƒ­ç‚¹FP8==

1.æƒé‡å‰¯æœ¬fp32

<img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250323155428008.png" alt="image-20250323155428008" style="zoom:50%;" />

2.æŸå¤±ç¼©æ”¾

3.è¾“å‡ºå­˜å‚¨åˆ°å•ç²¾åº¦ï¼Œæœ€ç»ˆå˜åŠç²¾åº¦



èˆå…¥è¯¯å·®ï¼ˆä¸‹æº¢ï¼‰

### 6. Apex





### 7.

1.ä¹‹å‰çœ‹é‚£ä¸ªstate_dictæœ€åå­˜å‚¨çš„keyå’Œvalueéƒ½æ˜¯tenserçš„è®¾è®¡ç­–ç•¥æ˜¯ä¸æ˜¯è·Ÿè¿™å„¿æœ‰å…³ç³»

2.å¤šæœºçš„æ—¶å€™æ˜¯ä¸æ˜¯ä¹Ÿéœ€è¦ssh å¯†é’¥ç»„ç½‘
