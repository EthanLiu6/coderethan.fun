# 基础内容面试点

## 1. Activation

> Time: 0407

- 常用的激活函数？有什么用？
- 优劣势比较。
- LSTM中的sigmoid和tanh啥作用？
- 工程中的使用？哪些更常用，为啥？
- 大模型的激活函数？GELU？Switch GELU？
- 计算公式？图像？导数及图像？等等

- GELU的GLU块，以及在MLP中的门控单元。



## 2. Opetrator

> Time：0408-0409

### 2.1 Norm

- Norm中为啥不用BN来做NLP？ Or  BN和LN的区别？（代码实现）

    > - NLP的主要数据格式是[batch_szie, seq_len, embedding_dim]
    > - Nlp 每个seq都是基本独立的，所以不能用Batch Norm
    > - LN对应的维度就是对embedding_dim进行的

- Norm在训练和推理时有何不同？Dropout呢？

- 如何实现训练和推理不同的情况？（相当于加锁或者if else）

    > 上面这俩可以结合torch源码解读

- BN期望和方差计算策略。

-  BN使用时需要注意什么？

- 不同的Norm的参数量分析。

    > 可学习参数和均值方差

- 不同Norm操作维度（再联系联系其他算子及不同场景的操作维度，比如：Conv，Polling，softmax等）

- 其他那些常用算子的底层是copy还是in-place？Or 有哪些常用算子是in-place操作和非in-place操作？

- 各个大模型常用的Norm？引出RMSNorm？

- RMS Norm 相比于 Layer Norm 有什么特点？公式？

- Pre-Norm和Post-Norm的区别（和效果，以及大模型使用的情况）？

    > 需要熟悉Transformer结构和常用大模型

- DeepNorm思路？论文中的的$\beta$是哪里的参数？该Norm有什么优势？伪代码实现。



### 2.1 细节算子

> PyTorch细节内容

- reshape、view、permute、transpose作用和区别。（底层细节）
- torch（或者Tensor）的`contiguous()`作用？（引入上面的算子）。推理阶段为什么要确保张量连续？



- Dropout训练和推理有啥不同？（联系Norm）