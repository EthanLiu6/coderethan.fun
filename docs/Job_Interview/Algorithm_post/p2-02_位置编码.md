# Position Embedding

## 1. 绝对位置编码

> 直接对word embedding和position embedding相加送入到模型

###  1.1 三角位置编码（早期Transformer）

- 具体公式：
  $$
  P^k_{2i} = sin(\frac{k}{10000^{2i/d_{model}}})
  $$

  $$
  P^k_{2i+1} = cos(\frac{k}{10000^{2i/d_{model}}})
  $$

- 其中：

  $k$表示第$k$个token，$2i$ 和 $2i+1$表示token对应的embedding（第奇数个和偶数个分别采用cos和sin）

### 1.2 可学习的位置编码（LPE）

> Bert、GPT-2等采用的就是这种方法

初始化一个position embedding，与word embedding进行相加得到，后续进行参数更新



## 2. 相对位置编码

### 2.1 T5的相对位置编码

> 最终结果是对Attention加上了一个偏置项
>
> 在Attention时候进行位置编码，而不是在最初





## 3 旋转位置编码——RoPE（大模型常用）

> 数学推导较为难理解
>
> 同相对位置编码一样，在Attention时候进行位置编码，只对q和k做位置编码，对value不做，value是结果或者说是token本身的特征信息

<img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250328102841239.png" alt="image-20250328102841239" style="zoom: 50%;" />

<img src="https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250328102744334.png" alt="image-20250328102744334" style="zoom:50%;" />

直观理解：

[RoPE理解](https://www.bilibili.com/video/BV1CQoaY2EU2?spm_id_from=333.788.player.switch&vd_source=d0891b7353b29ec2c50b1ea1f7004bfa)

[RoPE的远程衰减性](https://www.bilibili.com/video/BV1iuoYYNEcZ?spm_id_from=333.788.videopod.sections&vd_source=d0891b7353b29ec2c50b1ea1f7004bfa)
